<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习3.2-模型选择与调优策略（下） | 小漁头|小戴</title><meta name="author" content="小漁头&amp;小戴"><meta name="copyright" content="小漁头&amp;小戴"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文是深度学习的第六篇，主要介绍了四种模型的调优策略。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习3.2-模型选择与调优策略（下）">
<meta property="og:url" content="http://blog.dai2yutou.space/2022/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="小漁头|小戴">
<meta property="og:description" content="本文是深度学习的第六篇，主要介绍了四种模型的调优策略。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picbed.dai2yutou.space/web_img/19.png">
<meta property="article:published_time" content="2022-12-26T15:13:23.000Z">
<meta property="article:modified_time" content="2023-03-30T12:12:40.826Z">
<meta property="article:author" content="小漁头&amp;小戴">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="paddle">
<meta property="article:tag" content="深度学习基础_模型选择与调优策略">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picbed.dai2yutou.space/web_img/19.png"><link rel="shortcut icon" href="/img/basketball.png"><link rel="canonical" href="http://blog.dai2yutou.space/2022/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8B%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 小漁头&小戴","link":"链接: ","source":"来源: 小漁头|小戴","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习3.2-模型选择与调优策略（下）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-30 20:12:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/css.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/at.alicdn.com/t/c/font_3829236_a49e40pee5.css"><link rel="stylesheet" href="/css/font-awesome.css"><link rel="stylesheet" href="/css/progress_bar.css"><link rel="stylesheet" href="/css/nav_menu.css"><link rel="stylesheet" href="/css/color.css"><link rel="apple-touch-icon" href="/img/apple-touch-icon.jpg"><meta name="apple-mobile-web-app-title" content="小漁头🏀"><link rel="bookmark" href="/img/apple-touch-icon.jpg"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/apple-touch-icon.jpg" ><link rel="stylesheet" href="/css/card_author.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (ture) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picbed.dai2yutou.space/web_img/19.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小漁头|小戴</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习3.2-模型选择与调优策略（下）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-26T15:13:23.000Z" title="发表于 2022-12-26 23:13:23">2022-12-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-30T12:12:40.826Z" title="更新于 2023-03-30 20:12:40">2023-03-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">23.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>86分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习3.2-模型选择与调优策略（下）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>调优策略</h1>
<p>😎学习本节，希望你能够掌握以下知识点：</p>
<ol>
<li>针对系统中出现的问题选择不同的优化策略以提高系统的能力。</li>
</ol>
<hr>
<blockquote>
<h4 id="ppl-py文件">ppl.py文件</h4>
<p>将在此使用的相同的函数放到一起，作为一个库，在使用其中的函数时，直接<code>import ppl</code>导入即可，或者<code>from ppl import 函数名</code>；</p>
<p>我将ppl.py文件放在了<code>C:\Users\21431\Desktop\计算机学习\人工智能\人工智能山大培训\深度学习</code>；</p>
<p>在本地使用时，复制放在同一目录下即可；</p>
<p>AI Studio中放到项目中即可。</p>
</blockquote>
<hr>
<h2 id="一、解决欠拟合与过拟合">一、解决欠拟合与过拟合</h2>
<p>欠拟合是指模型不能再训练集上获得足够低的误差，即模型在训练集上的误差比人类水平达到的误差要高，此时模型还有提升的空间，可以通过增加模型深度和训练次数【迭代周期】或选择一些优化算法继续提高模型的表达能力。而过拟合是指学习时选择的模型包含的参数过多【模型的复杂度较高】，以至于这一模型对已知数据预测的很好，但对未知数据预测的很差。过拟合通常被称为模型的泛化能力不好，可以通过增加数据集、加入一些正则化方法或者改变超参数来调整。</p>
<p><strong>【模型复杂性】</strong></p>
<p>模型的训练程度可以用模型复杂度来衡量。当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近，很容易出现欠拟合；当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。</p>
<p>模型复杂性由什么构成是一个复杂的问题。你可以把它想象成模型复杂就是机器更智能。一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型也比较复杂。我们将重点介绍几个倾向于影响模型复杂度的因素：</p>
<ol>
<li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li>
<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>
</ol>
<p>例如，因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。</p>
<p><strong>【数据集大小】</strong></p>
<p>另一个重要因素是数据集的大小。<strong>数据的复杂度受到样本的个数、每个样本的元素个数、时间空间结构和多样性的影响</strong>。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小。</p>
<p>此外，一般来说，更多的数据不会有什么坏处。对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。能够拟合更复杂的模型可能是有益的。如果没有足够的数据，简单的模型可能更有用。</p>
<p>对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。从一定程度上来说，<strong>深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。</strong></p>
<p><strong>【实现多项式回归】</strong></p>
<p>我们现在可以通过多项式拟合来探索这些概念。通过对输入数据的控制分别展示欠拟合、正常拟合和过拟合的情况。</p>
<p>给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：</p>
<p>$$<br>
y = 5 + 1.2x - 3.4\frac{x^2} {2!} + 5.6 \frac{x^3} {3!} + \epsilon \text{ where }<br>
\epsilon \sim \mathcal{N}(0, 0.1^2).<br>
$$</p>
<p>噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值，因此我们将特征从$x<sup>i$调整为$\frac{x</sup>i} {i!}$，这样可以避免很大的$i$带来的特别大的指数值。我们将为训练集和测试集各生成100个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line">max_degree = <span class="number">20</span> <span class="comment"># 生成20维的数据样本，我们只需要前4个维度，后边的是噪声</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">true_w = np.zeros(shape=(max_degree,))</span><br><span class="line">true_w[<span class="number">0</span>: <span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))</span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">math.gamma(N)其表示N在N-1到0范围内的整数阶乘。</span></span><br><span class="line"><span class="string">公式为：gamma(N)=(N-1)*(N-2)*...*2*1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    paddle.to_tensor(x, dtype=paddle.float32) </span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(poly_features[:<span class="number">1</span>, :], <span class="string">&#x27;\n&#x27;</span>, labels[:<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Tensor(shape=[1, 20], dtype=float32, place=Place(cpu), stop_gradient=True,
       [[1.        , 1.28171623, 0.82139820, 0.35093310, 0.11244916, 0.02882558,
         0.00615770, 0.00112749, 0.00018064, 0.00002573, 0.00000330, 0.00000038,
         0.00000004, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000,
         0.00000000, 0.00000000]]) 
 Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
       [5.66237640])
</code></pre>
<p>首先让我们实现一个函数来评估模型在给定数据集上的损失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>) <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>现在读取数据并定义训练函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_array, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构造数据迭代器</span></span><br><span class="line">    dataset = TensorDataset(data_array)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter():</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, paddle.optimizer.Optimizer):</span><br><span class="line">            updater.clear_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            W, b = updater(W, b, X.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;updated W, b:&quot;</span>, W, b)</span><br><span class="line">            metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels, num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)<span class="comment">#均方损失</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias_attr=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    train_iter = load_array((train_features, train_labels.reshape((-<span class="number">1</span>, <span class="number">1</span>))), batch_size)</span><br><span class="line">    test_iter = load_array((test_features, test_labels.reshape((-<span class="number">1</span>, <span class="number">1</span>))), batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = paddle.optimizer.SGD(<span class="number">0.001</span>, net.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight)</span><br></pre></td></tr></table></figure>
<p><strong>1）三阶多项式函数拟合(正常)</strong></p>
<blockquote>
<p>如图所示：</p>
<p>train_loss就是训练误差；</p>
<p>test_loss就是泛化误差。</p>
</blockquote>
<p>现在我们正常将样本投入到训练中，学习到的模型参数也接近真实值$w = [5, 1.2, -3.4, 5.6]$。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d24d18e751094cc5a09e9fec7159ae410eb9d1df692b4f34b6e1796bcca3b7bf" width="350" hegiht="" ></center>
<center>图4：正常拟合 </center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<pre><code>weight: Parameter containing:
Tensor(shape=[4, 1], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[ 4.98475456],
        [ 1.18825924],
        [-3.37832952],
        [ 5.63073015]])
</code></pre>
<p><strong>2）线性函数拟合(欠拟合)</strong></p>
<p>当我们没有提供足量的数据，只提供前两列（数据不全），<br>
当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p>
<p>$w = [5, 1.2, -3.4, 5.6]$</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d8488a4e261c4493be2f8f916790a68c53f322094f9d4c8b93b2f44a2e3045d7" width="350" hegiht="" ></center>
<center>图5：欠拟合 </center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>], </span><br><span class="line">        labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<pre><code>weight: Parameter containing:
Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[3.09112906],
        [4.18816757]])
</code></pre>
<p><strong>3）高阶多项式函数拟合(过拟合)</strong></p>
<p>接下来把所有的数据都给模型，这个过于复杂的模型会轻易受到训练数据中噪声的影响。<br>
虽然训练损失可以有效地降低，但测试损失仍然很高。<br>
结果表明，复杂模型对数据造成了过拟合。</p>
<p>$w = [5, 1.2, -3.4, 5.6]$</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d0e8eec69cba4648bb56dcedf4f7c28f835fb7d2231944fd87bcf13591cf7855" width="350" hegiht="" ></center>
<center>图6：过拟合 </center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure>
<pre><code>weight: Parameter containing:
Tensor(shape=[20, 1], dtype=float32, place=Place(cpu), stop_gradient=False,
       [[ 4.96567392],
        [ 1.28106725],
        [-3.26861453],
        [ 5.17804003],
        [-0.45723167],
        [ 1.17682230],
        [ 0.58788991],
        [ 0.11369380],
        [-0.00772932],
        [ 0.34586436],
        [ 0.51956356],
        [ 0.03097064],
        [-0.40698799],
        [ 0.23049854],
        [-0.52523530],
        [-0.27161127],
        [-0.42720860],
        [-0.45747021],
        [-0.48819304],
        [ 0.42260706]])
</code></pre>
<h2 id="二、数值稳定性和模型初始化">二、数值稳定性和模型初始化</h2>
<p>​        到目前为止，我们实现的每个模型都是根据某个预先指定的分布来初始化模型的参数。初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起，我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。在本节中，我们将更详细地探讨这些主题，并讨论一些有用的启发式方法。</p>
<p>​        深度学习的训练本质是优化损失，优化的方式是计算梯度，然后通过优化算法更新参数。开始之前，我们先来回忆一下梯度公式的数学意义，它描述了函数在某点函数值增加最快的方向，它的模就等于函数在该点方向导数的最大值。用直观的解释就是，假设你现在位于一座山上，则这一点的梯度是在该点坡度（或者说斜度）最陡的方向，梯度的大小告诉我们坡度到底有多陡。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/7b41140018cd482eb6b1438dc713be416375e9d920a44f63895f6f9ca26485b2" width="500" hegiht="" ></center>
<center>图1：梯度更新的原理 </center>
<p>根据梯度链式传递法则可以发现，<strong>激活函数的求导值和权重值会以连乘的形式参与到该层权重的梯度计算中</strong>，而<strong>预测值与真实值的偏差以及神经元的输入值只是以常数的形式参与计算</strong>。</p>
<p>梯度是矩阵与梯度向量的乘积，因此，我们容易受到数值下溢问题的影响。</p>
<p>不稳定梯度带来的风险不止在于数值表示，不稳定梯度也威胁到我们优化算法的稳定性。我们可能面临一些问题，要么是<strong>梯度爆炸（gradient exploding）</strong> 问题：参数更新过大，破坏了模型的稳定收敛；要么是<strong>梯度消失（gradient vanishing）</strong> 问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p>
<p>总结一下 ，梯度消失与梯度爆炸是反向传播训练法则的先天性不足，本质是梯度反向传播中的连乘效应。梯度消失与梯度爆炸的产生原因在于神经网络的更新中，梯度的传递是采用连乘函数的形式。众所周知，指数级别的增长是爆炸式的，因此可能引起深层网络的浅层节点更新使用一个过小或者过大的梯度值，这种深度学习现象叫做梯度消失与梯度爆炸。</p>
<h3 id="2-1-梯度消失">2.1 梯度消失</h3>
<p>曾经sigmoid函数很流行，因为它类似于阈值函数。由于早期的人工神经网络受到生物神经网络的启发，神经元要么完全激活要么完全不激活（就像生物神经元）的想法很有吸引力。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/8e6c84ff23204602ae7e8a3bbe35c43fee9e0a818fd6452192594885139a4fb5" width="400" hegiht="" ></center>
<center>图2：softmax激活函数曲线 </center>
<p>然而，它却是导致梯度消失问题的一个常见的原因，当sigmoid函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。<br>
因此，更稳定的ReLU系列函数已经成为从业者的默认选择，ReLU函数的导数在正数部分是恒等于1的，因此在深层网络中使用更优的激活函数（例如ReLU等）可以减轻梯度消失的问题。</p>
<h3 id="2-2-梯度爆炸">2.2 梯度爆炸</h3>
<p>相反，梯度爆炸可能同样令人烦恼。为了更好地说明这一点，我们生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘。可以发现出现了梯度爆炸的情况，当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">M = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=[<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一个矩阵 \n&#x27;</span>,M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    M = paddle.mm(M,paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=[<span class="number">4</span>, <span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;乘以100个矩阵后\n&#x27;</span>, M)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">一个矩阵 </span></span><br><span class="line"><span class="string"> Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,</span></span><br><span class="line"><span class="string">       [[-1.19920492, -1.51656413, -0.66636622,  0.62597984],</span></span><br><span class="line"><span class="string">        [-0.00658856,  0.70853168,  0.45729700,  0.10293817],</span></span><br><span class="line"><span class="string">        [ 0.27025989, -0.19260350, -0.12201850, -1.42817235],</span></span><br><span class="line"><span class="string">        [-0.04502322,  0.19740644, -0.95305598,  0.35635617]])</span></span><br><span class="line"><span class="string">乘以100个矩阵后</span></span><br><span class="line"><span class="string"> Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,</span></span><br><span class="line"><span class="string">       [[ 303895516951572182466560. ,  232979873649558818914304. ,</span></span><br><span class="line"><span class="string">         -2250137589105981078372352., -5345116457122281304358912.],</span></span><br><span class="line"><span class="string">        [-136164064919334751830016. , -104411381703363541336064. ,</span></span><br><span class="line"><span class="string">          1008128975147034581401600.,  2394808197116681298903040.],</span></span><br><span class="line"><span class="string">        [ 113319943119065864732672. ,  86876427403012165599232.  ,</span></span><br><span class="line"><span class="string">         -839054477648321230929920. , -1993143315278582639493120.],</span></span><br><span class="line"><span class="string">        [ 13815522565998537342976.  ,  10588780127280403841024.  ,</span></span><br><span class="line"><span class="string">         -102303525940968309129216. , -243013515316971584880640. ]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>​		我们知道，梯度需要和学习率相结合，当出现过小的梯度（或为0），无论学习率怎么调整，我们都无法实现训练的进展，底部层梯度为0的情况会尤为严重，这种情况下，无论输入的值是多少，底部层都无法对数值进行处理，那么再深的网络结构都毫无意义，实际效果和单层神经网络的效果都是相同的。</p>
<p>​		同样地，梯度爆炸也会带来许多问题，这会使模型对学习率变得更加敏感，稍大的学习率就会导致参数动荡，无法收敛，始终处于一个大的参数值的状态；学习率小，虽然避免了上面的情况，但往往会导致训练无进展。因此我们需要在训练过程中不断地去调整学习率。</p>
<h3 id="2-3-合理的权重初始化的方法">2.3 合理的权重初始化的方法</h3>
<p>​		针对着上面的情况，我们可以采用很多方法来实现参数的稳定，目标是使梯度值在一个合理的范围内，最终推动训练的稳定。<em><strong>本节主要介绍合理的权重初始化的方法</strong></em>。对于模型而言参数初始化是初始阶段比较重要的阶段，糟糕的初始化会导致模型不收敛或者根本训练没反应。</p>
<p><strong>1）打破对称性</strong></p>
<p>​		当所有初始值都相同时，例如将每个权重初始化为0，然后在进行反向传播时，所有权重将获得相同的梯度，因此进行相同的更新。<br>
直观地说，这意味着所有节点都将学习相同的东西，而我们不希望那样，因为我们希望网络学习不同种类的特征。这就是所谓的打破对称性。</p>
<p>​		假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。在这种情况下，第一个隐藏单元与第二个隐藏单元没有什么特别的区别假设我们将隐藏层的所有参数初始化为$\mathbf{W}^{(1)} = c$，$c$为常量，会发生什么？在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数$\mathbf{W}<sup>{(1)}$对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代之后，$\mathbf{W}</sup>{(1)}$的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。这就是所谓的对称性。</p>
<p>​		因此，我们决不能把所有参数初始化为0，同样也不能初始化为任何相同的值，我们必须“打破对称性”！我们希望不同的节点（神经元）学习到不同的参数，但是如果参数相同以及输出值都一样，不同的节点根本学习不到不同的特征，这样就失去了网络学习特征的意义了。</p>
<p>​		解决（或至少减轻）上述问题的一种方法是进行参数初始化，这是通过随机初始化来实现的，因为这样梯度会不同，每个节点将变得与其他节点更加不同，从而实现多样化的特征提取。</p>
<p><strong>2）默认随机初始化</strong></p>
<p>​		在前面的部分中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。通常来说，$b$不用随机初始化，因为$w$随机之后，已经打破对称。</p>
<p><strong>3）Xavier初始化</strong></p>
<p>​		要描述“差异性”，首先就能想到概率统计中的方差这个基本统计量，为了使网络中的信息更好的流动，每一层输出的方差应该尽量相等。这就是现在标准且实用的Xavier初始化的基础。<br>
下面我们使用公式来解读。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/7163751298e8498ab850641790f1307f8636695442ad47429b7893c946390d5b" width="150" hegiht="" ></center>
<p>其中$n$是上一层神经元的数量。因此，根据概率统计里的两个随机变量乘积的方差展开：</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/f691ddd1dc114039ac7b48b644adbbb1fea2e2fd72a5465fba5087dd5b367cfa" width="650" hegiht="" ></center>
<p>$E(X)$表示变量$X$的期望。可以得到，如果$E(xi)=E(wi)=0$，那么就有:</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/4895a6a26c3b47908a5cc7ea9d0eeb89a1f8470b3bf44889bffc0ab24333f01a" width="290" hegiht="" ></center>
<p>如果随机变量$xi$和$wi$再满足独立同分布的话:</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/2fb25022dc284683b1818e56c52e2f07e9acccc8406b410c8b3abefbea183b29" width="450" hegiht="" ></center>
<p>​		根据文章《激活函数》，整个大型前馈神经网络无非就是一个超级大映射，将原始样本稳定的映射成它的类别,也就是将样本空间映射到类别空间。</p>
<p>​		试想，如果样本空间与类别空间的分布差异很大【这里样本空间就是x，类别空间就是z】，比如说类别空间特别稠密，样本空间特别稀疏辽阔，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。</p>
<p>​		同样，如果类别空间特别稀疏，样本空间特别稠密，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。</p>
<p>​		因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，也就是要让它们的<strong>方差尽可能相等</strong>。</p>
<p>​		因此为了得到$Var(z)=Var(x)$，只能让$n×Var(w)=1$，也就是$Var(w)=1/n$。<br>
同样的道理，正向传播时是从前往后计算的，因此$Var(w)=\frac{1} {n_{in} }$，反向传播时是从后往前计算的，因此$Var(w)=\frac{1} {n_{out} }$，然而$n_{in}$和$n_{out}$往往不相等，因此在这里我们选择他们的均值，即令：</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d4936358413f4939af9a804e516d7b36fe65e32d33334343830721086c686d08" width="180" hegiht="" ></center>
<p>​		其中$n_{in}$与$n_{out}$分别是输入层与输出层的神经元个数。通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2} {n_\mathrm{in} + n_\mathrm{out} }$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\frac{a^2} {3}$。将$\frac{a^2} {3}$代入到$\sigma^2$的条件中，将得到初始化值域：</p>
<p>$$<br>
U\left(-\sqrt{\frac{6} {n_\mathrm{in} + n_\mathrm{out} }}, \sqrt{\frac{6} {n_\mathrm{in} + n_\mathrm{out} }}\right).<br>
$$<br>
​		得到的这个结论就是Xavier初始化方法。这不是一个一劳永逸的方法，但除非你的网络设计的明显不满足Xavier的假设，否则使用Xavier往往不会出错。</p>
<p>（补充：若 X 服从 [ a , b ] 上的均匀分布，则数学期望 EX =（a + b）/ 2 ；方差 DX =（b - a）² / 12 ）</p>
<h3 id="2-4-小结">2.4 小结</h3>
<ul>
<li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
<li>随机初始化是保证在进行优化前打破对称性的关键。</li>
</ul>
<h2 id="三、降低偏差–优化算法">三、降低偏差–优化算法</h2>
<p>优化问题是指在满足一定条件下，在众多方案或参数值中寻找最优方案或参数值，以使得某个或多个功能指标达到最优，或使系统的某些性能指标达到最大值或最小值。</p>
<p>深度网络模型优化算法主要依据最小化或最大化目标函数，更新对模型的训练和表达能力造成影响的参数，使这些参数达到或尽可能接近目标函数的最优值，从而提高模型的学习能力，获得预期的网络模型。当模型出现欠拟合的状况时，可以通过调整优化算法来改善模型的训练，降低模型的预测偏差，提升模型的表达能力。</p>
<blockquote>
<p>神经网络中的优化是一个最小值的优化，优化的目标对象是损失。</p>
</blockquote>
<h3 id="【设置学习率】"><strong>【设置学习率】</strong></h3>
<p>在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：</p>
<ul>
<li>学习率不是越小越好。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如左图所示。</li>
<li>学习率不是越大越好。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如右图所示。</li>
</ul>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/4266917491ee489e8ca0296565f1fe5edc2cb68d4a4546f3b8979e1200691bb5" width="550" hegiht="" ></center>
<center>图1：不同学习率的影响 </center>
<p>在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察$Loss$下降的情况判断合理的学习率。<br>
学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和$Loss$的变化。经过研究员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam。</p>
<h3 id="3-1-梯度下降">3.1 梯度下降</h3>
<h4 id="3-1-1-梯度下降">3.1.1 梯度下降</h4>
<p>🎨<code>sec_gd</code></p>
<p>为什么梯度下降算法可以优化目标函数？一维中的梯度下降给我们很好的启发。</p>
<p>梯度下降法是一个一阶最优化算法，通常也称为最陡下降法。要使用梯度下降法找到一个函数的局部极小值，必须朝向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d44fbf84938642e1a0acf335cc7f4271444efbdbad5d4ff58222e19482fef8df" width="450" hegiht="" ></center>
<center>图2：梯度下降法 </center>
<p><strong>优化算法主要是针对凸函数</strong>。下面再来回顾以下有关理论：</p>
<ol>
<li>集合中任意两点的连线都在集合中，为凸集合。在二维中，我们可以将凸集视为一个形状，无论用什么线连接集中的两个点，都不会在集外。</li>
<li>二阶导数需大于0才是凸函数。二阶导数是一阶导数的导数。从原理上看，它表示一阶导数的变化率；从图形上看，它反映的是函数图像的凹凸性。</li>
<li>目标函数是凸函数，变量所属集合是凸集合的优化问题为凸优化问题，对于凸优化问题来说，局部最优解就是全局最优解。</li>
</ol>
<p>现在我们来看一些凸优化理论。我们知道，梯度下降法被应用于寻找代价函数的全局最小值。但是我们怎么知道存在一个全局最小值呢？当最小化函数时，凸函数可确保如果存在最小值，则它将是全局最小值。二次函数（例如线性最小二乘问题）是强凸的。这意味着该函数具有唯一的最小值，而该最小值是全局最小值。因此，当我们应用梯度下降算法时，我们可以确信它将收敛于正确的最小值。如果我们试图最小化的函数是非凸的，则梯度下降可能会收敛于局部最小值而不是全局最小值。这就是为什么使用非凸函数要困难得多，这很重要，因为许多机器学习模型（最著名的是神经网络）是非凸的。</p>
<h5 id="1）一维梯度下降"><strong>1）一维梯度下降</strong></h5>
<p>下面我们来展示如何实现梯度下降。</p>
<p>为了简单起见，我们选用目标函数$f(x)=x^2$。尽管我们知道$x=0$时$f(x)$能取得最小值，但我们仍然使用这个简单的函数来观察$x$的变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x</span>): <span class="comment"># 目标函数的梯度（导数）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x</span><br></pre></td></tr></table></figure>
<p><strong>梯度下降三步走</strong>：</p>
<ul>
<li>选定初始$X_0$</li>
<li>对$t=1,2,3…T$</li>
<li>求梯度：$X_t = X_{t-1} - \eta f’(X_{t-1})$</li>
</ul>
<p>接下来，我们使用$x=10$作为初始值，并假设$\eta=0.2$。<br>
使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gd</span>(<span class="params">eta, f_grad</span>):</span><br><span class="line">    x = <span class="number">10.0</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        x -= eta * f_grad(x)</span><br><span class="line">        results.append(<span class="built_in">float</span>(x))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch 10, x: %f&#x27;</span>%x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = gd(<span class="number">0.2</span>, f_grad)	<span class="comment"># 此时学习率为0.02，这是经过多次得出来的适中的值。</span></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 0.060466</span></span><br><span class="line"><span class="string">[10.0, 6.0, 3.5999999999999996, 2.1599999999999997, 1.2959999999999998, 0.7775999999999998, 0.46655999999999986, 0.2799359999999999, 0.16796159999999993, 0.10077695999999996, 0.06046617599999997]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>对进行$x$优化的过程可以绘制如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace</span>(<span class="params">results, f</span>):</span><br><span class="line">    n = <span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="built_in">min</span>(results)), <span class="built_in">abs</span>(<span class="built_in">max</span>(results)))</span><br><span class="line">    <span class="comment">#print(n)</span></span><br><span class="line">    f_line = paddle.arange(-n, n, <span class="number">0.01</span>, dtype=paddle.float32)</span><br><span class="line">    tensor_list = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> f_line:</span><br><span class="line">        tensor_list.append(f(x))</span><br><span class="line">        <span class="comment">#print(tensor_list)</span></span><br><span class="line">    final_tensor=paddle.stack(tensor_list, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(final_tensor)</span></span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plot([f_line, results], </span><br><span class="line">            [final_tensor, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> results]], </span><br><span class="line">            <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, fmts=[<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>])</span><br><span class="line"></span><br><span class="line">show_trace(results, f)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10.svg" alt="svg"></p>
<blockquote>
<h5 id="如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，梯度-学习率的值也就越来越小，更新的越来越慢。">如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，<code>梯度*学习率</code>的值也就越来越小，更新的越来越慢。</h5>
<h5 id="学习率适中，10轮后，得到的x值为0-06，已经非常接近最优解了">学习率适中，10轮后，得到的x值为0.06，已经非常接近最优解了</h5>
</blockquote>
<p>我们可以尝试调整学习率的大小来查看它对于模型优化的影响。例如，考虑同一优化问题中$\eta = 0.05$的进度。如下所示，尽管经过了10个步骤，我们仍然离最优解很远。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">0.05</span>, f_grad), f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 3.486784</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11.svg" alt="svg"></p>
<blockquote>
<h5 id="如上图所示-学习率太小，更新的幅度也就变小，经过轮过后，x值为3-48，与最优解相差较远。">如上图所示,学习率太小，更新的幅度也就变小，经过轮过后，x值为3.48，与最优解相差较远。</h5>
</blockquote>
<p>相反，学习率也不可过大，过大可能在优化过程中错过梯度为0的最优点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">1.1</span>, f_grad), f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 61.917364</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12.svg" alt="svg"></p>
<blockquote>
<h5 id="如上图所示，学习率过高，在最优解附近振荡，达不到最优解。">如上图所示，学习率过高，在最优解附近振荡，达不到最优解。</h5>
</blockquote>
<h5 id="2）局部最小值"><strong>2）局部最小值</strong></h5>
<p>为了演示非凸函数的梯度下降，考虑函数$f(x) = x \cdot \cos(cx)$，其中$c$为某常数。这个函数有无穷多个局部最小值。根据我们选择的学习率，我们最终可能只会得到许多解的一个。下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 局部最小值</span></span><br><span class="line">c = paddle.to_tensor(<span class="number">0.15</span> * np.pi) <span class="comment"># 0.15π</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * paddle.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.cos(c * x) - c * x * paddle.sin(c * x)</span><br></pre></td></tr></table></figure>
<p>对进行$x$优化的过程可以绘制如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace1</span>(<span class="params">results, f</span>):</span><br><span class="line">    n = <span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="built_in">min</span>(results)), <span class="built_in">abs</span>(<span class="built_in">max</span>(results)))</span><br><span class="line">    <span class="comment">#print(n)</span></span><br><span class="line">    f_line = paddle.arange(-n, n, <span class="number">0.01</span>, dtype=paddle.float32)</span><br><span class="line">    fx_in_fline = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> f_line:</span><br><span class="line">        fx_in_fline.append(f(x))</span><br><span class="line">        <span class="comment">#print(fx_in_fline)</span></span><br><span class="line">    fx_in_fline = paddle.stack(fx_in_fline, axis=<span class="number">0</span>)</span><br><span class="line">    fx_in_results = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> results:</span><br><span class="line">        fx_in_results.append(f(x))</span><br><span class="line">    fx_in_results = paddle.stack(fx_in_results, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plot([f_line, results], </span><br><span class="line">            [fx_in_fline, fx_in_results], </span><br><span class="line">            <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, fmts=[<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>])</span><br><span class="line"></span><br><span class="line">show_trace1(gd(<span class="number">2</span>, f_grad), f)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: -1.528165</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13.svg" alt="svg"></p>
<blockquote>
<h5 id="如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。">如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。</h5>
</blockquote>
<h5 id="3）多元梯度下降"><strong>3）多元梯度下降</strong></h5>
<p>我们构造一个目标函数$f(\mathbf{x})=x_1<sup>2+2x_2</sup>2$，并有二维向量$\mathbf{x} = [x_1, x_2]^\top$（列向量）作为输入，标量作为输出。梯度由$\nabla f(\mathbf{x}) = [2x_1, 4x_2]^\top$给出。我们将从初始位置$[-5, -2]$通过梯度下降观察$\mathbf{x}$的轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;多元梯度下降&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_2d</span>(<span class="params">trainer, steps=<span class="number">20</span>, f_grad=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用定制的训练机优化2D目标函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># s1 和 s2 是稍后将使用的内部状态变量</span></span><br><span class="line">    <span class="comment"># 状态变量：一个不可训练的变量，不会被优化器更新，但在评估或预测阶段可能是必要的。</span></span><br><span class="line">    <span class="comment"># 比如 BatchNorm 中的均值和方差。</span></span><br><span class="line">    <span class="comment"># 这里迭代周期我们定义为20，可以修改</span></span><br><span class="line">    x1, x2, s1, s2 = -<span class="number">5</span>, -<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">        <span class="keyword">if</span> f_grad: <span class="comment"># 如存在梯度，则需要更新梯度</span></span><br><span class="line">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)</span><br><span class="line">        results.append((x1, x2)) <span class="comment"># 获取x1和x2的坐标</span></span><br><span class="line">        <span class="comment">#print(f&#x27;epoch &#123;i+1&#125;, x1:&#123;float(x1): f&#125;, x2: &#123;float(x2):f&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace_2d</span>(<span class="params">f, results</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示优化过程中2D变量的轨迹&quot;&quot;&quot;</span></span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plt.plot(*<span class="built_in">zip</span>(*results), <span class="string">&#x27;-o&#x27;</span>, color=<span class="string">&#x27;#ff7f0e&#x27;</span>)</span><br><span class="line">    x1, x2 = paddle.meshgrid( <span class="comment"># 输入变量为 k 个一维张量, 输出 k 个 k 维张量</span></span><br><span class="line">        <span class="comment"># paddle.arange(start=0, end=None, step=1)</span></span><br><span class="line">        paddle.arange(-<span class="number">5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, dtype=paddle.float32),</span><br><span class="line">        paddle.arange(-<span class="number">3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, dtype=paddle.float32)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># plt.contour是python中用于画等高线的函数</span></span><br><span class="line">    ppl.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">&#x27;#1f77b4&#x27;</span>)</span><br><span class="line">    ppl.plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">    ppl.plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们观察学习率$\eta = 0.05$时优化变量$\mathbf{x}$的轨迹。可以看到，经过20步之后，$\mathbf{x}$的值接近其位于$[0, 0]$的最小值。虽然进展相当顺利，但相当缓慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d_grad</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数的梯度</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x1, <span class="number">4</span> * x2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gd_2d</span>(<span class="params">x1, x2, s1, s2, f_grad</span>):</span><br><span class="line">    g1, g2 = f_grad(x1, x2)</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * g1, x2 - eta * g2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.05</span>	<span class="comment"># 学习率</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14.svg" alt="svg"></p>
<h4 id="3-1-2-随机梯度下降">3.1.2 随机梯度下降</h4>
<p>🎨<code>sec_sgd</code></p>
<p>在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定$n$个样本的训练数据集，我们假设$f_i(\mathbf{x})$是关于索引$i$的训练样本的损失函数，其中$\mathbf{x}$是参数向量。然后我们得到目标函数</p>
<p>$$<br>
f(\mathbf{x}) = \frac{1} {n} \sum_{i = 1}^n f_i(\mathbf{x}).<br>
$$<br>
$\mathbf{x}$的目标函数的梯度计算为</p>
<p>$$<br>
\nabla f(\mathbf{x}) = \frac{1} {n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).<br>
$$<br>
如果使用梯度下降法，则每个自变量迭代的计算代价为$\mathcal{O}(n)$，它随$n$线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引$i$，其中$i\in{1,\ldots, n}$，并计算梯度$\nabla f_i(\mathbf{x})$以更新$\mathbf{x}$：</p>
<p>$$<br>
\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),<br>
$$<br>
其中$\eta$是学习率。我们可以看到，每次迭代的计算开销降到了常数级，而且随机梯度是对梯度的无偏估计，即使用随机梯度是对梯度的一个良好估计。</p>
<ul>
<li>梯度下降法是初始化模型参数，然后计算所有样本在本模型参数下的输出和梯度，再求梯度的均值，去更新网络参数。在下一次迭代，仍然计算全部样本。</li>
<li>随机梯度下降法是初始化模型参数，然后随机选出一个样本，计算该样本在当前参数下的输出和梯度，去更新一次模型参数。在下一次迭代中，再随机选择一个样本，计算输出和梯度再更新模型参数。</li>
</ul>
<p>由于随机梯度下降对每个样本都进行更新，使得参数的变化过于频繁，参数之间的方差偏高，每次更新可能并不会按照正确的方向进行，因此可以带来优化波动。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/8e183f5ad03545fdb5725146f5daaecbd9e092f889a9422d9ddaeb84f4a2bdc4" width="450" hegiht="" ></center>
<center>图3：SGD搅动 </center>
<p>现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数的梯度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x1, <span class="number">4</span> * x2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">x1, x2, s1, s2, f_grad</span>):</span><br><span class="line">    g1, g2 = f_grad(x1, x2)</span><br><span class="line">    <span class="comment"># paddle.normal(mean=0.0, std=1.0, shape=None)</span></span><br><span class="line">    g1 += paddle.normal(<span class="number">0.0</span>, <span class="number">1</span>, (<span class="number">1</span>, ))</span><br><span class="line">    g2 += paddle.normal(<span class="number">0.0</span>, <span class="number">1</span>, (<span class="number">1</span>, ))</span><br><span class="line">    eta_t = eta * lr()</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta_t * g1, x2 - eta_t * g2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constant_lr</span>(): <span class="comment"># 常量lr：学习率调度的任何功能都处于休眠状态（使用常量的学习率）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">lr = constant_lr <span class="comment"># eta*常量lr</span></span><br><span class="line">ppl.show_trace_2d(f, ppl.train_2d(sgd, steps=<span class="number">500</span>, f_grad=f_grad))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 500, x1:-0.085716, x2: -0.015954</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15.svg" alt="svg"></p>
<p>正如我们所看到的，随机梯度下降中变量的轨迹比我们在之前梯度下降中观察到的轨迹嘈杂得多。这是由于梯度的随机性质。也就是说，即使我们接近最小值，我们仍然受到通过$\eta \nabla f_i(\mathbf{x})$的瞬间梯度所注入的不确定性的影响。即使经过50次迭代，质量仍然不那么好。更糟糕的是，经过额外的步骤，它不会得到改善。</p>
<h4 id="3-1-3-小批量随机梯度下降">3.1.3 小批量随机梯度下降</h4>
<p>🎨<code>sec_minibatch_sgd</code></p>
<p>到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况：</p>
<ul>
<li>在<code>sec_gd</code>中使用整个训练数据集来计算梯度并更新参数，因此它有时也被称为<strong>批量梯度下降（batch gradient descent）</strong>。</li>
<li>在<code>sec_sgd</code>在每次迭代中只随机采样一个样本来计算梯度以取得进展。</li>
</ul>
<p>二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。而由于CPU和GPU无法充分利用向量化，硬件资源难得到完全利用，随机梯度下降并不特别“计算高效”。这暗示了两者之间可能有折中方案，这便涉及到<strong>小批量随机梯度下降（minibatch gradient descent）</strong>。批量为1就是随机梯度下降，为n就是梯度下降，为之间的值就是小批量随机梯度下降。</p>
<p><strong>1）向量化和缓存</strong></p>
<p>使用小批量的决策的核心是计算效率。减轻这些限制的方法是使用足够快的CPU缓存层次结构来为处理器提供数据。这是深度学习中批量处理背后的推动力。举一个简单的例子：矩阵-矩阵乘法。<br>
比如$\mathbf{A} = \mathbf{B}\mathbf{C}$，我们有很多方法来计算$\mathbf{A}$。例如，我们可以尝试以下方法：</p>
<ul>
<li>我们可以计算$\mathbf{A}<em>{ij} = \mathbf{B}</em>{i,:} \mathbf{C}_{:,j}^\top$，也就是说，我们可以通过点积进行逐元素计算。</li>
<li>我们可以计算$\mathbf{A}<em>{:,j} = \mathbf{B} \mathbf{C}</em>{:,j}^\top$，也就是说，我们可以一次计算一列。同样，我们可以一次计算一行。</li>
<li>我们可以简单地计算$\mathbf{A} = \mathbf{B} \mathbf{C}$。</li>
<li>我们可以将$\mathbf{B}$和$\mathbf{C}$分成较小的区块矩阵，然后一次计算$\mathbf{A}$的一个区块。</li>
</ul>
<p>如果我们使用第一个选择，每次我们计算一个元素$\mathbf{A}<em>{ij}$时，都需要将一行和一列向量复制到CPU中。更糟糕的是，由于矩阵元素是按顺序对齐的，因此当从内存中读取它们时，我们需要访问两个向量中许多不相交的位置；第二种选择相对更有利：我们能够在遍历$\mathbf{B}$的同时，将列向量$\mathbf{C}</em>{:,j}$保留在CPU缓存中，它将内存带宽需求减半，相应地提高了访问速度；第三种选择表面上是最可取的，然而大多数矩阵可能不能完全放入缓存中；第四种选择提供了一个实践上很有用的方案：我们可以将矩阵的区块移到缓存中然后在本地将它们相乘。</p>
<p>让我们来看看这些操作在实践中的效率如何。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pickletools <span class="keyword">import</span> optimize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line">timer = ppl.Timer()</span><br><span class="line"><span class="comment"># 定义三个256*256的矩阵</span></span><br><span class="line">A = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">B = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">C = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐元素计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        A[i, j] = paddle.dot(B[i, :], C[:, j]) <span class="comment"># 计算向量的内积</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：8.538275241851807</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐列计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">    A[:, j] = paddle.mv(B, C[:, j]) <span class="comment"># 计算矩阵和向量的乘积</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：0.06382012367248535</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次性计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line">A = paddle.mm(B, C) <span class="comment"># 矩阵相乘</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：0.0013420581817626953</span></span><br></pre></td></tr></table></figure>
<p><strong>2）小批量</strong></p>
<p>🎨<code>sec_minibatches</code></p>
<p>之前我们会理所当然地读取数据的小批量，而不是观测单个数据来更新参数，现在简要解释一下原因。处理单个观测值需要我们执行许多单一矩阵-矢量（甚至矢量-矢量）乘法，这耗费相当大，而且对应深度学习框架也要巨大的开销。这既适用于计算梯度以更新参数时，也适用于用神经网络预测。也就是说，每当我们执行$\mathbf{w} \leftarrow \mathbf{w} - \eta_t \mathbf{g}_t$时，消耗巨大。其中：</p>
<p>$$<br>
\mathbf{g}<em>t = \partial</em>{\mathbf{w} } f(\mathbf{x}_{t}, \mathbf{w})<br>
$$<br>
我们可以通过将其应用于一个小批量观测值来提高此操作的计算效率。也就是说，我们将梯度$\mathbf{g}_t$替换为一个小批量而不是单个观测值：</p>
<p>$$<br>
\mathbf{g}<em>t = \partial</em>{\mathbf{w} } \frac{1} {|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w})<br>
$$<br>
每个迭代周期的耗时在梯度下降和随机梯度下降之间。让我们看看这对$\mathbf{g}_t$的统计属性有什么影响：由于$\mathbf{x}_t$和小批量$\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的预期保持不变，保留了随机梯度下降的优势（无偏估计）。另一方面，方差（数据离散程度）显著降低。</p>
<h4 id="3-1-4-实现梯度下降法">3.1.4 实现梯度下降法</h4>
<p>让我们来看看如何从数据中有效地生成小批量。下面我们使用NASA开发的测试机翼的数据集<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise">不同飞行器产生的噪声</a>来比较这些优化算法。为方便起见，我们只使用前$1,500$样本并进行数据预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data = np.genfromtxt(</span><br><span class="line">        <span class="string">&#x27;work/airfoil_self_noise.dat&#x27;</span>, 	<span class="comment"># 数据集在飞将的work文件夹下</span></span><br><span class="line">        dtype=np.float32,</span><br><span class="line">        delimiter=<span class="string">&#x27;\t&#x27;</span> <span class="comment"># 水平制表：tab</span></span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br><span class="line">data = paddle.to_tensor((data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_ch11</span>(<span class="params">batch_size=<span class="number">10</span>, n=<span class="number">1500</span></span>):	<span class="comment"># 读取此数据集的前1500个数据，并按每10个数据打包</span></span><br><span class="line">    data = np.genfromtxt(</span><br><span class="line">        <span class="string">&#x27;work/airfoil_self_noise.dat&#x27;</span>,</span><br><span class="line">        dtype=np.float32,</span><br><span class="line">        delimiter=<span class="string">&#x27;\t&#x27;</span></span><br><span class="line">        )</span><br><span class="line">    data = paddle.to_tensor((data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>))</span><br><span class="line">    data_iter = ppl.load_array(</span><br><span class="line">        (data[:n, :-<span class="number">1</span>],</span><br><span class="line">        data[:n, -<span class="number">1</span>]),</span><br><span class="line">        batch_size,</span><br><span class="line">        is_train=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> data_iter, data.shape[<span class="number">1</span>]-<span class="number">1</span> </span><br></pre></td></tr></table></figure>
<p><strong>1）从零开始实现</strong></p>
<p>我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。具体来说，我们添加了一个状态输入<code>states</code>并将超参数放在字典<code>hyperparams</code>中。此外，我们将在训练函数里对各个小批量样本的损失求平均，因此优化算法中的梯度不需要除以批量大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="comment"># params为待更新的参数</span></span><br><span class="line">    <span class="comment"># states为状态变量</span></span><br><span class="line">    <span class="comment"># hyperparams为超参数</span></span><br><span class="line">    <span class="comment"># 在神经网络中我们能控制的就是这三个参数</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p.stop_gradient = <span class="literal">True</span>	<span class="comment"># 梯度停止更新</span></span><br><span class="line">        p.subtract_(hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad) <span class="comment"># subtract_算完之后即可进行赋值操作</span></span><br><span class="line">        p.grad.zero_()</span><br><span class="line">        p.stop_gradient = <span class="literal">False</span>	<span class="comment"># 梯度开始更新</span></span><br></pre></td></tr></table></figure>
<p>下面实现一个通用的训练函数【因为我们会学6大优化算法】，以方便本章后面介绍的其他优化算法使用。它初始化了一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其他算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch11</span>(<span class="params">trainer_fn, states, hyperparams, data_iter,</span></span><br><span class="line"><span class="params">               feature_dim, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    <span class="comment"># trainer_fn：线性回归模型</span></span><br><span class="line">    <span class="comment"># data_iter：数据集</span></span><br><span class="line">    <span class="comment"># feature_dim：特征数</span></span><br><span class="line">    <span class="comment"># num_epochs：迭代周期</span></span><br><span class="line">    w = paddle.normal(mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>, shape=(feature_dim, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: ppl.linreg(X, w, b), ppl.squared_loss</span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer_fn([w, b], states, hyperparams)</span><br><span class="line">            n += X.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter),</span><br><span class="line">                             (ppl.evaluate_loss(net, data_iter, loss),))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;timer.avg():<span class="number">.3</span>f&#125;</span> sec/epoch&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> timer.cumsum(), animator.Y[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_sgd</span>(<span class="params">lr, batch_size, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    data_iter, feature_dim = get_data_ch11(batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_ch11(</span><br><span class="line">        sgd, <span class="literal">None</span>, &#123;<span class="string">&#x27;lr&#x27;</span>: lr&#125;, data_iter, feature_dim, num_epochs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">gd_res = train_sgd(<span class="number">1</span>, <span class="number">1500</span>, <span class="number">10</span>)		<span class="comment"># 批量梯度下降</span></span><br><span class="line">sgd_res = train_sgd(<span class="number">0.005</span>, <span class="number">1</span>)		<span class="comment"># 随机梯度下降</span></span><br><span class="line">mini1_res = train_sgd(<span class="number">0.4</span>, <span class="number">100</span>)		<span class="comment"># 小批量随机梯度下降【以100为批量数batch】</span></span><br><span class="line">mini2_res = train_sgd(<span class="number">0.05</span>, <span class="number">10</span>)		<span class="comment"># 小批量随机梯度下降【以10为批量数batch】</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">上述四个模型是超参数不同的同类模型，它们要执行的任务都是同一个任务,都是去分析前面1500个关于测试机翼的数据，不同点是拿进来的批量不一样、学习率不一样，由此产生了不同的模型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ppl.set_figsize([<span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">ppl.plot(</span><br><span class="line">    *<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(gd_res, sgd_res, mini1_res, mini2_res))),</span><br><span class="line">    <span class="string">&#x27;time (sec)&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1e-2</span>, <span class="number">10</span>],</span><br><span class="line">    legend=[<span class="string">&#x27;gd&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>, <span class="string">&#x27;batch_size=100&#x27;</span>, <span class="string">&#x27;batch_size=10&#x27;</span>])</span><br><span class="line">ppl.plt.gca().set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/16.svg" alt="svg"></p>
<center>批量梯度下降</center>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17.svg" alt="svg"></p>
<center>随机梯度下降</center>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18.svg" alt="svg"></p>
<center>批量梯度下降</center>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19.svg" alt="svg"></p>
<center>四个优化算法汇总</center>
<p>现在我们可以比较前四个实验的时间与损失。可以看出，尽管在处理的样本数方面，随机梯度下降的收敛速度快于梯度下降，但与梯度下降相比，它需要更多的时间来达到同样的损失，因为逐个样本来计算梯度并不那么有效。小批量随机梯度下降能够平衡收敛速度和计算效率。大小为10的小批量比随机梯度下降更有效；大小为100的小批量在运行时间上甚至优于梯度下降。</p>
<p><strong>2）简洁实现</strong></p>
<p>下面用深度学习框架自带算法实现一个通用的训练函数，我们将在本章中其它小结使用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise_ch11</span>(<span class="params">trainer_fn, hyperparams, data_iter, num_epochs=<span class="number">4</span></span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            new_weight = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=m.weight.shape)</span><br><span class="line">            m.weight.set_value(new_weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line"></span><br><span class="line">    optimize = trainer_fn(parameters=net.parameters(), **hyperparams)</span><br><span class="line"></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimize.clear_grad()</span><br><span class="line">            out = net(X)</span><br><span class="line">            y = y.reshape(out.shape)</span><br><span class="line">            l = loss(out, y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            optimize.step()</span><br><span class="line">            n += X.shape[<span class="number">0</span>] <span class="comment"># 累加的批量（0--n，每次加b个）</span></span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(</span><br><span class="line">                    n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter),</span><br><span class="line">                    (ppl.evaluate_loss(net, data_iter, loss) / <span class="number">2</span>, ))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span> sec/epoch&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>下面使用这个训练函数，复现之前的实验。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_iter, _ = get_data_ch11(<span class="number">10</span>)</span><br><span class="line">trainer = paddle.optimizer.SGD</span><br><span class="line">train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20.svg" alt="svg"></p>
<h4 id="3-1-5-小结">3.1.5 小结</h4>
<ul>
<li>学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。</li>
<li>对于凸问题，我们可以证明，对于广泛的学习率选择，随机梯度下降将收敛到最优解。</li>
<li>如果学习率太小或太大，就会出现问题。实际上，通常只有经过多次实验后才能找到合适的学习率。</li>
<li>当训练数据集中有更多样本时，计算梯度下降的每次迭代的代价更高，因此在这些情况下，首选随机梯度下降。</li>
<li>随机梯度下降的最佳性保证在非凸情况下一般不可用，因为需要检查的局部最小值的数量可能是指数级的。</li>
<li>在迭代过程中，由于存在噪音，下降会朝向最小值，但是不会精确收敛，只会在附近摆动，更重要的原因在于：学习率是一个固定值。</li>
<li>由于减少了深度学习框架的额外开销，使用更好的内存方位以及CPU和GPU上的缓存，向量化使代码更加高效。</li>
<li>随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。</li>
<li>在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。</li>
<li>在训练期间降低学习率有助于训练。</li>
<li>一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。</li>
<li>批量合适很重要：过大那么计算代价大，过小虽然收敛快但是计算慢。</li>
</ul>
<h3 id="3-2-动量法">3.2 动量法</h3>
<p>🎨<code>sec_momentum</code></p>
<p>模型通过训练数据来拟合模型参数，使得模型能够根据输入来正确输出。那么如果输出距离我们正确答案有差异，我们将衡量差异的函数定义为目标函数（损失函数等）。<br>
我们希望目标函数为0，这样就表明我们模型完全正确输出我们的期望。但是因为特征维度的丰富，目标函数的解析解，不可求或者不好求，所以运用计算机强大的运算能力来求数值解，即：只要误差在可接受范围内，我们就认为这个解释我们的目标解。</p>
<p>我们求解数值解的方法，我们采用了<strong>迭代逼近</strong>的方法。一次次迭代x的值，使其逐渐逼近误差函数的最小值。<br>
但是这样速度太慢，为此我们考虑沿着反梯度方向可是实现最速下降，故引入了梯度下降法。</p>
<h4 id="3-2-1-质疑">3.2.1 质疑</h4>
<p>有人觉得仅仅考虑沿梯度下降还不够。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量，在当前位置下降，有人提出质疑：仅仅考虑当前位置不行的，<br>
要考虑整体方向，运动也有个惯性，历史也有大势所趋，肯定不能只看当前位置。所以在自变量更新的时候考虑引入速度变量，有速度就有惯性。惯性越大，越势不可挡。</p>
<p><strong>梯度下降的问题</strong></p>
<p>为了更好地了解动量法的几何属性，我们复习一下梯度下降。我们构造一个目标函数$f(\mathbf{x})=x_1<sup>2+2x_2</sup>2$，并有二维向量$\mathbf{x} = [x_1, x_2]^\top$作为输入，标量作为输出。在此基础上我们稍加改动。</p>
<p>$$<br>
f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.<br>
$$<br>
与之前一样，$f$在$(0, 0)$有最小值，该函数在$x_1$的方向上非常平坦。让我们看看在这个新函数上执行梯度下降时会发生什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1**<span class="number">2</span> + <span class="number">2</span> * x2**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gd_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(gd_2d))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-4.257978, x2: -0.061181</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/21.svg" alt="svg"></p>
<p>可以看到，同一位置上，目标函数在竖直方向比在水平方向的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。<br>
因此，我们陷入两难：如果选择较小的学习率，我们会确保解不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。</p>
<p>下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化：$x_1$方向上的收敛有所改善，但整体来看解的质量更差了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(gd_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.387814, x2: -1673.365109</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/22.svg" alt="svg"></p>
<p>在梯度下降中，我们需要一个较小的学习率从而避免自变量在梯度较大的方向上越过目标函数最优解，然而，这会造成自变量在梯度较小的方向上朝最优解移动变慢；我们试着将学习率调得稍大一点，此时自变量在梯度较大的方向又会不断越过最优解并逐渐发散。</p>
<h4 id="3-2-2-动量法地提出">3.2.2 动量法地提出</h4>
<p><strong>动量法（momentum）</strong> 使我们能够解决上面描述的梯度下降问题。动量法又被称作基于动量的梯度下降法(SGD with momentum)，是一种使梯度向量向相关方向加速变化、最终实现加速收敛的方法。动量法引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。</p>
<p>在中学物理中，刻画惯性的物理量是动量，沿山谷滚下的铁球会收到沿坡道向下的力和与左右山壁碰撞的弹力，向下的力（重力）稳定不变，产生的动量不断累积，速度越来越快；左右的弹力总是在不停切换，动量累积的结果是相互抵消，减弱了球的来回震荡。因此，与随机梯度下降相比，动量方法的收敛速度更快，收敛曲线也更稳定，见下图。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/da1bdcde82e445b8be8070b97eeef064ba5c62d5a2ff45c6bd203482e2df3be1" width="500" hegiht="" ></center>
<center>图4：SGD与带动量SGD </center>
<p>每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即<strong>每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加</strong> 得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）喜欢稍重篮球的比例较高，一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。</p>
<p><strong>1）动量法公式</strong></p>
<p>动量法引入了速度$\mathbf{v}_t$，它包含了参数在参数空间移动的方向和速率。使用$\mathbf{v}_t$而不是梯度$\mathbf{g}_t$可以生成以下更新等式：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>t &amp;\leftarrow \beta \mathbf{v}</em>{t-1} + \eta_t \mathbf{g}_{t}, \<br>
\mathbf{x}<em>t &amp;\leftarrow \mathbf{x}</em>{t-1} -  \mathbf{v}_t.<br>
\end{aligned}<br>
$$</p>
<p>这里的$\mathbf{v}_t$就是动量，它是一段时间内平均预测的值，$\beta$为动量系数。对于$\beta = 0$，我们恢复常规的梯度下降。<br>
在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。为了方便起见，我们在时间$t=0$初始化$\mathbf{v}_0 = 0$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">momentum_2d</span>(<span class="params">x1, x2, v1, v2</span>):</span><br><span class="line">    <span class="comment">#print(v1,v2)</span></span><br><span class="line">    v1 = beta * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = beta * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, beta = <span class="number">0.6</span>, <span class="number">0.5</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(momentum_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1: 0.007188, x2: 0.002553</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/23.svg" alt="svg"></p>
<p>正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。让我们看看当降低动量参数时会发生什么。将其减半至$\beta = 0.25$会导致一条几乎没有收敛的轨迹。尽管如此，它比没有动量时解将会发散要好得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta, beta = <span class="number">0.6</span>, <span class="number">0.25</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(momentum_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.126340, x2: -0.186632</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/24.svg" alt="svg"></p>
<p><strong>2）指数加权平均</strong></p>
<p>我们可以用指数加权移动平均理解动量法。在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。</p>
<p><strong>滑动平均(exponential moving average)</strong>，或者叫做<strong>指数加权平均(exponentially weighted moving average)</strong>，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。指数移动加权平均法，是指各数值的加权系数随时间呈指数式递减，越靠近当前时刻的数值加权系数就越大，较传统的平均法来说，一是不需要保存过去所有的数值；二是计算量显著减小。<br>
$$<br>
\begin{aligned}<br>
\mathbf{v}<em>t &amp;\leftarrow \beta \mathbf{v}</em>{t-1} +({1-\beta}) \mathbf{θ}_{t}<br>
\end{aligned}<br>
$$</p>
<p>上式中 $θt$为时刻$t$ 的实际值；系数 $β$ 表示加权下降的速率，其值越小下降的越快；$vt$ 为 $t$时刻EWMA（指数加权移动平均） 的值。在实践中，我们通常将$\mathbf{v}<em>t$ 看做是对最近 $\frac{1} {1-\beta}$个时间步的$\mathbf{θ}</em>{t}$值的加权平均。我们使用$\beta = 0.9$来验证这个公式。</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>{100} = 0.9 \mathbf{v}</em>{99} + 0.1 \mathbf{θ}_{100}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>{99} = 0.9 \mathbf{v}</em>{98} + 0.1 \mathbf{θ}_{99}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>{98} = 0.9 \mathbf{v}</em>{97} + 0.1 \mathbf{θ}_{98}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
…<br>
\end{aligned}<br>
$$</p>
<p>我们将式子一步一步的带入得到最终的推导：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>{n} = 0.1 \mathbf{θ}</em>{n} + (0.9)^{1}0.1 \mathbf{θ}<em>{n-1} + (0.9)^{2}0.1 \mathbf{θ}</em>{n-2} …+ (0.9)^{n-1}0.1 \mathbf{θ}_{1}<br>
\end{aligned}<br>
$$</p>
<p>所以我们计算出来的$\mathbf{v}<em>{n}$实际上是包含了以前所有的结果。对于$\mathbf{v}</em>{20}$各参数的数值如下图：</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/a21079e5e7f84d53b822c3786c1e750f1702d4a2f89542af8f6162a7e3ca08a4" width="600" hegiht="" ></center>
<center>图5：$\mathbf{v}_{20}$参数取值 </center>
<p>可以看出，对于$\mathbf{v}<em>{20}$来说，$\mathbf{θ}</em>{20}$的权重几乎是$\mathbf{θ}<em>{1}$的9倍，这个时候即使我们忽略$\mathbf{θ}</em>{1}$其实对结果也没有太大的影响，所以我们进一步规定，如果权重低于$\mathbf{θ}<em>{n}$的$\frac{1} {e}$，那么我们就认为该数值在平均中起很小的作用，在数学上一般会以$\frac{1} {e}$作为一个临界值，小于该值的加权系数的值不做考虑。这样上图我们可以将$\mathbf{v}</em>{20}$看作是红线以上的10个数据的指数加权平均，当$\beta = 0.9$时，指数加权平均考虑了估计点附近的10个数据。</p>
<p>现在我们可以对动量法的速度变量做变形：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>t &amp;\leftarrow \beta \mathbf{v}</em>{t-1} +({1-\beta}) (\frac{\eta_t} { {1-\beta} }\mathbf{g}_{t})<br>
\end{aligned}<br>
$$</p>
<p>指数加权平均是在将当前值用其前面时间的值表示，所以动量法中当前的动量就可以用前面的动量表示，再利用当前动量影响自变量。所以说，在动量法中，我们根据 $\beta$ 的值，确定当前的动量是根据前多少个时间步的动量做的指数加权移动平均，利用这些数据逐渐缩小参数收敛速率。</p>
<h4 id="3-2-3-实现动量法">3.2.3 实现动量法</h4>
<p>相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度，它与梯度以及优化问题的变量具有相同的形状。类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，而步长则是时间，前一次步伐好比前一时刻的速度。标准梯度下降算法在每次行动时，都忽略前一时刻的速度，而重新根据当前时刻的加速度和时间来行走，因此当加速度趋于零时就很难继续移动。<br>
而动量方法则考虑前一时刻速度和当前加速度的共同作用。</p>
<p><strong>1）从零开始实现</strong></p>
<p>在下面的实现中，我们称这些变量为<code>states</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>): <span class="comment"># 获取优化的特征个数，对应速度(状态变量)的个数</span></span><br><span class="line">    v_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;SGD动量&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>): <span class="comment"># 参数 状态变量 超参数</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + hyperparams[<span class="string">&#x27;learning_rate&#x27;</span>] * p.grad</span><br><span class="line">            p[:] -=  v</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_momentum</span>(<span class="params">lr, momentum, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    ppl.train_ch11(</span><br><span class="line">        sgd_momentum, init_momentum_states(feature_dim),</span><br><span class="line">        &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr, <span class="string">&#x27;momentum&#x27;</span>: momentum&#125;, data_iter,</span><br><span class="line">        feature_dim, num_epochs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>) <span class="comment"># 分别对应数据迭代器和特征个数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/25.svg" alt="svg"></p>
<p>在实际过程中，选择合适的动量参数对模型的收敛十分关键。当我们将动量超参数<code>momentum</code>增加到0.9时，它相当于有效样本数量增加到$\frac{1} {1 - 0.9} = 10$。可以看到，损失一直在动荡，甚至出现了剧增，模型不收敛，且效果不稳定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/26.svg" alt="svg"></p>
<p>降低学习率进一步解决了任何非平滑优化问题的困难，将其设置为$0.005$会产生良好的收敛性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.005</span>, <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/27.svg" alt="svg"></p>
<p><strong>2）简洁实现</strong></p>
<p>深度学习框架中的优化器早已构建了动量法。<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Overview_cn.html">paddle.optimizer</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Momentum</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.005</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/28.svg" alt="svg"></p>
<h4 id="3-2-4-小结">3.2.4 小结</h4>
<ul>
<li>动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。</li>
<li>对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。</li>
<li>动量法可以防止在随机梯度下降的优化过程停滞的问题。</li>
<li>动量法的实现非常简单，但它需要我们存储额外的状态向量（动量$\mathbf{v}$）。</li>
<li>由于对过去的数据进行了指数降权，有效梯度数为$\frac{1} {1-\beta}$。</li>
</ul>
<h3 id="3-3-AdaGrad">3.3 AdaGrad</h3>
<p>🎨<code>sec_adagrad</code></p>
<p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。在动量法一节里我们看到，当$x1$和$x2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。</p>
<p><strong>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能</strong>。本节我们介绍$AdaGrad$算法，<strong>它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率</strong>，从而避免统一的学习率难以适应所有维度的问题。这有两个好处：首先，我们不再需要决定梯度何时算足够大。其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。</p>
<h5 id="1）稀疏特征和学习率"><strong>1）稀疏特征和学习率</strong></h5>
<p>假设我们正在训练一个语言模型。为了获得良好的准确性，我们大多希望在训练的过程中降低学习率，原因在于稀疏特征（即只在偶尔出现的特征）的影响，这对自然语言来说很常见。例如，我们看到“预先条件”这个词比“学习”这个词的可能性要小得多。但是，它在计算广告学和个性化协同过滤等其他领域也很常见。只有在这些不常见的特征出现时，与其相关的参数才会得到有意义的更新。鉴于学习率下降，我们可能最终会面临这样的情况：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。</p>
<p>解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率【AdaGrad算法的核心思想】。</p>
<h5 id="2）AdaGrad算法"><strong>2）AdaGrad算法</strong></h5>
<p>AdaGrad优化算法在每次使用一个批量的数据进行参数更新的时候，计算所有参数的梯度，使用变量$\mathbf{s}_t$来累加过去的小批量随机梯度$\mathbf{g}_t^2$，接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整，如下所示：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{g}<em>t &amp; = \partial</em>{\mathbf{w} } l(y_t, f(\mathbf{x}_t, \mathbf{w})), \<br>
\mathbf{s}<em>t &amp; = \mathbf{s}</em>{t-1} + \mathbf{g}_t^2, \<br>
\mathbf{w}<em>t &amp; = \mathbf{w}</em>{t-1} - \frac{\eta} {\sqrt{\mathbf{s}_t + \epsilon} } \cdot \mathbf{g}_t.<br>
\end{aligned}<br>
$$<br>
在这里，操作是按照坐标顺序应用，这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。与之前一样，$\eta$是学习率，$\epsilon$是一个为维持数值稳定性而添加的常数，用来确保我们不会除以$0$。因为有可能 $\mathbf{s}$ 的值为 0，那么 0 出现在分母就会出现无穷大的情况。这样不同的参数由于梯度不同，他们对应的 $\mathbf{s}$ 大小也就不同，所以上面的公式得到的学习率也就不同，这也就实现了自适应的学习率。最后，我们初始化$\mathbf{s}_0 = \mathbf{0}$。</p>
<p>需要强调的是，小批量随机梯度按元素平方的累加变量$\large s_{t}$出现在学习率的分母项中。因此：</p>
<ul>
<li>如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快。</li>
<li>如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。</li>
</ul>
<p>我们使用自适应的学习率就可以帮助算法在梯度大的参数方向减缓学习速率，而在梯度小的参数方向加快学习速率，这就导致了神经网络的训练速度的加快。然而，由于累加变量$\large s_{t}$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率较小，可能较难找到一个有用的解。在深度学习中，我们可能希望更慢地降低学习率，这引出了许多AdaGrad算法的变体，我们将在后续章节中讨论它们。</p>
<p>眼下让我们先看看它在二次凸问题中的表现如何。我们仍然同一函数为例：</p>
<p>$$<br>
f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.<br>
$$<br>
我们将使用与之前相同的学习率来实现AdaGrad算法，即$\eta = 0.4$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span> <span class="comment"># 为维持数值稳定性而添加的常数</span></span><br><span class="line">    g1, g2 = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2 <span class="comment"># 自变量梯度</span></span><br><span class="line">    <span class="comment"># 累加变量 s1 s2 变量 x1 x2</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span> </span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(adagrad_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-2.382563, x2: -0.158591</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/29.svg" alt="svg"></p>
<p>可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。我们将学习率提高到$2$，可以看到更好的表现，这说明学习率降低的情况可能相当剧烈，我们需要确保参数能够适当地收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(adagrad_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.002295, x2: -0.000000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/30.svg" alt="svg"></p>
<h4 id="3）实现"><strong>3）实现</strong></h4>
<p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。不同点在于，这里需要使用<em><strong>更大的学习率</strong></em>来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adagrad_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;adagrad算法&#x27;&#x27;&#x27;</span>	</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] += paddle.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;learning_rate&#x27;</span>] * p.grad / paddle.sqrt(s + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch11</span>(<span class="params">trainer_fn, states, hyperparams, data_iter,</span></span><br><span class="line"><span class="params">               feature_dim, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    w = paddle.normal(mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>, shape=(feature_dim, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: ppl.linreg(X, w, b), ppl.squared_loss</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer_fn([w, b], states, hyperparams) <span class="comment"># 优化算法</span></span><br><span class="line">            n += X.shape[<span class="number">0</span>] <span class="comment"># 每次增加b个样本</span></span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter), <span class="comment"># x轴一轮迭代的时间坐标</span></span><br><span class="line">                             (ppl.evaluate_loss(net, data_iter, loss),))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="keyword">return</span> timer.avg(), animator.Y[<span class="number">0</span>][-<span class="number">1</span>] <span class="comment"># 返回每一轮迭代时间的平均值、最终的误差值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adagrad, init_adagrad_states(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter, feature_dim)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.062071673075358075, 0.24227345788478852)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/31.svg" alt="svg"></p>
<p>我们也可以直接使用深度学习框架中提供的AdaGrad算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adagrad</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/32.svg" alt="svg"></p>
<p>​		AdaGrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新，这就是Adagrad算法加快深层神经网络的训练速度的核心。<br>
类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。</p>
<h4 id="3-4-RMSProp">3.4 RMSProp</h4>
<p>🎨<code>sec_rmsprop</code></p>
<p>上一节AdaGrad算法中，我们提到，因为调整学习率时分母上的累加变量一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。<br>
因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。简言之：冲的太猛了。</p>
<p>问题在于，Adagrad算法将梯度$\mathbf{g}_t$的平方累加成状态矢量$\mathbf{s}<em>t = \mathbf{s}</em>{t-1} + \mathbf{g}_t^2$。因此，由于缺乏规范化，没有约束力，$\mathbf{s}_t$持续增长，几乎上是在算法收敛时呈线性递增。</p>
<p>RMSProp是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。</p>
<h5 id="1）RMSProp算法"><strong>1）RMSProp算法</strong></h5>
<p>RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。让我们详细写出这些方程式。</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{s}<em>t &amp; \leftarrow \gamma \mathbf{s}</em>{t-1} + (1 - \gamma) \mathbf{g}_t^2, \<br>
\mathbf{x}<em>t &amp; \leftarrow \mathbf{x}</em>{t-1} - \frac{\eta} {\sqrt{\mathbf{s}_t + \epsilon} } \odot \mathbf{g}_t.<br>
\end{aligned}<br>
$$<br>
常数$\epsilon &gt; 0$通常设置为$10^{-6}$，以确保我们不会因除以零或步长过大而受到影响。鉴于这种扩展，我们现在可以自由控制学习率$\eta$，而不考虑基于每个坐标应用的缩放。</p>
<h5 id="2）实现"><strong>2）实现</strong></h5>
<p>和之前一样，我们使用二次函数$f(\mathbf{x})=0.1x_1<sup>2+2x_2</sup>2$来观察RMSProp算法的轨迹。回想在 :numref:<code>sec_adagrad</code>一节中，当我们使用学习率为0.4的AdaGrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。RMSProp算法中不会发生这种情况，因为$\eta$是单独控制的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cProfile <span class="keyword">import</span> label <span class="comment"># 性能分析模块</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = gamma * s1 + (<span class="number">1</span> - gamma) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = gamma * s2 + (<span class="number">1</span> - gamma) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(rmsprop_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.010599, x2: 0.000000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/33.svg" alt="svg"></p>
<p>接下来，我们在深度网络中实现RMSProp算法。在这里，$\mathbf{s}$累加了过去的$1/(1-\gamma) = 10$次平方梯度观测值的平均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rmsprop_state</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;RMSProp算法&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    gamma, eps = hyperparams[<span class="string">&#x27;gamma&#x27;</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] = gamma * s + (<span class="number">1</span> - gamma) * paddle.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / paddle.sqrt(s + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    rmsprop, init_rmsprop_state(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter, feature_dim</span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.07110411326090495, 0.24332575956980387)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/34.svg" alt="svg"></p>
<p>我们可直接使用深度学习框架中提供的RMSProp算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.RMSProp</span><br><span class="line">ppl.train_concise_ch11(</span><br><span class="line">    trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/35.svg" alt="svg"></p>
<h4 id="3-5-Adadelta">3.5 Adadelta</h4>
<p>🎨<code>sec_adadelta</code></p>
<p>除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期较难找到有用解的问题做了改进：主要区别在于前者减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准。</p>
<h5 id="1）Adadelta算法"><strong>1）Adadelta算法</strong></h5>
<p>Adadelta使用两个状态变量，$\mathbf{s}_t$来接收小批量随机梯度按元素平方的指数加权移动平均变量：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{s}<em>t &amp; = \rho \mathbf{s}</em>{t-1} + (1 - \rho) \mathbf{g}<em>t^2.<br>
\end{aligned}<br>
$$<br>
与之前算法的不同点在于，额外增加了一个状态变量：$\Delta\mathbf{x}<em>t$，我们将$\Delta \mathbf{x}</em>{0}$初始化为$0$，使用$\Delta \mathbf{x}</em>{t-1}$【上一次更新的长度】来计算自变量的变化量，$\epsilon$（例如$10^{-5}$这样的小值）是为了保持数字稳定性而加入的：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{g}<em>t’ &amp; = \frac{\sqrt{\Delta\mathbf{x}</em>{t-1} + \epsilon} } {\sqrt{ {\mathbf{s}_t + \epsilon} }} \odot \mathbf{g}_t, \<br>
\end{aligned}<br>
$$</p>
<p>接着我们使用重新缩放的梯度$\mathbf{g}_t’$执行更新，即：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{x}<em>t  &amp; = \mathbf{x}</em>{t-1} - \mathbf{g}<em>t’. \<br>
\end{aligned}<br>
$$<br>
我们使用$\large \Delta x</em>{t}$来记录自变量变化量$\large {g}'_{t}$按元素平方的指数加权移动平均，即：</p>
<p>$$<br>
\begin{aligned}<br>
\Delta \mathbf{x}<em>t &amp; = \rho \Delta\mathbf{x}</em>{t-1} + (1 - \rho) {\mathbf{g}<em>t’}^2,<br>
\end{aligned}<br>
$$<br>
AdaDelta算法跟RMSProp算法的不同之处在于使用$\large \sqrt{\Delta x</em>{t-1} }$来替代学习率$\large \eta$。</p>
<h5 id="2）实现-v2"><strong>2）实现</strong></h5>
<p>Adadelta需要为每个变量维护两个状态变量，即$\mathbf{s}_t$和$\Delta\mathbf{x}_t$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adadelta_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w, s_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    delta_w, delta_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adadelta</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    rho, eps = hyperparams[<span class="string">&#x27;rho&#x27;</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] = rho * s + (<span class="number">1</span> - rho) * paddle.square(p.grad)</span><br><span class="line">            g = (paddle.sqrt(delta + eps) / paddle.sqrt(s + eps)) * p.grad</span><br><span class="line">            p[:] -= g</span><br><span class="line">            delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adadelta, init_adadelta_states(feature_dim), </span><br><span class="line">    &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter, feature_dim)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.08289804458618164, 0.24341165928045908)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/36.svg" alt="svg"></p>
<p>我们也可以简洁实现adadelta算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adadelta</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)	<span class="comment"># 没有学习率这个超参数了</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/37.svg" alt="svg"></p>
<blockquote>
<p>初始化没有数据支撑，就是随便下降，所以下降的比较缓慢。。</p>
</blockquote>
<h4 id="3-6-Adam">3.6 Adam</h4>
<p>🎨<code>sec_adam</code></p>
<p>由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这就是当前广泛应用的算法。</p>
<h5 id="1）Adam算法"><strong>1）Adam算法</strong></h5>
<p>Adam算法的关键组成部分之一是：它使用使用了动量法中的动量变量$\mathbf{v}_t$和RMSProp算法中的小批量随机梯度按元素平方的指数加权移动平均变量$\mathbf{s}_t$，即：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{v}<em>t &amp; \leftarrow \beta_1 \mathbf{v}</em>{t-1} + (1 - \beta_1) \mathbf{g}_t, \<br>
\mathbf{s}<em>t &amp; \leftarrow \beta_2 \mathbf{s}</em>{t-1} + (1 - \beta_2) \mathbf{g}_t^2.<br>
\end{aligned}<br>
$$<br>
这里$\beta_1$和$\beta_2$是非负加权参数。他们的常见设置是$\beta_1 = 0.9$和$\beta_2 = 0.999$。注意，如果我们初始化$\mathbf{v}_0 = \mathbf{s}_0 = 0$，就会获得一个相当大的初始偏差，原因在于每个最新数据值，都依赖于以前的数据结果。我们可以通过使用<strong>偏差修正（bias correction）</strong> 来解决这个问题，使用$\hat{\mathbf{v} }_t$和$\hat{\mathbf{s} }_t$，公式为：</p>
<p>$$<br>
\hat{\mathbf{v} }_t = \frac{\mathbf{v}_t} {1 - \beta_1^t} \text{ and } \hat{\mathbf{s} }_t = \frac{\mathbf{s}_t} {1 - \beta_2^t}.<br>
$$<br>
通过公式发现，随着$t$增加，$\beta^t$接近于0，所以当$t$很大的时候，偏差修正几乎没有作用，不过在开始学习阶段，偏差修正可以帮助我们更好预测。有了正确的估计，我们现在可以写出更新方程。首先，我们以非常类似于RMSProp算法的方式重新缩放梯度以获得</p>
<p>$$<br>
\mathbf{g}_t’ = \frac{\eta \hat{\mathbf{v} }_t} {\sqrt{\hat{\mathbf{s} }_t} + \epsilon}.<br>
$$<br>
与RMSProp不同，我们的更新使用动量$\hat{\mathbf{v} }_t$而不是梯度本身。此外，由于使用$\frac{1} {\sqrt{\hat{\mathbf{s} }_t} + \epsilon}$而不是$\frac{1} {\sqrt{\hat{\mathbf{s} }_t + \epsilon} }$进行缩放，两者会略有差异。前者在实践中效果略好一些，因此与RMSProp算法有所区分。通常，我们选择$\epsilon = 10^{-6}$，这是为了在数值稳定性和逼真度之间取得良好的平衡。最后，我们简单更新：</p>
<p>$$<br>
\mathbf{x}<em>t \leftarrow \mathbf{x}</em>{t-1} - \mathbf{g}_t’.<br>
$$</p>
<h5 id="2）实现-v3"><strong>2）实现</strong></h5>
<p>我们将时间步$t$存储在<code>hyperparams</code>字典中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w, v_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    s_w, s_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            s[:] = beta2 * s + (<span class="number">1</span> - beta2) * paddle.square(p.grad)</span><br><span class="line">            v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (paddle.sqrt(s_bias_corr) + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在，我们用以上Adam算法来训练模型，这里我们使用$\eta = 0.01$的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adam, init_adam_states(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, data_iter, feature_dim)</span><br></pre></td></tr></table></figure>
<pre><code>(0.07531051635742188, 0.24483665378888447)
</code></pre>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/38.svg" alt="svg"></p>
<p>此外，我们可以用深度学习框架自带算法应用Adam算法，这里我们只需要传递配置参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adam</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/39.svg" alt="svg"></p>
<h4 id="小结"><em><strong>小结</strong></em></h4>
<ul>
<li>AdaGrad算法会在单个坐标层面动态降低学习率。</li>
<li>AdaGrad算法利用梯度的大小作为调整速率的手段：用较小的学习率来补偿带有较大梯度的坐标。</li>
<li>如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。</li>
<li>AdaGrad算法对于稀疏特征特别有效，在此情况下由于不常出现的问题，学习率需要更慢地降低。</li>
<li>AdaGrad算法在迭代过程中不断调整学习率，并让目标函数自变量中每个元素都分别拥有自己的学习率。</li>
<li>使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</li>
<li>RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。</li>
<li>RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。</li>
<li>Adadelta没有学习率参数。相反，它使用参数本身的变化率来调整学习率。</li>
<li>Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。</li>
<li>Adam算法使用了偏差修正。</li>
<li>和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。</li>
</ul>
<h2 id="四、提高模型泛化能力–正则化方法">四、提高模型泛化能力–正则化方法</h2>
<p>降低偏差可以提高模型在训练数据上的表现。但实际上，评价一个机器学习模型的性能，不仅要看其在训练集上的表现，还要评估它在未观测到的数据上的表现，这就要求模型具有良好的泛化能力。本节介绍的正则化方法能够提高模型泛化能力，减少训练误差和测试误差的差距。下面将介绍几种常见的正则化方法。</p>
<h3 id="4-1-L2正则与L1正则">4.1 L2正则与L1正则</h3>
<p>正则化方法是在目标函数或代价函数后面加上一个正则项，对参数进行约束，来限制模型的学习能力，是应对过拟合的常用方法。回想一下，在多项式回归的例子中，我们可以通过调整拟合多项式的阶数来限制模型的容量，也就是限制参数值的选择范围。</p>
<p>$$<br>
L(\mathbf{w}, b) 满足 ||\mathbf{w}||^2 ≤ \lambda<br>
$$</p>
<p>这里我们使得权重的平方和小于一个值，以此来限制参数值以达到控制模型复杂度的目的，因为偏置对模型复杂度的影响较小，所以我们通常不限制。</p>
<h4 id="L2正则"><strong>L2正则</strong></h4>
<p>L2参数正则化方法也叫做权重衰减。<strong>权重衰减（weight decay）</strong> 是最广泛使用的正则化的技术之一。$L_2$范数惩罚项是模型权重参数每个元素的平方和与一个正的常数的乘积。我们通过正则化常数$\lambda$来描述这种权衡，$L_2$范数正则化在模型原损失函数的基础上添加$L_2$范数惩罚项，从而得到训练得到所需的最小化函数。当参数越多或取值越大时，该惩罚项越大：<br>
$$<br>
L(\mathbf{w}, b) + \frac{\lambda} {2} |\mathbf{w}|^2<br>
$$<br>
对于$\lambda = 0$，我们恢复了原来的损失函数。对于$\lambda &gt; 0$，我们限制$| \mathbf{w} |$的大小。这里我们仍然除以$2$：当我们取一个二次函数的导数时，$2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。</p>
<p>根据之前章节所讲的，我们根据估计值与观测值之间的差异来更新$\mathbf{w}$。然而，加入了正则化之后，我们同时也在试图将$\mathbf{w}$的大小进一步缩小。这就是为什么这种方法有时被称为权重衰减。我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的$\lambda$值对应较少约束的$\mathbf{w}$，而较大的$\lambda$值对$\mathbf{w}$的约束更大。</p>
<h4 id="L1正则"><strong>L1正则</strong></h4>
<p>L1正则化也是一种常见的正则化方法，它使模型的参数尽可能稀疏化。L1正则化被定义为各个参数的绝对值之和：</p>
<p>$$<br>
L(\mathbf{w}, b) + |\mathbf{w}|<br>
$$<br>
此外，你可能会问为什么我们首先使用$L_2$范数，而不是$L_1$范数。事实上，这个选择在整个统计领域中都是有效的和受欢迎的。使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为稳定。相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，而将其他权重清除为零。这称为<strong>特征选择（feature selection）</strong>，这可能是其他场景下需要的。</p>
<h4 id="1）从零开始实现"><strong>1）从零开始实现</strong></h4>
<p>下面我们将从头开始实现权重衰减，只需将$L_2$的平方惩罚添加到原始目标函数中。首先，我们像以前一样生成一些数据，生成公式如下：</p>
<p>$$<br>
y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }<br>
\epsilon \sim \mathcal{N}(0, 0.01^2).<br>
$$<br>
我们选择标签是关于输入的线性函数。标签同时被均值为0，标准差为0.01高斯噪声破坏。为了使过拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，并使用一个只包含20个样本的小训练集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.tensor <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_examples, <span class="built_in">len</span>(w)))<span class="comment">#均值为0，标准差为0.01，n个样本，w长度的特征</span></span><br><span class="line">    y = paddle.matmul(X, w) + b</span><br><span class="line">    y += paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)<span class="comment">#噪音</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment">#reshape(-1,1)转换成1列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_array, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构造数据迭代器</span></span><br><span class="line">    dataset = TensorDataset(data_array)<span class="comment"># 由张量列表定义的数据集</span></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=is_train) <span class="comment"># 之后从DataLoader中随机挑选b个样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># print(&quot;linreg: &quot;, X, w, b)</span></span><br><span class="line">    <span class="keyword">return</span> paddle.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(&quot;squared_loss: &quot;, y_hat, y)</span></span><br><span class="line">    loss = (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span> <span class="comment"># loss=1/2(y_hat-y)^2</span></span><br><span class="line">    <span class="comment">#print(&quot;loss&gt;&gt;&gt;&quot;, loss)</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>) <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line">true_w, true_b = paddle.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = load_array(train_data, batch_size)</span><br><span class="line">test_data = synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = load_array(test_data, batch_size)</span><br></pre></td></tr></table></figure>
<h5 id="【初始化模型参数】"><strong>【初始化模型参数】</strong></h5>
<p>首先，我们将定义一个函数来随机初始化模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<h5 id="【定义-L-2-范数惩罚】"><strong>【定义$L_2$范数惩罚】</strong></h5>
<p>实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h5 id="【定义训练代码实现】"><strong>【定义训练代码实现】</strong></h5>
<p>下面的代码将模型拟合训练数据集，并在测试数据集上进行评估。已知，线性网络和平方损失没有变化，唯一的变化是损失现在包括了惩罚项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib_inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        matplotlib_inline.backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line">        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = set_axes( self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):<span class="comment">#lambd超参数</span></span><br><span class="line">    w, b = init_params()<span class="comment">#初始化</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: linreg(X, w, b), squared_loss<span class="comment">#线性回归，均方损失</span></span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.03</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">                w -= lr * w.grad / X.shape[<span class="number">0</span>]</span><br><span class="line">                b -= lr * b.grad / batch_size</span><br><span class="line">                w.clear_grad()</span><br><span class="line">                b.clear_grad()</span><br><span class="line">                w.stop_gradient = <span class="literal">False</span></span><br><span class="line">                b.stop_gradient = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w的L2范数是：&quot;</span>, paddle.norm(w).item())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>①【忽略正则化直接训练】</strong></p>
<p>我们现在用<code>lambd = 0</code>禁用权重衰减后运行这个代码。这里训练误差有了减少，但测试误差没有减少，这意味着出现了严重的过拟合。</p>
<blockquote>
<p>粉色：测试误差</p>
<p>蓝色：训练误差</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/40.svg" alt="svg"></p>
<p><strong>②【使用权重衰减】</strong></p>
<p>下面，我们使用权重衰减来运行代码，这正是我们期望从正则化中得到的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/41.svg" alt="svg"></p>
<h4 id="2）简洁实现"><strong>2）简洁实现</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.set_value(paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=param.shape))</span><br><span class="line"></span><br><span class="line">    loss = paddle.nn.MSELoss()</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    sdg = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters(), weight_decay=<span class="built_in">float</span>(wd))	<span class="comment"># 注意：权重衰减的系数要是一个浮点数~</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            sdg.step()</span><br><span class="line">            sdg.clear_grad()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br><span class="line">    <span class="built_in">print</span>(net[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">train_concise(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>w的L2范数： 0.11938107758760452
Linear(in_features=200, out_features=1, dtype=float32)
</code></pre>
<h3 id="4-2-暂退法">4.2 暂退法</h3>
<p>🎨<code>sec_dropout</code></p>
<p><strong>暂退法（dropout）</strong> 是通过修改模型本身结构来实现的，计算方便但功能强大。这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代开始时，按照一定的概率随机选择一些神经元删除，即认为这些神经元不存在。</p>
<p>我们在之前完成了一个带有1个隐藏层和5个隐藏单元的多层感知机。当我们将暂退法应用到隐藏层，我们就可消除掉一些链接。例如下图，输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。这完全是随机的，这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/e4051dc691094456a4e8cfa4c8834df3c1e0711a262241ea9998c7b9a18efbf8" width="600" hegiht="" ></center>
<center>图1：丢弃前后的神经网络 </center>
<p>按照这样的网络计算梯度，进行梯度更新，删除的神经元不更新。在下一次迭代时，在随机选择一些神经元，重复上面的做法，直到训练结束。如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。这样的参数更新不再依赖于某些共同作用的隐藏层节点之间的关系，能够有效地防止过拟合。通常，我们在测试时不用暂退法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.vision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">layer</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(layer) == nn.Linear:</span><br><span class="line">        new_weight = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=layer.weight.shape)</span><br><span class="line">        <span class="comment">#print(new_weight)</span></span><br><span class="line">        layer.weight.set_value(new_weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure>
<pre><code>Sequential(
  (0): Flatten()
  (1): Linear(in_features=784, out_features=256, dtype=float32)
  (2): ReLU()
  (3): Dropout(p=0.2, axis=None, mode=upscale_in_train)
  (4): Linear(in_features=256, out_features=256, dtype=float32)
  (5): ReLU()
  (6): Dropout(p=0.5, axis=None, mode=upscale_in_train)
  (7): Linear(in_features=256, out_features=10, dtype=float32)
)
</code></pre>
<p>接下来，我们对模型进行训练和测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">trainer = paddle.optimizer.SGD(lr, net.parameters())</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">model=paddle.Model(net)</span><br><span class="line">model.prepare(trainer,loss,paddle.metric.Accuracy(topk=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)))</span><br><span class="line">model.fit(train_iter,epochs=num_epochs,batch_size=batch_size,verbose=<span class="number">1</span>)	<span class="comment"># 1开启一个可视化的进程</span></span><br><span class="line">eval_result = model.evaluate(test_iter, batch_size=<span class="number">256</span>,verbose=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#evaluate(eval_data, batch_size=1, log_freq=10, verbose=2, num_workers=0, callbacks=None)</span></span><br><span class="line"><span class="built_in">print</span>(eval_result)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The loss value printed in the log is the current step, and the metric is the average value of previous steps.</span></span><br><span class="line"><span class="string">Epoch 1/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.4857 - acc_top1: 0.5263 - acc_top2: 0.7206 - acc_top3: 0.8382 - 48ms/step         </span></span><br><span class="line"><span class="string">Epoch 2/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.5302 - acc_top1: 0.7777 - acc_top2: 0.9209 - acc_top3: 0.9783 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 3/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3104 - acc_top1: 0.8161 - acc_top2: 0.9399 - acc_top3: 0.9815 - 50ms/step         </span></span><br><span class="line"><span class="string">Epoch 4/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3551 - acc_top1: 0.8371 - acc_top2: 0.9505 - acc_top3: 0.9846 - 52ms/step         </span></span><br><span class="line"><span class="string">Epoch 5/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3623 - acc_top1: 0.8453 - acc_top2: 0.9541 - acc_top3: 0.9854 - 50ms/step         </span></span><br><span class="line"><span class="string">Epoch 6/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3932 - acc_top1: 0.8542 - acc_top2: 0.9579 - acc_top3: 0.9870 - 51ms/step         </span></span><br><span class="line"><span class="string">Epoch 7/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.4121 - acc_top1: 0.8600 - acc_top2: 0.9604 - acc_top3: 0.9876 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 8/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.2441 - acc_top1: 0.8646 - acc_top2: 0.9617 - acc_top3: 0.9882 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 9/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3528 - acc_top1: 0.8673 - acc_top2: 0.9632 - acc_top3: 0.9888 - 48ms/step         </span></span><br><span class="line"><span class="string">Epoch 10/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.2424 - acc_top1: 0.8722 - acc_top2: 0.9654 - acc_top3: 0.9891 - 50ms/step         </span></span><br><span class="line"><span class="string">Eval begin...</span></span><br><span class="line"><span class="string">step 40/40 [==============================] - loss: 0.1212 - acc_top1: 0.8572 - acc_top2: 0.9616 - acc_top3: 0.9878 - 48ms/step         </span></span><br><span class="line"><span class="string">Eval samples: 10000</span></span><br><span class="line"><span class="string">&#123;&#x27;loss&#x27;: [0.121200696], &#x27;acc_top1&#x27;: 0.8572, &#x27;acc_top2&#x27;: 0.9616, &#x27;acc_top3&#x27;: 0.9878&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">acc_top1：只拿一个的准确率</span></span><br><span class="line"><span class="string">acc_top2：拿两个的准确率</span></span><br><span class="line"><span class="string">acc_top3：拿三个的准确率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-小结">4.3 小结</h3>
<ul>
<li>L2正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li>
<li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li>
<li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li>
<li>暂退法仅在训练期间使用。</li>
<li>暂退法将一些输出项随机置0来控制模型复杂度。常用在隐藏层的输出上。</li>
<li>0.1,0.9,0.5是常用的丢弃法超参数的值。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space">小漁头&amp;小戴</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space/2022/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8B%EF%BC%89/">http://blog.dai2yutou.space/2022/12/26/深度学习3.2-模型选择与调优策略（下）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.dai2yutou.space" target="_blank">小漁头|小戴</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/paddle/">paddle</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5/">深度学习基础_模型选择与调优策略</a></div><div class="post_share"><div class="social-share" data-image="https://picbed.dai2yutou.space/web_img/19.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8A%EF%BC%89/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习3.1-模型选择与调优策略（上）</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.1-%E4%BD%BF%E7%94%A8%E9%A3%9E%E6%A1%A8%E5%AE%9E%E7%8E%B0%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习4.1-使用飞桨实现房价预测任务</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8A%EF%BC%89/" title="深度学习3.1-模型选择与调优策略（上）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-25</div><div class="title">深度学习3.1-模型选择与调优策略（上）</div></div></a></div><div><a href="/2022/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" title="深度学习1.1-深度学习概论"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-18</div><div class="title">深度学习1.1-深度学习概论</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="深度学习2.1-线性回归模型的实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.1-线性回归模型的实现</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/" title="深度学习2.2-神经网络中的分类任务"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.2-神经网络中的分类任务</div></div></a></div><div><a href="/2022/12/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="深度学习2.3-多层感知机的搭建与实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-23</div><div class="title">深度学习2.3-多层感知机的搭建与实现</div></div></a></div><div><a href="/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.1-%E4%BD%BF%E7%94%A8%E9%A3%9E%E6%A1%A8%E5%AE%9E%E7%8E%B0%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1/" title="深度学习4.1-使用飞桨实现房价预测任务"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-30</div><div class="title">深度学习4.1-使用飞桨实现房价预测任务</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="animate__fadeIn card-info card-widget wow" data-wow-delay="0" data-wow-duration="" data-wow-iteration="" data-wow-offset="" style="visibility: visible; animation-name: fadeIn;"><div class="author-info-top"><div class="card-info-avatar"><a class="avatar-img" data-pjax-state="" href="/about"><img class="entered loaded" alt="avatar" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apple-touch-icon.jpg" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;"/></a><div class="author-status-box"><div class="author-status"><g-emoji class="g-emoji" alias="palm_tree" fallback-src="/img/tree_icon.png">🐟</g-emoji><span>摸鱼中~</span></div></div></div></div><div class="author-info__sayhi" id="author-info__sayhi">晚安😴！我是</div><h1 class="author-info__name">XiaoYutou|XiaoDai</h1><div class="author-info__description">热爱生活点滴，分享时刻精彩。</div><a id="card-info-btn" data-pjax-state="" onclick="pjax.loadUrl(/about/)"><i></i><span style="padding-left:32px;font-weight:600;font-size:large">了解更多<i class="faa-passing animated" style="padding-left:-2px;display:inline-block;vertical-align:middle;"><span style="height:28px;width:28px;fill:currentColor;position:relative;top:-1.5px">💨</span></i></span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xiaoyutoua" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2143191301@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>主域名:<a target="_blank" rel="noopener" href="https://www.dai2yutou.space">小漁头|小戴</a><br><span>技术问题欢迎交流🧐</span><span color="#3eb8be">VX:yuguolong_001</span></center></div><div id="welcome-info"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">调优策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ppl-py%E6%96%87%E4%BB%B6"><span class="toc-text">ppl.py文件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%A7%A3%E5%86%B3%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">一、解决欠拟合与过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">二、数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-text">2.1 梯度消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-text">2.2 梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%90%88%E7%90%86%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">2.3 合理的权重初始化的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">2.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%99%8D%E4%BD%8E%E5%81%8F%E5%B7%AE%E2%80%93%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">三、降低偏差–优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E8%AE%BE%E7%BD%AE%E5%AD%A6%E4%B9%A0%E7%8E%87%E3%80%91"><span class="toc-text">【设置学习率】</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.1 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.1.1 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89%E4%B8%80%E7%BB%B4%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">1）一维梯度下降</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%B8%8A%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%8C%E5%8F%91%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%B6%8A%E6%9D%A5%E8%B6%8A%E6%85%A2%EF%BC%8C%E5%8E%9F%E5%9B%A0%E6%98%AF%E6%96%9C%E7%8E%87%E8%B6%8A%E6%9D%A5%E8%B6%8A%E5%B0%8F%EF%BC%8C%E6%A2%AF%E5%BA%A6-%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E5%80%BC%E4%B9%9F%E5%B0%B1%E8%B6%8A%E6%9D%A5%E8%B6%8A%E5%B0%8F%EF%BC%8C%E6%9B%B4%E6%96%B0%E7%9A%84%E8%B6%8A%E6%9D%A5%E8%B6%8A%E6%85%A2%E3%80%82"><span class="toc-text">如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，梯度*学习率的值也就越来越小，更新的越来越慢。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%82%E4%B8%AD%EF%BC%8C10%E8%BD%AE%E5%90%8E%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84x%E5%80%BC%E4%B8%BA0-06%EF%BC%8C%E5%B7%B2%E7%BB%8F%E9%9D%9E%E5%B8%B8%E6%8E%A5%E8%BF%91%E6%9C%80%E4%BC%98%E8%A7%A3%E4%BA%86"><span class="toc-text">学习率适中，10轮后，得到的x值为0.06，已经非常接近最优解了</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%B8%8A%E5%9B%BE%E6%89%80%E7%A4%BA-%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%A4%AA%E5%B0%8F%EF%BC%8C%E6%9B%B4%E6%96%B0%E7%9A%84%E5%B9%85%E5%BA%A6%E4%B9%9F%E5%B0%B1%E5%8F%98%E5%B0%8F%EF%BC%8C%E7%BB%8F%E8%BF%87%E8%BD%AE%E8%BF%87%E5%90%8E%EF%BC%8Cx%E5%80%BC%E4%B8%BA3-48%EF%BC%8C%E4%B8%8E%E6%9C%80%E4%BC%98%E8%A7%A3%E7%9B%B8%E5%B7%AE%E8%BE%83%E8%BF%9C%E3%80%82"><span class="toc-text">如上图所示,学习率太小，更新的幅度也就变小，经过轮过后，x值为3.48，与最优解相差较远。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%B8%8A%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E9%AB%98%EF%BC%8C%E5%9C%A8%E6%9C%80%E4%BC%98%E8%A7%A3%E9%99%84%E8%BF%91%E6%8C%AF%E8%8D%A1%EF%BC%8C%E8%BE%BE%E4%B8%8D%E5%88%B0%E6%9C%80%E4%BC%98%E8%A7%A3%E3%80%82"><span class="toc-text">如上图所示，学习率过高，在最优解附近振荡，达不到最优解。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC"><span class="toc-text">2）局部最小值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%B8%8A%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E9%AB%98%EF%BC%8C%E5%AF%BC%E8%87%B4%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%B7%B3%E8%BF%87%E4%BA%86%E6%9C%80%E4%BC%98%E8%A7%A3%EF%BC%8C%E7%84%B6%E5%90%8E%E7%BB%A7%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%8C%E6%9C%80%E7%BB%88%E6%89%BE%E5%88%B0%E7%9A%84%E6%98%AF%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E3%80%82"><span class="toc-text">如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3）多元梯度下降</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.1.2 随机梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.1.3 小批量随机梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-4-%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">3.1.4 实现梯度下降法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-5-%E5%B0%8F%E7%BB%93"><span class="toc-text">3.1.5 小结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-text">3.2 动量法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E8%B4%A8%E7%96%91"><span class="toc-text">3.2.1 质疑</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E5%8A%A8%E9%87%8F%E6%B3%95%E5%9C%B0%E6%8F%90%E5%87%BA"><span class="toc-text">3.2.2 动量法地提出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-%E5%AE%9E%E7%8E%B0%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-text">3.2.3 实现动量法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">3.2.4 小结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-AdaGrad"><span class="toc-text">3.3 AdaGrad</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89%E7%A8%80%E7%96%8F%E7%89%B9%E5%BE%81%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">1）稀疏特征和学习率</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89AdaGrad%E7%AE%97%E6%B3%95"><span class="toc-text">2）AdaGrad算法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89%E5%AE%9E%E7%8E%B0"><span class="toc-text">3）实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-RMSProp"><span class="toc-text">3.4 RMSProp</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89RMSProp%E7%AE%97%E6%B3%95"><span class="toc-text">1）RMSProp算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89%E5%AE%9E%E7%8E%B0"><span class="toc-text">2）实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-Adadelta"><span class="toc-text">3.5 Adadelta</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89Adadelta%E7%AE%97%E6%B3%95"><span class="toc-text">1）Adadelta算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89%E5%AE%9E%E7%8E%B0-v2"><span class="toc-text">2）实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-Adam"><span class="toc-text">3.6 Adam</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89Adam%E7%AE%97%E6%B3%95"><span class="toc-text">1）Adam算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89%E5%AE%9E%E7%8E%B0-v3"><span class="toc-text">2）实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E2%80%93%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-text">四、提高模型泛化能力–正则化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-L2%E6%AD%A3%E5%88%99%E4%B8%8EL1%E6%AD%A3%E5%88%99"><span class="toc-text">4.1 L2正则与L1正则</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99"><span class="toc-text">L2正则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99"><span class="toc-text">L1正则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">1）从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E3%80%90%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E3%80%91"><span class="toc-text">【初始化模型参数】</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E3%80%90%E5%AE%9A%E4%B9%89-L-2-%E8%8C%83%E6%95%B0%E6%83%A9%E7%BD%9A%E3%80%91"><span class="toc-text">【定义$L_2$范数惩罚】</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E3%80%90%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E3%80%91"><span class="toc-text">【定义训练代码实现】</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2）简洁实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%9A%82%E9%80%80%E6%B3%95"><span class="toc-text">4.2 暂退法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%B0%8F%E7%BB%93"><span class="toc-text">4.3 小结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/04/25/Python%E4%BC%A0%E5%8F%82%E6%96%B9%E5%BC%8F%EF%BC%9A%E5%8F%AF%E5%8F%98-%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/" title="Python传参方式：可变/不可变对象"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/Python/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python传参方式：可变/不可变对象"/></a><div class="content"><a class="title" href="/2023/04/25/Python%E4%BC%A0%E5%8F%82%E6%96%B9%E5%BC%8F%EF%BC%9A%E5%8F%AF%E5%8F%98-%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/" title="Python传参方式：可变/不可变对象">Python传参方式：可变/不可变对象</a><time datetime="2023-04-25T05:00:23.000Z" title="发表于 2023-04-25 13:00:23">2023-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/24/python%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8D%E4%BC%9A%E7%9A%84%E7%9F%A5%E8%AF%86/" title="python学习中遇到的基础不会的知识"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/Python/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python学习中遇到的基础不会的知识"/></a><div class="content"><a class="title" href="/2023/04/24/python%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8D%E4%BC%9A%E7%9A%84%E7%9F%A5%E8%AF%86/" title="python学习中遇到的基础不会的知识">python学习中遇到的基础不会的知识</a><time datetime="2023-04-24T15:53:05.000Z" title="发表于 2023-04-24 23:53:05">2023-04-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/12/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" title="数学建模图像处理"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/数学建模/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数学建模图像处理"/></a><div class="content"><a class="title" href="/2023/04/12/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" title="数学建模图像处理">数学建模图像处理</a><time datetime="2023-04-12T15:19:07.000Z" title="发表于 2023-04-12 23:19:07">2023-04-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 小漁头&小戴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.6/translate/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer src="/js/light.js"></script><canvas id="universe"></canvas><script defer src="/js/starry_sky.js"></script><script defer src="/js/console.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script async data-pjax src="/js/card_author.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JzK9w99AgP1g6fso",ck:"JzK9w99AgP1g6fso"})</script><script type="text/javascript" src ="/js/reward.js" ></script><script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.6.16/dist/sweetalert2.all.min.js"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="/js/txmap.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = 'b16a1fa0e63c46a4b8f28abfb06ae3fe';
  var gaud_map_key = 'e2b04289e870b005374ee030148d64fd&s=rsv3';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/4.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-02</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">英文水平不高，咋翻译论文？</a><div class="blog-slider__text">英文水平不高，咋翻译论文？</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/web_background2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-17</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">🐌博客搭建学习笔记</a><div class="blog-slider__text">这是再搭建博客已经写文章时遇到的bug和对博客的一些必要操作，不定时更新哦~</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/9.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">Butterfly外挂标签</a><div class="blog-slider__text">本文是撰写博客文章时可能会用到的外挂标签汇总，放到一起，便于查阅和使用</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/07/停车场管理模拟系统/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-07</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/07/停车场管理模拟系统/&quot;);" href="javascript:void(0);" alt="">停车场管理模拟系统</a><div class="blog-slider__text">本文是大二下学期程序设计与数据结构实训课设，模拟的是一个停车场管理系统，用C和C++语言编写，在此记录一下~</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/07/停车场管理模拟系统/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-09</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">第一篇文章</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">Hexo发生error：spawn failed错误的解决方法</a><div class="blog-slider__text">Hexo发生error：spawn failed错误的解决方法</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">Hexo博客备份与恢复</a><div class="blog-slider__text">本文旨在解决在不同电脑上都能维护博客或配置、发布的内容丢失可恢复的问题。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-24</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">Echarts社区地址</a><div class="blog-slider__text">一些Echarts图标的开源网站。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '2');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__bounceInRight');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/about/'|| '/about/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.dai2yutou.space/api?xiaoyutoua",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xiaoyutoua')
    }
  </script><!-- hexo injector body_end end --><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>