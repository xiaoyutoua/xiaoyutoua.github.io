<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python基本输出方法总结</title>
      <link href="/2023/05/09/python%E5%9F%BA%E6%9C%AC%E8%BE%93%E5%87%BA%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>/2023/05/09/python%E5%9F%BA%E6%9C%AC%E8%BE%93%E5%87%BA%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/16.jpg" style="border-radius:10px" width="1000px"></img></p><div class="tag link"><a class="link-card" title="Python官方教程" href="https://docs.python.org/zh-cn/3/tutorial/index.html"><div class="left"><img src="https://docs.python.org/zh-cn/3/_static/py.svg"/></div><div class="right"><p class="text">Python官方教程</p><p class="url">https://docs.python.org/zh-cn/3/tutorial/index.html</p></div></a></div><h2 id="一、print-函数基本输出"><a href="#一、print-函数基本输出" class="headerlink" title="一、print()函数基本输出"></a>一、print()函数基本输出</h2><h3 id="1-1-print-函数基本用法"><a href="#1-1-print-函数基本用法" class="headerlink" title="1.1 print()函数基本用法"></a>1.1 print()函数基本用法</h3><p>语法格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*objects, sep=<span class="string">&#x27; &#x27;</span>, end=<span class="string">&#x27;\n&#x27;</span>, file=sys.stdout)</span><br></pre></td></tr></table></figure><p>参数含义：</p><p>objects —表示输出的对象。输出多个对象时，需要用 , （逗号）分隔，如果直接输入多个字符串则可以不需要用逗号连接。</p><p>sep — 用来间隔多个对象。</p><p>end — 用来设定以什么结尾。默认值是换行符 \n，我们可以换成其他字符。</p><p>file — 要写入的文件对象。</p><h3 id="1-2-print-函数格式化输出"><a href="#1-2-print-函数格式化输出" class="headerlink" title="1.2 print()函数格式化输出"></a>1.2 print()函数格式化输出</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%[-][+][0][m][n]%</span><br><span class="line"># 具体含义见下图</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/Python/4.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/Python/5.png" alt="2"></p><p>如果需要在字符串中通过格式化字符输出多个值，则将每个对应值存放在一对圆括号()中，值与值之间使用英文逗号隔开。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">&quot;小渔头&quot;</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;%s已经%d了&#x27;</span> % (a, b))   <span class="comment"># 小渔头已经20了</span></span><br></pre></td></tr></table></figure><h2 id="二、格式化字符串字面值【f-字符串】"><a href="#二、格式化字符串字面值【f-字符串】" class="headerlink" title="二、格式化字符串字面值【f-字符串】"></a>二、格式化字符串字面值【f-字符串】</h2><p>在字符串前面加<code>f</code>或者<code>F</code>，通过<code>&#123;expression&#125;</code>表达式【包括变量和表达式】，把python表达式的值添加到字符串内。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">&quot;小渔头&quot;</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;我是<span class="subst">&#123;a&#125;</span>，已经<span class="subst">&#123;b&#125;</span>了&#x27;</span>)   <span class="comment"># 我是小渔头，已经20了</span></span><br></pre></td></tr></table></figure><p>f-字符串可以使用各种格式化选项，例如指定输出宽度、精度、对齐方式等等，写在表达式的后面，在<code>:</code>后传递参数。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pi = <span class="number">3.1415926</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The value of pi is approximately <span class="subst">&#123;pi:<span class="number">.2</span>f&#125;</span>.&#x27;</span>)<span class="comment"># The value of pi is approximately 3.14.</span></span><br></pre></td></tr></table></figure><p>还有一些修饰符可以在格式化前转换值。 <code>&#39;!a&#39;</code> 应用 <a href="https://docs.python.org/zh-cn/3/library/functions.html#ascii"><code>ascii()</code></a> ，<code>&#39;!s&#39;</code> 应用 <a href="https://docs.python.org/zh-cn/3/library/stdtypes.html#str"><code>str()</code></a>，<code>&#39;!r&#39;</code> 应用 <a href="https://docs.python.org/zh-cn/3/library/functions.html#repr"><code>repr()</code></a>：</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">&#x27;A&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;我是<span class="subst">&#123;a&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;我是<span class="subst">&#123;a!r&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在 f-字符串中使用 = 号说明符，可以将表达式展开为表达式的文本，等号，然后是求值表达式的表示形式。这个特性可以用于调试和理解代码中的表达式。这是一个新的特性，从 Python 3.8 开始引入。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">10</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line">c = <span class="string">f&quot;<span class="subst">&#123;a&#125;</span> + <span class="subst">&#123;b&#125;</span> = <span class="subst">&#123;a+b&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 输出：10 + 20 = 30</span></span><br><span class="line"></span><br><span class="line">d = <span class="string">f&quot;<span class="subst">&#123;a=&#125;</span> <span class="subst">&#123;b=&#125;</span> <span class="subst">&#123;a+b=&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(d)  <span class="comment"># 输出：a=10 b=20 a+b=30</span></span><br></pre></td></tr></table></figure><p>在上面的例子中，第一个 f-字符串中，我们使用了常规的字符串插值方式，将 a 和 b 的值插入到字符串中。在第二个 f-字符串中，我们使用了 = 号说明符，将表达式展开为表达式的文本，等号，然后是求值表达式的表示形式。这样，我们可以快速地查看表达式的值，并且可以方便地调试代码。</p><h2 id="三、字符串-format-方法"><a href="#三、字符串-format-方法" class="headerlink" title="三、字符串 format() 方法"></a>三、字符串 format() 方法</h2><p><code>str.format()</code></p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><ol><li>基本用法：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我是&#123;&#125;，今年&#123;&#125;了&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;小渔头&#x27;</span>,<span class="number">20</span>))    <span class="comment"># 我是小渔头，今年20了</span></span><br></pre></td></tr></table></figure><ol><li>根据位置填充</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我是&#123;0&#125;，今年&#123;1&#125;了&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;小渔头&#x27;</span>,<span class="number">20</span>))    <span class="comment"># 我是小渔头，今年20了</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我是&#123;1&#125;，今年&#123;0&#125;了&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;小渔头&#x27;</span>,<span class="number">20</span>))    <span class="comment"># 我是20，今年小渔头了</span></span><br></pre></td></tr></table></figure><ol><li>根据关键字填充</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我是&#123;name&#125;，今年&#123;age&#125;了&#x27;</span>.<span class="built_in">format</span>(name=<span class="string">&#x27;小渔头&#x27;</span>,age=<span class="number">20</span>))    <span class="comment"># 我是小渔头，今年20了</span></span><br></pre></td></tr></table></figure><ol><li><p>位置参数和关键字参数可以任意组合</p></li><li><p>根据字典填充</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dict</span>=&#123;‘obj’:’world’,’name’:’python’&#125; </span><br><span class="line"><span class="built_in">print</span>(‘hello &#123;names[obj]&#125; i am &#123;names[name]&#125;’.<span class="built_in">format</span>(names=<span class="built_in">dict</span>)) </span><br><span class="line"><span class="comment"># hello world i am python </span></span><br><span class="line"><span class="comment"># 注意访问字典的key，不用引号的</span></span><br></pre></td></tr></table></figure><p>如果不想分拆较长的格式字符串，最好按名称引用变量进行格式化，不要按位置。这项操作可以通过传递字典，并用方括号 <code>&#39;[]&#39;</code> 访问键来完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">table = &#123;<span class="string">&#x27;Sjoerd&#x27;</span>: <span class="number">4127</span>, <span class="string">&#x27;Jack&#x27;</span>: <span class="number">4098</span>, <span class="string">&#x27;Dcab&#x27;</span>: <span class="number">8637678</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Jack: &#123;0[Jack]:d&#125;; Sjoerd: &#123;0[Sjoerd]:d&#125;; &#x27;</span></span><br><span class="line">      <span class="string">&#x27;Dcab: &#123;0[Dcab]:d&#125;&#x27;</span>.<span class="built_in">format</span>(table))</span><br><span class="line"><span class="comment"># Jack: 4098; Sjoerd: 4127; Dcab: 8637678</span></span><br></pre></td></tr></table></figure><p>这也可以通过使用**符号将表字典作为关键字参数传递来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">table = &#123;<span class="string">&#x27;Sjoerd&#x27;</span>: <span class="number">4127</span>, <span class="string">&#x27;Jack&#x27;</span>: <span class="number">4098</span>, <span class="string">&#x27;Dcab&#x27;</span>: <span class="number">8637678</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Jack: &#123;Jack:d&#125;; Sjoerd: &#123;Sjoerd:d&#125;; Dcab: &#123;Dcab:d&#125;&#x27;</span>.<span class="built_in">format</span>(**table))</span><br><span class="line"><span class="comment"># Jack: 4098; Sjoerd: 4127; Dcab: 8637678</span></span><br></pre></td></tr></table></figure><p>“d”在这里表示将值格式化为十进制整数。在这个例子中，它告诉Python将每个键的值转换为十进制整数，并将它们插入到格式化字符串中的相应位置中。</p><ol><li>通过列表填充</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>=[<span class="string">&#x27;world&#x27;</span>,<span class="string">&#x27;python&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello &#123;names[0]&#125;  i am &#123;names[1]&#125;&#x27;</span>.<span class="built_in">format</span>(names=<span class="built_in">list</span>))</span><br><span class="line"><span class="comment"># 输出结果：hello world  i am python</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello &#123;0[0]&#125;  i am &#123;0[1]&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">list</span>)) </span><br><span class="line"><span class="comment"># 输出结果：hello world  i am python</span></span><br></pre></td></tr></table></figure><ol><li>通过类的属性填充</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Names</span>():</span><br><span class="line">    obj=<span class="string">&#x27;world&#x27;</span></span><br><span class="line">    name=<span class="string">&#x27;python&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello &#123;names.obj&#125; i am &#123;names.name&#125;&#x27;</span>.<span class="built_in">format</span>(names=Names))</span><br><span class="line"><span class="comment"># 输入结果hello world i am python</span></span><br></pre></td></tr></table></figure><h3 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h3><ol><li>填充和对齐字符串<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;:^10&#125;&quot;</span>.<span class="built_in">format</span>(text))</span><br><span class="line"><span class="comment">#   Hello   </span></span><br></pre></td></tr></table></figure></li><li>格式化数字<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">price = <span class="number">19.99</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The price is $&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(price))</span><br><span class="line"><span class="comment"># The price is $19.99</span></span><br></pre></td></tr></table></figure></li><li>格式化日期时间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">now = datetime.datetime.now()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Today is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(now.strftime(<span class="string">&quot;%Y-%m-%d&quot;</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The time is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(now.strftime(<span class="string">&quot;%H:%M:%S&quot;</span>)))</span><br><span class="line"><span class="comment"># Today is 2021-09-23</span></span><br><span class="line"><span class="comment"># The time is 16:05:23</span></span><br></pre></td></tr></table></figure></li></ol><p>其中，%Y表示年份，%m表示月份，%d表示日期，%H表示小时，%M表示分钟，%S表示秒数。strftime()方法将日期和时间对象转换为字符串。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python内置函数使用方法</title>
      <link href="/2023/05/08/python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>/2023/05/08/python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/Python/1.jpg" style="border-radius:10px" width="1000px"></img></p><div class="tag link"><a class="link-card" title="Python官方教程" href="https://docs.python.org/zh-cn/3/library/functions.html"><div class="left"><img src="https://docs.python.org/zh-cn/3/_static/py.svg"/></div><div class="right"><p class="text">Python官方教程</p><p class="url">https://docs.python.org/zh-cn/3/library/functions.html</p></div></a></div><h2 id="A"><a href="#A" class="headerlink" title="A"></a>A</h2><div class="tabs" id="a"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#a-1">abs()</button></li><li class="tab"><button type="button" data-href="#a-2">aiter()</button></li><li class="tab"><button type="button" data-href="#a-3">all()</button></li><li class="tab"><button type="button" data-href="#a-4">any()</button></li><li class="tab"><button type="button" data-href="#a-5">anext()</button></li><li class="tab"><button type="button" data-href="#a-6">ascii()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="a-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="a-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="a-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="a-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="a-5"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="a-6"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="B"><a href="#B" class="headerlink" title="B"></a>B</h2><div class="tabs" id="b"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#b-1">bin()</button></li><li class="tab"><button type="button" data-href="#b-2">bool()</button></li><li class="tab"><button type="button" data-href="#b-3">breakpoint()</button></li><li class="tab"><button type="button" data-href="#b-4">bytearray()</button></li><li class="tab"><button type="button" data-href="#b-5">bytes()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="b-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="b-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="b-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="b-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="b-5"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><div class="tabs" id="c"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#c-1">callable()</button></li><li class="tab"><button type="button" data-href="#c-2">chr()</button></li><li class="tab"><button type="button" data-href="#c-3">classmethod()</button></li><li class="tab"><button type="button" data-href="#c-4">compile()</button></li><li class="tab"><button type="button" data-href="#c-5">complex()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="c-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="c-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="c-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="c-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="c-5"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="D"><a href="#D" class="headerlink" title="D"></a>D</h2><div class="tabs" id="d"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#d-1">delattr()</button></li><li class="tab"><button type="button" data-href="#d-2">dict()</button></li><li class="tab"><button type="button" data-href="#d-3">dir()</button></li><li class="tab"><button type="button" data-href="#d-4">divmod()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="d-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="d-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="d-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="d-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="E"><a href="#E" class="headerlink" title="E"></a>E</h2><div class="tabs" id="e"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#e-1">enumerate()</button></li><li class="tab"><button type="button" data-href="#e-2">eval()</button></li><li class="tab"><button type="button" data-href="#e-3">exec()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="e-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="e-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="e-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="F"><a href="#F" class="headerlink" title="F"></a>F</h2><div class="tabs" id="f"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#f-1">filter()</button></li><li class="tab"><button type="button" data-href="#f-2">float()</button></li><li class="tab"><button type="button" data-href="#f-3">format()</button></li><li class="tab"><button type="button" data-href="#f-4">frozenset()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="f-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="f-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="f-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="f-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="G"><a href="#G" class="headerlink" title="G"></a>G</h2><div class="tabs" id="g"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#g-1">getattr()</button></li><li class="tab"><button type="button" data-href="#g-2">globals()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="g-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="g-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="H"><a href="#H" class="headerlink" title="H"></a>H</h2><div class="tabs" id="h"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#h-1">hasattr()</button></li><li class="tab"><button type="button" data-href="#h-2">hash()</button></li><li class="tab"><button type="button" data-href="#h-3">help()</button></li><li class="tab"><button type="button" data-href="#h-4">hex()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="h-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="h-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="h-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="h-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="I"><a href="#I" class="headerlink" title="I"></a>I</h2><div class="tabs" id="i"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#i-1">id()</button></li><li class="tab"><button type="button" data-href="#i-2">input()</button></li><li class="tab"><button type="button" data-href="#i-3">int()</button></li><li class="tab"><button type="button" data-href="#i-4">isinstance()</button></li><li class="tab"><button type="button" data-href="#i-5">issubclass()</button></li><li class="tab"><button type="button" data-href="#i-6">iter()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="i-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-5"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-6"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="L"><a href="#L" class="headerlink" title="L"></a>L</h2><div class="tabs" id="i"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#i-1">len()</button></li><li class="tab"><button type="button" data-href="#i-2">list()</button></li><li class="tab"><button type="button" data-href="#i-3">locals()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="i-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="i-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="M"><a href="#M" class="headerlink" title="M"></a>M</h2><div class="tabs" id="m"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#m-1">map()</button></li><li class="tab"><button type="button" data-href="#m-2">max()</button></li><li class="tab"><button type="button" data-href="#m-3">memoryview()</button></li><li class="tab"><button type="button" data-href="#m-4">min()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="m-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="m-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="m-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="m-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="N"><a href="#N" class="headerlink" title="N"></a>N</h2><div class="tabs" id="n"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#n-1">next()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="n-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="O"><a href="#O" class="headerlink" title="O"></a>O</h2><div class="tabs" id="o"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#o-1">object()</button></li><li class="tab"><button type="button" data-href="#o-2">oct()</button></li><li class="tab"><button type="button" data-href="#o-3">open()</button></li><li class="tab"><button type="button" data-href="#o-4">ord()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="o-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="o-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="o-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="o-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="P"><a href="#P" class="headerlink" title="P"></a>P</h2><div class="tabs" id="p"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#p-1">pow()</button></li><li class="tab"><button type="button" data-href="#p-2">print()</button></li><li class="tab"><button type="button" data-href="#p-3">property()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="p-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="p-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="p-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="R"><a href="#R" class="headerlink" title="R"></a>R</h2><div class="tabs" id="r"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#r-1">range()</button></li><li class="tab"><button type="button" data-href="#r-2">repr()</button></li><li class="tab"><button type="button" data-href="#r-3">reversed()</button></li><li class="tab"><button type="button" data-href="#r-4">round()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="r-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="r-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="r-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="r-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="S"><a href="#S" class="headerlink" title="S"></a>S</h2><div class="tabs" id="s"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#s-1">set()</button></li><li class="tab"><button type="button" data-href="#s-2">setattr()</button></li><li class="tab"><button type="button" data-href="#s-3">slice()</button></li><li class="tab"><button type="button" data-href="#s-4">sorted()</button></li><li class="tab"><button type="button" data-href="#s-5">staticmethod()</button></li><li class="tab"><button type="button" data-href="#s-6">str()</button></li><li class="tab"><button type="button" data-href="#s-7">sum()</button></li><li class="tab"><button type="button" data-href="#s-8">super()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="s-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-3"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-4"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-5"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-6"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-7"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="s-8"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="T"><a href="#T" class="headerlink" title="T"></a>T</h2><div class="tabs" id="t"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#t-1">tuple()</button></li><li class="tab"><button type="button" data-href="#t-2">type()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="t-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="t-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="Y"><a href="#Y" class="headerlink" title="Y"></a>Y</h2><div class="tabs" id="y"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#y-1">tuple()</button></li><li class="tab"><button type="button" data-href="#y-2">type()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="y-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="y-2"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="V"><a href="#V" class="headerlink" title="V"></a>V</h2><div class="tabs" id="v"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#v-1">vars()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="v-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h2><div class="tabs" id="z"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#z-1">zip()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="z-1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id=""><a href="#" class="headerlink" title="-"></a>-</h2><div class="tabs" id="-"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#--1">__import__()</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="--1"><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python传参方式：可变/不可变对象</title>
      <link href="/2023/04/25/Python%E4%BC%A0%E5%8F%82%E6%96%B9%E5%BC%8F%EF%BC%9A%E5%8F%AF%E5%8F%98-%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/"/>
      <url>/2023/04/25/Python%E4%BC%A0%E5%8F%82%E6%96%B9%E5%BC%8F%EF%BC%9A%E5%8F%AF%E5%8F%98-%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/Python/1.jpg" style="border-radius:10px" width="1000px"></img></p><p>在Python中，数字、字符串与元组是不可变类型，而列表、字典、集合是可变类型，两者区别如下：</p><ul><li><strong>不可变类型</strong>——该类型的对象所代表的值不能被改变。当改变某个变量时，由于其所指的值不能被改变，相当于把原来的值复制一份后再改变，这会开辟一个新的地址，变量再指向这个新的地址。（值传递）</li><li><strong>可变类型</strong>——该类型的对象所代表的值可以被改变。变量改变后，实际上是其所指的值直接发生改变，并没有发生复制行为，也没有开辟出新的地址。（地址传递）</li></ul><p>下面以一个例子说明</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>(<span class="params">alist</span>):</span><br><span class="line">    alist.append(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(alist)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(alist))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>(<span class="params">astr</span>):</span><br><span class="line">    astr += <span class="string">&#x27;.space&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(astr)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(astr))</span><br><span class="line"></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]    <span class="comment"># 可变对象</span></span><br><span class="line">str1 = <span class="string">&#x27;dai2yutou&#x27;</span>  <span class="comment"># 不可变对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加前</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;添加前&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(list1)    <span class="comment"># [1, 2, 3, 4]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(list1))    <span class="comment"># 2708903970752</span></span><br><span class="line"><span class="built_in">print</span>(str1)     <span class="comment"># dai2yutou</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(str1))     <span class="comment"># 2708906967920</span></span><br><span class="line"><span class="comment"># 添加后，函数中变量打印</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;添加后，函数中变量打印&quot;</span>)</span><br><span class="line">test1(list1)     <span class="comment"># [1, 2, 3, 4, 5]  2708903970752</span></span><br><span class="line">test2(str1)     <span class="comment"># dai2yutou.space   2708906977392</span></span><br><span class="line"><span class="comment"># 添加后，原变量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;添加后，原变量&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(list1)    <span class="comment"># [1, 2, 3, 4, 5]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(list1))    <span class="comment"># 2708903970752</span></span><br><span class="line"><span class="built_in">print</span>(str1)     <span class="comment"># dai2yutou</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(str1))     <span class="comment"># 2708906967920</span></span><br></pre></td></tr></table></figure><p>如上所示，test1函数传入的参数是列表，列表是可变序列,因此传入的是列表的地址值，对列表进行append，则传入的与之前的都会发生改变。而test2函数传入的参数是字符串，字符串是不可变序列，因此传入的是变量的值，修改后，之前的字符串不会发生变化，而是创建了一个新的字符串。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习中遇到的基础不会的知识</title>
      <link href="/2023/04/24/python%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8D%E4%BC%9A%E7%9A%84%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/04/24/python%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8D%E4%BC%9A%E7%9A%84%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/Python/1.jpg" style="border-radius:10px" width="1000px"></img></p><h1 id="一、杂"><a href="#一、杂" class="headerlink" title="一、杂"></a>一、杂</h1><p>1.注意：十进制转换进制后，就成为str类型了。<br>2.int()函数默认处理的是十进制，当处理其他进制时，需要添加条件</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">&#x27;0b1010&#x27;</span><span class="comment">#二进制</span></span><br><span class="line">b = <span class="built_in">int</span>(a,base=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(b)    <span class="comment"># 10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))  <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>3.<strong>关于round()函数</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a=<span class="built_in">round</span>(<span class="number">0.375</span>,<span class="number">2</span>)<span class="comment"># 四舍六入五留双</span></span><br><span class="line"><span class="built_in">print</span>(a)<span class="comment"># 0.38</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">“四”是指≤4时舍去，</span></span><br><span class="line"><span class="string">&quot;六&quot;是指≥6时进上。</span></span><br><span class="line"><span class="string">&quot;五&quot;指的是根据5后面的数字来定，当5后有数时，舍5入1；</span></span><br><span class="line"><span class="string">当5后无有效数字时，需要分两种情况来讲：5前为奇数，舍5入1；5前为偶数，舍5不进（0是偶数）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>4.编码与解码</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encode()</span><br><span class="line">decode()</span><br></pre></td></tr></table></figure><p>5.<strong>注意一些字符串的处理方法</strong></p><pre><code>- ###### strip()，join()</code></pre><hr><p>6.<strong>chr(n)</strong></p><p>  chr(n) 是 Python 内置函数之一，用于将 Unicode 码位值 n 转换为对应的字符。<br>  它返回一个字符串，表示 Unicode 码位值 n 所对应的单个字符。<br>  例如，chr(65) 返回字符 ‘A’，chr(8364) 返回字符 ‘€’。<br>7.<strong>sorted() 函数</strong></p><p>  <a href="http://c.biancheng.net/view/2239.html">Python sorted函数及用法 (biancheng.net)</a></p><p>  <a href="https://www.runoob.com/python/python-func-sorted.html">Python sorted() 函数</a></p><h1 id="二、关于文件处理："><a href="#二、关于文件处理：" class="headerlink" title="二、关于文件处理："></a>二、关于文件处理：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os <span class="comment"># Python的系统编程的操作模块,可以处理文件和目录</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">with语法是一种上下文的管理协议，用于简化try…except…finally的处理流程。</span></span><br><span class="line"><span class="string">with通过__enter__方法初始化</span></span><br><span class="line"><span class="string">然后在__exit__中做善后以及处理异常</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;##方法一##&#x27;</span></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(f.read())</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;##方法二##&#x27;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;fail to open&#x27;</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    f.close()</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;##方法三##&#x27;</span></span><br><span class="line"><span class="comment">#这里with自动帮我们执行了退出的操作，不需要额外进行后续的处理</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#注意读也可以读一行中的部分内容。或者哪几行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一行，某一个长度</span></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(f.readline()[<span class="number">2</span>:<span class="number">7</span>])</span><br><span class="line"><span class="comment"># 所有行，某一行</span></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(f.readlines()[<span class="number">1</span>:<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;work/诗歌.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(f.readlines())</span><br><span class="line"><span class="built_in">print</span>(f.read())</span><br><span class="line"><span class="built_in">print</span>(f.readline())</span><br><span class="line"><span class="comment">#[&#x27;床前明月光，疑是地上霜。&#x27;]</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(f.tell()) <span class="comment">#指针的位置</span></span><br><span class="line"><span class="comment">#字节的位置</span></span><br><span class="line"></span><br><span class="line">f.seek()<span class="comment">#都是字节量的移动</span></span><br></pre></td></tr></table></figure><h1 id="三、关于函数部分"><a href="#三、关于函数部分" class="headerlink" title="三、关于函数部分"></a>三、关于函数部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># *的扩展-统计得分-获取多个数值</span></span><br><span class="line"></span><br><span class="line">first, *mean, last = [<span class="number">100</span>, <span class="number">95</span>, <span class="number">75</span>, <span class="number">65</span>, <span class="number">60</span>]</span><br><span class="line"><span class="built_in">print</span>(first,mean,last)<span class="comment">#100 [95, 75, 65] 60</span></span><br><span class="line"><span class="built_in">print</span>(*mean)<span class="comment">#95 75 65</span></span><br><span class="line"><span class="built_in">print</span>(mean)<span class="comment">#[95, 75, 65]</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*mean, = <span class="number">95</span>, <span class="number">75</span></span><br><span class="line"><span class="comment">#进行多个值赋值的时候，前面的变量一定要变成元组</span></span><br><span class="line"><span class="comment">#*mean = 95, 75 # 错误用法</span></span><br><span class="line"><span class="built_in">print</span>(mean)     <span class="comment">#[95, 75]</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">param</span>(<span class="params">a, *args</span>):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;a =&quot;</span>, a)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;args =&quot;</span>, args)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">for</span> arg <span class="keyword">in</span> args:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;arg =&quot;</span>, arg)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">param(<span class="string">&quot;say:&quot;</span>,<span class="string">&quot;It&#x27;s&quot;</span>,<span class="string">&quot;a&quot;</span>,<span class="string">&quot;nice&quot;</span>,<span class="string">&quot;day&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">a = say:</span></span><br><span class="line"><span class="string">args = (&quot;It&#x27;s&quot;, &#x27;a&#x27;, &#x27;nice&#x27;, &#x27;day&#x27;)</span></span><br><span class="line"><span class="string">arg = It&#x27;s</span></span><br><span class="line"><span class="string">arg = a</span></span><br><span class="line"><span class="string">arg = nice</span></span><br><span class="line"><span class="string">arg = day</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">l = [(<span class="string">&quot;第一&quot;</span>, <span class="number">100</span>, <span class="string">&quot;很好&quot;</span>), (<span class="string">&quot;第二&quot;</span>, <span class="number">90</span>, <span class="string">&quot;好&quot;</span>), (<span class="string">&quot;第三&quot;</span>, <span class="number">80</span>, <span class="string">&quot;一般&quot;</span>), (<span class="string">&quot;第四&quot;</span>, <span class="number">60</span>, <span class="string">&quot;差&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> d, *p <span class="keyword">in</span> l:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;名次是<span class="subst">&#123;d&#125;</span>，分数是<span class="subst">&#123;p&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">名次是第一，分数是[100, &#x27;很好&#x27;]</span></span><br><span class="line"><span class="string">名次是第二，分数是[90, &#x27;好&#x27;]</span></span><br><span class="line"><span class="string">名次是第三，分数是[80, &#x27;一般&#x27;]</span></span><br><span class="line"><span class="string">名次是第四，分数是[60, &#x27;差&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="四、小练习题"><a href="#四、小练习题" class="headerlink" title="四、小练习题"></a>四、小练习题</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">demp</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">return</span> n*n</span><br><span class="line">lst = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: demp(x),lst)))     <span class="comment"># [1, 4, 9, 16, 25]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">x = [[random.randint(<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> x:</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;___________________&quot;</span>)</span><br><span class="line">y = <span class="built_in">sorted</span>(x,key=<span class="keyword">lambda</span> item:(item[<span class="number">1</span>],item[<span class="number">4</span>]))</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> y:</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[10, 1, 9, 1, 5]</span></span><br><span class="line"><span class="string">[3, 2, 3, 8, 5]</span></span><br><span class="line"><span class="string">[7, 4, 6, 7, 6]</span></span><br><span class="line"><span class="string">[7, 9, 9, 7, 8]</span></span><br><span class="line"><span class="string">[1, 9, 7, 8, 5]</span></span><br><span class="line"><span class="string">___________________</span></span><br><span class="line"><span class="string">[10, 1, 9, 1, 5]</span></span><br><span class="line"><span class="string">[3, 2, 3, 8, 5]</span></span><br><span class="line"><span class="string">[7, 4, 6, 7, 6]</span></span><br><span class="line"><span class="string">[1, 9, 7, 8, 5]</span></span><br><span class="line"><span class="string">[7, 9, 9, 7, 8]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学建模图像处理</title>
      <link href="/2023/04/12/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
      <url>/2023/04/12/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/23.jpg" alt="1"></p><h2 id="图像形态学运算"><a href="#图像形态学运算" class="headerlink" title="图像形态学运算"></a>图像形态学运算</h2><p>图像形态学运算是一种基于图像形态学理论的图像处理方法，用于对图像进行形态学操作，如膨胀、腐蚀、开运算、闭运算等。这些操作可以用于去除图像中的噪声、填补图像中的空洞、分离图像中的物体等应用。以下是常用的图像形态学运算：</p><ol><li>膨胀（Dilation）：膨胀操作可以将图像中的物体边界向外扩张，使物体变大，同时也可以填补物体内部的空洞。</li><li>腐蚀（Erosion）：腐蚀操作可以将图像中的物体边界向内收缩，使物体变小，同时也可以去除物体边界上的噪声。</li><li>开运算（Opening）：开运算是先进行腐蚀操作，再进行膨胀操作，用于去除图像中的小噪声和细小的物体。</li><li>闭运算（Closing）：闭运算是先进行膨胀操作，再进行腐蚀操作，用于填补图像中的小空洞和连接细小的物体。</li><li>梯度运算（Gradient）：梯度运算是对图像进行膨胀和腐蚀操作后得到的差值图像，用于分离物体边界。</li><li>顶帽运算（Top Hat）：顶帽运算是原始图像与开运算之间的差值图像，用于分离细小的物体。</li><li>底帽运算（Bottom Hat）：底帽运算是闭运算与原始图像之间的差值图像，用于分离大的物体。</li></ol><h2 id="imread"><a href="#imread" class="headerlink" title="imread()"></a>imread()</h2><p>imread函数是matlab中用于读取图像文件的函数。它的语法如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I = imread(filename)</span><br></pre></td></tr></table></figure><p>其中，filename是要读取的图像文件名，可以是绝对路径或相对路径。I是读取出的图像矩阵，它的类型和大小取决于原始图像文件的格式和颜色深度。</p><p>imread函数支持读取多种图像格式，包括bmp、jpg、png、tif等。如果读取的图像文件格式不受支持，则会返回一个错误。</p><p>除了读取图像文件外，imread函数还支持读取网络上的图像文件，只需要将filename指定为网络上的URL地址即可。</p><h2 id="imshow"><a href="#imshow" class="headerlink" title="imshow()"></a>imshow()</h2><p>imshow函数是matlab中用于显示图像的函数。它的语法如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">imshow(I)</span><br></pre></td></tr></table></figure><p>其中，I是要显示的图像矩阵。imshow函数会自动根据图像矩阵的类型和大小来确定显示的方式和颜色深度。</p><p>除了显示图像矩阵外，imshow函数还支持显示灰度图像、RGB图像、二值图像和彩色图像。可以使用不同的参数来指定显示的方式，例如：</p><ul><li><p>imshow(I,[]) % 显示灰度图像</p></li><li><p>imshow(I,[low high]) % 显示灰度图像，并指定灰度级的范围</p></li><li><p>imshow(RGB) % 显示RGB图像</p></li><li><p>imshow(BW) % 显示二值图像</p></li><li><p>imshow(X,map) % 显示索引图像</p></li><li><p>imshow(X,R) % 显示彩色图像，并指定颜色空间</p></li></ul><p>此外，imshow函数还支持一些交互式操作，例如放大、缩小、平移和旋转等。可以使用鼠标或键盘来进行操作。</p><h2 id="rgb2gray"><a href="#rgb2gray" class="headerlink" title="rgb2gray()"></a>rgb2gray()</h2><p><strong>原理：</strong></p><p>rgb2gray是一种将RGB图像转换为灰度图像的函数。其作用原理是将RGB图像中的三个颜色通道（红色、绿色和蓝色）进行加权平均，得到一个灰度值，用于表示该像素的亮度。</p><p>具体地，rgb2gray的计算公式为：</p><p>gray = 0.2989 <em> R + 0.5870 </em> G + 0.1140 * B</p><p>其中，R、G、B分别为该像素在RGB图像中的红、绿、蓝通道值，gray为计算得到的灰度值。这个公式中的权重值是经过实验得出的，其目的是使得计算出的灰度值更符合人类视觉感知的亮度。</p><p>通过rgb2gray函数将RGB图像转换为灰度图像后，每个像素只需要一个灰度值来表示，从而减少了存储空间，方便了图像处理和分析。</p><p><strong>使用方法</strong>：</p><p>在Matlab中，rgb2gray函数也是将RGB图像转换为灰度图像的函数，其使用方法和解释如下：</p><ol><li>导入需要处理的图像文件：</li></ol><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = imread(<span class="string">&#x27;example.jpg&#x27;</span>);</span><br></pre></td></tr></table></figure><ol><li>调用rgb2gray函数进行转换：</li></ol><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gray_img = rgb2gray(img);</span><br></pre></td></tr></table></figure><p>其中，img为原始的RGB图像，gray_img为转换后的灰度图像。在Matlab中，rgb2gray函数直接将RGB图像转换为灰度图像，无需传入颜色空间参数。</p><ol><li>可以使用imshow函数将原始图像和转换后的灰度图像显示出来：</li></ol><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">imshow(img);</span><br><span class="line">title(<span class="string">&#x27;Original Image&#x27;</span>);</span><br><span class="line"><span class="built_in">figure</span>;</span><br><span class="line">imshow(gray_img);</span><br><span class="line">title(<span class="string">&#x27;Gray Image&#x27;</span>);</span><br></pre></td></tr></table></figure><p>其中，imshow函数用于显示图像，title函数用于设置图像标题，figure函数用于创建新的图像窗口。</p><p>需要注意的是，在Matlab中，图像的颜色通道顺序是RGB，而不是OpenCV中的BGR。因此，在Matlab中读取的图像是RGB格式的，而不需要进行颜色通道的转换。</p><p>总的来说，Matlab中的rgb2gray函数使用方法简单，直接将RGB图像转换为灰度图像，适合处理小规模的图像数据。</p><p>也可以使用如下方法，将颜色映射转换为灰度：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[X,map] = imread(<span class="string">&#x27;corn.tif&#x27;</span>); </span><br><span class="line">imshow(X,map);</span><br><span class="line">newmap = rgb2gray(map);</span><br><span class="line">imshow(X,newmap)</span><br></pre></td></tr></table></figure><p>使用imread函数读取名为”corn.tif”的图像文件，并将生成的图像矩阵分配给变量X。它还读取名为”map”的颜色映射，并将其分配给变量map。然后，使用imshow函数使用颜色映射map显示由X表示的图像。</p><p>接下来，使用rgb2gray函数将颜色映射map转换为灰度，并将生成的颜色映射分配给变量newmap。最后，再次使用imshow函数使用新的灰度颜色映射newmap显示由X表示的图像。</p><h2 id="imresize"><a href="#imresize" class="headerlink" title="imresize()"></a>imresize()</h2><p>imresize是MATLAB的一个图像处理函数，用于调整图像的大小。它可以将图像缩小或放大到指定的尺寸，并支持多种插值方法。</p><p>imresize的使用方法如下：</p><p>imresize(A, scale)：将图像A按照比例scale进行缩放，scale可以是一个标量，表示缩放的比例；也可以是一个二元组(x_scale, y_scale)，表示在x和y方向上的缩放比例。</p><p>imresize(A, [m, n])：将图像A缩放到指定的大小m×n。</p><p>imresize(A, [m, n], method)：指定插值方法，其中method可以是以下方法之一：</p><ul><li>nearest：最近邻插值</li><li>bilinear：双线性插值</li><li>bicubic：双三次插值</li></ul><p>例如，下面的代码将读取一张图像，将其缩放到一半的大小，并使用双线性插值方法：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = imread(<span class="string">&#x27;example.jpg&#x27;</span>);</span><br><span class="line">scaled_img = imresize(img, <span class="number">0.5</span>, <span class="string">&#x27;bilinear&#x27;</span>);</span><br><span class="line">imshow(scaled_img);</span><br></pre></td></tr></table></figure><p><strong>关于插值方法：</strong></p><p>在图像缩放过程中，需要对原始图像中不存在的像素进行估计，这个过程称为插值。插值方法的作用是通过对已知像素的计算来推测未知像素的值，从而实现图像的缩放、旋转、变形等操作。</p><p>常见的插值方法有：</p><p>1.最近邻插值（nearest neighbor interpolation）：对于目标像素，选择距离最近的原始像素的值作为其值。</p><p>2.双线性插值（bilinear interpolation）：对于目标像素，利用其周围四个最近的原始像素的值进行加权平均。</p><p>3.三次样条插值（bicubic interpolation）：对于目标像素，利用其周围16个最近的原始像素的值进行加权平均。</p><p>不同的插值方法会对图像的质量产生不同的影响，选择合适的插值方法可以使得图像缩放后的质量更好。但同时也需要注意，使用更复杂的插值方法会增加计算的复杂度，导致处理时间变长。</p><h2 id="princomp"><a href="#princomp" class="headerlink" title="princomp()"></a>princomp()</h2><p>princomp()函数是matlab软件中的一个函数，用于进行主成分分析（PCA）。主成分分析是一种常用的数据降维方法，可以将高维数据转换为低维数据，同时保留数据的主要信息。</p><p>princomp()函数的语法如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[coeff,score,latent,tsquared,explained,mu] = princomp(X)</span><br><span class="line"><span class="comment">% 简单使用</span></span><br><span class="line">coeff = princomp(X)</span><br></pre></td></tr></table></figure><p>其中，X是一个m×n的矩阵，表示有m个样本，每个样本有n个特征。</p><p>函数的输出包括：</p><ul><li>coeff：主成分系数矩阵，表示将原始数据转换为主成分的系数。</li><li>score：主成分分数矩阵，表示每个样本在主成分上的投影值。</li><li>latent：主成分方差向量，表示每个主成分的方差大小。</li><li>tsquared：T2统计量向量，表示每个样本的T2统计量值。</li><li>explained：解释方差向量，表示每个主成分解释的方差所占比例。</li><li>mu：样本均值向量，表示每个特征的平均值。</li></ul><p>需要注意的是，princomp()函数默认对每个特征进行中心化处理，即将每个特征减去其均值。</p><p>使用princomp()函数进行主成分分析的步骤如下：</p><ol><li><p>准备数据：将需要进行主成分分析的数据存储在一个矩阵中，每行代表一个样本，每列代表一个特征。</p></li><li><p>调用princomp()函数：将数据矩阵作为输入参数传入princomp()函数，即可得到主成分系数矩阵coeff、主成分分数矩阵score、主成分方差向量latent、T2统计量向量tsquared、解释方差向量explained和样本均值向量mu。</p></li><li><p>选择主成分数量：根据解释方差向量explained的结果，可以选择保留前k个主成分，将数据降维为k维。</p></li><li><p>对新数据进行处理：使用得到的主成分系数矩阵coeff对新数据进行变换，得到新的主成分分数矩阵score_new，即可将高维数据转换为低维数据。</p></li></ol><p>使用princomp()函数进行主成分分析可以帮助我们更好地理解数据的结构和特征，同时也可以用于数据降维、数据可视化、特征提取等方面。</p><h2 id="im2col-和col2im"><a href="#im2col-和col2im" class="headerlink" title="im2col()和col2im()"></a>im2col()和col2im()</h2><p>im2col()和col2im()函数是MATLAB中用于图像处理的函数，主要作用是将图像转化为列向量或将列向量转化为图像。</p><p>im2col()函数的作用是将输入的图像矩阵按照指定的窗口大小和步长进行分块，并将每个块转化为列向量。该函数的语法如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B = im2col(A,[m n],<span class="string">&#x27;sliding&#x27;</span>);</span><br></pre></td></tr></table></figure><p>其中，A为输入的图像矩阵，[m n]为指定的窗口大小，‘sliding’为指定的步长方式。该函数的输出为列向量矩阵B。</p><p>col2im()函数的作用是将列向量矩阵转化为图像矩阵。该函数的语法如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = col2im(B,[m n],[M N],<span class="string">&#x27;sliding&#x27;</span>);</span><br></pre></td></tr></table></figure><p>其中，B为输入的列向量矩阵，[m n]为指定的窗口大小，[M N]为指定的输出图像大小，‘sliding’为指定的步长方式。该函数的输出为图像矩阵A。</p><p>综上，im2col()和col2im()函数在图像处理中具有重要的作用，它们可以方便地将图像转化为列向量进行处理，也可以将处理后的列向量转化为图像进行输出。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的计网——数据链路层</title>
      <link href="/2023/04/08/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
      <url>/2023/04/08/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/17.png" alt="1"></p><p><strong>参考博客：</strong></p><div class="tag link"><a class="link-card" title="BloothOfYouth]" href="https://www.jianshu.com/p/6b82134a4325"><div class="left"><img src="https://upload.jianshu.io/users/upload_avatars/24878825/a76b4b0e-6816-4240-af9a-5541e80af541.jpg?imageMogr2/auto-orient/stripimageView2/1/w/240/h/240"/></div><div class="right"><p class="text">BloothOfYouth]</p><p class="url">https://www.jianshu.com/p/6b82134a4325</p></div></a></div><p><strong>本笔记来源于：<a href="https://www.bilibili.com/video/BV1c4411d7jb">计算机网络微课堂——湖科大</a></strong></p><h1 id="一、数据链路层概述"><a href="#一、数据链路层概述" class="headerlink" title="一、数据链路层概述"></a>一、数据链路层概述</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p><strong>链路</strong>是从一个结点到相邻结点的一段物理线路，<strong>数据链路</strong>则是在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现）</p><p><strong>网络中的主机、路由器等都必须实现数据链路层</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011102531462.png" alt="image-20201011102531462"></p><p><strong>局域网中的主机、交换机等都必须实现数据链路层</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014004326549.png" alt="image-20201014004326549"></p><p><strong>从层次上来看数据的流动</strong></p><blockquote><p>主机H1将待发送的数据逐层封装后，通过物理层将构成数据包的各比特转换为电信号，发送到传输媒体，数据包进入路由器后，由下往上逐层解封到网络层，路由器根据数据包中的目的网络地址和自身转发表，确定数据包的转发端口，然后从网络层向下逐层封装数据包，并通过物理层将数据包发送到传输媒体。数据包最终到达主机H2时，还要由下往上逐层解封，最终解封出主机H1所发送的数据。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011102618878.png" alt="image-20201011102618878"></p><p><strong>仅从数据链路层观察帧的流动</strong></p><blockquote><p>这里我们只关注数据链路层，看作数据包只从数据链路层从左到右传送。</p><p>主机H1到主机H2的通信，可以看成是在4段不同的链路上的通信组成的。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011102653161.png" alt="image-20201011102653161"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011102733584.png" alt="image-20201011102733584"></p><blockquote><p>主机H1 到主机H2 所经过的网络可以是多种不同类型的</p><p><strong>注意：不同的链路层可能采用不同的数据链路层协议</strong></p></blockquote><p><strong>数据链路层使用的信道</strong></p><p>数据链路层属于计算机网路的低层。<strong>数据链路层使用的信道主要有以下两种类型：</strong></p><ul><li>点对点信道</li><li>广播信道</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014004459744.png" alt="image-20201014004459744"></p><blockquote><p><strong>局域网属于数据链路层</strong></p><p>局域网虽然是个网络。但我们并不把局域网放在网络层中讨论。这是因为在网络层要讨论的是多个网络互连的问题，是讨论分组怎么从一个网络，通过路由器，转发到另一个网络。</p><p>而在同一个局域网中，分组怎么从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，<strong>局域网仍属于数据链路层</strong>的范围</p></blockquote><h2 id="1-2-三个重要问题"><a href="#1-2-三个重要问题" class="headerlink" title="1.2 三个重要问题"></a>1.2 三个重要问题</h2><p>数据链路层传送的协议数据单元是<strong>帧</strong></p><p><strong>封装成帧</strong></p><blockquote><p>如下图所示，两台主机都会对所发送或接受的数据包进行五个层次的封装或解封，发送方将待发送的数据通过应用层封装为应用层协议数据单元，运输层为之添加协议首部，使之成为运输层协议数据单元，然后交付给网络层，网络层为其添加网络层协议首部，使之成为网络层协议数据单元，然后交付给数据链路层，数据链路层为其添加一个数据链路层协议首部，简称为帧头，还要给其添加一个帧尾，最终如下图所示</p></blockquote><ul><li><strong>封装成帧</strong> (framing) 就是在一段数据的前后分别添加帧头和帧尾，然后就构成了一个帧。</li><li>首部和尾部的一个重要作用就是进行<strong>帧定界</strong>，为了在链路层上以帧为单元来传送数据，也就是为了实现数据链路层本身的功能。</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011103650253.png" alt="image-20201011103650253"></p><p><strong>差错控制</strong></p><p>在传输过程中可能会产生<strong>比特差错</strong>：1 可能会变成 0， 而 0 也可能变成 1。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011103917512.png" alt="image-20201011103917512"></p><p>接收方主机如何判断帧在传输过程中是否出现了误码？</p><blockquote><p>发送方在发送帧之前，基于待发送的数据和检错算法计算出检错码，并将其封装在帧尾，接收方收到帧后，通过检错码和检错算法，就可以判断出帧在传输过程中是否出现了误码。</p></blockquote><p><strong>可靠传输</strong></p><p>接收方主机收到有误码的帧后，是不会接受该帧的，会将它丢弃</p><p>如果数据链路层向其上层提供的是不可靠服务，那么丢弃就丢弃了，不会再有更多措施</p><p><strong>如果数据链路层向其上层提供的是可靠服务，那就还需要其他措施，来确保接收方主机还可以重新收到被丢弃的这个帧的正确副本</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011105314053.png" alt="image-20201011105314053"></p><blockquote><p>以上三个问题都是使用<strong>点对点信道的数据链路层</strong>来举例的</p></blockquote><p><strong>如果使用广播信道的数据链路层除了包含上面三个问题外，还有一些问题要解决</strong></p><p>如图所示，主机A，B，C，D，E通过一根总线进行互连，主机A要给主机C发送数据，代表帧的信号会通过总线传输到总线上的其他各主机，那么主机B，D，E如何知道所收到的帧不是发送给她们的，主机C如何知道发送的帧是发送给自己的？</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011105824466.png" alt="image-20201011105824466"></p><p>可以用编址（地址）的来解决</p><p>将帧的目的地址添加在帧中一起传输</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011110017415.png" alt="image-20201011110017415"></p><p>还有数据碰撞问题</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011110129994.png" alt="image-20201011110129994"></p><blockquote><p>随着技术的发展，交换技术的成熟，</p><p>在 有线（局域网）领域 使用<strong>点对点链路</strong>和<strong>链路层交换机</strong>的<strong>交换式局域网</strong>取代了<del>共享式局域网</del></p><p>由于无线信道的广播天性，在无线局域网中仍然使用的是共享信道技术</p></blockquote><hr><h1 id="二、封装成帧"><a href="#二、封装成帧" class="headerlink" title="二、封装成帧"></a>二、封装成帧</h1><h2 id="2-1-介绍"><a href="#2-1-介绍" class="headerlink" title="2.1 介绍"></a>2.1 介绍</h2><p>封装成帧是指数据链路层给上层交付的协议数据单元添加帧头和帧尾使之成为帧</p><ul><li><strong>帧头和帧尾中包含有重要的控制信息</strong></li></ul><blockquote><p>PPP帧就是点对点的格式</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011110851301.png" alt="image-20201011110851301"></p><p>发送方的数据链路层将上层交付下来的协议数据单元封装成帧后，还要通过物理层，将构成帧的各比特，转换成电信号交给传输媒体，那么接收方的数据链路层如何从物理层交付的比特流中提取出一个个的帧？</p><p>答：需要帧头和帧尾来做<strong>帧定界</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011111334052.png" alt="image-20201011111334052"></p><p>如上图PPT帧的格式，在帧头和帧尾各包含一个长度为1字节的标志字段，其作用就是帧定界。</p><p>但比不是每一种数据链路层协议的帧都包含有帧定界标志，例如下面例子</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011111729324.png" alt="image-20201011111729324"></p><blockquote><p>前导码</p><ul><li>前同步码：作用是使接收方的时钟同步</li><li>帧开始定界符：表明其后面紧跟着的就是MAC帧</li></ul></blockquote><p>另外以太网还规定了帧间间隔为96比特时间，因此，MAC帧不需要帧结束定界符，如下图所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011112450187.png" alt="image-20201011112450187"></p><h2 id="2-2-透明传输"><a href="#2-2-透明传输" class="headerlink" title="2.2 透明传输"></a>2.2 透明传输</h2><blockquote><p><strong>透明</strong>：</p><p>指某一个实际存在的事物看起来却好像不存在一样。</p><p><strong>透明传输：</strong></p><p>指的是在计算机网络中，数据传输过程中不会改变数据的内容、格式、顺序等信息，使得数据传输的过程对用户来说是“透明”的，就好像数据直接从源端传输到目的端一样。这种传输方式可以提高网络的可靠性和稳定性，同时也能提升用户体验。</p></blockquote><p>透明传输是指<strong>数据链路层对上层交付的传输数据没有任何限制</strong>，好像数据链路层不存在一样。</p><p>帧定界标志也就是个特定数据值，如果在上层交付的协议数据单元中，恰好也包含这个特定数值，接收方就不能正确接收。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011113207944.png" alt="image-20201011113207944"></p><blockquote><p>如果数据链路层不采取其他措施，来避免接收方对帧是否结束的误判，就不能称为透明传输。所以数据链路层应该对上层交付的数据有限制，其内容不能包含帧定界符的值</p></blockquote><h3 id="2-2-1-解决透明传输问题"><a href="#2-2-1-解决透明传输问题" class="headerlink" title="2.2.1 解决透明传输问题"></a>2.2.1 解决透明传输问题</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011113804721.png" alt="image-20201011113804721"></p><ul><li><strong>解决方法</strong>：面向字节的物理链路使用<strong>字节填充</strong> (byte stuffing) 或<strong>字符填充</strong> (character stuffing)，面向比特的物理链路使用<strong>比特填充</strong>的方法【每五个连续的1后面插入一个比特0】实现透明传输。</li><li>发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面<strong>插入一个转义字符“ESC”</strong>(转义字符是一种特殊的控制字符，其长度为1个字节，十进制值为27，其十六进制编码是1B)。</li><li>接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。</li><li>如果转义字符也出现在数据当中，那么应在转义字符前面插入一个转义字符 ESC。当接收端收到连续的两个转义字符时，就删除其中前面的一个。</li></ul><h3 id="2-2-2-帧的数据部分长度"><a href="#2-2-2-帧的数据部分长度" class="headerlink" title="2.2.2 帧的数据部分长度"></a>2.2.2 帧的数据部分长度</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011115008209.png" alt="image-20201011115008209"></p><h2 id="2-3-总结"><a href="#2-3-总结" class="headerlink" title="2.3 总结"></a>2.3 总结</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011115049672.png" alt="image-20201011115049672"></p><hr><h1 id="三、差错检测"><a href="#三、差错检测" class="headerlink" title="三、差错检测"></a>三、差错检测</h1><h2 id="3-1-介绍"><a href="#3-1-介绍" class="headerlink" title="3.1 介绍"></a>3.1 介绍</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011133757804.png" alt="image-20201011133757804"></p><blockquote><p>如上图，帧尾的FCS是帧检验序列字段【检测码】，其作用就是让接收方的数据链路层检查帧在传输过程中是否产生了误码。</p></blockquote><h2 id="3-2-奇偶校验"><a href="#3-2-奇偶校验" class="headerlink" title="3.2 奇偶校验"></a>3.2 奇偶校验</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011234428217.png" alt="image-20201011234428217"></p><h2 id="3-3-循环冗余校验CRC-Cyclic-Redundancy-Check"><a href="#3-3-循环冗余校验CRC-Cyclic-Redundancy-Check" class="headerlink" title="3.3 循环冗余校验CRC(Cyclic Redundancy Check)"></a>3.3 循环冗余校验CRC(Cyclic Redundancy Check)</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011234605045.png" alt="image-20201011234605045"></p><blockquote><p>注意📌：添加到左边，即待发送的数据的后面</p><p>注意📌：这里的除法是<code>异或</code>运算</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011234701845.png" alt="image-20201011234701845"></p><p><strong>例题</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011235128869.png" alt="image-20201011235128869"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011235325022.png" alt="image-20201011235325022"></p><h2 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201011235726437.png" alt="image-20201011235726437"></p><blockquote><p>循环冗余校验 CRC 是一种检错方法，而帧校验序列 FCS 是添加在数据后面的冗余码</p></blockquote><hr><h1 id="四、可靠传输"><a href="#四、可靠传输" class="headerlink" title="四、可靠传输"></a>四、可靠传输</h1><h2 id="4-1-基本概念"><a href="#4-1-基本概念" class="headerlink" title="4.1 基本概念"></a>4.1 基本概念</h2><p><strong>可靠传输</strong>是指在计算机网络中，数据在传输过程中能够保证不丢失、不重复、不出错地到达目的地。</p><h3 id="4-1-1-下面是比特差错"><a href="#4-1-1-下面是比特差错" class="headerlink" title="4.1.1 下面是比特差错"></a>4.1.1 下面是比特差错</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012153605893.png" alt="image-20201012153605893"></p><h3 id="4-1-2-其他传输差错"><a href="#4-1-2-其他传输差错" class="headerlink" title="4.1.2 其他传输差错"></a>4.1.2 其他传输差错</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012153811724.png" alt="image-20201012153811724"></p><ul><li>分组丢失</li></ul><p>路由器输入队列快满了，主动丢弃收到的分组</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012154910921.png" alt="image-20201012154910921"></p><ul><li>分组失序</li></ul><p>数据并未按照发送顺序依次到达接收端</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012155300937.png" alt="image-20201012155300937"></p><ul><li>分组重复</li></ul><p>由于某些原因，有些分组在网络中滞留了，没有及时到达接收端，这可能会造成发送端对该分组的重发，重发的分组到达接收端，但一段时间后，滞留在网络的分组也到达了接收端，这就造成<strong>分组重复</strong>的传输差错</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012160026362.png" alt="image-20201012160026362"></p><h2 id="4-2-三种可靠协议"><a href="#4-2-三种可靠协议" class="headerlink" title="4.2 三种可靠协议"></a>4.2 三种可靠协议</h2><ul><li>停止-等待协议SW</li><li>回退N帧协议GBN</li><li>选择重传协议SR</li></ul><blockquote><p>这三种可靠传输实现机制的基本原理并不仅限于数据链路层，可以应用到计算机网络体系结构的各层协议中</p></blockquote><h3 id="4-2-1-停止-等待协议SW-Stop-and-Wait"><a href="#4-2-1-停止-等待协议SW-Stop-and-Wait" class="headerlink" title="4.2.1 停止-等待协议SW(Stop-and-Wait)"></a>4.2.1 停止-等待协议SW(Stop-and-Wait)</h3><h4 id="1）停止-等待协议可能遇到的四个问题"><a href="#1）停止-等待协议可能遇到的四个问题" class="headerlink" title="1）停止-等待协议可能遇到的四个问题"></a>1）停止-等待协议可能遇到的四个问题</h4><blockquote><p>每发送一个数据分组，就需要停止等待，收到确认分组，发送方才能继续发送下一个DATA。</p></blockquote><p><strong>1.确认与否认</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012162009780.png" alt="image-20201012162009780" style="zoom:67%;" /></p><blockquote><p>如上图所示，收发双方基于互联网进行通信，而不是局限在一条点对点的数据链路，纵坐标为时间，发送方给接收方发送数据分组DATA。接收方收到后对其进行差错检测，若没有误码，则接受该数据分组，并给发送方发送确认分组，简称为ACK。发送方收到对所发送数据分组的确认分组后，才能发送下一个数据分组。假设这个分组在传输过程中出现了误码，接收方收到后对其进行差错检测，发现了误码，则丢弃该数据分组，并给发送方发送否认分组，简称NAK。发送方收到对所发送数据分组的确认分组后，就知道了之前所发送的数据分组出现了差错而被接收方拒绝，于是立刻重传该数据分组。</p><p>因此发送方发送完一个数据分组后，并不能立即将数据分组从缓存中删除，只有在收到针对该数据分组的确认分组后，才能将其从缓存中删除。</p></blockquote><p><strong>2.超时重传</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012162112151.png" alt="image-20201012162112151" style="zoom:67%;" /></p><blockquote><p>对于数据链路层点对点信道而言，不太容易出现DATA在传输道路上丢失的情况，但对于多个网路通过多个路由器互联的复杂互联网环境而言，这种情况是经常出现的。</p></blockquote><p><strong>3.确认丢失</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012162318298.png" alt="image-20201012162318298" style="zoom:67%;" /></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012162348428.png" alt="image-20201012162348428" style="zoom:67%;" /></p><blockquote><p>根据SW的停等特性，一个比特来编号，即0/1，用来确定与上一个DATA不是同一个即可！！！</p><p>既然数据分组需要编号，确认分组是否需要编号？</p><p>要。如下图所示</p></blockquote><p><strong>4.确认迟到</strong></p><blockquote><p>对于数据链路层的点对点信道，往返时间比较固定，不会出现确认迟到的情况。因此，如果只在数据链路层实现SW，可以不用给确认分组编号。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012162815885.png" alt="image-20201012162815885" style="zoom:67%;" /></p><blockquote><p>注意，图中最下面那个数据分组与之前序号为0的那个数据分组不是同一个数据分组</p></blockquote><p><strong>注意事项</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012164008780.png" alt="image-20201012164008780"></p><h4 id="2）停止-等待协议的信道利用率"><a href="#2）停止-等待协议的信道利用率" class="headerlink" title="2）停止-等待协议的信道利用率"></a>2）停止-等待协议的信道利用率</h4><p>假设收发双方之间是一条直通的信道，发送方发送一个数据分组后，就停止发送，并等待接收方对该数据分组的确认，当收到确认分组后，才可以发送下一个数据分组。</p><ul><li><strong>TD</strong>：是发送方发送数据分组所耗费的发送时延【数据帧的发送时延】</li><li><strong>RTT</strong>：是收发双方之间的往返时间【是发送方与接收方之间的两段单程传播时延】</li><li><strong>TA</strong>：是接收方发送确认分组所耗费的发送时延</li></ul><p>图中忽略了接收方对数据分组的处理时延和发送方对确认分组的处理时延。</p><p>TA一般都远小于TD，可以忽略，当RTT远大于TD时，信道利用率会非常低。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012164924635.png" alt="image-20201012164924635"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012181005719.png" alt="image-20201012181005719"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012181047665.png" alt="image-20201012181047665"></p><blockquote><p>像停止-等待协议这样通过确认和重传机制实现的可靠传输协议，常称为自动请求重传协议ARQ(<strong>A</strong>utomatic <strong>R</strong>epeat re<strong>Q</strong>uest)，意思是重传的请求是自动进行，因为不需要接收方显式地请求-&gt;发送方重传某个发送的分组。</p></blockquote><h3 id="4-2-2-回退N帧协议GBN-Go-Back-N"><a href="#4-2-2-回退N帧协议GBN-Go-Back-N" class="headerlink" title="4.2.2 回退N帧协议GBN(Go-Back-N)"></a>4.2.2 回退N帧协议GBN(Go-Back-N)</h3><h4 id="1）为什么用回退N帧协议"><a href="#1）为什么用回退N帧协议" class="headerlink" title="1）为什么用回退N帧协议"></a>1）为什么用回退N帧协议</h4><p>在相同的时间内，使用停止-等待协议的发送方只能发送一个数据分组，而采用流水线传输的发送方，可以发送多个数据分组。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012190027828.png" alt="image-20201012190027828"></p><p>回退N帧协议在流水线传输的基础上，利用发送窗口来限制发送方可连续发送数据分组的个数。</p><p>下图举例说明：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012190632086.png" alt="image-20201012190632086"></p><blockquote><p>如果WT的取值为1，则就是SW协议。</p><p>在GBN中，WR只能取值为1。</p></blockquote><h4 id="2）无差错情况流程"><a href="#2）无差错情况流程" class="headerlink" title="2）无差错情况流程"></a>2）无差错情况流程</h4><p>发送方将序号落在发送窗口内的0~4号数据分组，依次连续发送出去</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012191936466.png" alt="image-20201012191936466"></p><p>他们经过互联网传输正确到达接收方，就是没有乱序和误码，接收方按序接收它们，每接收一个，接收窗口就向前滑动一个位置，并给发送方发送针对所接收分组的确认分组，在通过互联网的传输正确到达了发送方</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012192932035.png" alt="image-20201012192932035"></p><p>发送方每接收一个、发送窗口就向前滑动一个位置，这样就有新的序号落入发送窗口，发送方可以将收到确认的数据分组从缓存中删除了，而接收方可以择机将已接收的数据分组交付上层处理</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012193212419.png" alt="image-20201012193212419"></p><h4 id="3）累计确认"><a href="#3）累计确认" class="headerlink" title="3）累计确认"></a>3）累计确认</h4><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012194304696.png" alt="，image-20201012194304696"></p><blockquote><p>累计确认</p><p>优点:</p><ul><li>即使确认分组丢失，发送方也可能不必重传！</li><li>减小接收方的开销！</li><li>减小对网络资源的占用！</li></ul><p>缺点：</p><ul><li>不能向发送方及时反映出接收方已经正确接收的数据分组信息！</li></ul></blockquote><h4 id="4）有差错情况"><a href="#4）有差错情况" class="headerlink" title="4）有差错情况"></a>4）有差错情况</h4><p>例如</p><p>在传输数据分组时，5号数据分组出现误码，接收方通过数据分组中的检错码发现了错误</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012195440780.png" alt="image-20201012195440780"></p><p>于是丢弃该分组，而后续到达的这剩下四个分组与接收窗口的序号不匹配</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012195629368.png" alt="image-20201012195629368"></p><p>接收方同样也不能接收它们，将它们丢弃，并对之前按序接收的最后一个数据分组进行确认，发送ACK4，<strong>每丢弃一个数据分组，就发送一个ACK4</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012195836902.png" alt="image-20201012195836902"></p><p>当收到重复的ACK4时，就知道之前所发送的数据分组出现了差错，于是可以不等超时计时器超时就立刻开始重传，具体收到几个重复确认就立刻重传，根据具体实现决定</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012200120166.png" alt="image-20201012200120166"></p><p>如果收到这4个重复的确认并不会触发发送立刻重传，一段时间后。超时计时器超时，也会将发送窗口内以发送过的这些数据分组全部重传</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012200454557.png" alt="image-20201012200454557"></p><p><strong>若WT超过取值范围，例如WT=8，会出现什么情况？</strong></p><blockquote><p>如下图所示：</p><p>发送方将序号落在发送窗口内的0~7号这8个数据分组，依次连续的发送出去。它们经过了互联网的传输，正确到达了接收方，接收方正确按序接收后，给发送方发回累计确认ACK7。假设ACK7在传输过程中丢失了，这将导致发送方的超时重传，重传的0~7号数据分组到达接收方。但现在问题来了接收方根据当前接收窗口内的序号，会对这8个数据分组按序接收，但是接收方之前已经接受过这8个数据分组了，现在是重复接受，无法分辨新、旧数据分组，进而会产生分组重复这一传输差错。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012201109774.png" alt="image-20201012201109774"></p><h4 id="5）习题"><a href="#5）习题" class="headerlink" title="5）习题"></a>5）习题</h4><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012202419107.png" alt="image-20201012202419107"></p><h4 id="6）总结"><a href="#6）总结" class="headerlink" title="6）总结"></a>6）总结</h4><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012202222138.png" alt="image-20201012202222138"></p><ul><li>回退N帧协议在流水线传输的基础上利用发送窗口来限制发送方连续发送数据分组的数量，是一种连续ARQ协议</li><li>在协议的工作过程中发送窗口和接收窗口不断向前滑动，因此这类协议又称为滑动窗口协议<ul><li>由于回退N帧协议的特性，当通信线路质量不好时，其信道利用率并不比停止-等待协议高</li></ul></li></ul><h3 id="4-2-3-选择重传协议SR-Selective-Request"><a href="#4-2-3-选择重传协议SR-Selective-Request" class="headerlink" title="4.2.3 选择重传协议SR(Selective Request)"></a>4.2.3 选择重传协议SR(Selective Request)</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012203638722.png" alt="image-20201012203638722"></p><p><a href="https://www.bilibili.com/video/BV1c4411d7jb?p=27">具体流程请看视频！！！</a></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/ab2d1c59f757ffc79330fdd2a40ff90.png" alt="image-20201012203638722"></p><blockquote><p>如果发送窗口和接收窗口超过最大值，会出现接收方无法分辨新、旧数据分组，进而出现分组重复这种差错。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/14adb997c61c313ea80e93110da4d84.png" alt="image-20201012203638722"></p><p><strong>习题</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012205250996.png" alt="image-20201012205250996"></p><p><strong>总结</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012204742870.png" alt="image-20201012204742870"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012205133924.png" alt="image-20201012205133924"></p><hr><h1 id="五、点对点协议PPP"><a href="#五、点对点协议PPP" class="headerlink" title="五、点对点协议PPP"></a>五、点对点协议PPP</h1><ul><li>点对点协议PPP（Point-to-Point Protocol）是目前使用最广泛的点对点数据链路层协议</li><li>PPP协议是因特网工程任务组IEIF在1992年制定的。经过1993年和1994年的修订，现在的PPP协议已成为因特网的正式标准[RFC1661，RFC1662]</li><li>数据链路层使用的一种协议，它的特点是：简单；只检测差错，而不是纠正差错；不使用序号，也不进行流量控制；可同时支持多种网络层协议</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012210844629.png" alt="image-20201012210844629"></p><blockquote><p>点对点PPP协议也广泛应用于广域网路由器之间的专用线路。</p></blockquote><ul><li>PPPoE 是为宽带上网的主机使用的链路层协议</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012211423528.png" alt="image-20201012211423528"></p><h2 id="5-1-帧格式"><a href="#5-1-帧格式" class="headerlink" title="5.1 帧格式"></a>5.1 帧格式</h2><p>必须规定特殊的字符作为帧定界符</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012211826281.png" alt="image-20201012211826281"></p><h2 id="5-2-透明传输"><a href="#5-2-透明传输" class="headerlink" title="5.2 透明传输"></a>5.2 透明传输</h2><p>必须保证数据传输的透明性💥</p><blockquote><p>当PPP帧的数据部分出现帧首和帧尾中的标志字段时，如果不采取措施，则会造成接收方对PPP帧是否结束的误判。</p></blockquote><h4 id="实现透明传输的方法【取决于所使用的链路的类型】"><a href="#实现透明传输的方法【取决于所使用的链路的类型】" class="headerlink" title="实现透明传输的方法【取决于所使用的链路的类型】"></a>实现透明传输的方法【取决于所使用的链路的类型】</h4><ul><li>面向字节的异步链路：字节填充法（插入“转义字符”）</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012212148803.png" alt="image-20201012212148803"></p><ul><li>面向比特的同步链路：比特填充法（插入“比特0”）</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012212255550.png" alt="image-20201012212255550"></p><h2 id="5-3-差错检测"><a href="#5-3-差错检测" class="headerlink" title="5.3 差错检测"></a>5.3 差错检测</h2><p>能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012212558654.png" alt="image-20201012212558654"></p><h2 id="5-4-工作状态"><a href="#5-4-工作状态" class="headerlink" title="5.4 工作状态"></a>5.4 工作状态</h2><ul><li>当用户拨号接入 ISP 时，路由器的调制解调器对拨号做出确认，并建立一条物理连接。</li><li>PC 机向路由器发送一系列的 LCP 分组（封装成多个 PPP 帧）。</li><li>这些分组及其响应选择一些 PPP 参数，并进行网络层配置，NCP 给新接入的 PC 机</li><li>分配一个临时的 IP 地址，使 PC 机成为因特网上的一个主机。</li><li>通信完毕时，NCP 释放网络层连接，收回原来分配出去的 IP 地址。接着，LCP 释放数据链路层连接。最后释放的是物理层的连接。</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201012213021860.png" alt="image-20201012213021860"></p><blockquote><p>可见，PPP 协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。</p></blockquote><hr><h1 id="六、媒体接入控制（介质访问控制）——广播信道"><a href="#六、媒体接入控制（介质访问控制）——广播信道" class="headerlink" title="六、媒体接入控制（介质访问控制）——广播信道"></a>六、媒体接入控制（介质访问控制）——广播信道</h1><p><strong>媒体接入控制（介质访问控制）使用一对多的广播通信方式</strong></p><blockquote><p><strong>Medium Access Control</strong>翻译成媒体接入控制，有些翻译成介质访问控制</p></blockquote><p><strong>局域网的数据链路层</strong></p><ul><li>局域网最主要的<strong>特点</strong>是：<ul><li>网络为一个单位所拥有；</li><li>地理范围和站点数目均有限。</li></ul></li><li>局域网具有如下<strong>主要优点</strong>：<ul><li>具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。</li><li>便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。</li><li>提高了系统的可靠性、可用性和残存性。</li></ul></li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013201521915.png" alt="image-20201013201521915"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013201533445.png" alt="image-20201013201533445"></p><p><strong>数据链路层的两个子层</strong></p><p>为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成<strong>两个子层</strong>：</p><ol><li><strong>逻辑链路控制</strong> LLC (Logical Link Control)子层；</li><li><strong>媒体接入控制</strong> MAC (Medium Access Control)子层。</li></ol><p>与接入到传输媒体有关的内容都放在 MAC子层，而 LLC 子层则与传输媒体无关。<br><strong>不管采用何种协议的局域网，对 LLC 子层来说都是透明的。</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013201133903.png" alt="image-20201013201133903"></p><h2 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a>6.1 基本概念</h2><p>为什么要媒体接入控制（介质访问控制）？</p><p><strong>共享信道带来的问题</strong></p><p>若多个设备在共享信道上同时发送数据，则会造成彼此干扰，导致发送失败。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013152007335.png" alt="image-20201013152007335"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013152453425.png" alt="image-20201013152453425"></p><blockquote><p>随着技术的发展，交换技术的成熟和成本的降低，具有更高性能的使用点对点链路和链路层交换机的交换式局域网在有线领域已完全取代了共享式局域网，但由于无线信道的广播天性，无线局域网仍然使用的是共享媒体技术。</p></blockquote><h2 id="6-2-静态划分信道"><a href="#6-2-静态划分信道" class="headerlink" title="6.2 静态划分信道"></a>6.2 静态划分信道</h2><h3 id="6-2-1-信道复用概念"><a href="#6-2-1-信道复用概念" class="headerlink" title="6.2.1 信道复用概念"></a>6.2.1 信道复用概念</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013153642544.png" alt="image-20201013153642544"></p><h3 id="6-2-2-常见的信道复用技术"><a href="#6-2-2-常见的信道复用技术" class="headerlink" title="6.2.2 常见的信道复用技术"></a>6.2.2 常见的信道复用技术</h3><p><strong>频分复用FDM (Frequency Division Multiplexing)</strong></p><ul><li><p>将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。</p></li><li><p><strong>频分复用</strong>的所有用户在同样的时间<strong>占用不同的带宽资源</strong>（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。</p></li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013153947668.png" alt="image-20201013153947668"></p><p><strong>时分复用TDM (Time Division Multiplexing)</strong></p><ul><li><strong>时分复用</strong>则是将时间划分为一段段等长的<strong>时分复用帧（TDM帧）</strong>。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。</li><li>每一个用户所占用的时隙是<strong>周期性地出现</strong>（其周期就是TDM帧的长度）的。</li><li>TDM 信号也称为<strong>等时</strong> (isochronous) 信号。</li><li><strong>时分复用的所有用户在不同的时间占用同样的频带宽度。</strong></li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013154142540.png" alt="image-20201013154142540"></p><p><strong>波分复用 WDM(Wavelength Division Multiplexing)</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013202218132.png" alt="image-20201013202218132"></p><blockquote><p>波分复用就是光的频分复用，使用一根光纤来同时传输多个光载波信号</p><p>光信号传输一段距离后悔衰减，所以要用 掺铒光纤放大器 放大光信号</p></blockquote><p><strong>码分复用 CDM  (Code Division Multiplexing)</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013203126625.png" alt="image-20201013203126625"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013203324709.png" alt="image-20201013203324709"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013203459640.png" alt="image-20201013203459640"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013203819578.png" alt="image-20201013203819578"></p><h2 id="6-3-动态接入控制"><a href="#6-3-动态接入控制" class="headerlink" title="6.3 动态接入控制"></a>6.3 动态接入控制</h2><p>受控接入</p><p><strong>受控接入在局域网中使用得较少，本书不再讨论</strong></p><p><strong>随机接入</strong></p><p><strong>重点</strong></p><h3 id="关于退避算法："><a href="#关于退避算法：" class="headerlink" title="关于退避算法："></a>关于退避算法：</h3><blockquote><p>退避算法是指在网络传输过程中，如果发现冲突（即多个设备同时发送数据导致冲突），则需要进行退避操作。退避操作指的是将当前的数据包延迟一段时间后再次发送，以避免再次发生冲突。</p><p>退避算法通常会出现在以太网中，因为以太网采用的是CSMA/CD协议，即载波监听多路访问/冲突检测。当多个设备同时发送数据导致冲突时，就需要进行退避操作。</p><p>退避算法的目的是为了避免网络中的冲突，从而提高网络的效率和稳定性。如果没有退避算法，冲突会不断发生，导致网络的传输速度变慢，影响网络的正常运行。因此，退避算法是网络中必不可少的一部分。</p></blockquote><h3 id="时隙与争用期："><a href="#时隙与争用期：" class="headerlink" title="时隙与争用期："></a>时隙与争用期：</h3><blockquote><p>时隙和争用期是无线通信中的两个重要概念。</p><p>时隙是无线通信中用于传输数据的时间段，也称为时间片。一个时隙通常包括发送和接收数据的时间，以及一些必要的控制信息。</p><p>争用期是指当多个设备在同一时隙内试图传输数据时，它们之间的竞争时间。在争用期内，设备会竞争使用时隙，以便传输数据。如果多个设备同时尝试使用同一时隙，就会发生冲突，导致数据传输失败。</p><p>为了避免冲突和提高通信效率，通常会使用一些协议和技术来管理时隙和争用期。例如，无线局域网中使用的CSMA/CA协议就是一种用于管理争用期的技术。在CSMA/CA中，设备在发送数据前会先监听信道，以确保信道上没有其他设备正在传输数据。如果信道空闲，设备就可以开始传输数据。如果信道有其他设备正在传输数据，设备就会等待一段时间后再次尝试传输。这样可以减少冲突，提高通信效率。</p></blockquote><h2 id="6-4-随机接入（CSMA-CD协议）"><a href="#6-4-随机接入（CSMA-CD协议）" class="headerlink" title="6.4 随机接入（CSMA/CD协议）"></a>6.4 随机接入（CSMA/CD协议）</h2><p><strong>总线局域网使用协议：CSMA/CD</strong></p><h3 id="6-4-1-基本概念"><a href="#6-4-1-基本概念" class="headerlink" title="6.4.1 基本概念"></a>6.4.1 基本概念</h3><p><strong>最初</strong>的<strong>以太网</strong>是将许多计算机都连接到一根总线上。易于实现广播通信。当初认为这样的连接方法既简单又可靠，因为总线上没有有源器件。</p><blockquote><p><strong>以太网（Ethernet）</strong>是一种计算机<strong>局域网技术</strong>。IEEE组织的IEEE 802.3标准制定了<strong>以太网（Ethernet）</strong>的技术标准</p><p>以太网采用无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢弃，其他什么也不做</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013211620687.png" alt="image-20201013211620687"></p><p class='p red'>早期的共享式以太网采用载波监听多址接入/碰撞检测，也就是CSMA/CD协议来解决该问题。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013213102777.png" alt="image-20201013213102777"></p><blockquote><p>上图中的<code>96比特时间</code>是指发送96比特所耗费的时间，也称为帧间最小间隔。其作用是使接收方可以检测出一个帧的结束，同时也能使得其他站点都能有机会平等竞争信道并发送帧。</p></blockquote><h3 id="6-4-2-多址接入MA"><a href="#6-4-2-多址接入MA" class="headerlink" title="6.4.2 多址接入MA"></a>6.4.2 多址接入MA</h3><p>表示许多主机以多点接入的方式连接在一根总线上。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013215400688.png" alt="image-20201013215400688"></p><h3 id="6-4-3-载波监听CS"><a href="#6-4-3-载波监听CS" class="headerlink" title="6.4.3 载波监听CS"></a>6.4.3 载波监听CS</h3><p>是指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013215530979.png" alt="image-20201013215530979"></p><p>总线上并没有什么“载波”。因此， <strong>“载波监听”就是用电子技术检测总线上有没有其他计算机发送的数据信号。</strong></p><h3 id="6-4-4-碰撞检测CD"><a href="#6-4-4-碰撞检测CD" class="headerlink" title="6.4.4 碰撞检测CD"></a>6.4.4 碰撞检测CD</h3><ul><li><strong>“碰撞检测”</strong>就是计算机<strong>边发送数据边检测</strong>信道上的信号电压大小。</li><li>当几个站同时在总线上发送数据时，总线上的信号电压摆动值将会增大（互相叠加）。</li><li>当一个站检测到的信号电压摆动值超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。</li><li><strong>所谓“碰撞”就是发生了冲突。因此“碰撞检测”也称为“冲突检测”。</strong></li><li>在发生碰撞时，总线上传输的信号产生了严重的失真，无法从中恢复出有用的信息来。</li><li><strong>每一个正在发送数据的站，一旦发现总线上出现了碰撞，就要立即停止发送，免得继续浪费网络资源，然后等待一段随机时间后再次发送。</strong></li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013221240514.png" alt="image-20201013221240514"></p><blockquote><p>为什么要进行碰撞检测？ 因为信号传播时延对载波监听产生了影响</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013221834942.png" alt="image-20201013221834942"></p><p>A 需要单程传播时延的 2 倍的时间，才能检测到与 B 的发送产生了冲突</p></blockquote><h3 id="6-4-5-CSMA-CD-协议工作流程"><a href="#6-4-5-CSMA-CD-协议工作流程" class="headerlink" title="6.4.5 CSMA/CD 协议工作流程"></a>6.4.5 CSMA/CD 协议工作流程</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013221705893.png" alt="image-20201013221705893"></p><h3 id="6-4-6-CSMA-CD-协议工作——争用期（碰撞窗口）"><a href="#6-4-6-CSMA-CD-协议工作——争用期（碰撞窗口）" class="headerlink" title="6.4.6 CSMA/CD 协议工作——争用期（碰撞窗口）"></a>6.4.6 CSMA/CD 协议工作——争用期（碰撞窗口）</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013223235305.png" alt="image-20201013223235305"></p><h3 id="6-4-7-CSMA-CD-协议工作——最小帧长"><a href="#6-4-7-CSMA-CD-协议工作——最小帧长" class="headerlink" title="6.4.7 CSMA/CD 协议工作——最小帧长"></a>6.4.7 CSMA/CD 协议工作——最小帧长</h3><p>​        假设主机A正在给主机D发送一个很短的帧，边发送边检测碰撞，主机A很快就将该帧发送完毕了，之后就不再针对该帧检测碰撞。在该帧的传输过程中，主机C也要发送帧，主机C检测到总线空闲96比特时间后就立即发送帧尽管总线现在实际上并不空闲，这必然会产生碰撞。主机D最终会收到主机A发送的、并遭遇碰撞的帧，主机D会将该帧丢弃。但对于主机A而言，它并不知道自己已发送完毕的该帧，在总线上传输的过程中遭遇了碰撞，因此不会重发该帧。因此使用CSMA/CD协议的以太网的帧长不能太短。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013224051932.png" alt="image-20201013224051932"></p><blockquote><p>根据端到端时延，可以求出争用期，再乘以数据传输速率，即为最小帧长。</p></blockquote><h3 id="6-4-8-CSMA-CD-协议工作——最大帧长"><a href="#6-4-8-CSMA-CD-协议工作——最大帧长" class="headerlink" title="6.4.8 CSMA/CD 协议工作——最大帧长"></a>6.4.8 CSMA/CD 协议工作——最大帧长</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013225400777.png" alt="image-20201013225400777"></p><h3 id="6-4-9-CSMA-CD-协议工作——截断二进制指数退避算法"><a href="#6-4-9-CSMA-CD-协议工作——截断二进制指数退避算法" class="headerlink" title="6.4.9 CSMA/CD 协议工作——截断二进制指数退避算法"></a>6.4.9 CSMA/CD 协议工作——截断二进制指数退避算法</h3><p>📌此小节介绍退避时间的计算方法</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013230717856.png" alt="image-20201013230717856"></p><h3 id="6-4-10-CSMA-CD-协议工作——信道利用率"><a href="#6-4-10-CSMA-CD-协议工作——信道利用率" class="headerlink" title="6.4.10 CSMA/CD 协议工作——信道利用率"></a>6.4.10 CSMA/CD 协议工作——信道利用率</h3><blockquote><p>发送成功后，还有信道中端到端的传播时延，所以还会多一个t</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013231430295.png" alt="image-20201013231430295"></p><h3 id="6-4-11-CSMA-CD-协议工作——帧发送流程"><a href="#6-4-11-CSMA-CD-协议工作——帧发送流程" class="headerlink" title="6.4.11 CSMA/CD 协议工作——帧发送流程"></a>6.4.11 CSMA/CD 协议工作——帧发送流程</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/4122fc42639b194625028bce9779b2e.png" alt="image-20201013231703302"></p><h3 id="6-4-12-CSMA-CD-协议工作——帧接收流程"><a href="#6-4-12-CSMA-CD-协议工作——帧接收流程" class="headerlink" title="6.4.12 CSMA/CD 协议工作——帧接收流程"></a>6.4.12 CSMA/CD 协议工作——帧接收流程</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201013231703302.png" alt="image-20201013231703302"></p><h3 id="6-4-13-CSMA-CD-协议的重要特性"><a href="#6-4-13-CSMA-CD-协议的重要特性" class="headerlink" title="6.4.13 CSMA/CD 协议的重要特性"></a>6.4.13 CSMA/CD 协议的重要特性</h3><ul><li>使用 CSMA/CD 协议的以太网不能进行全双工通信而<strong>只能进行双向交替通信（半双工通信）。</strong></li><li>每个站在发送数据之后的一小段时间内，存在着遭遇碰撞的可能性。</li><li>这种<strong>发送的不确定性</strong>使整个以太网的平均通信量远小于以太网的最高数据率。</li></ul><blockquote><p>CSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。</p><p><strong>现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CS协议</strong></p></blockquote><h2 id="6-5-随机接入（CSMA-CA协议）"><a href="#6-5-随机接入（CSMA-CA协议）" class="headerlink" title="6.5 随机接入（CSMA/CA协议）"></a>6.5 随机接入（CSMA/CA协议）</h2><p><strong>无线局域网使用的协议：CSMA/CA</strong></p><h3 id="6-5-1-为什么无线局域网要使用CSMA-CA协议"><a href="#6-5-1-为什么无线局域网要使用CSMA-CA协议" class="headerlink" title="6.5.1 为什么无线局域网要使用CSMA/CA协议"></a>6.5.1 为什么无线局域网要使用CSMA/CA协议</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014192811760.png" alt="image-20201014192811760"></p><h3 id="6-5-2-帧间间隔IFS（InterFrame-Space）"><a href="#6-5-2-帧间间隔IFS（InterFrame-Space）" class="headerlink" title="6.5.2 帧间间隔IFS（InterFrame Space）"></a>6.5.2 帧间间隔IFS（InterFrame Space）</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014200149717.png" alt="image-20201014200149717"></p><h3 id="6-5-3-CSMA-CA协议的工作原理"><a href="#6-5-3-CSMA-CA协议的工作原理" class="headerlink" title="6.5.3 CSMA/CA协议的工作原理"></a>6.5.3 CSMA/CA协议的工作原理</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014200833233.png" alt="image-20201014200833233"></p><blockquote><p><strong>源站为什么在检测到信道空闲后还要再等待一段时间DIFS？</strong></p><ul><li>考虑到可能有其他的站有高优先级的帧要发送。若有，就要让高优先级帧先发送</li></ul><p><strong>目的站为什么正确接收数据帧后还要等待一段时间SIFS才能发送ACK帧？</strong></p><ul><li>SIFS是最短的帧间间隔，用来分隔开属于一次对话的各帧，在这段时间内，一个站点应当能够从发送方式切换到接收方式</li></ul></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014201511741.png" alt="image-20201014201511741"></p><blockquote><p><strong>信道由忙转为空闲且经过DIFS时间后，还要退避一段随机时间才能使用信道？</strong></p><p>防止多个站点同时发送数据而产生碰撞</p></blockquote><p><strong>使用退避算法的时机</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014201927680.png" alt="image-20201014201927680"></p><h3 id="6-5-4-CSMA-CA协议的退避算法"><a href="#6-5-4-CSMA-CA协议的退避算法" class="headerlink" title="6.5.4 CSMA/CA协议的退避算法"></a>6.5.4 CSMA/CA协议的退避算法</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014202213766.png" alt="image-20201014202213766"></p><p><strong>退避算法的示例</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014202819851.png" alt="image-20201014202819851"></p><p>上图A、B、C、D、E是五个无线站点，横坐标为时间，假设A正在占用无线信道发送帧，在A的发送过程中，B、C、D也要发送帧，我们用向上的箭头来表示，于是进行载波监听，发现信道忙，需要退避，根据退避算法选择出一个随机时间，并在每个时隙对信道进行一次检测，当检测到信道由忙转为空闲状态，且经过帧间间隔DIFS后，退避计时器开始倒计时，假设C的退避时间最短，当C的退避计时器到时后，C立即开始发送帧，此时信道由空闲状态转换为忙状态，当B和D检测到信道忙后，就冻结各自剩余的退避时间。当B和D检测到信道由忙转化为空闲状态且经过帧间间隔DIFS后，退避计时器重新开始从上次冻结的退避剩余时间开始倒计时，倒计时结束后，再立即发送帧。</p><h3 id="6-5-5-CSMA-CA协议的信道预约和虚拟载波监听"><a href="#6-5-5-CSMA-CA协议的信道预约和虚拟载波监听" class="headerlink" title="6.5.5 CSMA/CA协议的信道预约和虚拟载波监听"></a>6.5.5 CSMA/CA协议的信道预约和虚拟载波监听</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014203119710.png" alt="image-20201014203119710"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014203506878.png" alt="image-20201014203506878"></p><p><strong>虚拟载波监听机制能减少隐蔽站带来的碰撞问题的示例</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014203859033.png" alt=" "></p><hr><h1 id="七、MAC地址、IP地址以及ARP协议"><a href="#七、MAC地址、IP地址以及ARP协议" class="headerlink" title="七、MAC地址、IP地址以及ARP协议"></a>七、MAC地址、IP地址以及ARP协议</h1><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014222831663.png" alt="image-20201014222831663"></p><h2 id="7-1-MAC地址"><a href="#7-1-MAC地址" class="headerlink" title="7.1 MAC地址"></a>7.1 MAC地址</h2><blockquote><ul><li>使用点对点信道的数据链路层不需要使用地址就可以通信，因此连接在信道上的主机只有它们两个。</li><li>使用共享信道的总线型局域网：使用广播信道的数据链路层必须使用地址来区分各主机。</li></ul></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014223659993.png" alt="image-20201014223659993"></p><h3 id="7-1-1-广播信道的数据链路层必须使用地址（MAC）"><a href="#7-1-1-广播信道的数据链路层必须使用地址（MAC）" class="headerlink" title="7.1.1 广播信道的数据链路层必须使用地址（MAC）"></a>7.1.1 广播信道的数据链路层必须使用地址（MAC）</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014224732019.png" alt="image-20201014224732019"></p><blockquote><p>📌插入一些基础知识</p><p><code>网络适配器</code>（network adapter）是一种硬件设备，也称为网络接口卡（Network Interface Card，NIC），它允许计算机与网络通信。它通常安装在计算机的主板上，通过网络线连接到计算机和网络之间。网络适配器的主要作用是将计算机的数字信号转换为能够在网络中传输的模拟信号，同时也将从网络中接收到的模拟信号转换为计算机可以理解的数字信号。网络适配器可以支持不同的网络协议和速率，如以太网、无线局域网等。</p><p><code>交换机</code>（Switch）是一种网络设备，用于在局域网（LAN）中传输数据。交换机可以将传入的数据包解析并将其发送到目标设备，从而实现网络设备之间的通信。交换机可以提供多个端口，以支持多个设备连接到同一个局域网。交换机通常使用MAC地址（Media Access Control Address）来识别和转发数据包。它是现代计算机网络的关键设备之一，被广泛应用于企业、学校、医院、政府机构等组织的局域网中。</p><p><code>交换机</code>通常是一种类似于路由器的设备，它通常是一个方形的盒子，有多个端口和指示灯。交换机的外观和大小可以因厂商和型号而异，但大多数交换机都具有类似的特征。它们通常被安装在机房或网络中心的机柜中，以便管理和维护。</p><p><code>路由器</code>通常是一个小盒子，上面有一些连接口，如WAN口、LAN口、USB口、电源口等。它的外形和大小因品牌和型号而异。路由器是一种网络设备，用于将多个网络连接在一起，并在它们之间转发数据包，使得数据能够在不同的网络之间传递。</p><p><code>交换机</code>和<code>路由器</code>是不同的设备。交换机主要用于在局域网内连接多个设备，实现设备之间的通信。而路由器则是用于连接不同网络之间的设备，实现不同网络之间的通信（广域网必须使用）。交换机和路由器的外形和功能也有所不同。交换机通常只有局域网的接口，而路由器则有连接不同网络的接口。</p><p><code>广播地址【单播对立】</code>是指一个网络中所有主机都能接收到广播消息的地址。在IPv4网络中，广播地址通常是该网络的最后一个IP地址，例如，对于192.168.1.0/24网络，广播地址为192.168.1.255。在IPv6网络中，广播地址使用特殊的地址格式，例如，对于fe80::/10网络，广播地址为ff02::1。<em>广播地址通常用于向网络中的所有主机发送消息</em>，例如DHCP请求、ARP请求等。</p><p><strong>MAC地址又称为硬件地址或物理地址</strong>。请注意：不要被 “物理” 二字误导认为物理地址属于物理层范畴，物理地址属于数据链路层范畴</p></blockquote><h3 id="7-1-2-IEEE-802局域网的MAC地址格式"><a href="#7-1-2-IEEE-802局域网的MAC地址格式" class="headerlink" title="7.1.2 IEEE 802局域网的MAC地址格式"></a>7.1.2 IEEE 802局域网的MAC地址格式</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014225358570.png" alt="image-20201014225358570"></p><blockquote><p><strong>组织唯一标识符OUI</strong></p><ul><li>生产网络设备的厂商，需要向IEEE的注册管理机构申请一个或多个OUI</li></ul><p><strong>网络接口标识符</strong></p><ul><li>由获得OUI的厂商自行随意分配</li></ul><p><strong>EUI-48</strong></p><ul><li>48是这个MAC地址的位数</li></ul></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014230248959.png" alt="image-20201014230248959"></p><blockquote><p>对于使用EUI-48空间的应用程序，IEEE的目标寿命为100年（直到2080年），但是鼓励采用EUI-64作为替代</p></blockquote><p><strong>关于无效的 MAC 帧</strong></p><ul><li>数据字段的长度与长度字段的值不一致；</li><li>帧的长度不是整数个字节；</li><li>用收到的帧检验序列 FCS 查出有差错；</li><li>数据字段的长度不在 46 ~ 1500 字节之间。</li><li>有效的 MAC 帧长度为 64 ~ 1518 字节之间。</li></ul><blockquote><p><strong>对于检查出的无效</strong> <strong>MAC</strong> <strong>帧就简单地丢弃。以太网不负责重传丢弃的帧。</strong></p></blockquote><h3 id="7-1-3-IEEE-802局域网的MAC地址发送顺序"><a href="#7-1-3-IEEE-802局域网的MAC地址发送顺序" class="headerlink" title="7.1.3 IEEE 802局域网的MAC地址发送顺序"></a>7.1.3 IEEE 802局域网的MAC地址发送顺序</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014230625182.png" alt="image-20201014230625182"></p><h3 id="7-1-4-单播MAC地址举例"><a href="#7-1-4-单播MAC地址举例" class="headerlink" title="7.1.4 单播MAC地址举例"></a>7.1.4 单播MAC地址举例</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014230822305.png" alt="image-20201014230822305"></p><blockquote><p>主机B给主机C发送<strong>单播帧</strong>，主机B首先要构建该<strong>单播帧</strong>，<strong>在帧首部中的目的地址字段填入主机C的MAC地址</strong>，源地址字段填入自己的MAC地址，再加上帧首部的其他字段、数据载荷以及帧尾部，就构成了该<strong>单播帧</strong></p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014231244655.png" alt="image-20201014231244655"></p><blockquote><p>主机B将该<strong>单播帧</strong>发送出去，主机A和C都会收到该<strong>单播帧</strong></p><p>主机A的网卡发现该<strong>单播帧</strong>的目的MAC地址与自己的MAC地址不匹配，丢弃该帧</p><p>主机C的网卡发现该<strong>单播帧</strong>的目的MAC地址与自己的MAC地址匹配，接受该帧,并将该帧交给其上层处理。</p></blockquote><h3 id="7-1-5-广播MAC地址举例"><a href="#7-1-5-广播MAC地址举例" class="headerlink" title="7.1.5 广播MAC地址举例"></a>7.1.5 广播MAC地址举例</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014231754669.png" alt="image-20201014231754669"></p><blockquote><p>假设主机B要发送一个<strong>广播帧</strong>，主机B首先要构建该<strong>广播帧</strong>，<strong>在帧首部中的目的地址字段填入广播地址</strong>，也就是十六进制的全F，源地址字段填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该<strong>广播帧</strong></p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014232132424.png" alt="image-20201014232132424"></p><blockquote><p>主机B讲该<strong>广播帧</strong>发送出去，主机A和C都会收到该<strong>广播帧</strong>，<strong>发现该帧首部中的目的地址字段的内容是广播地址</strong>，就知道该帧是<strong>广播帧</strong>，主机A和主机C都接受该帧，并将该帧交给上层处理</p></blockquote><h3 id="7-1-6-多播MAC地址举例"><a href="#7-1-6-多播MAC地址举例" class="headerlink" title="7.1.6 多播MAC地址举例"></a>7.1.6 多播MAC地址举例</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201014232714791.png" alt="image-20201014232714791"></p><blockquote><p>假设主机A要发送<strong>多播帧</strong>给该<strong>多播地址</strong>。将该<strong>多播地址</strong>的左起第一个字节写成8个比特，第一个字节的最低比特位是1，这就表明该地址是<strong>多播地址</strong>。</p><p>📢快速判断地址是不是<strong>多播地址</strong>，就是上图所示箭头所指的第十六进制数不能整除2（1,3,5,7,9,B,D,F），则该地址是<strong>多播地址</strong></p><p>假设主机B，C和D支持多播，各用户给自己的主机配置多播组列表<strong>如下所示</strong></p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015001243584.png" alt="image-20201015001243584"></p><blockquote><p>主机B属于两个多播组，主机C也属于两个多播组，而主机D不属于任何多播组</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015001535528.png" alt="image-20201015001535528"></p><blockquote><p>主机A首先要构建该<strong>多播帧</strong>，<strong>在帧首部中的目的地址字段填入该多播地址</strong>，源地址点填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该<strong>多播帧</strong></p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015002054876.png" alt="image-20201015002054876"></p><blockquote><p>主机A将该<strong>多播帧</strong>发送出去，主机B、C、D都会收到该<strong>多播帧</strong></p><p><strong>主机B和C发现该多播帧的目的MAC地址在自己的多播组列表中</strong>，主机B和C都会接受该帧，并送交上层处理。</p><p>主机D发现该<strong>多播帧</strong>的目的MAC地址不在自己得多播组列表中，则丢弃该<strong>多播帧</strong></p><p>给主机配置多播组列表进行私有应用时，不得使用公有的标准多播地址</p></blockquote><h3 id="7-1-7-关于随机MAC地址"><a href="#7-1-7-关于随机MAC地址" class="headerlink" title="7.1.7 关于随机MAC地址"></a>7.1.7 关于随机MAC地址</h3><p>​        据斯诺登爆料，美国国家安全局有一套系统，通过监视电子设备的MAC地址，来跟踪城市中每个人的行动。因此开始出现提供随机MAC地址功能的公司。目前大多数移动设备已经采用了随机MAC地址技术。</p><h2 id="7-2-IP地址"><a href="#7-2-IP地址" class="headerlink" title="7.2 IP地址"></a>7.2 IP地址</h2><p>IP地址属于网络层的范畴，不属于数据链路层的范畴=</p><p>下面内容讲的是IP地址的使用，详细的IP地址内容在网络层中介绍</p><h3 id="7-2-1-基本概念"><a href="#7-2-1-基本概念" class="headerlink" title="7.2.1 基本概念"></a>7.2.1 基本概念</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015104441580.png" alt="image-20201015104441580"></p><h3 id="7-2-2-从网络体系结构看IP地址与MAC地址"><a href="#7-2-2-从网络体系结构看IP地址与MAC地址" class="headerlink" title="7.2.2 从网络体系结构看IP地址与MAC地址"></a>7.2.2 从网络体系结构看IP地址与MAC地址</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015104913755.png" alt="image-20201015104913755"></p><h3 id="7-2-3-数据包转发过程中IP地址与MAC地址的变化情况"><a href="#7-2-3-数据包转发过程中IP地址与MAC地址的变化情况" class="headerlink" title="7.2.3 数据包转发过程中IP地址与MAC地址的变化情况"></a>7.2.3 数据包转发过程中IP地址与MAC地址的变化情况</h3><p>图上各主机和路由器各接口的IP地址和MAC地址用简单的标识符来表示</p><p>假设主机H1要给主机H2发送一个数据包；</p><p>注意📢：主机中有完整的网络体系结构，而路由器的最高层为网络层。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015105455043.png" alt="image-20201015105455043"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20210103212224961.png" alt="image-20210103212224961"></p><blockquote><p>如何从IP地址找出其对应的MAC地址？</p><p>ARP协议</p></blockquote><h2 id="7-3-ARP协议"><a href="#7-3-ARP协议" class="headerlink" title="7.3 ARP协议"></a>7.3 ARP协议</h2><p>如何从IP地址找出其对应的MAC地址？</p><p>ARP（地址解析协议）</p><h3 id="7-3-1-流程"><a href="#7-3-1-流程" class="headerlink" title="7.3.1 流程"></a>7.3.1 流程</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015113826197.png" alt="image-20201015113826197"></p><p>ARP高速缓存表</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015114052206.png" alt="image-20201015114052206"></p><blockquote><p>当主机B要给主机C发送数据包时，会首先在自己的ARP高速缓存表中查找主机C的IP地址所对应的MAC地址，但未找到，因此，主机B需要发送ARP请求报文，来获取主机C的MAC地址</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015114444263.png" alt="image-20201015114444263"></p><blockquote><p>ARP请求报文有具体的格式，上图的只是简单描述</p><p>ARP请求报文被封装在MAC帧中发送，目的地址为广播地址</p><p>主机B发送封装有ARP请求报文的广播帧，总线上的其他主机都能收到该广播帧</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015114811501.png" alt="image-20201015114811501"></p><blockquote><p>收到ARP请求报文的主机A和主机C会把ARP请求报文交给上层的ARP进程</p><p>主机A发现所询问的IP地址不是自己的IP地址，因此不用理会</p><p>主机C的发现所询问的IP地址是自己的IP地址，需要进行相应处理，如上图所示</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015115212170.png" alt="image-20201015115212170"></p><blockquote><p>主机C给主机B发送封装有ARP响应报文的单播帧，总线上的其他主机都能收到该单播帧。主机A的网卡收到该单播帧后，发现其目的MAC地址与自己的MAC地址不匹配，直接丢弃该帧。</p><p>主控B的网卡收到该单播帧后，发现其目的MAC地址就是自己的MAC地址，将其交付上层处理。上层的ARP进程解析ARP响应报文，将其所包含的主机C的ip地址与MAC地址记录到自己的ARP高速缓存表中。</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015115236673.png" alt="image-20201015115236673"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015115252972.png" alt="image-20201015115252972"></p><p><strong>动态与静态的区别</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015115831543.png" alt="image-20201015115831543"></p><p><strong>ARP协议只能在一段链路或一个网络上使用，而不能跨网络使用</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015120108028.png" alt="image-20201015120108028"></p><blockquote><p>ARP协议的使用是逐段链路进行的</p></blockquote><h3 id="7-3-2-总结"><a href="#7-3-2-总结" class="headerlink" title="7.3.2 总结"></a>7.3.2 总结</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015120707150.png" alt="image-20201015120707150"></p><blockquote><p>ARP表中的IP地址与MAC地址的对应关系记录，是<strong>会定期自动删除的</strong>，<strong>因为IP地址与MAC地址的对应关系不是永久性的</strong></p></blockquote><hr><h1 id="八、集线器与交换机的区别"><a href="#八、集线器与交换机的区别" class="headerlink" title="八、集线器与交换机的区别"></a>八、集线器与交换机的区别</h1><h2 id="8-1-集线器-在物理层扩展以太网"><a href="#8-1-集线器-在物理层扩展以太网" class="headerlink" title="8.1 集线器-在物理层扩展以太网"></a>8.1 集线器-在物理层扩展以太网</h2><h3 id="8-1-1-概念"><a href="#8-1-1-概念" class="headerlink" title="8.1.1 概念"></a>8.1.1 概念</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015144628691.png" alt="image-20201015144628691"></p><blockquote><ul><li>传统以太网最初是使用粗同轴电缆，后来演进到使用比较便宜的细同轴电缆，最后发展为使用更便宜和更灵活的双绞线。</li><li>采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做<strong>集线器</strong> (hub)。</li><li><strong>集线器</strong>是也可以看做多口中继器，每个端口都可以成为一个中继器，中继器是对减弱的信号进行放大和发送的设备。</li><li><strong>集线器</strong>的以太网在逻辑上仍是个<code>总线</code>网，需要使用CSMA/CD协议来协调各主机争用总线，只能工作在半双工模式，收发帧不能同时进行。</li></ul></blockquote><h3 id="8-1-2-集线器HUB在物理层扩展以太网"><a href="#8-1-2-集线器HUB在物理层扩展以太网" class="headerlink" title="8.1.2 集线器HUB在物理层扩展以太网"></a>8.1.2 集线器HUB在物理层扩展以太网</h3><p>集线器只工作在物理层。</p><p><strong>使用集线器扩展</strong>：将多个以太网段连成更大的、多级星形结构的以太网</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015145732275.png" alt="image-20201015145732275"></p><blockquote><ul><li><strong>优点</strong><ol><li>使原来属于不同碰撞域的以太网上的计算机能够进行跨碰撞域的通信。</li><li>扩大了以太网覆盖的地理范围。</li></ol></li><li><strong>缺点</strong><ol><li>碰撞域增大了，但总的吞吐量并未提高。</li><li>如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。</li></ol></li></ul></blockquote><p><strong>碰撞域</strong></p><ul><li><strong>碰撞域（collision domain）</strong>又称为<strong>冲突域</strong>，是指网络中一个站点发出的帧会与其他站点发出的帧产生碰撞或冲突的那部分网络。</li><li>碰撞域越大，发生碰撞的概率越高。</li></ul><h2 id="8-2-以太网交换机-在数据链路层扩展以太网"><a href="#8-2-以太网交换机-在数据链路层扩展以太网" class="headerlink" title="8.2 以太网交换机-在数据链路层扩展以太网"></a>8.2 以太网交换机-在数据链路层扩展以太网</h2><h3 id="8-2-1-概念"><a href="#8-2-1-概念" class="headerlink" title="8.2.1 概念"></a>8.2.1 概念</h3><ul><li>扩展以太网更常用的方法是在数据链路层进行。</li><li>早期使用<strong>网桥</strong>，现在使用<strong>以太网交换机</strong>。</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015150620067.png" alt="image-20201015150620067"></p><blockquote><p><strong>网桥</strong></p><ul><li>网桥工作在数据链路层。</li><li>它根据 MAC 帧的目的地址对收到的帧进行转发和过滤。</li><li>当网桥收到一个帧时，并不是向所有的接口转发此帧，而是先检查此帧的目的MAC 地址，然后再确定将该帧转发到哪一个接口，或把它丢弃。</li></ul><p><strong>交换机</strong></p><ul><li>1990 年问世的交换式集线器 (switching hub) 可明显地提高以太网的性能。</li><li>交换式集线器常称为<strong>以太网交换机</strong> (switch) 或<strong>第二层交换机</strong> (L2 switch)，强调这种交换机工作在数据链路层。</li><li>以太网交换机实质上就是一个<strong>多接口的网桥</strong></li></ul></blockquote><h3 id="8-2-2-集线器HUB与交换机SWITCH区别"><a href="#8-2-2-集线器HUB与交换机SWITCH区别" class="headerlink" title="8.2.2 集线器HUB与交换机SWITCH区别"></a><strong>8.2.2 集线器HUB与交换机SWITCH区别</strong></h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015152232158.png" alt="image-20201015152232158"></p><blockquote><p>使用<strong>集线器</strong>互连而成的共享总线式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧会通过共享总线传输到<strong>总线上的其他各个主机</strong></p><p>使用交换机互连而成的交换式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧进入交换机后，交换机会将该单播帧转发给目的主机，<strong>而不是网络中的其他各个主机</strong></p><p><strong>这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了</strong></p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015152858146.png" alt="image-20201015152858146"></p><blockquote><p>全工作方式：发送帧和接收帧可以同时进行。</p><p><strong>以太网交换机的交换方式</strong></p><ul><li>存储转发方式<ul><li>把整个数据帧<strong>先缓存</strong>后再进行处理。</li></ul></li><li>直通 (cut-through) 方式<ul><li>接收数据帧的同时就<strong>立即按数据帧的目的 MAC 地址决定该帧的转发接口</strong>，因而提高了帧的转发速度。</li><li><strong>缺点</strong>是它不检查差错就直接将帧转发出去，因此有可能也将一些无效帧转发给其他的站。</li></ul></li></ul><p><strong>这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了</strong></p></blockquote><p><strong>对比集线器和交换机</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015153907268.png" alt="image-20201015153907268"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015154523036.png" alt="image-20201015154523036"></p><blockquote><p>多台主机同时给另一台主机发送单播帧</p><p>集线器以太网：会产生碰撞，遭遇碰撞的帧会传播到总线上的各主机</p><p>交换机以太网：会将它们缓存起来，然后逐个转发给目的主机，不会产生碰撞</p><p><strong>这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了</strong></p></blockquote><p><strong>集线器扩展以太网和交换机扩展以太网区别</strong></p><p><strong>单播</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015155408692.png" alt="image-20201015155408692"></p><p><strong>广播</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015155440402.png" alt="image-20201015155440402"></p><p><strong>多个单播</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015155526386.png" alt="image-20201015155526386"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015155706698.png" alt="image-20201015155706698"></p><p>广播域（broadcast domain）：指这样一部分网络，其中任何一台设备发出的广播通信都能被该部分网络中的所有其他设备所接收。</p><h2 id="8-3-总结"><a href="#8-3-总结" class="headerlink" title="8.3 总结"></a>8.3 总结</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015160146482.png" alt="image-20201015160146482"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015160526999.png" alt="image-20201015160526999"></p><blockquote><p>工作在数据链路层的以太网交换机，其性能远远超过工作在物理层的集线器，而且价格并不贵，这就使得集线器逐渐被市场淘汰</p></blockquote><hr><h1 id="九、以太网交换机自学习和转发帧的流程"><a href="#九、以太网交换机自学习和转发帧的流程" class="headerlink" title="九、以太网交换机自学习和转发帧的流程"></a>九、以太网交换机自学习和转发帧的流程</h1><h2 id="9-1-概念"><a href="#9-1-概念" class="headerlink" title="9.1 概念"></a>9.1 概念</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015161015165.png" alt="image-20201015161015165"></p><h2 id="9-2-自学习和转发帧的例子"><a href="#9-2-自学习和转发帧的例子" class="headerlink" title="9.2 自学习和转发帧的例子"></a>9.2 自学习和转发帧的例子</h2><p>以下例子假设各主机知道网络中其他各主机的MAC地址（无需首先进行ARP来获取目的主机的MAC地址）</p><p><strong><span style="color:red;">A -&gt; B</span></strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015161458528.png" alt="image-20201015161458528"></p><blockquote><ol><li>A 先向 B 发送一帧。该帧从接口 1 进入到交换机</li><li>交换机收到帧后，先查找（图中左边）交换表。没有查到应从哪个接口转发这个帧给 B</li><li>交换机把这个帧的源地址 A 和接口 1 写入（图中左边）交换表中</li><li>交换机向除接口 1 以外的所有的接口广播这个帧</li><li>接口 4到接口 2，先查找（图中右边）交换表。没有查到应从哪个接口转发这个帧给 B</li><li>交换机把这个帧的源地址 A 和接口 1 写入（图中右边）交换表中</li><li>除B主机之外与该帧的目的地址不相符，将丢弃该帧</li><li>主机B发现是给自己的帧，接受该帧</li></ol></blockquote><p><strong><span style="color:red;">B-&gt; A</span></strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015162310922.png" alt="image-20201015162310922"></p><blockquote><ol><li>B 向 A 发送一帧。该帧从接口 3 进入到交换机</li><li>交换机收到帧后，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。</li><li>主机 A 发现目的地址是它，就接受该帧</li><li>交换机把这个帧的源地址 B 和接口 3 写入（图中左边）交换表中</li></ol></blockquote><p><strong><span style="color:red;">E -&gt; A</span></strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015162622462.png" alt="image-20201015162622462"></p><blockquote><ol><li>E 向 A发送一帧</li><li>交换机收到帧后，先查找（图中右边）交换表。发现（图中右边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口2转发出去。于是就把这个帧传送到接口 2 转发给 接口 4。</li><li>交换机把这个帧的源地址 E 和接口 3 写入（图中右边）交换表中</li><li>接口 4 到 左边的交换机，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。</li><li>交换机把这个帧的源地址 E 和接口 4 写入（图中左边）交换表中</li><li>主机 A 发现目的地址是它，就接受该帧</li></ol></blockquote><p><strong><span style="color:red;">G -&gt; A</span></strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015163157140.png" alt="image-20201015163157140"></p><blockquote><p>主机 A、主机 G、交换机 1的接口 1就共享同一条总线（相当于总线式网络，可以想象成用集线器连接了）</p><ol><li>主机 G 发送给 主机 A 一个帧</li><li>主机 A 和 交换机接口 1都能接收到</li><li>主机 A 的网卡收到后，根据帧的目的MAC地址A，就知道是发送给自己的帧，就接受该帧</li><li>交换机 1收到该帧后，首先进行登记工作</li><li>然后交换机 1对该帧进行转发，该帧的MAC地址是A，在（图中左边）交换表查找MAC 地址有 A</li><li>MAC 地址为 A的接口号是1，但是该帧正是从接口 1 进入交换机的，交换机不会再从该接口 1 将帧转发出去，因为这是没有必要，于是丢弃该帧</li></ol></blockquote><p>随着网络中各主机都发送了帧后，网络中的各交换机就可以学习到各主机的MAC地址，以及它们与自己各接口的对应关系</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015164210543.png" alt="image-20201015164210543"></p><blockquote><p>考虑到可能有时要在交换机的接口更换主机，或者主机要更换其网络适配器，这就需要更改交换表中的项目。为此，在交换表中每个项目都设有一定的<strong>有效时间</strong>。<strong>过期的项目就自动被删除</strong>。</p><p><strong>以太网交换机的这种自学习方法使得以太网交换机能够即插即用，不必人工进行配置，因此非常方便。</strong></p></blockquote><h2 id="9-3-总结"><a href="#9-3-总结" class="headerlink" title="9.3 总结"></a>9.3 总结</h2><p><strong>交换机自学习和转发帧的步骤归纳</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015170656500.png" alt="image-20201015170656500"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015170739679.png" alt="image-20201015170739679"></p><hr><h1 id="十、以太网交换机的生成树协议STP"><a href="#十、以太网交换机的生成树协议STP" class="headerlink" title="十、以太网交换机的生成树协议STP"></a>十、以太网交换机的生成树协议STP</h1><h2 id="10-1-如何提高以太网的可靠性"><a href="#10-1-如何提高以太网的可靠性" class="headerlink" title="10.1 如何提高以太网的可靠性"></a>10.1 如何提高以太网的可靠性</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015171453001.png" alt="image-20201015171453001"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015171515481.png" alt="image-20201015171515481"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015171900775.png" alt="image-20201015171900775"></p><p><strong>广播风暴📌</strong></p><blockquote><p>如图所示，例如广播从H1主机发送出去，经过一系列的转发，该广播帧将在各交换机之间反复转发，分别按顺时针和逆时针方向同时兜圈，这就是所谓的广播风暴！</p></blockquote><h2 id="10-2-生成树协议STP"><a href="#10-2-生成树协议STP" class="headerlink" title="10.2 生成树协议STP"></a>10.2 生成树协议STP</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015172204419.png" alt="image-20201015172204419"></p><blockquote><ul><li>IEEE 802.1D 标准制定了一个<strong>生成树协议 STP</strong>  (Spanning Tree Protocol)。</li><li>其<strong>要点</strong>是：<strong>不改变</strong>网络的实际拓扑，但在逻辑上则切断某些链路，使得从一台主机到所有其他主机的路径是<strong>无环路的树状结构</strong>，从而消除了兜圈子现象。</li></ul></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015202257756.png" alt="image-20201015202257756"></p><hr><h1 id="十一、虚拟局域网VLAN"><a href="#十一、虚拟局域网VLAN" class="headerlink" title="十一、虚拟局域网VLAN"></a>十一、虚拟局域网VLAN</h1><h2 id="11-1-为什么要虚拟局域网VLAN"><a href="#11-1-为什么要虚拟局域网VLAN" class="headerlink" title="11.1 为什么要虚拟局域网VLAN"></a>11.1 为什么要虚拟局域网VLAN</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015202859124.png" alt="image-20201015202859124"></p><p><strong>分割广播域的方法</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015203113654.png" alt="image-20201015203113654"></p><blockquote><p>📌路由器默认情况下不对广播数据包进行转发。</p><p>为了分割广播域，所以虚拟局域网VLAN技术应运而生</p></blockquote><h2 id="11-2-概念"><a href="#11-2-概念" class="headerlink" title="11.2 概念"></a>11.2 概念</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015203559548.png" alt="image-20201015203559548"></p><blockquote><ul><li>利用以太网交换机可以很方便地实现虚拟局域网 VLAN (Virtual LAN)。</li><li>IEEE 802.1Q 对虚拟局域网 VLAN 的<strong>定义</strong>：<br><strong>虚拟局域网 VLAN</strong> 是由一些局域网网段构成的<strong>与物理位置无关的逻辑组</strong>，而这些网段具有某些共同的需求。每一个 VLAN 的帧都有一个明确的标识符，指明发送这个帧的计算机是属于哪一个 VLAN。</li><li>同一个VLAN内部可以广播通信，不同VLAN不可以广播通信</li><li><strong>虚拟局域网其实只是局域网给用户提供的一种服务，而并不是一种新型局域网。</strong></li><li>由于虚拟局域网是用户和网络资源的逻辑组合，因此可按照需要将有关设备和资源非常方便地重新组合，使用户从不同的服务器或数据库中存取所需的资源。</li></ul></blockquote><h2 id="11-3-虚拟局域网VLAN的实现机制"><a href="#11-3-虚拟局域网VLAN的实现机制" class="headerlink" title="11.3 虚拟局域网VLAN的实现机制"></a>11.3 虚拟局域网VLAN的实现机制</h2><p>虚拟局域网VLAN技术是在交换机上实现的，需要交换机能够实现以下功能</p><ul><li>能够处理带有VLAN标记的帧——IEEE 802.1 Q帧</li><li>交换机的各端口可以支持不同的端口类型，不同端口类型的端口对帧的处理方式有所不同</li></ul><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015204639599.png" alt="image-20201015204639599"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015204749141.png" alt="image-20201015204749141"></p><h4 id="Access端口"><a href="#Access端口" class="headerlink" title="Access端口"></a>Access端口</h4><p>交换机与用户计算机之间的互连</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015205311757.png" alt="image-20201015205311757"></p><blockquote><p>同一个VLAN内部可以广播通信，不同VLAN不可以广播通信</p></blockquote><p><strong>举个例子🤓</strong></p><blockquote><p>如图，主机A发送一个广播帧，该帧从交换机的端口1进入交换机，由于端口1的类型为Access，它会对接受到的“未打标签”的普通以太网MAC帧“打标签”，也就是插入4字节的VLAN标记字段。如图所示，由于端口1的PVID值等于2，因此，所插入的4字节VLAN标记字段中的VID的值也等于2。广播帧中的VID的取值与端口2的PVID取值都等于2，因此，交换机会从端口2对帧进行“去标签”转发，但其他端口不会。</p></blockquote><h4 id="Truck端口"><a href="#Truck端口" class="headerlink" title="Truck端口"></a>Truck端口</h4><p>交换机之间或交换机与路由器之间的互连</p><blockquote><p>发送还有：Trunk端口对VID不等于PVID的帧是直接转发的！！！</p><p>接受还有：接手“已打标签的帧”，直接进行发送</p></blockquote><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015205947636.png" alt="image-20201015205947636"></p><p><strong><span style="color:red;">        通过本例可以看出：在由多个交换机互连而成的交换式以太网中划分VLAN时，连接主机的交换机端口应设置为<code>Access</code>类型，交换机之间互连的端口应设置为<code>Trunk</code>类型。</span></strong></p><p><strong>小例题</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015210417695.png" alt="image-20201015210417695"></p><h4 id="华为交换机私有的Hybrid端口类型"><a href="#华为交换机私有的Hybrid端口类型" class="headerlink" title="华为交换机私有的Hybrid端口类型"></a>华为交换机私有的Hybrid端口类型</h4><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015211031361.png" alt="image-20201015211031361"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015211349531.png" alt="image-20201015211349531"></p><h2 id="11-4-总结"><a href="#11-4-总结" class="headerlink" title="11.4 总结"></a>11.4 总结</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/计算机网络第三章（数据链路层）.assets/image-20201015211512622.png" alt="image-20201015211512622"></p><blockquote><p><strong>虚拟局域网优点</strong></p><p>虚拟局域网（VLAN）技术具有以下主要优点：</p><ol><li>改善了性能</li><li>简化了管理</li><li>降低了成本</li><li>改善了安全性 </li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>操作系统基本概念练习题</title>
      <link href="/2023/04/05/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%BB%83%E4%B9%A0%E9%A2%98/"/>
      <url>/2023/04/05/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%BB%83%E4%B9%A0%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p class='p center logo huge black'>操作系统</p><div class="note success flat"><p>本习题是XOSCATSX的基本概念测试题，记录下来方便及时复习！！！</p></div><details class="folding-tag" blue><summary> 基本概念_ALL </summary>              <div class='content'>              <ol><li>操作系统的设计目标：<code>方便性、有效性、可扩充性、开放性</code>。</li><li>操作系统的基本特征：<code>并发性、共享性、虚拟性、异步性</code>。</li><li>操作系统的基本类型：<code>批处理系统、分时系统、实时系统</code>。</li><li>进程三种基本状态：<code>就绪状态、执行状态、阻塞状态</code>。</li><li>操作系统的主要功能：<code>处理机管理、存储器管理、设备管理、文件管理、用户接口</code>。</li><li>进程同步机制应遵循的准则：<code>空闲让进、忙则等待、有限等待、让权等带</code>。</li><li>产生进程死锁的必要条件：<code>互斥条件、请求和保持条件、不剥夺条件、环路等待条件</code>。</li><li>设备分配中的主要数据结构（英文缩写）：<code>DCT、COCT、CHCT、SDT</code>。</li><li>目录管理的主要功能：<code>按名存取、提高检索速度、文件共享、允许文件重名</code>。</li><li>目前比较流行的操作系统（实例）：<code>Windows、UNIX、Linux</code>。</li><li>影响文件安全性的主要因素：<code>人为因素、系统因素、自然因素</code>。</li><li>进程、文件、线程在系统中存在的唯一标志（英文缩写）：<code>PCB、FCB、TCB</code>。</li><li>抢占式进程调度方式基于的主要原则：<code>优先权原则、短进程优先原则、时间片原则</code>。</li><li>1968年，Peter J. Denning 指出程序执行时呈现出：<code>时间局限性、空间局限性</code>。</li><li>请求分页系统的主要硬件支持：<code>请求页表机制、缺页中断机构、地址变换机构</code>。</li><li>通常采用解除死锁的两种方法：<code>剥夺资源、撤消进程</code>。</li><li>在操作系统中，实现进程同步的机制：<code>信号量机制、管程机制</code>。</li><li>解决<code>通道</code>“瓶颈”问题最有效的方法是增加设备到主机间的<code>通路</code>。</li><li>产生进程死锁的原因可归结为两点：<code>竞争资源、进程间推进顺序非法</code>。</li><li>目前，常用的文件（外存）分配方法：<code>连续分配、链接分配、索引分配</code>。</li><li>按设备的固有属性分类，将I/O设备分为：<code>独占设备、共享设备、虚拟设备</code>。</li><li>为了实现设备的独立性，系统必须设置（英文缩写）：<code>LUT</code>。</li><li>操作系统的用户接口：<code>命令接口、程序接口、图形用户接口</code>。</li><li>UNIX系统最本质的特征（英文缩写）：<code>OSI</code>。</li><li><code>UNIX</code>系统的内核结构可分成两大部分：<code>进程控制子系统、文件子系统</code>。</li><li>将一台物理I/O设备虚拟为多台逻辑I/O设备的技术：<code>SPOOLing</code>。</li><li>虚拟存储器的理论依据：<code>局部性原理</code>。</li><li>保证文件系统安全性的主要措施：<code>存取控制、容错技术、后备系统</code>。</li><li><code>并发性</code>和<code>共享性</code>是多用户、多任务操作系统两个最基本的特征。</li><li><code>并发性</code>是多用户、多任务操作系统最重要的特征。</li><li>通过建立<code>后备系统</code>，防止由<code>自然因素</code>所造成的文件系统的不安全性。</li><li>通过<code>存取控制</code>机制，防止由<code>人为因素</code>所造成的文件系统的不安全性。</li><li>通过采取<code>容错技术</code>，防止由<code>系统因素</code>所造成的文件系统的不安全性。</li><li>方便性<code>和</code>有效性`是操作系统设计中最重要的两个目标。</li><li>1990年后，<code>开放性</code>已成为新系统或软件能否被广泛应用的至关重要的因素。</li><li>在操作系统基本类型中，可靠性是<code>实时系统</code>最重要的特征。</li><li>在<code>局部性原理</code>中，产生<code>时间局限性</code>的典型原因是在程序中存在着大量的循环操作。</li><li>在<code>局部性原理</code>中，产生<code>空间局限性</code>的典型情况是程序的顺序执行。</li><li>在死锁的条件中，<code>不剥夺条件</code>是指进程已获得的资源只能在使用完时由自己释放。</li><li>在死锁的条件中，<code>互斥条件</code>是指在一段时间内，某资源只能被一个进程占用。</li><li>进程所请求的一次I/O完成后,将使进程状态从<code>阻塞状态</code>变为<code>就绪状态</code>。</li><li>操作系统中处于<code>执行状态</code>的进程时间片用完后,进程状态将转变为<code>就绪状态</code>。</li><li>操作系统中处于<code>执行状态</code>的进程提出I/O请求后,进程状态将转变为<code>阻塞状态</code>。</li><li>在文件系统中，文件属性信息存储在数据结构(英文缩写)<code>FCB</code>中。</li><li>在操作系统接口中，<code>程序接口</code>亦称为<code>系统调用</code>。</li><li>操作系统利用数据结构（英文缩写）<code>PCB</code>描述进程的基本情况和活动过程。</li><li>资源的按序分配法是摒弃死锁条件中的<code>环路等待条件</code>来预防死锁的发生。</li><li>现代操作系统产生死锁的条件中，<code>互斥条件</code>是不能被摒弃来预防死锁的发生。</li><li>系统将被中断进程的CPU现场信息保存在该进程的数据结构(英文缩写)<code>PCB</code>中。</li><li>在OS基本特征中，<code>异步性</code>是指进程是以人们不可预知的速度向前推进的。</li><li>1965年，荷兰学者Dijkstra提出的<code>信号量机制</code>是一种卓有成效的进程同步工具。</li><li>同步机制准则中，<code>让权等待</code>是指当进程不能进入自己的临界区时，应立即释放处理机。</li><li>进程三种基本状态中，<code>就绪状态</code>是指进程已分配到除CPU以外的所有必要资源。</li><li>在索引节点中设置链接引用(links)计数的目的是为了实现目录管理的<code>文件共享</code>功能。</li><li>按设备的固有属性分类中，<code>独占设备</code>属于临界资源，即进程临界区访问的资源。</li><li>按设备的固有属性分类中，典型的<code>独占设备</code>有打印机、磁带机等。</li><li>按设备的固有属性分类中，典型的<code>共享设备</code>有磁盘、光盘等。</li><li>在假脱机打印机系统中，按设备的固有属性分类，是将<code>独占设备</code>改造为<code>共享设备</code>。</li><li>在假脱机打印机系统中，按设备的固有属性分类，实现了<code>虚拟设备</code>功能。</li><li><code>SPOOLing</code>技术是对脱机I/O系统的模拟，或称为假脱机技术。</li><li>在请求分页系统的硬件支持中，当所要访问的页面不在内存时，由<code>缺页中断机构</code>实现。</li><li>在请求分页系统的硬件支持中，页面置换算法需要应用<code>请求页表机制</code>实现。</li><li>在设备分配中，用于记录每一个设备情况的数据结构(英文缩写)：<code>DCT</code>。</li><li>在设备分配中，用于记录全部设备情况的数据结构(英文缩写)：<code>SDT</code>。</li><li>在设备分配中，用于记录每一个控制器情况的数据结构(英文缩写)：<code>COCT</code>。</li><li>实现“<code>按名存取</code>”是文件系统目录管理中最基本的功能。</li><li>实现“<code>按名存取</code>”是文件系统向用户提供的最基本的服务。</li></ol>              </div>            </details><details class="folding-tag" red><summary> 基本概念_ALL_Practice </summary>              <div class='content'>              <ol><li>操作系统的设计目标：<psw>方便性、有效性、可扩充性、开放性</psw>。</li><li>操作系统的基本特征：<psw>并发性、共享性、虚拟性、异步性</psw>。</li><li>操作系统的基本类型：<psw>批处理系统、分时系统、实时系统</psw>。</li><li>进程三种基本状态：<psw>就绪状态、执行状态、阻塞状态</psw>。</li><li>操作系统的主要功能：<psw>处理机管理、存储器管理、设备管理、文件管理、用户接口</psw>。</li><li>进程同步机制应遵循的准则：<psw>空闲让进、忙则等待、有限等待、让权等带</psw>。</li><li>产生进程死锁的必要条件：<psw>互斥条件、请求和保持条件、不剥夺条件、环路等待条件</psw>。</li><li>设备分配中的主要数据结构（英文缩写）：<psw>DCT、COCT、CHCT、SDT</psw>。</li><li>目录管理的主要功能：<psw>按名存取、提高检索速度、文件共享、允许文件重名</psw>。</li><li>目前比较流行的操作系统（实例）：<psw>Windows、UNIX、Linux</psw>。</li><li>影响文件安全性的主要因素：<psw>人为因素、系统因素、自然因素</psw>。</li><li>进程、文件、线程在系统中存在的唯一标志（英文缩写）：<psw>PCB、FCB、TCB</psw>。</li><li>抢占式进程调度方式基于的主要原则：<psw>优先权原则、短进程优先原则、时间片原则</psw>。</li><li>1968年，Peter J. Denning 指出程序执行时呈现出：<psw>时间局限性、空间局限性</psw>。</li><li>请求分页系统的主要硬件支持：<psw>请求页表机制、缺页中断机构、地址变换机构</psw>。</li><li>通常采用解除死锁的两种方法：<psw>剥夺资源、撤消进程</psw>。</li><li>在操作系统中，实现进程同步的机制：<psw>信号量机制、管程机制</psw>。</li><li>解决<psw>通道</psw>“瓶颈”问题最有效的方法是增加设备到主机间的<psw>通路</psw>。</li><li>产生进程死锁的原因可归结为两点：<psw>竞争资源、进程间推进顺序非法</psw>。</li><li>目前，常用的文件（外存）分配方法：<psw>连续分配、链接分配、索引分配</psw>。</li><li>按设备的固有属性分类，将I/O设备分为：<psw>独占设备、共享设备、虚拟设备</psw>。</li><li>为了实现设备的独立性，系统必须设置（英文缩写）：<psw>LUT</psw>。</li><li>操作系统的用户接口：<psw>命令接口、程序接口、图形用户接口</psw>。</li><li>UNIX系统最本质的特征（英文缩写）：<psw>OSI</psw>。</li><li><psw>UNIX</psw>系统的内核结构可分成两大部分：<psw>进程控制子系统、文件子系统</psw>。</li><li>将一台物理I/O设备虚拟为多台逻辑I/O设备的技术：<psw>SPOOLing</psw>。</li><li>虚拟存储器的理论依据：<psw>局部性原理</psw>。</li><li>保证文件系统安全性的主要措施：<psw>存取控制、容错技术、后备系统</psw>。</li><li><psw>并发性</psw>和<psw>共享性</psw>是多用户、多任务操作系统两个最基本的特征。</li><li><psw>并发性</psw>是多用户、多任务操作系统最重要的特征。</li><li>通过建立<psw>后备系统</psw>，防止由<psw>自然因素</psw>所造成的文件系统的不安全性。</li><li>通过<psw>存取控制</psw>机制，防止由<psw>人为因素</psw>所造成的文件系统的不安全性。</li><li>通过采取<psw>容错技术</psw>，防止由<psw>系统因素</psw>所造成的文件系统的不安全性。</li><li><psw>方便性</psw>和<psw>有效性</psw>是操作系统设计中最重要的两个目标。</li><li>1990年后，<psw>开放性</psw>已成为新系统或软件能否被广泛应用的至关重要的因素。</li><li>在操作系统基本类型中，可靠性是<psw>实时系统</psw>最重要的特征。</li><li>在<psw>局部性原理</psw>中，产生 <psw>时间局限性</psw>的典型原因是在程序中存在着大量的循环操作。</li><li>在<psw>局部性原理</psw>中，产生<psw>空间局限性</psw>的典型情况是程序的顺序执行。</li><li>在死锁的条件中，<psw>不剥夺条件</psw>是指进程已获得的资源只能在使用完时由自己释放。</li><li>在死锁的条件中，<psw>互斥条件</psw>是指在一段时间内，某资源只能被一个进程占用。</li><li>进程所请求的一次I/O完成后,将使进程状态从<psw>阻塞状态</psw>变为<psw>就绪状态</psw>。</li><li>操作系统中处于<psw>执行状态</psw>的进程时间片用完后,进程状态将转变为<psw>就绪状态</psw>。</li><li>操作系统中处于<psw>执行状态</psw>的进程提出I/O请求后,进程状态将转变为<psw>阻塞状态</psw>。</li><li>在文件系统中，文件属性信息存储在数据结构(英文缩写)<psw>FCB</psw>中。</li><li>在操作系统接口中，<psw>程序接口</psw>亦称为<psw>系统调用</psw>。</li><li>操作系统利用数据结构（英文缩写）<psw>PCB</psw>描述进程的基本情况和活动过程。</li><li>资源的按序分配法是摒弃死锁条件中的<psw>环路等待条件</psw>来预防死锁的发生。</li><li>现代操作系统产生死锁的条件中，<psw>互斥条件</psw>是不能被摒弃来预防死锁的发生。</li><li>系统将被中断进程的CPU现场信息保存在该进程的数据结构(英文缩写)<psw>PCB</psw>中。</li><li>在OS基本特征中，<psw>异步性</psw>是指进程是以人们不可预知的速度向前推进的。</li><li>1965年，荷兰学者Dijkstra提出的<psw>信号量机制</psw>是一种卓有成效的进程同步工具。</li><li>同步机制准则中，<psw>让权等待</psw>是指当进程不能进入自己的临界区时，应立即释放处理机。</li><li>进程三种基本状态中，<psw>就绪状态</psw>是指进程已分配到除CPU以外的所有必要资源。</li><li>在索引节点中设置链接引用(links)计数的目的是为了实现目录管理的<psw>文件共享</psw>功能。</li><li>按设备的固有属性分类中，<psw>独占设备</psw>属于临界资源，即进程临界区访问的资源。</li><li>按设备的固有属性分类中，典型的<psw>独占设备</psw>有打印机、磁带机等。</li><li>按设备的固有属性分类中，典型的<psw>共享设备</psw>有磁盘、光盘等。</li><li>在假脱机打印机系统中，按设备的固有属性分类，是将<psw>独占设备</psw>改造为<psw>共享设备</psw>。</li><li>在假脱机打印机系统中，按设备的固有属性分类，实现了<psw>虚拟设备</psw>功能。</li><li><psw>SPOOLing</psw>技术是对脱机I/O系统的模拟，或称为假脱机技术。</li><li>在请求分页系统的硬件支持中，当所要访问的页面不在内存时，由<psw>缺页中断机构</psw>实现。</li><li>在请求分页系统的硬件支持中，页面置换算法需要应用<psw>请求页表机制</psw>实现。</li><li>在设备分配中，用于记录每一个设备情况的数据结构(英文缩写)：<psw>DCT</psw>。</li><li>在设备分配中，用于记录全部设备情况的数据结构(英文缩写)：<psw>SDT</psw>。</li><li>在设备分配中，用于记录每一个控制器情况的数据结构(英文缩写)：<psw>COCT</psw>。</li><li>实现“<psw>按名存取</psw>”是文件系统目录管理中最基本的功能。</li><li>实现“<psw>按名存取</psw>”是文件系统向用户提供的最基本的服务。</li></ol>              </div>            </details><details class="folding-tag" yellow><summary> 基本概念_分专题 </summary>              <div class='content'>              <div class="tabs" id="基本概念_分专题"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#基本概念_分专题-1">专题1</button></li><li class="tab"><button type="button" data-href="#基本概念_分专题-2">专题2</button></li><li class="tab"><button type="button" data-href="#基本概念_分专题-3">专题3</button></li><li class="tab"><button type="button" data-href="#基本概念_分专题-4">专题4</button></li><li class="tab"><button type="button" data-href="#基本概念_分专题-5">专题5</button></li><li class="tab"><button type="button" data-href="#基本概念_分专题-6">专题6</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="基本概念_分专题-1"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/1.png" alt="专题1"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="基本概念_分专题-2"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/2.png" alt="专题2"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="基本概念_分专题-3"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/3.png" alt="专题3"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="基本概念_分专题-4"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/4.png" alt="专题4"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="基本概念_分专题-5"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/5.png" alt="专题5"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="基本概念_分专题-6"><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/6.png" alt="专题6"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><p><img src="https://picbed.dai2yutou.space/article_img/操作系统/cover.png" style="width:800px"></img></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文心一言上手体验</title>
      <link href="/2023/04/04/%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E4%B8%8A%E6%89%8B%E4%BD%93%E9%AA%8C/"/>
      <url>/2023/04/04/%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E4%B8%8A%E6%89%8B%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/16.png" alt="img"></p><p>时隔了18天，终于收到了短信，2023-4-4</p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/11.png" alt="文心一言"></p><p>主要想用一下它的作图功能，那么就体验一下吧………</p><h4 id="画出文心一言的logo"><a href="#画出文心一言的logo" class="headerlink" title="画出文心一言的logo"></a>画出文心一言的logo</h4><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/12.png" alt="画出文心一言的logo"></p><h4 id="帮我画鸡蛋灌饼-创意图"><a href="#帮我画鸡蛋灌饼-创意图" class="headerlink" title="帮我画鸡蛋灌饼#创意图"></a>帮我画鸡蛋灌饼#创意图</h4><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/13.png" alt="帮我画鸡蛋灌饼#创意图#"></p><h4 id="给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义"><a href="#给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义" class="headerlink" title="给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义"></a>给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义</h4><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/14.png" alt="给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义"></p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/15.png" alt="给我画一个以篮球、篮筐、天空为主题，3D，具有向往远方意义#创意图#"></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>画图方面对我自己来说感觉能用但不多，应该对我近期也会有些有帮助。</p>]]></content>
      
      
      <categories>
          
          <category> 日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文心一言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于相对定位和绝对定位的区别</title>
      <link href="/2023/03/31/%E5%85%B3%E4%BA%8E%E7%9B%B8%E5%AF%B9%E5%AE%9A%E4%BD%8D%E5%92%8C%E7%BB%9D%E5%AF%B9%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2023/03/31/%E5%85%B3%E4%BA%8E%E7%9B%B8%E5%AF%B9%E5%AE%9A%E4%BD%8D%E5%92%8C%E7%BB%9D%E5%AF%B9%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/21.png" alt="1"></p><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>参考：<a href="https://blog.csdn.net/caseywei/article/details/81049536">关于相对定位与绝对定位的区别_相对定位和绝对定位的区别_CaseyWei的博客-CSDN博客</a></p></div><h2 id="一、理论解释"><a href="#一、理论解释" class="headerlink" title="一、理论解释"></a>一、理论解释</h2><p><strong>相对定位</strong>：该元素相对于自己原有位置，偏移一定距离。相对的是自己。</p><p><strong>绝对定位</strong>：该元素相对于其父元素，偏移一定距离。相对的是父元素，重点是这个父元素也需要是设置了position属性。从最近的父元素开始找，直到找到body位置为止。</p><h2 id="二、例子"><a href="#二、例子" class="headerlink" title="二、例子"></a>二、例子</h2><h3 id="1）相对定位"><a href="#1）相对定位" class="headerlink" title="1）相对定位"></a>1）相对定位</h3><blockquote><p>p1蓝色，p2黄色</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p1&quot;</span>&gt;</span>相对定位：相对于自己原来的位置，偏移一些距离<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p2&quot;</span>&gt;</span>相对定位,相对的是自己<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#test</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">background</span>: gray;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*p标签本身会有padding和margin值*/</span></span><br><span class="line"><span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">0px</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p1</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">11</span>, <span class="number">187</span>, <span class="number">200</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p2</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">80px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">80px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">223</span>, <span class="number">200</span>, <span class="number">20</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行后效果是：(这时没有设置position属性呢)</p><p><img src="https://picbed.dai2yutou.space/article_img/css/3.png" alt="1"></p><p>然后，给p1设置相对定位,代码如下所示：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.p1</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">11</span>, <span class="number">187</span>, <span class="number">200</span>);</span><br><span class="line">    <span class="comment">/*设置相对定位*/</span></span><br><span class="line">+   <span class="attribute">position</span>: relative;</span><br><span class="line">    <span class="comment">/*相对于左边偏移20px,相对于上边偏移20px*/</span></span><br><span class="line">+   <span class="attribute">left</span>: <span class="number">20px</span>;</span><br><span class="line">+   <span class="attribute">top</span>:<span class="number">20px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行后效果如下： </p><p><img src="https://picbed.dai2yutou.space/article_img/css/4.png" alt="1"></p><p>如图我们发现，p1方块向左移动了20px，向下移动了20px，并且是相对于自身原来的位置。</p><span class='p red'>需要注意的是：p1虽然移动了，但还是占着原来的位置，即后面的元素并不会补上去。</span><h3 id="2）绝对定位"><a href="#2）绝对定位" class="headerlink" title="2）绝对定位"></a>2）绝对定位</h3><blockquote><p>p1蓝色，p2黄色，p3粉色，p4绿色</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p1&quot;</span>&gt;</span>相对定位：相对于自己原来的位置，偏移一些距离<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p2&quot;</span>&gt;</span>相对定位,相对的是自己<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;test2&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p3&quot;</span>&gt;</span>绝对定位：相对于自己父元素的位置，偏移一些距离<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;p4&quot;</span>&gt;</span>绝对定位,相对的是父元素<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#test</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">background</span>: gray;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*p标签本身会有padding和margin值*/</span></span><br><span class="line"><span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">0px</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p1</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">11</span>, <span class="number">187</span>, <span class="number">200</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p2</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">80px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">80px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">223</span>, <span class="number">200</span>, <span class="number">20</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-id">#test2</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">67</span>, <span class="number">62</span>, <span class="number">63</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p3</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">231</span>, <span class="number">116</span>, <span class="number">150</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.p4</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">90px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">90px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">79</span>, <span class="number">163</span>, <span class="number">105</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上所示，我们又新增了一个盒子，里面有两个p1，将p1的相对定位删除，最终如下所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/css/5.png" alt="1"></p><p>然后我们在给p3设置绝对定位</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.p3</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">231</span>, <span class="number">116</span>, <span class="number">150</span>);</span><br><span class="line">    <span class="comment">/* 設置绝对定位 */</span></span><br><span class="line">+   <span class="attribute">position</span>: absolute;</span><br><span class="line">+   <span class="attribute">left</span>: <span class="number">30px</span>;</span><br><span class="line">+   <span class="attribute">top</span>: <span class="number">30px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/css/6.png" alt="1"></p><p>如图我们发现跟我们想的好像不一样，p3方块怎么跑上边去了？？？</p><p>这是因为：虽然text2是p3的父级元素，我们都知道设置绝对定位后，移动的位置是相对父类的，但是此时text2并没有设置position属性，所有p3就会继续往上找，找text2的父元素，如果一直没有position属性，那么就找到body停止，然后以body作为相对元素移动，于是就出现了上面的效果。</p><p>如果我们想要p3相对于text2偏移，那么只需要对text2设置position属性即可，如下所示：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#test2</span>&#123;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">rgb</span>(<span class="number">67</span>, <span class="number">62</span>, <span class="number">63</span>);</span><br><span class="line">+   <span class="attribute">position</span>: relative;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上面代码所示，我们增加了position:relative属性，最终效果如下：</p><span class='p red'>注意：绝对定位不占据原来的位置，所以上图我们可以看到p4图片在text2左上角，也就是p3原来的位置。</span>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> css </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微信云开发中的增删查改操作</title>
      <link href="/2023/03/30/%E5%BE%AE%E4%BF%A1%E4%BA%91%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%A2%9E%E5%88%A0%E6%9F%A5%E6%94%B9%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/03/30/%E5%BE%AE%E4%BF%A1%E4%BA%91%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%A2%9E%E5%88%A0%E6%9F%A5%E6%94%B9%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/20.png" alt="1"></p><div class="note success simple"><p><strong>官方文档📢：<a href="https://developers.weixin.qq.com/miniprogram/dev/wxcloud/reference-sdk-api/database/Collection.html">Collection | 微信开放文档 (qq.com)</a></strong></p></div><h2 id="一、底层原理"><a href="#一、底层原理" class="headerlink" title="一、底层原理"></a>一、底层原理</h2><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>在学习微信云开发中的增删查改前，我们首先要了解微信云开发中的数据库底层原理。</p></div><p>微信云开发中的数据库底层原理使用的是腾讯云的云数据库服务。云数据库是一种基于云计算的数据库服务，提供了高可用性、高可扩展性、自动备份和恢复、自动故障转移等功能。微信云开发中的数据库服务是在腾讯云的云数据库服务上进行的封装和定制，使其更符合微信小程序的开发需求。</p><p>微信云开发中的数据库服务采用的是非关系型数据库存储模型，即NoSQL。NoSQL数据库与传统的关系型数据库不同，不使用结构化查询语言（SQL），而是使用其他数据查询语言或API来访问和管理数据。微信云开发中的数据库服务使用的是文档型数据库，即使用JSON格式存储数据，每个文档相当于一条记录。</p><p>微信云开发中的数据库服务支持实时同步和数据访问权限控制等功能。同时，它还支持云函数触发器，可以在数据库中的数据发生变化时触发云函数。这种触发器可以用于实现实时通知、自动化处理和数据分析等功能。</p><p>总的来说，微信云开发中的数据库底层原理采用了腾讯云的云数据库服务，使用文档型数据库存储模型，支持实时同步和数据访问权限控制等功能，同时还支持云函数触发器。</p><h2 id="二、基本增删查改操作"><a href="#二、基本增删查改操作" class="headerlink" title="二、基本增删查改操作"></a>二、基本增删查改操作</h2><p>我们在云数据库中定义了<code>getData</code>集合，里面初始放了两条记录，每条记录有三个字段，分别为<code>_id、username、password</code>，如下图所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/微信小程序/1.png" alt="1"></p><p>我们在js文件中定义了如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义db变量连接数据库，并连接到getData这个表</span></span><br><span class="line"><span class="keyword">const</span> db = wx.<span class="property">cloud</span>.<span class="title function_">database</span>().<span class="title function_">collection</span>(<span class="string">&quot;getData&quot;</span>)</span><br><span class="line"><span class="title class_">Page</span>(&#123;</span><br><span class="line">  <span class="comment">//页面的初始数据</span></span><br><span class="line">  <span class="attr">data</span>: &#123;</span><br><span class="line">    <span class="attr">username</span>:<span class="string">&quot;&quot;</span>,<span class="comment">//初始化username数据，用来到数据库中传参username字段</span></span><br><span class="line">    <span class="attr">psd</span>:<span class="string">&quot;&quot;</span><span class="comment">//初始化psd数据，用来到数据库中传参password字段</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="2-1-增加"><a href="#2-1-增加" class="headerlink" title="2.1 增加"></a>2.1 增加</h3><p>wxml文件，两个输入框，一个添加按钮</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">placeholder</span>=<span class="string">&quot;输入账号&quot;</span> <span class="attr">bindinput</span>=<span class="string">&quot;addName&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">placeholder</span>=<span class="string">&quot;输入密码&quot;</span> <span class="attr">bindinput</span>=<span class="string">&quot;addPsd&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bindtap</span>=<span class="string">&quot;addData&quot;</span> <span class="attr">type</span>=<span class="string">&quot;primary&quot;</span>&gt;</span>添加<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><p>js文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">Page</span>(&#123;</span><br><span class="line">   <span class="comment">//获取添加的账号</span></span><br><span class="line">  <span class="attr">addName</span>:<span class="keyword">function</span>(<span class="params">e</span>)&#123;</span><br><span class="line">    <span class="variable language_">this</span>.<span class="title function_">setData</span>(&#123;<span class="attr">username</span>:e.<span class="property">detail</span>.<span class="property">value</span>&#125;)</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">//获取添加的密码</span></span><br><span class="line">  <span class="attr">addPsd</span>:<span class="keyword">function</span>(<span class="params">e</span>)&#123;</span><br><span class="line">    <span class="variable language_">this</span>.<span class="title function_">setData</span>(&#123;<span class="attr">psd</span>:e.<span class="property">detail</span>.<span class="property">value</span>&#125;)</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">//点击添加按钮添加操作</span></span><br><span class="line">  <span class="comment">//其中data里边为传入的参数，为json格式，：前面的为数据库中的字段，后面的为要传入的值，</span></span><br><span class="line">  <span class="comment">//此时就可以将数据添加到数据库中</span></span><br><span class="line">  <span class="attr">addData</span>:<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">    db.<span class="title function_">add</span>(&#123;</span><br><span class="line">      <span class="attr">data</span>:&#123;</span><br><span class="line">        <span class="attr">username</span>:<span class="variable language_">this</span>.<span class="property">data</span>.<span class="property">username</span>,</span><br><span class="line">        <span class="attr">password</span>:<span class="variable language_">this</span>.<span class="property">data</span>.<span class="property">psd</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">then</span>(<span class="function"><span class="params">res</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;添加成功&quot;</span>,res);</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">catch</span>(<span class="function"><span class="params">err</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;添加失败&quot;</span>,err);</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><blockquote><p>当我们在输入框输入账号和密码后，点击添加按钮，会在getData集合中生成一个新的记录，其中包括三个字段，_id为自动生成，username、password为我们输入的内容。</p></blockquote><p>添加成功的结果示例如下：</p><p><img src="https://picbed.dai2yutou.space/article_img/微信小程序/3.png" alt="1"></p><h3 id="2-2-删除"><a href="#2-2-删除" class="headerlink" title="2.2 删除"></a>2.2 删除</h3><p>wxml文件，为一个输入账号的input和一个删除按钮；</p><p>我们选择通过输入的账号删除</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">placeholder</span>=<span class="string">&quot;输入账号&quot;</span> <span class="attr">bindinput</span>=<span class="string">&quot;del&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bindtap</span>=<span class="string">&quot;delData&quot;</span> <span class="attr">type</span>=<span class="string">&quot;primary&quot;</span> &gt;</span>删除<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><p>js文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">delData</span>:<span class="keyword">function</span>(<span class="params">e</span>)&#123;</span><br><span class="line">  db.<span class="title function_">where</span>(&#123;</span><br><span class="line">    <span class="comment">//这是是根据where语句进行查询，查询集合中username字段的值为this.data.username的记录</span></span><br><span class="line">    <span class="attr">username</span>:<span class="variable language_">this</span>.<span class="property">data</span>.<span class="property">username</span></span><br><span class="line">  &#125;)</span><br><span class="line">  .<span class="title function_">remove</span>()</span><br><span class="line">  .<span class="title function_">then</span>(<span class="function"><span class="params">res</span>=&gt;</span>&#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;删除成功&quot;</span>,res)</span><br><span class="line">  &#125;)</span><br><span class="line">  .<span class="title function_">catch</span>(<span class="function"><span class="params">err</span>=&gt;</span>&#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;删除失败&quot;</span>,err)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>当我们在输入账号时，会调用del函数，将输入的账号赋值给了data里边的username，当我们点击删除按钮，会调用delData方法，根据data里面的username，到数据库中找username字段相等的记录，并执行remove()函数，删除此条数据，.then是删除成功后执行的，.catch是删除失败执行的。</p></blockquote><p>删除成功的结果示例如下：</p><p><img src="https://picbed.dai2yutou.space/article_img/微信小程序/2.png" alt="1"></p><h3 id="2-3-修改"><a href="#2-3-修改" class="headerlink" title="2.3 修改"></a>2.3 修改</h3><p>wxml文件，两个input，一个修改按钮，第一个input需填入要修改的账号，第二个填入修改后的密码。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">placeholder</span>=<span class="string">&quot;输入要修改账号的用户名&quot;</span> <span class="attr">bindinput</span>=<span class="string">&quot;updUsername&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">placeholder</span>=<span class="string">&quot;输入修改后的密码&quot;</span> <span class="attr">bindinput</span>=<span class="string">&quot;updPsd&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bindtap</span>=<span class="string">&quot;updData&quot;</span> <span class="attr">type</span>=<span class="string">&quot;primary&quot;</span> &gt;</span>修改<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><p>js文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">updUsername</span>:<span class="keyword">function</span>(<span class="params">e</span>)&#123;</span><br><span class="line">    <span class="variable language_">this</span>.<span class="title function_">setData</span>(&#123;<span class="attr">username</span>:e.<span class="property">detail</span>.<span class="property">value</span>&#125;)</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">updPsd</span>:<span class="keyword">function</span>(<span class="params">e</span>)&#123;</span><br><span class="line">    <span class="variable language_">this</span>.<span class="title function_">setData</span>(&#123;<span class="attr">psd</span>:e.<span class="property">detail</span>.<span class="property">value</span>&#125;)</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">updData</span>:<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">    db.<span class="title function_">where</span>(&#123;</span><br><span class="line">      <span class="attr">username</span>:<span class="variable language_">this</span>.<span class="property">data</span>.<span class="property">username</span></span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">update</span>(&#123;</span><br><span class="line">      <span class="attr">data</span>:&#123;</span><br><span class="line">        <span class="attr">password</span>:<span class="variable language_">this</span>.<span class="property">data</span>.<span class="property">psd</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">then</span>(<span class="function"><span class="params">res</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;修改成功&quot;</span>,res)</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">catch</span>(<span class="function"><span class="params">err</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;修改失败&quot;</span>,err)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><blockquote><p>当我们输入账号名和密码时，会调用updUsername和updPsd函数，将输入的内容赋值到data里面的数据，我们点击修改按钮后，会调用updData函数，然后根据where查询语句，在getData集合中查找usernam字段的值与刚刚输入的账号名相等的记录，并根据updata()函数将password字段的值修改为this.data.psd。</p></blockquote><p>修改成功的结果示例如下：</p><p><img src="https://picbed.dai2yutou.space/article_img/微信小程序/4.png" alt="1"></p><h3 id="2-4-查询"><a href="#2-4-查询" class="headerlink" title="2.4 查询"></a>2.4 查询</h3><p>wxml文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bindtap</span>=<span class="string">&quot;getData&quot;</span> <span class="attr">type</span>=<span class="string">&quot;primary&quot;</span>&gt;</span>查询<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><p>js文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">getData</span>:<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">    db.<span class="title function_">get</span>()</span><br><span class="line">    .<span class="title function_">then</span>(<span class="function"><span class="params">res</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;查询成功&quot;</span>,res)</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="title function_">catch</span>(<span class="function"><span class="params">err</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;查询失败&quot;</span>,err)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><blockquote><p>当我们点击查询按钮后，会得到所以结果，结果会存放在res的data下</p></blockquote><p>查询成功结果示例如下：</p><p><img src="https://picbed.dai2yutou.space/article_img/微信小程序/5.png" alt="1"></p><h2 id="三、where条件查询"><a href="#三、where条件查询" class="headerlink" title="三、where条件查询"></a>三、where条件查询</h2><div class="note success simple"><p>具体查看📢：<a href="https://developers.weixin.qq.com/miniprogram/dev/wxcloud/reference-sdk-api/database/Command.html">Command | 微信开放文档 (qq.com)</a></p></div><div class="note warning flag flat"><p>这是我们定义了<code>db=wx.cloud.database()</code>，与上文不同</p></div><p>微信小程序的增删查改操作需要使用小程序云开发的数据库 API，在使用 where 条件进行查询、更新或删除时，需要注意以下几点：</p><p>1.where 条件可以是一个对象，对象的属性名为字段名，属性值为匹配条件。例如，查询 name 字段等于“张三”的记录，可以使用以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">where</span>(&#123;</span><br><span class="line">  <span class="attr">name</span>: <span class="string">&#x27;张三&#x27;</span></span><br><span class="line">&#125;).<span class="title function_">get</span>()</span><br></pre></td></tr></table></figure><p>2.where 条件可以使用多个属性，多个属性之间的关系是“与”的关系。例如，查询 name 字段等于“张三”且 age 字段大于等于 18 的记录，可以使用以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">where</span>(&#123;</span><br><span class="line">  <span class="attr">name</span>: <span class="string">&#x27;张三&#x27;</span>,</span><br><span class="line">  <span class="attr">age</span>: db.<span class="property">command</span>.<span class="title function_">gte</span>(<span class="number">18</span>)</span><br><span class="line">&#125;).<span class="title function_">get</span>()</span><br></pre></td></tr></table></figure><p>其中，db.command.gte(18) 表示 age 大于等于 18。</p><p>3.where 条件支持多种比较操作符，包括：等于（eq）、不等于（neq）、小于（lt）、小于等于（lte）、大于（gt）、大于等于（gte）、包含于（in）、不包含于（nin）、以什么开头（regex）、以什么结尾（regex）、包含（and）、或者（or）等。例如，查询 age 字段大于等于 18 且小于等于 30 的记录，可以使用以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">where</span>(&#123;</span><br><span class="line">  <span class="attr">age</span>: db.<span class="property">command</span>.<span class="title function_">and</span>(db.<span class="property">command</span>.<span class="title function_">gte</span>(<span class="number">18</span>), db.<span class="property">command</span>.<span class="title function_">lte</span>(<span class="number">30</span>))</span><br><span class="line">&#125;).<span class="title function_">get</span>()</span><br></pre></td></tr></table></figure><p>其中，db.command.and() 表示“与”的关系，db.command.gte() 表示大于等于，db.command.lte() 表示小于等于。</p><p>4.where 条件支持嵌套，可以使用对象嵌套的方式表示复杂的查询条件。例如，查询 age 字段大于等于 18 且小于等于 30，且 (name 字段等于“张三”或者 gender 字段等于“女”) 的记录，可以使用以下代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">where</span>(&#123;</span><br><span class="line">  <span class="attr">age</span>: db.<span class="property">command</span>.<span class="title function_">and</span>(db.<span class="property">command</span>.<span class="title function_">gte</span>(<span class="number">18</span>), db.<span class="property">command</span>.<span class="title function_">lte</span>(<span class="number">30</span>)),</span><br><span class="line">  db.<span class="property">command</span>.<span class="title function_">or</span>([&#123;</span><br><span class="line">    <span class="attr">name</span>: <span class="string">&#x27;张三&#x27;</span></span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="attr">gender</span>: <span class="string">&#x27;女&#x27;</span></span><br><span class="line">  &#125;])</span><br><span class="line">&#125;).<span class="title function_">get</span>()</span><br></pre></td></tr></table></figure><p>其中，db.command.or() 表示“或”的关系，可以将多个条件用数组表示。</p><h2 id="四、如何获取前端页面点击的某条记录的信息？"><a href="#四、如何获取前端页面点击的某条记录的信息？" class="headerlink" title="四、如何获取前端页面点击的某条记录的信息？"></a>四、如何获取前端页面点击的某条记录的信息？</h2><blockquote><p>当我们将数据库中一个集合的数据利用for循环到前端页面展示的时候，当我们想点击后删除对应的记录，如何操作？</p></blockquote><p>可以给每个展示的数据绑定一个点击事件，在事件处理函数中获取被点击的数据的信息。</p><p>例如，假设展示的数据是一个数组，可以在模板中给每个数据项绑定一个点击事件：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">view</span> <span class="attr">wx:for</span>=<span class="string">&quot;&#123;&#123;dataList&#125;&#125;&quot;</span> <span class="attr">wx:key</span>=<span class="string">&quot;id&quot;</span> <span class="attr">bindtap</span>=<span class="string">&quot;handleClick&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">text</span>&gt;</span>&#123;&#123;item.name&#125;&#125;<span class="tag">&lt;/<span class="name">text</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">text</span>&gt;</span>&#123;&#123;item.age&#125;&#125;<span class="tag">&lt;/<span class="name">text</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br></pre></td></tr></table></figure><p>其中，<code>dataList</code> 是数据数组，<code>item</code> 是每个数据项，<code>handleClick</code> 是点击事件处理函数。</p><p>事件处理函数中可以通过 <code>event</code> 参数获取当前点击的数据项的信息：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">handleClick</span>: <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> item = event.<span class="property">currentTarget</span>.<span class="property">dataset</span>.<span class="property">item</span>;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(item);</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>获取后，我们就可以在后台进行一系列的操作了！</p><p>其中，<code>event.currentTarget</code> 表示当前触发事件的组件，<code>dataset</code> 是组件上的自定义数据集合，可以通过 <code>event.currentTarget.dataset</code> 获取到当前组件的自定义数据。在上面的模板中，我们可以在每个数据项上绑定一个 <code>data-item</code> 属性，存储当前数据项的信息：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;view <span class="attr">wx</span>:<span class="keyword">for</span>=<span class="string">&quot;&#123;&#123;dataList&#125;&#125;&quot;</span> <span class="attr">wx</span>:key=<span class="string">&quot;id&quot;</span> bindtap=<span class="string">&quot;handleClick&quot;</span> data-item=<span class="string">&quot;&#123;&#123;item&#125;&#125;&quot;</span>&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">text</span>&gt;</span>&#123;&#123;item.name&#125;&#125;<span class="tag">&lt;/<span class="name">text</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">text</span>&gt;</span>&#123;&#123;item.age&#125;&#125;<span class="tag">&lt;/<span class="name">text</span>&gt;</span></span></span><br><span class="line">&lt;/view&gt;</span><br></pre></td></tr></table></figure><p>这样，在事件处理函数中，就可以通过 <code>event.currentTarget.dataset.item</code> 获取当前点击的数据项的信息了。</p><h2 id="五、调用云函数进行增删查改"><a href="#五、调用云函数进行增删查改" class="headerlink" title="五、调用云函数进行增删查改"></a>五、调用云函数进行增删查改</h2><h3 id="5-1-wx-cloud-callFunction"><a href="#5-1-wx-cloud-callFunction" class="headerlink" title="5.1 wx.cloud.callFunction"></a>5.1 wx.cloud.callFunction</h3><p><code>wx.cloud.callFunction</code> 是微信小程序中调用云函数的方法，它的参数有以下几个：</p><ul><li><code>name</code>：云函数名称，必填项。</li><li><code>data</code>：传递给云函数的参数，可选项。</li><li><code>config</code>：配置选项，可选项。</li><li><code>success</code>：成功回调函数，可选项。</li><li><code>fail</code>：失败回调函数，可选项。</li><li><code>complete</code>：完成回调函数，可选项。</li></ul><p>其中，<code>name</code> 参数指定要调用的云函数的名称，是必填项。<code>data</code> 参数是传递给云函数的数据，可以是任何类型的数据，例如字符串、数字、对象等。<code>config</code> 参数是配置选项，可以用来设置超时时间、环境 ID 等。<code>success</code>、<code>fail</code> 和 <code>complete</code> 参数是回调函数，分别在调用成功、调用失败和调用完成时执行。</p><p>📌具体来说，<code>config</code> 参数有以下几个选项：</p><ul><li><code>env</code>：指定云环境 ID，如果不指定则默认使用当前小程序的云环境 ID。</li><li><code>timeout</code>：调用云函数的超时时间，单位为毫秒，默认为 3000 毫秒。</li></ul><p>📌​<code>data</code>参数具体如下：</p><ul><li><code>action</code>：表示要执行的操作，例如<code>action: &#39;add&#39;</code>或者<code>action:&#39;delete&#39;</code>等等</li><li><code>data</code>：表示要处理的数据，例如 <code>&#123;name: &#39;小明&#39;, age: 18&#125;</code> 表示要添加的数据是一个对象，包含了名字和年龄两个属性。</li></ul><h3 id="5-2-云函数"><a href="#5-2-云函数" class="headerlink" title="5.2 云函数"></a>5.2 云函数</h3><p>微信小程序中的云函数是一种在云端运行的 JavaScript 代码，它可以被小程序调用，用于处理一些复杂的业务逻辑或需要访问敏感数据的操作。与传统的客户端代码不同，云函数不会被打包到小程序中，而是在云端运行，可以在多个小程序中共享使用。</p><p>通过云函数，开发者可以在小程序中使用云端数据库、云存储、云函数等云服务，并在云端完成数据的增删查改等操作。云函数可以直接调用云服务 API，也可以通过云函数 SDK 来访问云服务。</p><p>使用云函数可以将一些复杂的计算、数据处理、业务逻辑等操作从客户端移动到云端，减轻客户端的负担，提高小程序的性能和安全性。同时，云函数的运行环境是在微信的安全沙箱中，保证了代码的安全性，可以访问小程序的敏感数据而不会泄露。</p><p>使用云函数需要先在小程序管理后台开通云开发功能，并在开发工具中创建云函数。然后，开发者可以在云函数中编写 JavaScript 代码，使用云函数 SDK 访问云服务，或直接调用云服务 API。在小程序中调用云函数时，可以传递参数给云函数，云函数可以接收并处理参数，然后返回结果给小程序。</p><p>在云函数中，入口函数的参数包括 <code>event</code> 和 <code>context</code> 两个参数，它们分别代表了当前云函数被触发时的事件和上下文信息。</p><p>具体来说，<code>event</code> 参数是一个包含了事件信息的对象，其中包含了调用云函数时传递的参数，例如小程序调用云函数时传递的 <code>data</code> 参数。我们可以通过 <code>event</code> 参数来获取传递的数据，然后根据不同的操作进行处理。</p><p>而 <code>context</code> 参数是一个包含了上下文信息的对象，其中包含了当前云函数的一些元数据，例如所属环境 ID、当前时间戳、请求 ID 等。我们可以通过 <code>context</code> 参数来获取当前云函数的一些元信息，例如获取当前时间戳。</p><p>例如，以下是一个云函数的入口函数示例：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exports</span>.<span class="property">main</span> = <span class="keyword">async</span> (event, context) =&gt; &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(event) <span class="comment">// 打印传递的参数</span></span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(context) <span class="comment">// 打印上下文信息</span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> 处理业务逻辑</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在以上代码中，我们通过 <code>console.log</code> 打印了 <code>event</code> 和 <code>context</code> 两个参数。在实际开发中，我们可以根据需要来获取传递的数据和上下文信息，然后根据不同的操作进行处理。</p><h3 id="5-3-示例"><a href="#5-3-示例" class="headerlink" title="5.3 示例"></a>5.3 示例</h3><p>以下是一个简单的微信小程序中调用云函数进行增删查改的代码示例：</p><ol><li>在小程序中调用云函数</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在页面中调用云函数</span></span><br><span class="line">wx.<span class="property">cloud</span>.<span class="title function_">callFunction</span>(&#123;</span><br><span class="line">  <span class="attr">name</span>: <span class="string">&#x27;crud&#x27;</span>, <span class="comment">// 云函数名称</span></span><br><span class="line">  <span class="attr">data</span>: &#123;</span><br><span class="line">    <span class="attr">action</span>: <span class="string">&#x27;add&#x27;</span>, <span class="comment">// 要执行的操作，例如添加数据</span></span><br><span class="line">    <span class="attr">data</span>: &#123; <span class="comment">// 要添加的数据</span></span><br><span class="line">      <span class="attr">name</span>: <span class="string">&#x27;小明&#x27;</span>,</span><br><span class="line">      <span class="attr">age</span>: <span class="number">18</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">success</span>: <span class="function"><span class="params">res</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(res.<span class="property">result</span>) <span class="comment">// 打印云函数返回的结果</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">fail</span>: <span class="function"><span class="params">err</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ol><li>在云函数中处理数据</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 云函数入口文件</span></span><br><span class="line"><span class="keyword">const</span> cloud = <span class="built_in">require</span>(<span class="string">&#x27;wx-server-sdk&#x27;</span>)</span><br><span class="line">cloud.<span class="title function_">init</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> db = cloud.<span class="title function_">database</span>() <span class="comment">// 获取数据库实例</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 云函数入口函数</span></span><br><span class="line"><span class="built_in">exports</span>.<span class="property">main</span> = <span class="keyword">async</span> (event, context) =&gt; &#123;</span><br><span class="line">  <span class="keyword">const</span> action = event.<span class="property">action</span> <span class="comment">// 获取要执行的操作</span></span><br><span class="line">  <span class="keyword">const</span> data = event.<span class="property">data</span> <span class="comment">// 获取要处理的数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">switch</span> (action) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&#x27;add&#x27;</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="title function_">addData</span>(data) <span class="comment">// 添加数据</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">&#x27;delete&#x27;</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="title function_">deleteData</span>(data) <span class="comment">// 删除数据</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">&#x27;update&#x27;</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="title function_">updateData</span>(data) <span class="comment">// 更新数据</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">&#x27;get&#x27;</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="title function_">getData</span>(data) <span class="comment">// 查询数据</span></span><br><span class="line">    <span class="attr">default</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="string">&#x27;unknown action&#x27;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加数据</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">addData</span>(<span class="params">data</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">add</span>(&#123;</span><br><span class="line">      <span class="attr">data</span>: data</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(err)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除数据</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">deleteData</span>(<span class="params">data</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">doc</span>(data.<span class="property">_id</span>).<span class="title function_">remove</span>()</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(err)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新数据</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">updateData</span>(<span class="params">data</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">doc</span>(data.<span class="property">_id</span>).<span class="title function_">update</span>(&#123;</span><br><span class="line">      <span class="attr">data</span>: data</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(err)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询数据</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">getData</span>(<span class="params">data</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> db.<span class="title function_">collection</span>(<span class="string">&#x27;users&#x27;</span>).<span class="title function_">where</span>(data).<span class="title function_">get</span>()</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(err)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码示例中，我们在小程序中调用云函数时传入了要执行的操作和要处理的数据，然后在云函数中根据不同的操作进行增删查改的操作，并返回操作结果。在实际开发中，我们可以根据需要进行修改和扩展。</p><h2 id="六、模糊查询"><a href="#六、模糊查询" class="headerlink" title="六、模糊查询"></a>六、模糊查询</h2><h3 id="6-1-基本原理代码"><a href="#6-1-基本原理代码" class="headerlink" title="6.1 基本原理代码"></a>6.1 基本原理代码</h3><p><strong>微信小程序模糊查询可以通过使用正则表达式来实现，具体步骤如下：</strong></p><ol><li>获取需要查询的字符串和查询的关键字。</li><li>构造正则表达式，将关键字转化为一个正则表达式，并添加模糊匹配符号。</li><li>使用正则表达式对字符串进行匹配，返回匹配结果。</li></ol><p>示例代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 获取需要查询的字符串和查询的关键字</span><br><span class="line">let str = &quot;hello world, hello everyone&quot;;</span><br><span class="line">let keyword = &quot;hello&quot;;</span><br><span class="line"></span><br><span class="line">// 构造正则表达式</span><br><span class="line">let regStr = `.*$&#123;keyword&#125;.*`;</span><br><span class="line">let reg = new RegExp(regStr, &quot;g&quot;);</span><br><span class="line"></span><br><span class="line">// 使用正则表达式对字符串进行匹配</span><br><span class="line">let result = str.match(reg);</span><br><span class="line"></span><br><span class="line">console.log(result); // [&quot;hello world&quot;, &quot;hello everyone&quot;]</span><br></pre></td></tr></table></figure><p>这样就可以实现简单的模糊查询，可以根据需要进行优化，例如忽略大小写、支持多关键字查询等。</p><h3 id="6-2-request请求模糊查询"><a href="#6-2-request请求模糊查询" class="headerlink" title="6.2 request请求模糊查询"></a>6.2 request请求模糊查询</h3><p><strong>代码示例：</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wx.<span class="title function_">request</span>(&#123;</span><br><span class="line">  <span class="attr">url</span>: <span class="string">&#x27;https://example.com/api/search&#x27;</span>,</span><br><span class="line">  <span class="attr">method</span>: <span class="string">&#x27;GET&#x27;</span>,</span><br><span class="line">  <span class="attr">data</span>: &#123;</span><br><span class="line">    <span class="attr">keyword</span>: <span class="string">&#x27;关键词&#x27;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">success</span>: <span class="keyword">function</span>(<span class="params">res</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(res.<span class="property">data</span>);</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">fail</span>: <span class="keyword">function</span>(<span class="params">err</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(err);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>其中，url 为要请求的接口地址，method 为请求方法，data 为请求参数，包括要匹配的关键词。在接口的后端代码中，可以使用类似以下的代码进行模糊查询：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> keyword = req.<span class="property">query</span>.<span class="property">keyword</span>;</span><br><span class="line"><span class="keyword">const</span> regex = <span class="keyword">new</span> <span class="title class_">RegExp</span>(keyword, <span class="string">&#x27;i&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> results = <span class="keyword">await</span> db.<span class="title function_">collection</span>(<span class="string">&#x27;collectionName&#x27;</span>).<span class="title function_">where</span>(&#123; <span class="attr">name</span>: regex &#125;).<span class="title function_">get</span>();</span><br><span class="line">res.<span class="title function_">status</span>(<span class="number">200</span>).<span class="title function_">json</span>(results);</span><br></pre></td></tr></table></figure><p>其中，req.query.keyword 为前端传递的关键词，regex 为正则表达式对象，i 表示不区分大小写，db.collection(‘collectionName’) 为数据库集合对象，name 为要匹配的字段名。最终将查询结果以 JSON 格式返回给前端。</p><h3 id="6-3-云数据库模糊查询"><a href="#6-3-云数据库模糊查询" class="headerlink" title="6.3 云数据库模糊查询"></a>6.3 云数据库模糊查询</h3><p><strong>代码示例：</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> db = wx.<span class="property">cloud</span>.<span class="title function_">database</span>();</span><br><span class="line"><span class="keyword">const</span> keyword = <span class="string">&#x27;关键词&#x27;</span>;</span><br><span class="line">db.<span class="title function_">collection</span>(<span class="string">&#x27;collectionName&#x27;</span>).<span class="title function_">where</span>(&#123;</span><br><span class="line">  <span class="attr">name</span>: db.<span class="title class_">RegExp</span>(&#123;</span><br><span class="line">    <span class="attr">regexp</span>: keyword,</span><br><span class="line">    <span class="attr">options</span>: <span class="string">&#x27;i&#x27;</span>,</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;).<span class="title function_">get</span>(&#123;</span><br><span class="line">  <span class="attr">success</span>: <span class="keyword">function</span>(<span class="params">res</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(res.<span class="property">data</span>);</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">fail</span>: <span class="keyword">function</span>(<span class="params">err</span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(err);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>其中，db.RegExp 为云数据库的正则表达式对象，regexp 为要匹配的关键词，options 为正则表达式选项，’i’ 表示不区分大小写。最终将查询结果以 JSON 格式返回给前端。    </p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微信小程序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微信小程序如何判断一个字符串是否包含另一个字符串</title>
      <link href="/2023/03/29/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2023/03/29/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/18.png" alt="1"></p><p>在小程序中，可以使用JavaScript中的<code>String</code>对象的<code>includes()</code>方法来判断一个字符串是否包含另一个字符串。</p><p><code>includes()</code>方法接收一个参数，表示要查找的字符串，如果被查找的字符串包含这个参数，则返回<code>true</code>，否则返回<code>false</code>。</p><p>例如，在小程序中可以使用以下代码来判断一个字符串是否包含另一个字符串：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> str1 = <span class="string">&#x27;Hello World&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> str2 = <span class="string">&#x27;World&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (str1.<span class="title function_">includes</span>(str2)) &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;str1包含str2&#x27;</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;str1不包含str2&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上述代码中，<code>str1</code>包含<code>str2</code>，因此会输出<code>str1包含str2</code>。</p><p>需要注意的是，<code>includes()</code>方法是ES6中新增的方法，如果需要在低版本的微信小程序中使用该方法，可以使用其他方法代替，例如使用<code>indexOf()</code>方法进行判断：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (str1.<span class="title function_">indexOf</span>(str2) !== -<span class="number">1</span>) &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;str1包含str2&#x27;</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;str1不包含str2&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上述代码中，如果<code>str2</code>在<code>str1</code>中出现，则<code>indexOf()</code>方法会返回<code>str2</code>在<code>str1</code>中的位置，否则返回<code>-1</code>。因此，可以通过判断<code>indexOf()</code>方法的返回值是否为<code>-1</code>来判断一个字符串是否包含另一个字符串。    </p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微信小程序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微信小程序中如何得到自己选择的view的属性等信息</title>
      <link href="/2023/03/29/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%A6%82%E4%BD%95%E5%BE%97%E5%88%B0%E8%87%AA%E5%B7%B1%E9%80%89%E6%8B%A9%E7%9A%84view%E7%9A%84%E5%B1%9E%E6%80%A7%E7%AD%89%E4%BF%A1%E6%81%AF/"/>
      <url>/2023/03/29/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%A6%82%E4%BD%95%E5%BE%97%E5%88%B0%E8%87%AA%E5%B7%B1%E9%80%89%E6%8B%A9%E7%9A%84view%E7%9A%84%E5%B1%9E%E6%80%A7%E7%AD%89%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/18.png" alt="1"></p><p>我们可以为每个<code>view</code>设置一个唯一的标识符，然后在点击事件中获取当前点击的<code>view</code>的标识符。</p><p>具体实现方法如下：</p><ol><li>在<code>wxml</code>中为每个<code>view</code>设置一个<code>data-id</code>属性，值为唯一标识符，例如：</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">view</span> <span class="attr">data-id</span>=<span class="string">&quot;view1&quot;</span> <span class="attr">bindtap</span>=<span class="string">&quot;onTap&quot;</span>&gt;</span>View 1<span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">view</span> <span class="attr">data-id</span>=<span class="string">&quot;view2&quot;</span> <span class="attr">bindtap</span>=<span class="string">&quot;onTap&quot;</span>&gt;</span>View 2<span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">view</span> <span class="attr">data-id</span>=<span class="string">&quot;view3&quot;</span> <span class="attr">bindtap</span>=<span class="string">&quot;onTap&quot;</span>&gt;</span>View 3<span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li>在<code>js</code>文件中，使用<code>event.currentTarget.dataset.id</code>获取当前点击的<code>view</code>的标识符，例如：</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">Page</span>(&#123;</span><br><span class="line">  <span class="attr">onTap</span>: <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> viewId = event.<span class="property">currentTarget</span>.<span class="property">dataset</span>.<span class="property">id</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Clicked view id: &quot;</span> + viewId);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>这样就可以在点击事件中获取当前点击的view的标识符了。</p><p>除了<code>data-id</code>属性外，还可以设置其他自定义属性，例如<code>data-name</code>、<code>data-type</code>等，用法和<code>data-id</code>类似。在点击事件中，可以通过<code>event.currentTarget.dataset</code>获取所有自定义属性的值。例如：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;view data-id=<span class="string">&quot;view1&quot;</span> data-name=<span class="string">&quot;View One&quot;</span> data-type=<span class="string">&quot;primary&quot;</span> bindtap=<span class="string">&quot;onTap&quot;</span>&gt;<span class="title class_">View</span> <span class="number">1</span>&lt;/view&gt;</span><br><span class="line"></span><br><span class="line"><span class="title class_">Page</span>(&#123;</span><br><span class="line">  <span class="attr">onTap</span>: <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> viewId = event.<span class="property">currentTarget</span>.<span class="property">dataset</span>.<span class="property">id</span>;</span><br><span class="line">    <span class="keyword">var</span> viewName = event.<span class="property">currentTarget</span>.<span class="property">dataset</span>.<span class="property">name</span>;</span><br><span class="line">    <span class="keyword">var</span> viewType = event.<span class="property">currentTarget</span>.<span class="property">dataset</span>.<span class="property">type</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Clicked view id: &quot;</span> + viewId);</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Clicked view name: &quot;</span> + viewName);</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;Clicked view type: &quot;</span> + viewType);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>这样就可以在点击事件中获取所有自定义属性的值了。    </p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微信小程序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新必应在DEV中无法使用解决方法</title>
      <link href="/2023/03/26/%E6%96%B0%E5%BF%85%E5%BA%94%E5%9C%A8DEV%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
      <url>/2023/03/26/%E6%96%B0%E5%BF%85%E5%BA%94%E5%9C%A8DEV%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/6.png" alt="1"></p><h2 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h2><p>之前在<code>DEV</code>中可以开全局代理可以正常使用新必应的服务，但是最近一直出现<code>Sorry, looks like your network settings are preventing access to this feature.</code>的错误信息，这是因为微软的此应用完全对<code>China</code>禁止，多次更换节点仍然无法解决。</p><h2 id="二、解决方法"><a href="#二、解决方法" class="headerlink" title="二、解决方法"></a>二、解决方法</h2><p>我们解决的核心就是让<code>Bing</code>认为你是一个彻头彻尾的美国用户，所以解决方案要尽量在整个使用过程中伪装成美国用户。</p><ol><li><p>修改全局至美国地区，确保提供的服务网络状态较好。</p></li><li><p>将微软账号设置为美国地区</p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/8.png" alt="1"></p></li><li><p>用<code>Header Editor</code>插件更改请求中的DNS，将头内容修改为<code>1.1.1.1</code></p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/7.png" alt="1"></p></li></ol><h2 id="三、效果展示"><a href="#三、效果展示" class="headerlink" title="三、效果展示"></a>三、效果展示</h2><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/9.png" alt="1"></p>]]></content>
      
      
      <categories>
          
          <category> bug记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 新必应 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVAWEB中访问一个Servlet类出现404的一种奇葩原因</title>
      <link href="/2023/03/25/JAVAWEB%E4%B8%AD%E8%AE%BF%E9%97%AE%E4%B8%80%E4%B8%AAServlet%E7%B1%BB%E5%87%BA%E7%8E%B0404%E7%9A%84%E4%B8%80%E7%A7%8D%E5%A5%87%E8%91%A9%E5%8E%9F%E5%9B%A0/"/>
      <url>/2023/03/25/JAVAWEB%E4%B8%AD%E8%AE%BF%E9%97%AE%E4%B8%80%E4%B8%AAServlet%E7%B1%BB%E5%87%BA%E7%8E%B0404%E7%9A%84%E4%B8%80%E7%A7%8D%E5%A5%87%E8%91%A9%E5%8E%9F%E5%9B%A0/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/16.jpg" alt="1"></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在<code>servlet</code>技术中，如果在一个项目名字为<code>aaa</code>的目录的<code>src</code>的新建立了一个<code>User_login</code>的包，在包里建立了一个名字为<code>LoginServlet</code>的类，注解为<code>@WebServlet(name = &quot;LoginServlet&quot;,urlPatterns = &quot;/User_login/LoginServlet&quot;)；</code>然后又在根目录的<code>src</code>下直接建立了一个名为<code>LoginServlet</code>的类，注解为<code>@WebServlet(name = &quot;LoginServlet&quot;,urlPatterns = &quot;/LoginServlet&quot;)</code>。</p><p><strong>结构如下：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- aaa</span><br><span class="line">- src</span><br><span class="line">- User_login</span><br><span class="line">- LoginServlet</span><br><span class="line">- LoginServlet</span><br></pre></td></tr></table></figure><p><strong>User_login包下的<code>LoginServlet</code>:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import javax.servlet.annotation.WebServlet;</span><br><span class="line">import javax.servlet.http.HttpServlet;</span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import javax.servlet.http.HttpServletResponse;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">@WebServlet(name = &quot;LoginServlet&quot;,urlPatterns = &quot;/User_login/LoginServlet&quot;)</span><br><span class="line">public class LoginServlet extends HttpServlet &#123;</span><br><span class="line">    public void doGet(HttpServletRequest req, HttpServletResponse rep) throws IOException &#123;</span><br><span class="line">        rep.getWriter().print(&quot;我是User_login包下的LoginServlet!&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    public void doPost(HttpServletRequest req, HttpServletResponse rep) throws IOException &#123;</span><br><span class="line">        doGet(req,rep);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>根目录下的<code>LoginServlet</code>:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import javax.servlet.annotation.WebServlet;</span><br><span class="line">import javax.servlet.http.HttpServlet;</span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import javax.servlet.http.HttpServletResponse;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">@WebServlet(name = &quot;LoginServlet&quot;,urlPatterns = &quot;/LoginServlet&quot;)</span><br><span class="line">public class LoginServlet extends HttpServlet &#123;</span><br><span class="line">    public void doGet(HttpServletRequest req, HttpServletResponse rep) throws IOException &#123;</span><br><span class="line">        rep.getWriter().print(&quot;我是根目录下的LoginServlet!&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    public void doPost(HttpServletRequest req, HttpServletResponse rep) throws IOException &#123;</span><br><span class="line">        doGet(req,rep);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时我们访问<code>http://localhost:8080/aaa/User_login/LoginServlet</code>会报错404。</p><p><img src="https://picbed.dai2yutou.space/article_img//JAVA/11.png" alt="1"></p><p>而能访问<code>http://localhost:8080/aaa/LoginServlet</code>，可以正常访问。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/12.png" alt="1"></p><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>当我们利用<code>@WebServlet</code>注解访问<code>url</code>的映射地址时，如果<code>name</code>为<code>LoginServlet</code>，<code>JAVA</code>会从根目录下一级一级的往下加载，寻找类名，当加载到根目录下的<code>LoginServlet</code>时，就不会往下加载了；</p><p>此时当访问<code>http://localhost:8080/aaa/User_login/LoginServlet时</code>，会先匹配到<code>/User_login/LoginServlet</code>，但是在<code>User_login</code>包下的<code>LoginServlet</code>类并没有被加载，因为在根目录下的<code>LoginServlet</code>类已经被加载了，所以会直接调用根目录下的<code>LoginServlet</code>类。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>如果想要访问<code>http://localhost:8080/aaa/User_login/LoginServlet</code></p><ul><li>可以将根目录下的<code>LoginServlet</code>的类删除。</li><li>可以修改<code>@WebServlet</code>注解中的<code>name</code>值，使两者不同。</li></ul>]]></content>
      
      
      <categories>
          
          <category> bug记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVAWEB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的计网学习——计算机网络物理层</title>
      <link href="/2023/03/23/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%89%A9%E7%90%86%E5%B1%82/"/>
      <url>/2023/03/23/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%89%A9%E7%90%86%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/17.png" alt="1"></p><p>参考博客：<div class="tag link"><a class="link-card" title="Nimrod" href="https://blog.csdn.net/Nimrod__/article/details/113271631"><div class="left"><img src="https://profile.csdnimg.cn/F/6/8/1_nimrod__"/></div><div class="right"><p class="text">Nimrod</p><p class="url">https://blog.csdn.net/Nimrod__/article/details/113271631</p></div></a></div></p><p>本笔记来源于：<a href="https://www.bilibili.com/video/BV1c4411d7jb">计算机网络微课堂——湖科大</a></p><div class="note warning flat"><p>物理层的协议众多，这是因为物理层的连接方式比较多，物理层下面的传输媒体也比较多，因此再学习物理层时，我们应将重点放在物理层的基本概念上，而不是局限于某一个具体的物理层协议。</p></div><h2 id="一、物理层的基本概念"><a href="#一、物理层的基本概念" class="headerlink" title="一、物理层的基本概念"></a>一、物理层的基本概念</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h3><p>物理层是计算机网络的第一层，是整个计算机网络系统的基础，为数据传输提供可靠的环境。</p><p>物理层要解决的基本问题：如何在各种传输媒体上传输比特0和1的问题。进而给数据链路层提供透明传输比特流的服务。</p><p>物理层为数据链路层屏蔽了各种传输媒体的差异，使数据链路层只需要考虑如何完成本层的协议和服务，而不必考虑网络具体的传输媒体是什么。</p><h3 id="1-2-物理层协议的主要任务"><a href="#1-2-物理层协议的主要任务" class="headerlink" title="1.2 物理层协议的主要任务"></a>1.2 物理层协议的主要任务</h3><p>定义物理层的四个特性：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/34.png" alt="1"></p><p>由于传输媒体种类众多（双绞线、光纤等），物理连接方式也很多（点对点连接、广播连接等），因此物理协议有很多种，但是<strong>每一种都需要包括以上四个特性。</strong></p><h2 id="二、物理层下面的传输媒体"><a href="#二、物理层下面的传输媒体" class="headerlink" title="二、物理层下面的传输媒体"></a>二、物理层下面的传输媒体</h2><h3 id="2-1-导引型传输媒体"><a href="#2-1-导引型传输媒体" class="headerlink" title="2.1 导引型传输媒体"></a>2.1 导引型传输媒体</h3><p>在导引型传输媒体中，<strong>电磁波被导引沿着固体传播媒体传播。</strong></p><p>常见的导引型传播媒体：</p><div class="tabs" id="常见的导引型传播媒体"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#常见的导引型传播媒体-1">同轴电缆</button></li><li class="tab"><button type="button" data-href="#常见的导引型传播媒体-2">双绞线</button></li><li class="tab"><button type="button" data-href="#常见的导引型传播媒体-3">光纤</button></li><li class="tab"><button type="button" data-href="#常见的导引型传播媒体-4">电力线</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="常见的导引型传播媒体-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/35.png" alt="1"></p><p>生活中最常见的就是电视线。用于<strong>模拟传输</strong>。<br>价格较贵，布线不够灵活方便。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的导引型传播媒体-2"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/36.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/37.png" alt="1"></p><p>双绞线是最古老又常用的传输媒体。也就是生活中最常见的网线。</p><p>双绞线的构成就是把两个互相绝缘的铜导线并排放在一起。 按照一定规则绞合起来。因此称为双绞线。</p><p>绞合的作用：</p><ul><li>抵御部分来自外界的电磁波干扰。</li><li>减少来自相邻导线的电磁干扰。</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的导引型传播媒体-3"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/38.png" alt="1"></p><p>每一根光纤是非常细的，因此需要将他做成很结实的光缆，一根光缆少则只有一根光纤，多则可能有数十根甚至数百跟光纤。 光纤的芯非常细。</p><p><strong>光纤优缺点：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/39.png" alt="1"></p><p><strong>光纤的工作原理：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/40.png" alt="1"></p><p>重复进行此过程，使光一直不断地全反射指导到达终点。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的导引型传播媒体-4"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/41.png" alt="1"></p><p>电力线是一种比较古老的技术。</p><p>应用电力线传输信号的实例最早使电力线电话。</p><p>但是在目前，以电力线构建局域网已不能满足需求。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="2-2-非导引型传输媒体"><a href="#2-2-非导引型传输媒体" class="headerlink" title="2.2 非导引型传输媒体"></a>2.2 非导引型传输媒体</h3><p><strong>非导引型的传播媒体是自由空间。</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/42.png" alt="1"></p><p>电磁波频率过大对人体有害，因此一般使用介于无线电波到红外的频率来进行信息传播。</p><p>常见的非导引型传播媒体：</p><div class="tabs" id="常见的非导引型传播媒体"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#常见的非导引型传播媒体-1">无线电波</button></li><li class="tab"><button type="button" data-href="#常见的非导引型传播媒体-2">微波</button></li><li class="tab"><button type="button" data-href="#常见的非导引型传播媒体-3">红外线</button></li><li class="tab"><button type="button" data-href="#常见的非导引型传播媒体-4">可见光</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="常见的非导引型传播媒体-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/43.png" alt="1"></p><p>无线电波用于<strong>国际广播、海事和航空通讯</strong>等。</p><p>无线电波中的低频和中频端主要以地面波形式传播。高频和甚高频主要用电离层的反射传播。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的非导引型传播媒体-2"><p>微波用于<strong>无线电话、无线网络</strong>、雷达、人造卫星接受等。在数据通信中占有重要地位。</p><p>微波在空间中主要以直线传播。</p><p>传统的微波通信主要有<strong>地面微波接力通信和卫星通信。</strong></p><p>其传播距离一般只有50公里左右。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的非导引型传播媒体-3"><p>利用红外线传输数据，例如电视遥控等。</p><p>红外通信属于<strong>点对点无线传输</strong>。</p><p>不能越障，传输距离短，传输速率低。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="常见的非导引型传播媒体-4"><p>LIFI，可以实现使用<strong>可见光通信</strong>，但是目前还在实验室阶段。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h4 id="无线电频谱管理机构"><a href="#无线电频谱管理机构" class="headerlink" title="无线电频谱管理机构"></a>无线电频谱管理机构</h4><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/44.png" alt="1"></p><h3 id="三、传输方式"><a href="#三、传输方式" class="headerlink" title="三、传输方式"></a>三、传输方式</h3><h3 id="3-1-串行传输和并行传输"><a href="#3-1-串行传输和并行传输" class="headerlink" title="3.1 串行传输和并行传输"></a>3.1 串行传输和并行传输</h3><div class="tabs" id="串行传输和并行传输"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#串行传输和并行传输-1">串行传输</button></li><li class="tab"><button type="button" data-href="#串行传输和并行传输-2">并行传输</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="串行传输和并行传输-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/45.png" alt="1"></p><p>数据通过一条线路传输，一个一个比特依次传送，因此只需要一条线路。</p><p>数据在进行<strong>远距离传输</strong>的时候用的是<strong>串行传输，不是并行传输。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="串行传输和并行传输-2"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/46.png" alt="1"></p><p>数据通过多条线路传输。</p><p>并行传输的优点是速度为<strong>串行传输的n倍</strong>。</p><p>缺点是<strong>成本高</strong>。</p><p><strong>计算机内部数据的传输一般使用并行传输。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="3-2-同步传输和异步传输"><a href="#3-2-同步传输和异步传输" class="headerlink" title="3.2 同步传输和异步传输"></a>3.2 同步传输和异步传输</h3><div class="tabs" id="同步传输和异步传输"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#同步传输和异步传输-1">同步传输</button></li><li class="tab"><button type="button" data-href="#同步传输和异步传输-2">异步传输</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="同步传输和异步传输-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/47.png" alt="1"></p><p>数据库以稳定的比特流形式传输，字节之间没有间隔。</p><p>接收端在每个比特信号的中间时刻进行检测，以判别接收到的是比特0还是比特1。</p><p>由于不同设备的时钟周期存在差异，不能做到完全相同，在传输大量数据的过程中判别时刻的累计误差会导致接收端对比特信号的判别位置错位。</p><p>因此需要采取方法使收发双方的始终保持同步。</p><p>有两种同步方法：</p><ul><li><strong>外同步：</strong>在收发双方之间添加一条单独的时钟信号线；发送端在发送数据信号的同时，另外发送一条时钟信号，接收端按照同步信号的节奏来接受数据。</li><li><strong>内同步：</strong>发送端将时钟同步信号编码到发送数据中一起传输（例如曼彻斯特编码）；传统以太网采用的就是曼彻斯特编码的内同步方式。</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="同步传输和异步传输-2"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/48.png" alt="1"></p><p>使用异步传输方式时，以子节为独立的传输单位，<strong>子节之间的间隔时间不固定</strong>的（因此成为异步传输）。</p><p>接收端仅在每个字节的<strong>起始处</strong>对子节内的比特实现同步。</p><p>为此，通常要在每个子节的前后分别加上起始位和结束位。</p><p><strong>异步指的是子节之间异步。</strong></p><p><strong>字节之间的时间间隔不固定。</strong></p><p><strong>但是，字节中的每个比特依旧要同步，各个比特的持续时间要相同。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="3-3-单工、半双工、全双通信"><a href="#3-3-单工、半双工、全双通信" class="headerlink" title="3.3 单工、半双工、全双通信"></a>3.3 单工、半双工、全双通信</h3><div class="tabs" id="单工、半双工、全双通信"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#单工、半双工、全双通信-1">单工</button></li><li class="tab"><button type="button" data-href="#单工、半双工、全双通信-2">半双工</button></li><li class="tab"><button type="button" data-href="#单工、半双工、全双通信-3">全双通信</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="单工、半双工、全双通信-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/49.png" alt="1"></p><p>通信双方只<strong>有一个数据传输方向</strong>。</p><p>例如无线电、广播。</p><p><strong>只需要一条信道。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="单工、半双工、全双通信-2"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/50.png" alt="1"></p><p>通信双方有<strong>两个数据传输方向，但是不可以同时通信。</strong></p><p><strong>需要两条信道。</strong></p><p>例如对讲机就是这种通信方式。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="单工、半双工、全双通信-3"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/51.png" alt="1"></p><p>通信双方有<strong>两个数据传输方向，可以进行同时通信。</strong></p><p><strong>需要两条信道。</strong></p><p>例如实时电话就是这种通信方式。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="四、编码与调制"><a href="#四、编码与调制" class="headerlink" title="四、编码与调制"></a>四、编码与调制</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/52.png" alt="1"></p><p>信号分为数字基带信号和模拟信号。</p><p>数字基带信号例如计算机和CPU与内存之间传输的信号。</p><p>模拟基带信号例如麦克风收到声音后产生的音频信号。</p><p>信号需要在信道中传输，信道有数字信道和模拟信道。</p><h3 id="4-1-编码"><a href="#4-1-编码" class="headerlink" title="4.1 编码"></a>4.1 编码</h3><p>编码就是在不改变信号性质的情况下仅对数字基带信号的波形进行变换。编码后产生的信号认为数字信号。可以在信道种传输。</p><p>有两种情况：</p><p>数字信号转换为另一种数字信号， 在数字信道中传输。</p><p>例如，以太网使用曼彻斯特编码、4B/5B、8B/10B等编码。</p><p>模拟信号转换为数字信号 ，在数字信道中传输。</p><p>例如，对音频信号进行编码的脉码调制PCM。</p><h3 id="4-2-调制"><a href="#4-2-调制" class="headerlink" title="4.2 调制"></a>4.2 调制</h3><p>把数据基带信号的频率范围，搬移到较高的频段，并转换为模拟信号，称为调制。调制后产生模拟信号，在模拟信道种传输。</p><p>有两种情况：</p><p>数字信号转换为模拟信号，在模拟信道中传输。</p><p>例如WiFi，采用补码键控CCK/直接序列扩频DSSS/正交频分复用OFDM等调制方式。</p><p>模拟信号转换为另一种模拟信号，在模拟信道中传输。</p><p>例如，语音数据加载到模拟的载波信号中传输。<br>频分复用FDM技术，充分利用带宽资源。</p><h3 id="4-3-常用编码"><a href="#4-3-常用编码" class="headerlink" title="4.3 常用编码"></a>4.3 常用编码</h3><p><strong>码元：构成信号的一段波形。</strong></p><p><strong>不归零编码：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/53.png" alt="1"></p><p>在整个码元时间内，不会出现零电平。</p><p>为了辨别码元个数，在发送信号时需要发送方和接收方做到<strong>严格的同步</strong>。</p><p>一般需要<strong>额外一根传输线来传输时钟信号</strong>，实现同步的目的。接收方按照时钟信号的节拍来逐个接收码元。</p><p>但是额外一根线资源造成浪费，在计算机网络中的数据传输一般不采用这类编码。</p><p><strong>归零编码：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/54.png" alt="1"></p><p>在传输过程中，<strong>每个码元传输结束后都要”归零“</strong>。<br>因此接收方只要在信号归零后进行采样，而不需要单独的时钟信号。</p><p>归零编码相当于把时钟信号用”归零“的方式放在了数据之内，看作一种<strong>自同步</strong>的信号。</p><p>但是在传输过程中，大部分的数据带宽都用来传输零电平数据，造成资源浪费。编码效率低。</p><p><strong>曼彻斯特编码：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/55.png" alt="1"></p><p>在<strong>码元中间时刻会产生跳变</strong>，跳变的含义可以自己定义。例如由低电平到高电平代表1/0信号，由高电平到滴电平代表0/1信号。</p><p>码元中间时刻的跳变既代表时钟，又代表数据。</p><p>传统以太网使用的就是满测斯特编码。</p><p><strong>差分曼彻斯特编码：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/56.png" alt="1"></p><p>在差分曼彻斯特编码中间也有跳变，但是此跳变只代表时钟信号。</p><p>具体的数据由码元开始处电平是否变化来表示。</p><p>例如在<strong>码元结束和码元开始的时候电平做对比，有变化代表0/1，无变化代表1/0</strong>。</p><h3 id="4-4-基本调制方法"><a href="#4-4-基本调制方法" class="headerlink" title="4.4 基本调制方法"></a>4.4 基本调制方法</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/57.png" alt="1"></p><h5 id="调幅："><a href="#调幅：" class="headerlink" title="调幅："></a>调幅：</h5><p>对基带信号的波幅进行调整，例如将上图的1信号波幅进行改变，在接收方读取时有载波输出为1.</p><h5 id="调频："><a href="#调频：" class="headerlink" title="调频："></a>调频：</h5><p>对基带信号频率进行调整，例如将0信号调制为频率f1，1信号调制为频率f2。</p><h5 id="调相："><a href="#调相：" class="headerlink" title="调相："></a>调相：</h5><p>对基带信号相位进行调制，例如将0信号的初相位调整为0度，1信号的初相位调整为180度。</p><p><strong>但是在以上基本调制方法中，1个码元只能包含1个比特信息。</strong></p><h5 id="混合调制："><a href="#混合调制：" class="headerlink" title="混合调制："></a>混合调制：</h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/58.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/59.png" alt="1"></p><p><strong>正交振幅调制QAM：</strong></p><p>QAM-16：</p><p>在QAM-16中有12种相位，每种相位有1或2种振幅可选择。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/60.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/65.png" alt="1"></p><p>由于此调制方法可以调制出16种码元，要完整的表示这16种情况，码元内是二进制数据，因此至少需要4个二进制数，也就是4个比特数据，因此<strong>在QAM-16调制方法中，每个码元可以表示4个比特数据</strong>。</p><p>为了防止传输出错导致错误，相邻码元之间的对应关系使用<strong>格雷码</strong>（相邻二进制数只有一位不同）。</p><h2 id="五、信道的极限容量"><a href="#五、信道的极限容量" class="headerlink" title="五、信道的极限容量"></a>五、信道的极限容量</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/61.png" alt="1"></p><p>在实际传输过程中由于通信质量的问题会导致传输信号被干扰，导致信号波形出现了码元之间的清晰界限，这种现象叫码元串扰。</p><h3 id="5-2-产生失真的因素主要有"><a href="#5-2-产生失真的因素主要有" class="headerlink" title="5.2 产生失真的因素主要有"></a>5.2 产生失真的因素主要有</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/62.png" alt="1"></p><p>其中码元传输速率（与调制方法有关）</p><p>因为以上情况，防止信道数据过大导致码间串扰，因此做出了信号极限容量的预测。</p><p>其中最著名的就是奈氏准则。</p><h5 id="奈氏准则："><a href="#奈氏准则：" class="headerlink" title="奈氏准则："></a>奈氏准则：</h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/63.png" alt="1"></p><p>由于奈氏准则是一种理想环境下的情况，在实际中极限容量要明显小于该值。</p><h5 id="香农公式："><a href="#香农公式：" class="headerlink" title="香农公式："></a>香农公式：</h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/64.png" alt="1"></p><p>由于在实际信道中，信号还有收到其他的一些损伤，例如个脉冲干扰等，因此实际信道能达成的信息速率要比计算出来的极限速率还要小。</p><p><strong>综合来看：在信道带宽一定的情况下，根据奈氏准则和香农公式，想要提高信息的传输速率必须采用多元制或者更好的调制方法，以及努力提高信道中的信噪比。</strong></p><h2 id="六、相关习题"><a href="#六、相关习题" class="headerlink" title="六、相关习题"></a>六、相关习题</h2><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/66.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/67.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/68.png" alt="1"></p><p>如果题目中没有指明信道是带通信道，也就是给出了信道频率的上下限,则信道属于低通信道。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/69.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/70.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/71.png" alt="1"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的计网学习——计算机网络概述</title>
      <link href="/2023/03/23/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A6%82%E8%BF%B0/"/>
      <url>/2023/03/23/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%AE%A1%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/17.png" alt="1"></p><p>参考博客：<div class="tag link"><a class="link-card" title="Nimrod" href="https://blog.csdn.net/Nimrod__/article/details/113127311#comments_25526717"><div class="left"><img src="https://profile-avatar.csdnimg.cn/92b5cd2dc5c148d78574e480109516d3_nimrod__.jpg!1"/></div><div class="right"><p class="text">Nimrod</p><p class="url">https://blog.csdn.net/Nimrod__/article/details/113127311#comments_25526717</p></div></a></div></p><p>本笔记来源于：<a href="https://www.bilibili.com/video/BV1c4411d7jb">计算机网络微课堂——湖科大</a></p><h2 id="一、计算机网络中在信息时代的作用"><a href="#一、计算机网络中在信息时代的作用" class="headerlink" title="一、计算机网络中在信息时代的作用"></a>一、计算机网络中在信息时代的作用</h2><p>计算机网络已由一种通信基础设施发展成为一种重要的信息服务基础设施。</p><p>计算机网络已经成为像水、电这些基础设时一样，成为我们生活重不可或缺的一部分。</p><h2 id="二、因特网概述"><a href="#二、因特网概述" class="headerlink" title="二、因特网概述"></a>二、因特网概述</h2><h3 id="2-1网络、互联网和因特网"><a href="#2-1网络、互联网和因特网" class="headerlink" title="2.1网络、互联网和因特网"></a>2.1网络、互联网和因特网</h3><p>网络：网络由<strong>若干节点和连接这些节点的链路</strong>组成。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/1.png" alt="1"></p><p>互联网：是一个覆盖范围更大的网络，由多个网络还可以通过路由器连接起来，可以理解为<strong>互联网就是网络的网络</strong>。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/2.png" alt="1"></p><p>因特网：就是<strong>世界上最大的互联网</strong>。</p><p><strong>区分：</strong></p><p>internet：互联网，这是一个通用名词，泛指由多个计算机网络互联而成的网络，网络之间的通信协议可以是任意的。</p><p>Internet：因特网，专有名词，指当前全球最大的、开放的、由众多网络相互连接而成的特定计算机网络，是采用TCP/IP写一族作为通信的规则。</p><h3 id="2-2因特网发展的三个阶段"><a href="#2-2因特网发展的三个阶段" class="headerlink" title="2.2因特网发展的三个阶段"></a>2.2因特网发展的三个阶段</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/3.png" alt="1"></p><p><strong>因特网服务提供者ISP(Internet Service Provider)</strong></p><p>提供给用户IP地址的角色，每个用户通过ISP提供的IP地址使用互联网，没有IP地址不可以使用互联网。</p><p>生活中电信等便是最经典的例子，其中在互联网中每个用户都可以是ISP。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/4.png" alt="1"></p><p>基于ISP的三个结构互联网</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/5.png" alt="1"></p><h3 id="2-3因特网的标准化工作"><a href="#2-3因特网的标准化工作" class="headerlink" title="2.3因特网的标准化工作"></a>2.3因特网的标准化工作</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/6.png" alt="1"></p><h3 id="2-4因特网的组成"><a href="#2-4因特网的组成" class="headerlink" title="2.4因特网的组成"></a>2.4因特网的组成</h3><p>边缘部分：由所有连接在因特网的主机组成，是用户<strong>直接使用的，用来通信和资源共享的设备</strong>。</p><p>核心部分：由<strong>大量网络和连接这些网络的路由器组成</strong>，这部分是为边缘服务提供服务的。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/7.png" alt="1"></p><h2 id="三、三种交换方式"><a href="#三、三种交换方式" class="headerlink" title="三、三种交换方式"></a>三、三种交换方式</h2><h3 id="3-1-三种交换概述"><a href="#3-1-三种交换概述" class="headerlink" title="3.1 三种交换概述"></a>3.1 三种交换概述</h3><div class="tabs" id="三种交换概述"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#三种交换概述-1">电路交换</button></li><li class="tab"><button type="button" data-href="#三种交换概述-2">分组交换</button></li><li class="tab"><button type="button" data-href="#三种交换概述-3">报文交换</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="三种交换概述-1"><p>电话交换机接通电话线的方式称为电路。</p><p>从通信资源的分配角度来看，交换就是按照某种方式<strong>动态的分配传输线路的资源</strong>。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/8.png" alt="1"></p><p>在图中，每个电路交换都需要建立一条物理通路，每条连接都是不可抢占，一直存在的。</p><p><strong>电路交换的步骤：</strong></p><ul><li>建立连接（分配通信资源）</li><li>通话（一直占用通信资源）</li><li>释放连接（归还通信资源）</li></ul><p>在建立后不论通信资源有没有使用，都不会进行中断，除非用户中断。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="三种交换概述-2"><p>把整块数据/信息分为多个数据段（相对于报文传输而言），数据段分开传输。在数据段前面加上首部后，成为一个分组。以分组为单位传输。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/9.png" alt="1"></p><p>在图中各个节点有<strong>存储转发</strong>的功能。</p><p>各个分组交换机会把分组先存储下来，然后根据其头部信息种的目的地地址，发送给下一个交换机。各个交换机进行对分组信息的储存转发后，最终到达主机H2。</p><p>目的地处理及再去除分组首部，还原出报文。传输完成。</p><p><strong>在转发过程中有两个特点：</strong></p><ul><li><strong>各分组从源站到达目的地可以走不同的路径</strong>。</li><li><strong>分组到达目的站的顺序不定，可能出现顺序变化</strong>。</li></ul><p><strong>在分组传输中各个角色主要功能：</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/10.png" alt="1"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="三种交换概述-3"><p>每一个结点接收整个报文，检查目标结点地址，然后根据网络中的通信情况在适当的时候转发到下一个结点。经过多次的存储——转发，最后到达目标，因而这样的网络叫存储——转发网络。其中的交换结点要有足够大的存储空间（一般是磁盘），用以缓冲收到的长报文。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="3-2-三种交换对比"><a href="#3-2-三种交换对比" class="headerlink" title="3.2 三种交换对比"></a>3.2 三种交换对比</h3><div class="tabs" id="三种交换对比"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#三种交换对比-1">电路交换</button></li><li class="tab"><button type="button" data-href="#三种交换对比-2">报文交换</button></li><li class="tab"><button type="button" data-href="#三种交换对比-3">分组交换</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="三种交换对比-1"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/11.png" alt="1"></p><p>在电路交换过程中要先进行建立连接，连接建立完成后直接进行报文传输，传输完成后释放连接。传输时以<strong>比特流直通形式</strong>传输。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="三种交换对比-2"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/12.png" alt="1"></p><p>在报文交换中，把整个报文先传送到相邻节点交换机，节点交换机进行储存下来后进行查表转发，转发到下一个节点交换机。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="三种交换对比-3"><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/13.png" alt="1"></p><p>分组交换可以随时发送分组，不需要事先建立连接。<br>将原始报文拆分成一个个分组，依次在各节点交换机上储存转发。各节点在发送分组的同时还需要缓存分组，提升效率。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="3-3-优缺点对比"><a href="#3-3-优缺点对比" class="headerlink" title="3.3 优缺点对比"></a>3.3 优缺点对比</h3><div class="tabs" id="优缺点对比"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#优缺点对比-1">电路交换</button></li><li class="tab"><button type="button" data-href="#优缺点对比-2">报文交换</button></li><li class="tab"><button type="button" data-href="#优缺点对比-3">分组交换</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="优缺点对比-1"><p><strong>优点：</strong></p><ul><li>通信时延小，适合传输大量数据</li><li>有序传输，只在一条固定线路传输，不会失序</li><li>没有冲突，只在一条线路传输，不会引发冲突</li><li>适用范围广，适合模拟信号和数字信号</li><li>实时性强</li><li>控制简单，结构简单，易于控制</li></ul><p><strong>缺点：</strong></p><ul><li>建立连接时间长</li><li>线路独占，使用效率低</li><li>灵活性差</li><li>难以规格化</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="优缺点对比-2"><p><strong>优点：</strong></p><ul><li>无需建立连接，可以随时发送报文</li><li>动态分配线路</li><li>提高线路可靠性，若线路故障会选择正常线路</li><li>提高线路利用率</li><li>提供多目标服务，一个报文可以同时发送给多个地址</li></ul><p><strong>缺点：</strong></p><ul><li>引发了转发时延，在节点中转发储存时间花费多</li><li>需要较大储存缓存空间</li><li>需要传输额外的信息量，报文中有源地址点多余信息</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="优缺点对比-3"><p><strong>优点：</strong></p><ul><li>无需建立连接</li><li>线路利用率高</li><li>简化了储存管理，分组长度固定，缓冲区固定，易于管理</li><li>加速传输，节点的转发和缓存同时进行，提高速度</li><li>减少出错概率和重发数据量</li></ul><p><strong>缺点：</strong></p><ul><li>引发了转发时延</li><li>需要传输额外的信息量，分组中有源地址等额外信息量</li><li>对于数据报服务，处在失序、丢失货重复分组的问题</li><li>对于虚电路服务，存在呼叫建立、数据传输和虚电路释放三个过程</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="四、计算机网络的定义和分类"><a href="#四、计算机网络的定义和分类" class="headerlink" title="四、计算机网络的定义和分类"></a>四、计算机网络的定义和分类</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><p>计算机在实际上<strong>没有精确统一的定义</strong>。</p><p>但是有一个最简单的定义：<strong>互连、自治的计算机集合。</strong><br>其中：</p><ul><li><strong>互连</strong>指的是计算机之间可以通过有线或者无线的方式进行数据通信。</li><li><strong>自治</strong>指的是独立的子算计，拥有自己的硬件和软件，可以独立运行。</li><li><strong>计算机集合</strong>指的是至少要有两台计算机。</li></ul><p>比较全面的定义是：</p><p>计算机网络主要是一些通用的、可编程的硬件互连而成的，而硬件并非专门用来实现某一特定目的的，这些可编程的硬件能够用来传送多种不同类型的数据（如音频、数据），这些可编程的软件能支持广泛和日益增长的应用。</p><p>在实际生活中不同的发展阶段对计算机网络的定义不同，对其的定义反应了当时的网络技术发展水平。</p><h3 id="分类："><a href="#分类：" class="headerlink" title="分类："></a>分类：</h3><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/14.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/15.png" alt="1"></p><p>公用网指的是电信公司构建的大型网络，<strong>面对公众开放</strong>，只要交钱既可使用，又叫公众网。</p><p>专用网指的是某个部门为本单位的特殊业务工作需要而建造的网络。例如铁路部门、电信部门<strong>专门为自己构建的，不对外开放</strong>的网络。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/16.png" alt="1"></p><p>有线网络主要双绞线(日常使用网线)、光纤。</p><p>无线网络主要是WIFI。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/17.png" alt="1"></p><p>广域网WAN：覆盖面积通常为几十公里到几千公里，可以覆盖国家范围、洲际范围，有时也成为远程网。负责互连分布在不同区域的城域网和局域网，是最大范围的网络。</p><p>城域网MAN：覆盖范围一般是一个城市。作用距离为5到50公里。通常作为城市骨干网，互连大量企业、机构、学校。</p><p>局域网LAN:局域网一般是微信计算机或工作站通过告诉线路相连，范围一般是一个实验室、一栋楼或一个校园。通常由某个单位单独拥有、使用和维护。</p><p>个域网PAN：个人区域网络。非用来连接普通计算机，耳式在个人工作的地方把个人使用的电子设备，鼠标、键盘、耳机等用无线的方式连接起来形成的个人网络系统。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/18.png" alt="1"></p><p>总线型网络：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/19.png" alt="1"></p><p>总线型网络用<strong>单根传输线</strong>把计算机连接起来。</p><p>优点是<strong>建网容易，增减节点方便，节省线路</strong>。</p><p>缺点是<strong>重负载时通信效率不高，任意一处出现故障后全网瘫痪</strong>。</p><p>星型结构：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/20.png" alt="1"></p><p>星型网络是将每个计算机都以单独的线路与中央设备相连。中央设备现在一般是交换机。</p><p>优点：便于网络的<strong>集中控制和管理</strong>。</p><p>缺点：<strong>成本高，中央设备对故障敏感</strong>。</p><p>环形结构：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/21.png" alt="1"></p><p>环形网络是将所有计算机网络接口连接成一个环。环可以是单环或者双环，环中信号是单项传输的。</p><p>网状型结构：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/22.png" alt="1"></p><p>在网状结构中，每个节点至少由两条路径与其他节点相连接，多用在广域网中。</p><p>优点：<strong>可靠性高</strong>。</p><p>缺点：<strong>控制复杂，线路成本高</strong>。</p><p><strong>在以上四种结构中可以组合任意的更加复杂有效的结构。</strong></p><h2 id="五、计算机网络中的性能指标"><a href="#五、计算机网络中的性能指标" class="headerlink" title="五、计算机网络中的性能指标"></a>五、计算机网络中的性能指标</h2><p>性能指标可以从不同的方面来度量计算机网络的性能。</p><p>常用的性能指标有8个 ⬇</p><div class="tabs" id="性能指标"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#性能指标-1">速率</button></li><li class="tab"><button type="button" data-href="#性能指标-2">带宽</button></li><li class="tab"><button type="button" data-href="#性能指标-3">吞吐量</button></li><li class="tab"><button type="button" data-href="#性能指标-4">时延</button></li><li class="tab"><button type="button" data-href="#性能指标-5">时延宽带积</button></li><li class="tab"><button type="button" data-href="#性能指标-6">往返时间RTT</button></li><li class="tab"><button type="button" data-href="#性能指标-7">利用率</button></li><li class="tab"><button type="button" data-href="#性能指标-8">丢包率</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="性能指标-1"><p>首先先了解比特：</p><p>比特：计算机中<strong>数据量的单位</strong>，也是信息论中信息量的单位。一个比特就是二进制数字中的一个1或0。</p><p>基本单位：bit(b) 比特<br>常用单位：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">8 bit = 1 Byte</span><br><span class="line">KB = 2^10B</span><br><span class="line">MB = K KB = 2^20 B</span><br><span class="line">GB = K MB = 2^30 B</span><br><span class="line">TB = K GB = 2^40 B</span><br></pre></td></tr></table></figure><p>速率就是<strong>连接在计算机网络上的主机在数字信道上传送比特的速率</strong>，也称为<strong>比特率或数据率</strong>。</p><p>基本单位：bit/s(b/s，bps)</p><p>常用单位：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kb/s = 10^3b/s</span><br><span class="line">Mb/s = K Kb/s = 10^6 b/s</span><br><span class="line">Gb/s = k Mb/s = 10^9 b/s</span><br><span class="line">Tb/s = k Gb/s = 10^12 b/s</span><br></pre></td></tr></table></figure><p>注意！！</p><p><strong>数据单位bit中K = 2^10B</strong></p><p><strong>数据单位bit中K = 10^3B</strong></p><p>所以在计算过程中二者存在一定差距，不要弄错了。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-2"><p>带宽在模拟信号系统中表示的是信号所包含的各种不同频率成分所占据的频率范围，也就是传输过程中最大频率与最小频率的范围。</p><p>带宽中计算机网络中的意义：</p><p>用来表示网络的通信线路所能传送数据的能力，因此网络带宽表示在单位时间内从网络中的某一点到另一个点所能通过的“最高数据率。</p><p>单位：b/s(kb/s, Mb/s, Gb/s, Tb/s)，与速率相同。</p><p>在日常生活中的宽带带宽除以8，一般就是平时使用中的最高传输速率。（1B = 8bit）</p><p>是一个很重要的计算机网络性能指标。直接关系网络的应用体验。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-3"><p>吞吐量表示<strong>在单位时间内通过某个网络（或信道、接口）的数据量。</strong></p><p>吞吐量被经常用于对现实世界中的网络的一种测量，以便知道实际上到底<strong>有多少数据量能够通过网络</strong>。</p><p>吞吐量受网络的<strong>带宽或额定速率</strong>的限制。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-4"><p>发送时延：源主机将分组发送出去产生的时延。</p><p>计算方式： 分组长度(b) / 发送速率(b/s)。</p><p>发送速率 = min[网卡发送速率，信道带宽，交换机或路由器的接口速率]</p><p>发送效率是一个短板效应，由速率最低的模块决定，所以在选择网线、路由器等设备时，要考虑到整体的速率平衡问题。</p><p>传播时延:分组在线路中传播产生的时延。</p><p>计算方式：信道长度(m) / 电磁波传播速率(m/s)</p><p>处理时延：路由器收到分组后对其进行存储转发产生的时延。</p><p>一般不方便计算。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-5"><p>传播时延和带宽的乘积。</p><p>若发送端连续发送数据，则在所发送的第一个比特即将到达终点时，发送端就已经发送了时延带宽积个比特。</p><p>实际上就是<strong>一个比特到达终点的单位时间内传输的比特个数</strong>。</p><p>链路的时延带宽积又称为以比特为单位的链路长度。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-6"><p>一个网络请求双向交互一次所需的时间。</p><p>往返时间RTT也是一个重要的性能指标</p><p>可以更好的了解到网络的情况。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-7"><p><strong>信道利用率</strong>：用来表示某信道有百分之几的时间是被利用的（有数据通过）。</p><p><strong>网络利用率</strong>是全网络的信道利用率的加权平均。</p><p>根据排队论，当某信道的利用率增大时，该信道引起的时延也会迅速增大。如图所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/23.png" alt="1"></p><p>当网络利用率达到50％时，时延就要加倍。</p><p>当网络利用率超过50％时，时延就急剧增大。</p><p>当网络利用率接近100％时，时延趋于无穷大。</p><p>因此要控制信道利用率不超过50％，否则就要准备扩容。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能指标-8"><p>丢包率即分组丢失率，是指在一定的时间范围内，传输过程中丢失的分组数量与总分组数量的比率。</p><p>丢包率具体可分为接口丢包率、结点丢包率、链路丢包率、路径丢包率、网络丢包率等。</p><p>分组在传输过程中出现误码，被结点丢弃。</p><p>分组到达一台队列已满的分组交换机时被丢弃，在通信量较大时就可能造成网络拥塞。</p><p>丢包率反映了网络的拥塞情况。</p><p>无拥塞时丢包率为0</p><p>轻度拥塞时丢包率为1％—4％</p><p>严重拥塞时丢包率为5％-15％</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="六、计算机网络体系结构"><a href="#六、计算机网络体系结构" class="headerlink" title="六、计算机网络体系结构"></a>六、计算机网络体系结构</h2><h3 id="6-1-常见的计算机网络体系结构"><a href="#6-1-常见的计算机网络体系结构" class="headerlink" title="6.1 常见的计算机网络体系结构"></a>6.1 常见的计算机网络体系结构</h3><h5 id="OSI体系结构：是法律上的国际标准。"><a href="#OSI体系结构：是法律上的国际标准。" class="headerlink" title="OSI体系结构：是法律上的国际标准。"></a><strong>OSI体系结构</strong>：<strong>是法律上的国际标准。</strong></h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/24.png" alt="1"></p><h5 id="TCP-IP体系结构：是实际上的国际标准。"><a href="#TCP-IP体系结构：是实际上的国际标准。" class="headerlink" title="TCP/IP体系结构：是实际上的国际标准。"></a><strong>TCP/IP体系结构</strong>：<strong>是实际上的国际标准。</strong></h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/25.png" alt="1"></p><p>TCP/IP体系结构相当于：</p><p>将OSI结构的物理层和数据链路层合并成为网络接口层，去掉了会话层和表示层。</p><p>在TCP/IP协议中的网络接口层并没有规定什么具体的内容，目的是为了允许任何形式的网络接口使用TCP/IP协议。</p><p>所以实际上TCP/IP协议只有三层：网际层、运输层、应用层。</p><p>其中：</p><p>网际层核心协议是IP协议。</p><p>运输层的两个重要协议：TCP(可靠传输)、UDP(不可靠传输)。</p><p>应用层有大量的应用协议：HTTP、SMTP等等。</p><p>层次之间关系：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/26.png" alt="1"></p><h5 id="原理体系结构"><a href="#原理体系结构" class="headerlink" title="原理体系结构"></a>原理体系结构</h5><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/27.png" alt="1"></p><p>此种结构的出现时便于学习。</p><p>此种结构相当于把TCP/IP协议中的网络接口层还原成数据链路层、物理层。网际层还原成网络层。</p><p><strong>在接下来的学习中主要使用该种模型。</strong></p><h3 id="6-2-计算机网络结构分层的必要性"><a href="#6-2-计算机网络结构分层的必要性" class="headerlink" title="6.2 计算机网络结构分层的必要性"></a>6.2 计算机网络结构分层的必要性</h3><ol><li>计算机网络是个非常复杂的系统</li><li>“分层”可将庞大而复杂的问题，转化为若干较小的局部问题</li></ol><p>基于以上原因选择对计算机网络进行分层。</p><h5 id="其中各层的主要解决问题："><a href="#其中各层的主要解决问题：" class="headerlink" title="其中各层的主要解决问题："></a>其中各层的主要解决问题：</h5><div class="tabs" id="各层解决问题"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#各层解决问题-1">物理层</button></li><li class="tab"><button type="button" data-href="#各层解决问题-2">数据链路层</button></li><li class="tab"><button type="button" data-href="#各层解决问题-3">网络层</button></li><li class="tab"><button type="button" data-href="#各层解决问题-4">运输层</button></li><li class="tab"><button type="button" data-href="#各层解决问题-5">应用层</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="各层解决问题-1"><ul><li>采用怎样的传输媒体（介质）？</li><li>采用怎样的物理接口？</li><li>使用怎样的信号表示比特0和1？</li></ul><p><strong>解决以上问题后就可以实现01信号在计算机之间的传输。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="各层解决问题-2"><ul><li>如何标识网络中的各主机（主机编址问题，例如MAC地址）？</li><li>如何从信号所表示的一连串比特流中区分出地址和数据？</li><li>如何协调各主机通信（例如，各主机争用总线，交换机的实现原理）？</li></ul><p><strong>解决此问题后可以实现分组在一个网络上传输。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="各层解决问题-3"><ul><li>如何标识各网络以及网络中的各主机（网络和主机共同编址的问题，例如IP地址）？</li><li>路由器如何转发分组，如何进行路由选择?</li></ul><p><strong>解决此问题后可以实现分组在网络间传输。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="各层解决问题-4"><ul><li>如何解决进程之间基于网络的通信？</li><li>出现传输错误时如何处理？</li></ul><p><strong>解决此问题后可以实现进程之间基于网络的通信。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="各层解决问题-5"><p>通过应用进程间的交互来完成特定的网络应用。</p><p>例如：支持万维网应用的HTTP协议，支持电子邮件的SMTP协议，支持文件传送的FTP协议。</p><p><strong>解决此问题后可以实现计算机网络所解决的所有问题。</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p><em>在此一层一层的问题解决中边逐步从物理层、数据链路层、网络层、运输层、应用层结局问题，实现目的，这也是分层思想解决问题的最好提现。</em></p><h3 id="6-3-计算机网络体系结构分层思想举例"><a href="#6-3-计算机网络体系结构分层思想举例" class="headerlink" title="6.3 计算机网络体系结构分层思想举例"></a>6.3 计算机网络体系结构分层思想举例</h3><p>以易于学习的五层结构思想为例：</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/28.png" alt="1"></p><p>在五层结构中，各层负责任务如下：</p><p>应用层：按照http请求的协议，构建报文。然后交付给运输层处理。</p><p>运输层：给http请求加上一个头部，使其成为一个TOP报文段。其头部的作用是识别进程和实现可靠传输（TCP的特点）。而后交给网络层处理。</p><p>网络层：给TCP报文段加上一个IP头部，使其成为IP数据报。其首部作用是为了使该数据段可以在互联网上传输。而后将其交付给数据链路层处理。</p><p>数据链路层：给IP数据报添加一个首部和一个尾部，使其成为帧（按帧传输）。</p><p>其首部的作用是使其能够在一段链路或者网络上传输，以及被目的交换机接收并处理。</p><p>其尾部的作用是为了让目的主机检查是否有误码。</p><p>最后将其交给物理层。</p><p>物理层：将帧看作是比特流（01编码），由于是在以太网传输，因而给其加上前导码，便于传输。并且将其变成相应的信号发送到传输媒体。</p><p>此时，发送端处理结束。</p><p>由于接收端的过程就是发送端的逆过程，不再进行啰嗦。</p><h3 id="6-4-计算机网络体系结构中的术语"><a href="#6-4-计算机网络体系结构中的术语" class="headerlink" title="6.4 计算机网络体系结构中的术语"></a>6.4 计算机网络体系结构中的术语</h3><h5 id="实体："><a href="#实体：" class="headerlink" title="实体："></a>实体：</h5><p>实体是指<strong>任何可发送或接收信息的硬件或软件进程。</strong></p><p>对等实体是指<strong>通信双方相同层次中的实体。</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/29.png" alt="1"></p><p>在此图中，A、B等是实体，A、H是对等实体。</p><h5 id="协议："><a href="#协议：" class="headerlink" title="协议："></a>协议：</h5><p>协议是<strong>控制两个对等实体进行逻辑通信的规则的集合</strong>，例如TCP/IP协议。</p><p>协议的三要素：语法，语义，同步。</p><p>语法定义所交换信息的格式。例如，IP数据报的格式。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/30.png" alt="1"></p><p>语法定义了所交换信息由那些字段以及何种顺序构成。</p><p>语义定义通信双方所要完成的操作。例如，主机HTTP的GET请求给Web服务器，Web服务器收到后执行相应的操作，然后给主机发回HTTP的响应。</p><p>同步定义通信双方的时序关系。例如，TCP的“三报文握手”建立连接。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/31.png" alt="1"></p><p>对等层次之间传送的数据包称为该层的协议数据单元PDU。<br>举例：</p><p>应用层：报文(message)</p><p>运输层：TCP报文段(segment)或UDP用户数 据报(datagram)</p><p>网络层：分组(packet)或IP数据报</p><p>数据链路层：帧(frame)</p><p>物理层：比特流(bit stram)</p><p>以上的便是各层的协议数据单元PDU。</p><p>协议对于实体来说是抽象的，看不见内在，只能看见提供的功能。</p><h5 id="服务："><a href="#服务：" class="headerlink" title="服务："></a>服务：</h5><p>在协议中，每层不但要实现本层协议，还要使用下一层所提供的服务。</p><p>并且在协议的控制下，两个对等实体间的逻辑通信要是的本层能够向上一层提供服务。</p><p>协议是<strong>水平</strong>的，服务是<strong>垂直的</strong>。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/32.png" alt="1"></p><p>服务访问点：在同一系统中相邻两层的实体交换信息的逻辑接口，用于区分不同的服务类型。</p><p>示例：</p><p>数据链路层的服务访问点为帧的“类型”字段。</p><p>网络层的服务访问点为IP数据报首部中的“协议字段”。</p><p>运输层的服务访问点为“端口号”。</p><h5 id="服务原语："><a href="#服务原语：" class="headerlink" title="服务原语："></a>服务原语：</h5><p>上层使用下层所提供的服务必须通过与下层交换一些命令。</p><h5 id="服务数据单元SDU："><a href="#服务数据单元SDU：" class="headerlink" title="服务数据单元SDU："></a>服务数据单元SDU：</h5><p>同一系统内，层与层之间交换的数据包。</p><p>多个SDU可以合成为一个PDU；一个SDU也可划分为几个PDU。</p><p><img src="https://picbed.dai2yutou.space/article_img/计算机网络/33.png" alt="1"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github_copilot使用</title>
      <link href="/2023/03/21/github-copilot%E4%BD%BF%E7%94%A8/"/>
      <url>/2023/03/21/github-copilot%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/5.png" alt="1"></p><p>前几天<code>gpt-4</code>发布会展出全新的人工智能，写代码能力超强，导致小破站上又人才济济的讨论，它的超强功能在代码方便，但是每个月<strong>10美元</strong>的费用实在是消费不起，于是我又关注到了<code>github copilot</code>，也是成功使用了，怎么说呢，使用<code>vscode</code>借助这个工具感觉还蛮好用的。</p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/3.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/4.png" alt="1"></p><h2 id="Github-Copilot是什么？"><a href="#Github-Copilot是什么？" class="headerlink" title="Github Copilot是什么？"></a>Github Copilot是什么？</h2><p><code>GitHub Copilot</code>是一款基于人工智能的代码自动生成工具，它由<code>GitHub</code>和<code>OpenAI</code>联合开发。它使用了一个庞大的预训练模型，即<code>OpenAI</code>的<code>GPT</code>模型，来理解程序员所输入的自然语言描述，并生成与描述相关的代码片段。使用<code>Copilot</code>，程序员可以通过输入自然语言描述来生成代码，从而提高编写代码的效率和准确性。<code>Copilot</code>可以在多种编程语言中生成代码，并可以自动完成函数、类、变量等的定义和命名，还可以根据上下文生成更加精确的代码。<code>Copilot</code>还可以在<code>GitHub</code>上的代码库中搜索相关的代码片段，并根据上下文推荐最佳的代码实现方式。Copilot是一个非常有潜力的工具，它可以帮助程序员更快、更准确地编写代码，并可能改变未来编程的方式。</p><h2 id="如何在VsCode中使用？"><a href="#如何在VsCode中使用？" class="headerlink" title="如何在VsCode中使用？"></a>如何在VsCode中使用？</h2><blockquote><p>其他的例如JetBrains中使用方法是一样的，大差不差！</p></blockquote><p>首先已经在vscode中下载了插件，并登录成功！</p><p><code>GitHub Copilot</code>是一款基于人工智能技术的代码助手，它可以为你自动生成代码，并提供一些代码提示。当你在编写代码时，<code>Copilot</code>会在编辑器中显示一些浅色的代码提示，并在你按下<code>Tab</code>键时自动补全代码。</p><p>如果你希望<code>Copilot</code>自动为你生成代码，可以在命令面板中搜索“<strong>GitHub Copilot: 建议下一行</strong>”或“<strong>GitHub Copilot: 建议下一个片段</strong>”，然后选择你想要的建议。<code>Copilot</code>将会自动为你生成代码，并在光标位置插入。</p><p>如果你发现<code>Copilot</code>的代码提示颜色太浅，难以看清，你可以尝试更改编辑器的主题或调整<code>Copilot</code>的颜色设置。你可以在<code>VS Code</code>的设置中搜索<code>“copilot”</code>，然后更改相应的设置，例如<code>“copilot.showSuggestionDecorations”</code>和<code>“copilot.suggestionDecorationColor”</code>。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> copilot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vercel部署网站</title>
      <link href="/2023/03/14/Vercel%E9%83%A8%E7%BD%B2%E7%BD%91%E7%AB%99/"/>
      <url>/2023/03/14/Vercel%E9%83%A8%E7%BD%B2%E7%BD%91%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/16.jpg" alt="1"></p><div class="note orange icon-padding flat"><i class="note-icon fas fa-battery-half"></i><p>用来记录一下将网站代码提交到Github上，并使用Vercel进行部署，进行有效管理！😁</p></div><div class="note info simple"><p><strong>参考: <a href="https://www.fomal.cc/posts/d7fb1ba1.html">免费图床综合教程 | Fomalhaut🥝</a></strong></p></div><h2 id="一、建立Github仓库clone到本地"><a href="#一、建立Github仓库clone到本地" class="headerlink" title="一、建立Github仓库clone到本地"></a>一、建立Github仓库clone到本地</h2><ol><li>进入<a href="https://github.com/">Github官网</a>注册并登录自己的账号，到自己的个人主页，点击右上角的<code>+</code>，并选择<code>New Repository</code>创建自己的仓库。</li></ol><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/1.jpg" alt="1"></p><ol><li>仓库名字随意，描述也可以自由发挥，可见性最好选<code>public</code>，<code>Readme</code>文件可以创建，然后点击<code>Create Repository</code>创建仓库。</li></ol><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/2.jpg" alt="1"></p><ol><li><p>让你的计算机与Github建立通信，主要是<code>创建公钥</code>、<code>上传公钥</code>、<code>检查通信</code>这几步，确保计算机有权限访问远程仓库，这些已经先前创建好。</p></li><li><p>先在本地创建一个文件夹，文件夹位置和名字随意就可以，进入该文件夹后右键打开<code>Git Bash</code>，然后输入以下代码把之前创建的仓库拷贝下来，其中<code>git clone</code>后面的东西要替换成自己的仓库信息，可以通过自己仓库的ssh链接来获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:xiaoyutoua/blog.git</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/3.jpg" alt="1"></p></li></ol><h2 id="二、项目推送"><a href="#二、项目推送" class="headerlink" title="二、项目推送"></a>二、项目推送</h2><p>我们在带有<code>Readme.md</code>文件夹右击打开<code>Git Bash</code>（注意看清楚是什么文件夹），将需要push到github上的文件复制到该目录下或者对文件进行修改等操作，然后依次输入以下命令把更改推送到远程仓库，最后一步不成功可能要多试几次。</p><p>注意📌:</p><blockquote><p>关键文件是.git文件，它的作用就是与github中你的仓库关联</p></blockquote><p>❗❗❗</p><blockquote><p>如果你的目的是部署网站，那么只需要上传静态页面文件，并利用Verel一键部署，其他的全部代码文件本地修改，之后生成静态页面文件即可。<br>github仓库中和vercel中都只是静态页面文件<br>如果需要推送全部的代码文件到github上存储，那么需要再新建立一个库，clone到本地，并推送全部代码文件等！！！</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将更改提交</span></span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;填入你此次更改的内容啥的（啥都可以）&quot;</span></span><br><span class="line"><span class="comment"># 推送至github仓库</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>最后看见如下信息就代表推送成功了</p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/4.jpeg" alt="1"></p><p>我们可以在Github上看到此次推送的内容</p><p>以后我们在本地更改此项目后，直接执行上述命令即可修改Github上的内容。</p><h2 id="三、Staticaly-CDN加速"><a href="#三、Staticaly-CDN加速" class="headerlink" title="三、Staticaly CDN加速"></a>三、Staticaly CDN加速</h2><p>直接访问Github仓库的资源是非常慢的！因此我们要用一些免费的CDN进行加速，<code>Staticaly CDN</code>是目前免费CDN中比较好用的啦，他的应用规则如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 格式 其中 user是用户名  repo是仓库名  version代表版本(tag或者分支 默认为main)  flie是文件路径 </span><br><span class="line">https://cdn.staticaly.com/gh/user/repo@version/file</span><br><span class="line"></span><br><span class="line"># 比如我的示例仓库就是加速地址就是这个大家可以参考参考</span><br><span class="line">https://cdn.staticaly.com/gh/fomalhaut1998/pic_bed@main/img/p2.webp</span><br></pre></td></tr></table></figure><h2 id="四、Vercel-部署"><a href="#四、Vercel-部署" class="headerlink" title="四、Vercel 部署"></a>四、Vercel 部署</h2><p><strong>4.1 进入<a href="https://vercel.com/dashboard">Vercel控制面板</a>新建项目，并<code>通过Github继续</code>，选择导入刚刚创建的仓库，然后直接部署即可</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/5.jpeg" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/6.jpeg" alt="1"></p><p><strong>4.2 在进入该项目控制台后，选择右上角的<code>View Domains</code>添加自定义域名</strong></p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/7.jpg" alt="1"></p><p>然后在这里输入你需要绑定的域名。</p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/8.jpg" alt="1"></p><p>如果你是二级域名，则添加后他会提示你添加一条DNS解析记录。</p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/10.jpg" alt="1"></p><p>如果你是新买的域名，直接输入你新买的一级域名即可，例如<code>demo123.com</code>，他会推荐你将<code>demo123.com</code>重定向至<code>www.demo123.com</code>，点<code>ADD</code>即可，然后他会提示你添加两条解析记录，一个是<code>@</code>开头的和<code>CNAME</code>开头的，添加记录的方法和二级域名一致。</p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/9.jpg" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/11.jpg" alt="1"></p><p><strong>4.3 域名解析</strong></p><p>在<strong>阿里云</strong>或者<strong>腾讯云</strong>域名解析记录里面添加如下记录，其中记录类型对应<code>Type</code>，主机记录对应<code>Name</code>，记录值对应<code>Value</code>，其他的设置默认即可。</p><p>以下以示例在腾讯云解析步骤：</p><ul><li><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/12.jpg" alt="1"></p></li><li><p>点击解析后-&gt;添加记录</p></li><li><p>记录类型为<code>Type</code>，主机记录为<code>Name</code>，记录值为<code>Value</code></p><p><img src="https://picbed.dai2yutou.space/article_img/Vercel部署项目/13.jpg" alt="1"></p></li></ul><p><strong>4.4 回到Vercel刚刚查看域名的地方，如果操作没问题，应该会显示域名配置成功的提示，此时就可以通过自定义域名来访问我们网站或图片等资源了.</strong></p><p><strong>4.5 访问</strong></p><p>直接输入域名进行访问，如果需要访问其中内部的图片文件，比如你的网站是用来作为图床作用的，则可以直接域名/根目录下的文件…</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vercel </tag>
            
            <tag> 网站部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板字符串</title>
      <link href="/2023/03/08/%E6%A8%A1%E6%9D%BF%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2023/03/08/%E6%A8%A1%E6%9D%BF%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/15.png" alt="1"></p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>模板字符串（Template Strings）是 JavaScript 中一种特殊的字符串，用来定义复杂的字符串模板，支持变量替换和特定语法。它们通常也被称为字符串插值（String Interpolation）。</p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>模板字符串有助于在字符串模板中使用变量、表达式、以及函数，而不需要使用复杂的字符串拼接。</p><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p>模板字符串使用反引号（`）来代替普通字符串中的引号（’ 或 “），变量使用 ${} 来代替普通字符串中的变量</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> name = <span class="string">&#x27;Jack&#x27;</span>;</span><br><span class="line"><span class="keyword">let</span> age = <span class="number">22</span>;</span><br><span class="line"><span class="keyword">let</span> message = <span class="string">`My name is <span class="subst">$&#123;name&#125;</span>, and I am <span class="subst">$&#123;age&#125;</span> years old.`</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(message);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JavaScript </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Servlet技术</title>
      <link href="/2023/03/08/Servlet%E6%8A%80%E6%9C%AF/"/>
      <url>/2023/03/08/Servlet%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/web_img/14.png" alt="1"></p><h3 id="1、创建servlet类，并在页面输出Hello-World"><a href="#1、创建servlet类，并在页面输出Hello-World" class="headerlink" title="1、创建servlet类，并在页面输出Hello World"></a>1、创建servlet类，并在页面输出Hello World</h3><p>我们在首先创建一个<code>JAVA</code>项目，并进行<code>JAVAWEB</code>和<code>Tomcat</code>的配置</p><p>然后在<code>src</code>文件夹下创建一个名为<code>servlet</code>的<code>Servlet</code>类，如下所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/6.png" alt="1"></p><p>定义<code>@WebServlet(urlPatterns=&quot;/login.do&quot;)</code>，即将此<code>Servlet</code>类映射为<code>login.do</code>，然后运行此<code>Servlet</code>类，</p><p>将网址路径改为<strong>localhost:8000/项目名/login.do</strong>，如下图所示：</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/7.png" alt="1"></p><p>可以看到在页面上输出了<code>Hello World</code>。</p><h3 id="2、创建表单页面并提交到Servlet类中"><a href="#2、创建表单页面并提交到Servlet类中" class="headerlink" title="2、创建表单页面并提交到Servlet类中"></a>2、创建表单页面并提交到Servlet类中</h3><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>首先在项目目录下的<code>WEB</code>目录下创建一个<code>html</code>页面，例如<code>a.html</code>，然后写上表单项：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;/Test/login.do&quot;</span> <span class="attr">method</span>=<span class="string">&quot;post&quot;</span>&gt;</span></span><br><span class="line">    账号：<span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">&quot;username&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    密码：<span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">&quot;psd&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span> <span class="attr">value</span>=<span class="string">&quot;登录&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：上图中的<code>action</code>路径为<code>/Test/login.do</code>，这是<strong>绝对路径</strong>的写法，也可以直接/login.do</p><p>也可以使用<strong>相对路径</strong>的写法：<code>../login.do</code>，找到上一级目录级web文件夹所在目录，然后直接找到<code>login.do</code>的url所在的类；也可以直接<code>login.do</code>，因为这个类与这个html的表单页面是同级的！！！</p></blockquote><p>然后在项目目录下的<code>src</code>目录下创建一个<code>Servlet</code>类，用来接收信息，并进行处理。</p><p>例如我们创建了一个名为<code>Test</code>的项目，并在其<code>src</code>目录下创建一个名为<code>Servlet01</code>的<code>Servlet</code>类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> javax.servlet.ServletException;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.annotation.WebServlet;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServlet;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletRequest;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletResponse;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@WebServlet(urlPatterns=&quot;/login.do&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Servlet01</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line"><span class="comment">//        PrintWriter out = response.getWriter();</span></span><br><span class="line"><span class="comment">//        out.println(&quot;hello Servlet&quot;);</span></span><br><span class="line">        String name=request.getParameter(<span class="string">&quot;username&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">psd</span> <span class="operator">=</span> request.getParameter(<span class="string">&quot;psd&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;账号为：&quot;</span>+name+<span class="string">&quot;，密码为：&quot;</span>+psd);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.doPost(request,response);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上所示，我们在创建<code>Servlet</code>类的时候，在类前面加了一个注解<code>@WebServlet(urlPatterns=&quot;/login.do&quot;)</code>，即将此Servlet类映射为路径<code>login.do</code>，然后在表单中<code>action=“/Test/login.do”</code>，此时在表单中输入的内容便会发送到此<code>Servlet</code>类中，并打印出来。</p><p>结果如下：</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/8.png" alt="1"></p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/9.png" alt="1"></p><h4 id="底层原理："><a href="#底层原理：" class="headerlink" title="底层原理："></a>底层原理：</h4><p>若想让<code>Servlet</code>正确的运行在服务器中并处理请求信息，我们必须进行适当的配置。<code>Servlet</code>的配置主要有两种方式：</p><h5 id="⭕使用web-xml配置Servlet"><a href="#⭕使用web-xml配置Servlet" class="headerlink" title="⭕使用web.xml配置Servlet"></a>⭕使用<code>web.xml</code>配置<code>Servlet</code></h5><p>在<code>web.xml</code>文件中，通过<code>&lt;servlet&gt;</code>标签进行注册</p><ul><li>在<code>&lt;servlet&gt;</code>标签下，使用<code>&lt;servlet-name&gt;</code>指定该<code>servlet</code>的名称，一般与<code>Servlet</code>类名相同，且唯一；</li><li>然后在<code>&lt;servlet&gt;</code>标签下，继续使用<code>&lt;servlet-class&gt;</code>指定该<code>Servlet</code>类的位置，包括包名和类名。</li></ul><p>在<code>web.xml</code>文件中，使用<code>&lt;servlet-mapping&gt;</code>标签进行映射，将<code>Servlet</code>映射到<code>URL</code>地址</p><ul><li>使用<code>&lt;servlet-name&gt;</code>子标签指定要映射的Servlet名称，名称要和之前在<code>&lt;servlet&gt;</code>标签下注册的相同；</li><li>使用<code>&lt;url-pattern&gt;</code>子标签映射url地址，地址前必须加“/”，否则访问不到。</li></ul><p>例如上示例用<code>web.xml</code>配置<code>Servlet</code>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">servlet</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">servlet-name</span>&gt;</span>Servlet01<span class="tag">&lt;/<span class="name">servlet-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">servet-class</span>&gt;</span>Servlet01<span class="tag">&lt;/<span class="name">servet-class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">servlet</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">servlet-mapping</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">servlet-name</span>&gt;</span>Servlet01<span class="tag">&lt;/<span class="name">servlet-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url-pattern</span>&gt;</span>/Test/login.do<span class="tag">&lt;/<span class="name">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">servlet-mapping</span>&gt;</span></span><br></pre></td></tr></table></figure><p>我们使用表单进行数据发送的时候，<code>action=“/Test/login.do”</code>，<code>JAVA</code>会自动帮我们到<code>web.xml</code>去找<code>&lt;servlet-mapping&gt;</code>标签下的<code>&lt;url-pattern&gt;</code>标签，然后找到对应的<code>servlet-name</code>，然后再到<code>&lt;servlet&gt;</code>标签找到对应的<code>servlet-name</code>，然后根据这个找到<code>&lt;servet-class&gt;</code>标签放的类的位置，即可将数据发送到此<code>Servlet</code>类中。</p><h5 id="⭕使用-WebServlet注解配置Servlet"><a href="#⭕使用-WebServlet注解配置Servlet" class="headerlink" title="⭕使用@WebServlet注解配置Servlet"></a>⭕使用<code>@WebServlet</code>注解配置<code>Servlet</code></h5><p>如上面例子中的<code>@WebServlet(name=&quot;Servlet01&quot;,urlPatterns=&quot;/login.do&quot;)</code></p><p>我们再使用时仅使用<code>name</code>和<code>urlPatterns</code>即可；</p><p>其中<code>name</code>为属性值指定<code>Servlet</code>的<code>name</code>属性，等价于<code>&lt;Servlet-name&gt;</code>，如果没有设置<code>name</code>属性，其默认值是<code>Servlet</code>类的完整名称。</p><p><code>urlPatterns</code>属性值用于指定一组<code>Servlet</code>的<code>URL</code>的匹配模式，等价于<code>&lt;url-pattern&gt;</code>标签。</p><h3 id="3、表单action路径方法"><a href="#3、表单action路径方法" class="headerlink" title="3、表单action路径方法"></a>3、表单action路径方法</h3><blockquote><p><code>servlet</code>是包名，在<code>myapp</code>这个项目的<code>src</code>下新建了一个名为<code>servlet</code>的包，里面放了名为<code>HelloServlet</code>的类</p></blockquote><p>表单的action路径可以通过多种方式来指定，下面是一些常见的方式：</p><ol><li>相对路径</li></ol><p>相对路径是相对于当前页面的路径来指定的，例如：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;servlet/HelloServlet&quot;</span> <span class="attr">method</span>=<span class="string">&quot;post&quot;</span>&gt;</span></span><br><span class="line">    //...</span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上述代码表示将表单提交的数据发送到相对路径为servlet/HelloServlet的Servlet上。</p><ol><li>绝对路径</li></ol><p>绝对路径是相对于Web应用程序的根目录来指定的，例如：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;/servlet/HelloServlet&quot;</span> <span class="attr">method</span>=<span class="string">&quot;post&quot;</span>&gt;</span></span><br><span class="line">    //...</span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上述代码表示将表单提交的数据发送到绝对路径为/servlet/HelloServlet的Servlet上。</p><ol><li>完整URL地址</li></ol><p>也可以直接指定完整的URL地址，例如：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;http://localhost:8080/myapp/servlet/HelloServlet&quot;</span> <span class="attr">method</span>=<span class="string">&quot;post&quot;</span>&gt;</span></span><br><span class="line">    //...</span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上述代码表示将表单提交的数据发送到完整的URL地址为<a href="http://localhost:8080/myapp/servlet/HelloServlet的Servlet上。">http://localhost:8080/myapp/servlet/HelloServlet的Servlet上。</a></p><p>当表单提交数据时，客户端会向指定的URL地址发送一个HTTP请求，服务器会根据URL地址找到对应的Servlet，并调用相应的doGet或doPost方法，从而获得表单提交的数据。因此，通过URL地址访问到Servlet时，需要在@WebServlet注解中指定该Servlet所映射的URL地址，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(urlPatterns = &quot;/servlet/HelloServlet&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloServlet</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码表示将HelloServlet类映射到URL地址为/servlet/HelloServlet的资源上。</p><h3 id="4、关于ServletConfig和ServletContext"><a href="#4、关于ServletConfig和ServletContext" class="headerlink" title="4、关于ServletConfig和ServletContext"></a>4、关于ServletConfig和ServletContext</h3><p>下面的图用来加强理解，不给出具体代码</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/10.png" alt="1"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVAWEB </tag>
            
            <tag> Servlet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>box-sizing的属性及作用详细介绍【CSS3】</title>
      <link href="/2023/03/06/box-sizing%E7%9A%84%E5%B1%9E%E6%80%A7%E5%8F%8A%E4%BD%9C%E7%94%A8%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%E3%80%90CSS3%E3%80%91/"/>
      <url>/2023/03/06/box-sizing%E7%9A%84%E5%B1%9E%E6%80%A7%E5%8F%8A%E4%BD%9C%E7%94%A8%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%E3%80%90CSS3%E3%80%91/</url>
      
        <content type="html"><![CDATA[<p><strong>本内容参考自：<a href="http://t.csdn.cn/fXtR4">让你五行代码</a></strong></p><details class="folding-tag" green><summary> 博主前言 </summary>              <div class='content'>              <p>因为本人跟着原专业的几位同学参加了今年的互联网+比赛，我的任务就是完成一个关于项目的小程序，所以重新捡起了微信小程序的开发，在学习过程中发现了<code>box-sizing</code>这个属性，由于在学习web开发的过程中，并没有用到过<code>box-sizing</code>这个属性，所以对此很陌生，因此研究了一下。</p><p>这个也不是很难，就是一个关于盒子模型的属性，我记录下来，防止遗忘。</p>              </div>            </details><p><strong>W3C标准盒子模型</strong></p><p>标准模式下，一个块的宽度 = width+padding(内边距)+border(边框)+margin(外边距)<br><strong>IE盒子模型</strong></p><p>怪异模式下，一个块的宽度 = width+margin(外边距) （即怪异模式下，width包含了border以及padding</p><p>当一个盒子的总宽度确定之后，要想给盒子添加边框或<a href="https://so.csdn.net/so/search?q=内边距&amp;spm=1001.2101.3001.7020">内边距</a>，往往需要更改 width属性值，才能保证盒子总宽度不变，操作起来烦琐且容易出错，运用CSS3的box-sizing属性可以轻松解决这个问题。box-sizing属性用于定义盒子的宽度值和高度值是否包含元素的内边距和边框，其基本语法格式如下。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">box-sizing</span>: content-box/border-box;</span><br></pre></td></tr></table></figure><p>在上面的语法格式中，<a href="https://so.csdn.net/so/search?q=box-sizing属性&amp;spm=1001.2101.3001.7020">box-sizing属性</a>的取值可以为content-box或border-box、inherit，对它们的解释如下。</p><ul><li>content-box：浏览器对盒模型的解释遵从W3C标准，当定义width和height时，它的参数值不包括border和padding，此时将采用标准模式的盒子模型标准。</li><li>border-box：当定义width和height时，border和padding的参数值被包含在width和height之内，此时将采用怪异模式的盒子模型标准。</li><li>inherit：规定应从父元素继承 box-sizing 属性的值。</li></ul><p>下面通过一个案例对box-sizing属性进行演示，如下所示。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>box-sizing属性用法<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.box1</span>&#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">300px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">height</span>: <span class="number">100px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">padding-right</span>: <span class="number">10px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background</span>: <span class="number">#F90</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">border</span>: <span class="number">10px</span> solid <span class="number">#ccc</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">box-sizing</span>: content-box;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.box2</span>&#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">300px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">height</span>: <span class="number">100px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">padding-right</span>: <span class="number">10px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background</span>: <span class="number">#F90</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">border</span>: <span class="number">10px</span> solid <span class="number">#ccc</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">box-sizing</span>: border-box;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;box1&quot;</span>&gt;</span>content_box属性<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;box2&quot;</span>&gt;</span>border_box属性<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在上面案例中定义了两个盒子，并对它们设置相同的宽、高、右内边距和边框样式。并且，对第一个盒子定义“box-sizing: content-box;”样式，对第二个盒子定义“box-sizing: border-box;”样式。</p><p><img src="https://picbed.dai2yutou.space/article_img/css/1.jpeg" alt="1"></p><p>可以发现应用了“box-sizing: content-box;”样式的盒子1，宽度比width参数值多出30px，总宽度变为330px;而应用了“box-sizing: border-box;”样式的盒子 2，宽度等于width参数值，总宽度仍为300px。应用“box-sizing: border-box;”样式后，盒子border和padding的参数值是被包含在width和height之内的。 </p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> css </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>完美解决idea配置JavaWeb环境并成功运行，一部到底！</title>
      <link href="/2023/03/04/%E5%AE%8C%E7%BE%8E%E8%A7%A3%E5%86%B3idea%E9%85%8D%E7%BD%AEJavaWeb%E7%8E%AF%E5%A2%83%E5%B9%B6%E6%88%90%E5%8A%9F%E8%BF%90%E8%A1%8C%EF%BC%8C%E4%B8%80%E9%83%A8%E5%88%B0%E5%BA%95%EF%BC%81/"/>
      <url>/2023/03/04/%E5%AE%8C%E7%BE%8E%E8%A7%A3%E5%86%B3idea%E9%85%8D%E7%BD%AEJavaWeb%E7%8E%AF%E5%A2%83%E5%B9%B6%E6%88%90%E5%8A%9F%E8%BF%90%E8%A1%8C%EF%BC%8C%E4%B8%80%E9%83%A8%E5%88%B0%E5%BA%95%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<div class="note info flat"><p>本博主看到别人帖子总结的，自用！</p></div><details class="folding-tag" cyan><summary> 前言 </summary>              <div class='content'>              <p>搞了一个晚上终于成功写出了JavaWeb文件</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/1.png" alt="1"></p><p>配置环境真的真的太难受了，这真是每次想学习一个新的东西的时候最不想经历的。</p><p>在之前学习JavaSE的时候，没有根据老师的给的idea和jdk的版本来下载，直接去官网和网上找到了最新的版本，然后大二下学期开JavaWeb课的时候的时候环境一直配不好。idea2022版我根本是配置不了这个web环境，不是缺这就缺那，那么干脆直接降低idea的版本，下面为降低版本，并进行相关web文件的创建和环境配置的过程整理，方便日后查看😪。</p>              </div>            </details><h2 id="一、降低idea版本"><a href="#一、降低idea版本" class="headerlink" title="一、降低idea版本"></a>一、降低idea版本</h2><p>直接删除电脑原有的2022版本，并删除<strong>注册表</strong>中的内容。</p><p>然后去网上寻找2020年的破解版本（当然支持正版💪），直接下载即可。<br>我的参考：<a href="https://www.cnblogs.com/technicist/p/15229615.html">https://www.cnblogs.com/technicist/p/15229615.html</a></p><h2 id="二、降低jdk版本"><a href="#二、降低jdk版本" class="headerlink" title="二、降低jdk版本"></a>二、降低jdk版本</h2><p class='p red'>这里我们需要选择适合当前使用的idea版本的jdk版本。</p><p>否则我们在idea软件上运行java项目的时候会出现如下报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error:Cannot determine path to ‘tools.jar‘ library for 16</span><br></pre></td></tr></table></figure><p>我们可以查看自己idea上对应的jdk最高的版本！！！</p><h3 id="查看idea对应的jdk最高版本"><a href="#查看idea对应的jdk最高版本" class="headerlink" title="查看idea对应的jdk最高版本"></a>查看idea对应的jdk最高版本</h3><p>打开IDEA 点击右上角的file 再点击Project Structure </p><p><img src="https://img-blog.csdnimg.cn/20210618194834414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dFQVNZRA==,size_16,color_FFFFFF,t_70" alt="2"></p><p>之后点击第一个第一条，Project 查看Project language level 打开下拉条，查看你的IDEA能支持的JDK最高版本</p><p><img src="https://img-blog.csdnimg.cn/20210618195028304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dFQVNZRA==,size_16,color_FFFFFF,t_70" alt="3"></p><p><img src="https://img-blog.csdnimg.cn/20210618195258829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dFQVNZRA==,size_16,color_FFFFFF,t_70" alt="4"></p><p>如图所示，我的这个最高支持到14版本，所以使用JDK16是不可以的，可以下载14以下版本的JDK</p><p>在Project SDK 中选择你合适的jdk文件夹，并在Project language level中选择对应的版本号，</p><p>所以当使用jdk16时，Project language level中没有对应的16版本导致了出现这种错误</p><p class='p yellow'>上述内容转载链接如下：</p><div class="tag link"><a class="link-card" title="卖女孩的小火柴คิดถึง" href="https://blog.csdn.net/WEASYD/article/details/118032654"><div class="left"><img src="https://profile.csdnimg.cn/D/8/A/3_weasyd"/></div><div class="right"><p class="text">卖女孩的小火柴คิดถึง</p><p class="url">https://blog.csdn.net/WEASYD/article/details/118032654</p></div></a></div><p>然后直接到官网上下载相应版本的JDK，并配置环境变量JAVA_HOME等，自行去百度！<br>如果你之前下载了其他版本的JDK，则新下载的会自动覆盖之前的，但也要修改环境变量、注册表等等。</p><h2 id="三、创建一个Java-web项目并配置Tomcat服务器"><a href="#三、创建一个Java-web项目并配置Tomcat服务器" class="headerlink" title="三、创建一个Java web项目并配置Tomcat服务器"></a>三、创建一个Java web项目并配置Tomcat服务器</h2><p class='p yellow'>下述内容转载链接如下：</p><div class="tag link"><a class="link-card" title="是小崔崔崔崔崔" href="https://blog.csdn.net/weixin_43716048/article/details/108639475"><div class="left"><img src="https://profile.csdnimg.cn/7/4/6/1_weixin_43716048"/></div><div class="right"><p class="text">是小崔崔崔崔崔</p><p class="url">https://blog.csdn.net/weixin_43716048/article/details/108639475</p></div></a></div><h3 id="1、创建Java-Web项目"><a href="#1、创建Java-Web项目" class="headerlink" title="1、创建Java Web项目"></a>1、创建Java Web项目</h3><p>打开IDEA之后新建一个普通的Java项目</p><p><img src="https://img-blog.csdnimg.cn/20200917114707339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="5"></p><p>选中红框之后点<code>next</code></p><p><img src="https://img-blog.csdnimg.cn/20200917114730888.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="6"></p><p>选择位置,工程名.然后点<code>finish</code></p><p><img src="https://img-blog.csdnimg.cn/20200917114857500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="6"></p><p><strong>重点:<code>找到刚才新建的Java项目,然后右键,点击 Add Framework Support...</code></strong></p><p><img src="https://img-blog.csdnimg.cn/20200917115153924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="7"></p><p><strong>然后勾选Web Application,点击ok</strong></p><p><img src="https://img-blog.csdnimg.cn/20200917115259728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="8"></p><p>这个时候Java web项目就创建好了</p><h3 id="2、配置Tomcat"><a href="#2、配置Tomcat" class="headerlink" title="2、配置Tomcat"></a>2、配置Tomcat</h3><p class='p red'>首先直接去官网下载对应版本的Tomcat，然后配置相应的环境变量！</p><p>点击导航栏的<code>Run</code>-&gt;<code>Edit Configurations...</code></p><p>也可以直接点击绿色小锤子🔨旁边的结构配置即可。</p><p><img src="https://img-blog.csdnimg.cn/20200917164617857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="9"></p><p>点击+添加，下滑找到<code>Tomcat</code>（注意这里不是<code>tomEE</code>，然后一定要选择<code>Local</code>📌）</p><p><img src="https://img-blog.csdnimg.cn/20200917164834517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="10"></p><p>按照图上的标注进行选择，其中<code>Configure...</code>点后要选中Tomcat的安装路径</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/2.png" alt="11"></p><p>下方有报错的话直接点击Fix，由于缺少一个jar包导致，点击Fix之后能直接自动获取。</p><p><img src="https://img-blog.csdnimg.cn/20200917165242741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxNjA0OA==,size_16,color_FFFFFF,t_70#pic_center" alt="12"></p><p><strong>注意📌</strong>：更改<code>Application context</code>为<code>/+项目名</code>，例如项目名如果为text，则为<code>/text</code></p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/4.png" alt="1"></p><p>然后点击ok即配置成功。</p><h3 id="3、配置完成之后运行项目"><a href="#3、配置完成之后运行项目" class="headerlink" title="3、配置完成之后运行项目"></a>3、配置完成之后运行项目</h3><p>点项目根文件，然后点击<code>Run Tomcat 8.5.57</code></p><div class='checkbox times red checked'><input type="checkbox" checked="checked"/>            <p>注意:在运行Java Web项目的时候，不要提前运行了tomcat🐱</p>            </div><div class='checkbox times red checked'><input type="checkbox" checked="checked"/>            <p>注意:每次在建立一个新的Java Web项目的时候</p>            </div><h3 id="4、报错解决方法"><a href="#4、报错解决方法" class="headerlink" title="4、报错解决方法"></a>4、报错解决方法</h3><h4 id="4-1-IntelliJ-IDEA关于“cannot-resolve-symbol-servlet”的解决"><a href="#4-1-IntelliJ-IDEA关于“cannot-resolve-symbol-servlet”的解决" class="headerlink" title="4.1 IntelliJ IDEA关于“cannot resolve symbol servlet”的解决"></a>4.1 IntelliJ IDEA关于“cannot resolve symbol servlet”的解决</h4><p>如果我们遇到了<code>cannot resolve symbol servlet</code>，如下图所示</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/5.png" alt="1"></p><p>出现这个问题的原因是缺少servlet-api.jar包造成的。</p><ol><li><p>右键项目-&gt;open module setting</p></li><li><p>libraries-&gt; + -&gt;Java-&gt;选择路径（tomcat安装目录下lib-&gt;tomcat-api.jar）</p></li></ol><p>完成之后就没有cannot resolve symbol servlet的提示错误了</p><details class="folding-tag" cyan><summary> 另一个方法 </summary>              <div class='content'>              <p><strong>一、新建Web项目</strong></p><p>选择IDEA主页的“Create New Project”选项，进入新建项目的界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/13.png" alt="1"></p><p>在New Projec界面中，选择左侧栏的“Java”选项，然后勾选”Web Application”选项。选择完毕之后，单击”Next”按钮进入填写项目信息的界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/14.png" alt="1"></p><p>在New Projec界面中，”Project name”选项用于指项目的名称，”Project localtion”选项用于指定Web项目的根目录。这里采用默认设置的目录，将chapter04作为Web项目的名称。设置完成之后，单击“Finish”按钮，进入开发界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/15.png" alt="1"></p><p><strong>二、添加Tomcat的Servlet-api.jar包</strong></p><p>创建好Web项目后，接下来，需要在项目中添加Tomcat的Servlet-api.jar包，单击IDEA开发工具右上角的“   ”按钮，进入Project Structure界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/16.png" alt="1"></p><p>在Project Structure界面中，单击【Libraries】→【+】→【Java】，弹出Select Library Files界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/17.png" alt="1"></p><p>在Select Library Files界面，选择项目所在的目录后单击”OK”按钮，弹出选择项目类型界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/18.png" alt="1"></p><p>在选择项目类型界面，选中Classes选项，单击“OK”按钮，会显示项目名称界面“Choose Modules”，在“Choose Modules”界面直接单击”OK”按钮，此时项目chapter04就加载到IDEA开发工具中了。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/19.png" alt="1"></p><p>单击标记的“+”按钮，查找Tomcat 包下lib包的Servlet-api.jar，将其导入到chapter04项目中。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/20.png" alt="1"></p><p>在单击”OK”按钮后，就可以将Servlet-api.jar添加到chapter04的项目中。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/21.png" alt="1"></p><p>右击chapter04项目的src文件，选择【new】→【Create New Servlet】选项，进入创建Servlet类的界面。</p><p><img src="https://picbed.dai2yutou.space/article_img/JAVA/22.png" alt="1"></p><p><strong>三、创建Servlet类</strong></p><p>此时IDEA工具会自动生成Servlet代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet(name = &quot;TestServlet01&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestServlet01</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest request, HttpServletResponse </span></span><br><span class="line"><span class="params">                 response)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doGet</span><span class="params">(HttpServletRequest request, HttpServletResponse </span></span><br><span class="line"><span class="params">                 response)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>              </div>            </details><p><img src="https://picbed.dai2yutou.space/article_img/博客美化/2.jpg" alt="13"></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java Web </tag>
            
            <tag> Tomcat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error:listen EADDRINUSE:address already in use :::4000</title>
      <link href="/2023/03/02/node%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8/"/>
      <url>/2023/03/02/node%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<div class="note warning flat"><p>本人亲测有效！</p></div><p>今天在对本人的博客目录下运行<code>hexo s</code>命令的时候，出现了一个报错。</p><p>报错如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: listen EADDRINUSE: address already in use :::4000</span><br></pre></td></tr></table></figure><h4 id="问题原因："><a href="#问题原因：" class="headerlink" title="问题原因："></a>问题原因：</h4><p><code>hexo s</code>命令是<code>hexo server</code>命令的缩写，作用是本地启动博客服务器。默认情况下，我们访问的网址为<a href="http://localhost:4000/，出现上述原因是我们已经启动过一次博客，导致4000端口被占用，然后我们再次启动进行访问时此端口无法访问，出现报错。">http://localhost:4000/，出现上述原因是我们已经启动过一次博客，导致4000端口被占用，然后我们再次启动进行访问时此端口无法访问，出现报错。</a></p><p class='p red'>注意：这里4000的端口只是一个端口号，每个人的不同，不是固定值。</p><h4 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h4><p>1、在当前文件的终端下查看端口占用的情况，也可以直接在电脑的命令行窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano</span><br></pre></td></tr></table></figure><p>2、找到对应4000端口的进程的PID值（例如是5327）</p><p>3、执行如下语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskkill -f /pid 5327</span><br></pre></td></tr></table></figure><p>4、最后重启进程即可</p><h4 id=""><a href="#" class="headerlink" title=""></a><div class='checkbox green checked'><input type="checkbox" checked="checked"/>            <p>一天一个小bug</p>            </div></h4><p><img src="https://picbed.dai2yutou.space/article_img/博客美化/1.jpg" alt="一天一个小bug"></p>]]></content>
      
      
      <categories>
          
          <category> bug记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PHP页面间传值的几种方法</title>
      <link href="/2023/02/28/PHP%E9%A1%B5%E9%9D%A2%E9%97%B4%E4%BC%A0%E5%80%BC%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
      <url>/2023/02/28/PHP%E9%A1%B5%E9%9D%A2%E9%97%B4%E4%BC%A0%E5%80%BC%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>这两天我一个部门的学弟在利用原生写法完成一篇博客，它问我如何在PHP页面点击一个框将值传到另一个页面中，这也就是PHP传值，所有我也借此机会再次学习了一下，以下为我总结的PHP传值的几种方法。</p><h2 id="方法1：利用require-once来实现"><a href="#方法1：利用require-once来实现" class="headerlink" title="方法1：利用require_once来实现"></a>方法1：利用require_once来实现</h2><p>利用<code>require_once</code>来实现PHP页面间传值的方法是：通过<code>require_once</code>语句，可以将一个PHP脚本的内容引入另一个PHP脚本。例如，如果有两个PHP文件a.php和b.php，其中<code>a.php</code>中有一个变量<code>$var = &quot;hello world&quot;</code>，那么在<code>b.php</code>中可以这样写：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="keyword">require_once</span>(<span class="string">&quot;a.php&quot;</span>);</span><br><span class="line"><span class="keyword">echo</span> <span class="variable">$var</span>;</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>这样，访问b.php会得到：hello world！。这种方法的好处是可以避免因失误而导致的重复引入²。</p><h2 id="方法2：GET方法"><a href="#方法2：GET方法" class="headerlink" title="方法2：GET方法"></a>方法2：GET方法</h2><p>使用 GET 方法可以通过 URL 参数将数据传递给另一个页面。在发送数据的页面中，可以使用 PHP 的 <code>http_build_query()</code> 函数将数据构建成一个查询字符串，并将其附加到 URL 上。在接收数据的页面中，可以使用 <code>$_GET</code> 数组获取数据。</p><p>发送数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$data</span> = <span class="keyword">array</span>(<span class="string">&#x27;name&#x27;</span> =&gt; <span class="string">&#x27;John&#x27;</span>, <span class="string">&#x27;age&#x27;</span> =&gt; <span class="number">30</span>);</span><br><span class="line"><span class="variable">$query_string</span> = <span class="title function_ invoke__">http_build_query</span>(<span class="variable">$data</span>);</span><br><span class="line"><span class="variable">$url</span> = <span class="string">&#x27;http://example.com/receive.php?&#x27;</span> . <span class="variable">$query_string</span>;</span><br><span class="line"><span class="title function_ invoke__">header</span>(<span class="string">&#x27;Location: &#x27;</span> . <span class="variable">$url</span>);</span><br></pre></td></tr></table></figure><p>接收数据的页面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$name = $_GET[&#x27;name&#x27;];</span><br><span class="line">$age = $_GET[&#x27;age&#x27;];</span><br></pre></td></tr></table></figure><h2 id="方法3：POST方法-表单"><a href="#方法3：POST方法-表单" class="headerlink" title="方法3：POST方法(表单)"></a>方法3：POST方法(表单)</h2><p>使用 POST 方法可以将数据作为 HTTP 请求的主体发送给另一个页面。在发送数据的页面中，可以使用 HTML 的 <code>&lt;form&gt;</code> 元素来创建一个表单，并将数据作为表单字段的值提交到另一个页面。在接收数据的页面中，可以使用 <code>$_POST</code> 数组获取数据。</p><p>发送数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;form action=<span class="string">&quot;http://example.com/receive.php&quot;</span> method=<span class="string">&quot;post&quot;</span>&gt;</span><br><span class="line">  &lt;input type=<span class="string">&quot;text&quot;</span> name=<span class="string">&quot;name&quot;</span> value=<span class="string">&quot;John&quot;</span>&gt;</span><br><span class="line">  &lt;input type=<span class="string">&quot;text&quot;</span> name=<span class="string">&quot;age&quot;</span> value=<span class="string">&quot;30&quot;</span>&gt;</span><br><span class="line">  &lt;button type=<span class="string">&quot;submit&quot;</span>&gt;Submit&lt;/button&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure><p>接收数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$name</span> = <span class="variable">$_POST</span>[<span class="string">&#x27;name&#x27;</span>];</span><br><span class="line"><span class="variable">$age</span> = <span class="variable">$_POST</span>[<span class="string">&#x27;age&#x27;</span>];</span><br></pre></td></tr></table></figure><h2 id="方法4：使用超链接来传递参数"><a href="#方法4：使用超链接来传递参数" class="headerlink" title="方法4：使用超链接来传递参数"></a>方法4：使用超链接来传递参数</h2><p>使用超链接来传递参数PHP的方法是：在超链接的href属性中添加问号和参数名=参数值的形式，然后在目标页面中用$_GET数组来获取参数值。例如，如果有两个PHP文件a.php和b.php，其中a.php中有一个超链接：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;b.php?name=张三&amp;age=18&quot;</span>&gt;</span>点击跳转<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><p>那么在b.php中可以这样写：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="variable">$name</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;name&#x27;</span>];</span><br><span class="line"><span class="variable">$age</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;age&#x27;</span>];</span><br><span class="line"><span class="keyword">echo</span> <span class="string">&quot;你好，<span class="subst">$name</span>，你的年龄是<span class="subst">$age</span>&quot;</span>;</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>这样，点击跳转后会得到：你好，张三，你的年龄是18。这种方法的好处是简单方便，但是缺点是参数会暴露在地址栏中。</p><h2 id="方法5：利用SESSION会话来实现"><a href="#方法5：利用SESSION会话来实现" class="headerlink" title="方法5：利用SESSION会话来实现"></a>方法5：利用SESSION会话来实现</h2><p>（SESSION是全局变量，只要被声明，在不关闭网页或者没有到SESSION的周期在所有页面都是可用的，而POST和GET只要php执行完毕就会立刻被释放没有）。使用 SESSION 方法可以在不同页面之间共享数据。在发送数据的页面中，可以将数据存储在 <code>$_SESSION</code> 变量中。在接收数据的页面中，可以从 <code>$_SESSION</code> 变量中获取数据。需要注意的是，使用 SESSION 方法需要先调用 <code>session_start()</code> 函数。</p><p>发送数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">session_start</span>();</span><br><span class="line"><span class="variable">$_SESSION</span>[<span class="string">&#x27;name&#x27;</span>] = <span class="string">&#x27;John&#x27;</span>;</span><br><span class="line"><span class="variable">$_SESSION</span>[<span class="string">&#x27;age&#x27;</span>] = <span class="number">30</span>;</span><br></pre></td></tr></table></figure><p>接收数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">session_start</span>();</span><br><span class="line"><span class="variable">$name</span> = <span class="variable">$_SESSION</span>[<span class="string">&#x27;name&#x27;</span>];</span><br><span class="line"><span class="variable">$age</span> = <span class="variable">$_SESSION</span>[<span class="string">&#x27;age&#x27;</span>];</span><br></pre></td></tr></table></figure><h2 id="方法6：利用COOKIE-方法来实现"><a href="#方法6：利用COOKIE-方法来实现" class="headerlink" title="方法6：利用COOKIE 方法来实现"></a>方法6：利用COOKIE 方法来实现</h2><p>cookie是存放在客户端上（也是全局变量），session是存放在服务器上，这是两者唯一的不同。使用 COOKIE 方法将数据存储在客户端，并在不同页面之间共享数据。在发送数据的页面中，可以使用 PHP 的 <code>setcookie()</code> 函数设置一个 cookie。在接收数据的页面中，可以使用 <code>$_COOKIE</code> 数组获取 cookie 的值。</p><p>发送数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">setcookie</span>(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;John&#x27;</span>, <span class="title function_ invoke__">time</span>() + <span class="number">3600</span>);</span><br><span class="line"><span class="comment">//创建一个名为name的cookie变量，它的值是jojn。它将在一个小时以后过期，也就是不能访问了</span></span><br><span class="line"><span class="title function_ invoke__">setcookie</span>(<span class="string">&#x27;age&#x27;</span>, <span class="number">30</span>, <span class="title function_ invoke__">time</span>() + <span class="number">3600</span>);</span><br><span class="line"><span class="comment">//创建一个名为age的cookie变量，它的值是30。它将在一个小时以后过期，也就是不能访问了</span></span><br></pre></td></tr></table></figure><p>接收数据的页面：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//注意需要刷新一下才生效</span></span><br><span class="line"><span class="variable">$name</span> = <span class="variable">$_COOKIE</span>[<span class="string">&#x27;name&#x27;</span>];</span><br><span class="line"><span class="variable">$age</span> = <span class="variable">$_COOKIE</span>[<span class="string">&#x27;age&#x27;</span>];</span><br></pre></td></tr></table></figure><h2 id="方法7：存入数据库再取出"><a href="#方法7：存入数据库再取出" class="headerlink" title="方法7：存入数据库再取出"></a>方法7：存入数据库再取出</h2><p>存入数据库再取出PHP的方法是：使用MySQLi或PDO扩展来连接MySQL数据库，然后使用SQL语句来插入和查询数据¹。例如，如果有一个MySQL数据库myDB，其中有一个表MyGuests，有三个字段id，name和age，那么可以这样写：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="comment">// 创建连接</span></span><br><span class="line"><span class="variable">$conn</span> = <span class="keyword">new</span> <span class="title function_ invoke__">mysqli</span>(<span class="string">&quot;localhost&quot;</span>, <span class="string">&quot;username&quot;</span>, <span class="string">&quot;password&quot;</span>, <span class="string">&quot;myDB&quot;</span>);</span><br><span class="line"><span class="comment">// 检测连接</span></span><br><span class="line"><span class="keyword">if</span> (<span class="variable">$conn</span>-&gt;connect_error) &#123;</span><br><span class="line">    <span class="keyword">die</span>(<span class="string">&quot;连接失败: &quot;</span> . <span class="variable">$conn</span>-&gt;connect_error);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入数据</span></span><br><span class="line"><span class="variable">$sql</span> = <span class="string">&quot;INSERT INTO MyGuests (name, age) VALUES (&#x27;张三&#x27;, 18)&quot;</span>;</span><br><span class="line"><span class="keyword">if</span> (<span class="variable">$conn</span>-&gt;<span class="title function_ invoke__">query</span>(<span class="variable">$sql</span>) === <span class="literal">TRUE</span>) &#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;插入成功&quot;</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;插入失败: &quot;</span> . <span class="variable">$conn</span>-&gt;error;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询数据</span></span><br><span class="line"><span class="variable">$sql</span> = <span class="string">&quot;SELECT id, name, age FROM MyGuests&quot;</span>;</span><br><span class="line"><span class="variable">$result</span> = <span class="variable">$conn</span>-&gt;<span class="title function_ invoke__">query</span>(<span class="variable">$sql</span>);</span><br><span class="line"><span class="keyword">if</span> (<span class="variable">$result</span>-&gt;num_rows &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 输出数据</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="variable">$row</span> = <span class="variable">$result</span>-&gt;<span class="title function_ invoke__">fetch_assoc</span>()) &#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">&quot;id: &quot;</span> . <span class="variable">$row</span>[<span class="string">&quot;id&quot;</span>]. <span class="string">&quot;, name: &quot;</span> . <span class="variable">$row</span>[<span class="string">&quot;name&quot;</span>]. <span class="string">&quot;, age: &quot;</span> . <span class="variable">$row</span>[<span class="string">&quot;age&quot;</span>]. <span class="string">&quot;&lt;br&gt;&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;没有结果&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 关闭连接</span></span><br><span class="line"><span class="variable">$conn</span>-&gt;<span class="title function_ invoke__">close</span>();</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>这样，运行后会得到：</p><p>插入成功</p><p>id: 1, name: 张三, age: 18</p><p><strong>好处</strong>：可以持久化保存数据，并且可以进行复杂的查询和操作。</p><p><strong>缺点</strong>：需要额外的数据库服务器和配置，耗费资源大。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PHP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGPT与新必应简单体验</title>
      <link href="/2023/02/28/OpenAI-new%E5%BF%85%E5%BA%94/"/>
      <url>/2023/02/28/OpenAI-new%E5%BF%85%E5%BA%94/</url>
      
        <content type="html"><![CDATA[<h3 id="首先简单说明一下我从接触ChatGPT与新必应到使用的过程"><a href="#首先简单说明一下我从接触ChatGPT与新必应到使用的过程" class="headerlink" title="首先简单说明一下我从接触ChatGPT与新必应到使用的过程"></a>首先简单说明一下我从接触ChatGPT与新必应到使用的过程</h3><p>在2022年寒假的时候，在看b站的时候发现ChatGPT开始出现在视野中，也是出于好奇，了解了一下，但当时感觉需求不是很大，而且有很多别的任务，也没有在关注它。</p><p>在寒假结束的时候突然想起来女朋友让我帮忙写的一个爬虫作业，由于对python爬虫的技术没有接触过，想到了ChatGPT，就去研究并注册了一个账号。</p><p><strong>结果如下：</strong></p><blockquote><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/1.png" alt="1"></p><p>编写网络爬虫需要一定的编程能力和网络基础知识。以下是一个简单的 Python 网络爬虫程序，可以用于采集山东省各城市未来7天的天气预报数据。</p><p>首先，需要安装 <code>requests</code> 和 <code>beautifulsoup4</code> 两个 Python 包。可以通过以下命令进行安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure><p>接下来，可以使用以下代码来实现爬虫程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置请求头信息</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取城市列表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_city_list</span>():</span><br><span class="line">    url = <span class="string">&#x27;http://www.weather.com.cn/textFC/hb.shtml&#x27;</span></span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    response.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    city_list = soup.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;lqcontentBoxheader&#x27;</span>&#125;).find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    city_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> city <span class="keyword">in</span> city_list:</span><br><span class="line">        city_name = city.get_text()</span><br><span class="line">        city_url = city[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">        city_dict[city_name] = city_url</span><br><span class="line">    <span class="keyword">return</span> city_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取城市天气信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_city_weather</span>(<span class="params">city_url</span>):</span><br><span class="line">    url = city_url</span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    response.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    weather_list = []</span><br><span class="line">    weather_div = soup.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;7d&#x27;</span>&#125;)</span><br><span class="line">    ul = weather_div.find(<span class="string">&#x27;ul&#x27;</span>)</span><br><span class="line">    li_list = ul.find_all(<span class="string">&#x27;li&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">        date = li.find(<span class="string">&#x27;h1&#x27;</span>).get_text()</span><br><span class="line">        weather = li.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line">        weather_info = weather[<span class="number">0</span>].get_text().strip()</span><br><span class="line">        temperature_info = weather[<span class="number">1</span>].get_text().strip()</span><br><span class="line">        wind_info = weather[<span class="number">2</span>].get_text().strip()</span><br><span class="line">        weather_list.append(&#123;<span class="string">&#x27;date&#x27;</span>: date, <span class="string">&#x27;weather_info&#x27;</span>: weather_info, <span class="string">&#x27;temperature_info&#x27;</span>: temperature_info, <span class="string">&#x27;wind_info&#x27;</span>: wind_info&#125;)</span><br><span class="line">    <span class="keyword">return</span> weather_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有城市的天气信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weather_info</span>():</span><br><span class="line">    city_dict = get_city_list()</span><br><span class="line">    weather_info_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> city_name, city_url <span class="keyword">in</span> city_dict.items():</span><br><span class="line">        weather_info_dict[city_name] = get_city_weather(city_url)</span><br><span class="line">    <span class="keyword">return</span> weather_info_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出天气信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_weather_info</span>(<span class="params">weather_info_dict</span>):</span><br><span class="line">    <span class="keyword">for</span> city_name, weather_info_list <span class="keyword">in</span> weather_info_dict.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;城市：<span class="subst">&#123;city_name&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> weather_info <span class="keyword">in</span> weather_info_list:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;日期：<span class="subst">&#123;weather_info[<span class="string">&quot;date&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;天气：<span class="subst">&#123;weather_info[<span class="string">&quot;weather_info&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;温度：<span class="subst">&#123;weather_info[<span class="string">&quot;temperature_info&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;风力：<span class="subst">&#123;weather_info[<span class="string">&quot;wind_info&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行程序</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    weather_info_dict = get_weather_info()</span><br><span class="line">    print_weather_info(weather_info_dict)</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote><p>也是发现了ChatGPT的强大之处，然后紧接着新必应出现了，由于开学前一天电脑充电器坏了，然后电脑好几天没有开机，最近才收到新必应的申请通过邮件，并立即去体验了一下，体验感也是非常的好。</p><p>新必应=必应+chatGPT😂</p><p><img src="https://picbed.dai2yutou.space/article_img/OpenAI/2.png" alt="2"></p><h3 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h3><p>对于这两个人工智能，都需要我们科学上网，并建议在注册ChatGPT时使用谷歌邮箱注册！！！</p><h3 id="感触"><a href="#感触" class="headerlink" title="感触"></a>感触</h3><p>感觉ChatGPT和新必应将来对我近数月的计算机学习和开发有很大的帮助。</p><p>新必应和chatGPT优势各不相同，新必应可能更加知识渊博，并且能博古通今。计算能力也是准确的，不会编瞎话，有自己的立场和坚持。而chatGPT更像是个工具人，以你为中心，你可以训练他让他变得更加趁手，但是不能作为计算器，满嘴跑火车，擅长说无用的废话。</p>]]></content>
      
      
      <categories>
          
          <category> 日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenAI </tag>
            
            <tag> 新必应 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WampServer运行呈橙色该怎么办？</title>
      <link href="/2023/02/06/WampServer%E5%9B%BE%E6%A0%87%E9%BB%84%E8%89%B2/"/>
      <url>/2023/02/06/WampServer%E5%9B%BE%E6%A0%87%E9%BB%84%E8%89%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://picbed.dai2yutou.space/article_img/PHP/1.png" alt="image-20230206001037121"></p><h3 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h3><p>​        当你打算使用<code>WordPress</code>搭建一个属于自己的博客或者想学习<code>php</code>时，避免不了要安装<code>wampserver</code>集成环境，在安装此环境之前，可能有些人已经安装过Apache、PHP、MySql，此时可能会出现<code>WampServer</code>图标呈现红色，或者黄色。</p><h3 id="二、解决方法"><a href="#二、解决方法" class="headerlink" title="二、解决方法"></a>二、解决方法</h3><p>这里直接暴力解决即可！</p><p>首先打运行<code>WampServer</code>环境，发现图标呈现红色或者黄色。</p><ul><li>红色一般为Apache、PHP、MySql此三个服务未打开。</li><li>黄色一般为Apache、PHP、MySql此三个服务端口被其他程序占用。</li></ul><h4 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h4><p>1.<code>win+Q</code>打开电脑搜索。</p><p><img src="https://picbed.dai2yutou.space/article_img/PHP/2.png" alt="image-20230205235602889"></p><p>2.然后输入<strong>服务</strong>进行搜索。</p><p>3.找到<code>wampapache</code>、<code>wampmariadb</code>、<code>wampmysqld</code>，观察第三列是否显示正在运行，如不是，则右击然后点击启动即可。</p><p><img src="https://picbed.dai2yutou.space/article_img/PHP/3.png" alt="image-20230205235812819"></p><p>4.对<code>WampServer</code>右击刷新，如未变成绿色进行下一步。</p><h4 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h4><p>在<strong>服务</strong>中找到已经安装的<code>mysql</code>，如显示正在运行，右击停止，并建议选择手动，不是自动打开。</p><p><img src="https://picbed.dai2yutou.space/article_img/PHP/4.png" alt="image-20230206000350082"></p><p>然后对<code>WampServer</code>右击刷新，如未变成绿色再进行下一步。</p><h4 id="第三步："><a href="#第三步：" class="headerlink" title="第三步："></a>第三步：</h4><p>1.<strong>win+Q</strong>打开电脑搜索。</p><p>2.输入<strong>计算机管理</strong>进行搜搜。</p><p>3.点击服务和应用程序，然后点击<strong>IIS管理器</strong>，如果显示正在运行，则点击停止，然后重新刷新<code>WampServer</code>。</p><p><img src="https://picbed.dai2yutou.space/article_img/PHP/5.png" alt="image-20230206000607935"></p><p><img src="https://picbed.dai2yutou.space/article_img/PHP/6.png" alt="image-20230206000751059"></p><p>4.刷新成功后，建议再重新启动<code>IIS</code>。</p><h3 id="三、问题解决🥰"><a href="#三、问题解决🥰" class="headerlink" title="三、问题解决🥰"></a>三、问题解决🥰</h3>]]></content>
      
      
      <categories>
          
          <category> bug记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PHP </tag>
            
            <tag> WampServer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英文水平不高，咋翻译论文？</title>
      <link href="/2023/02/02/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
      <url>/2023/02/02/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<div class="note info flat"><p>参考：<a href="https://mp.weixin.qq.com/s/YODR8nYweL6iRMVJ13HGDg">英文水平不高，咋翻译论文？</a></p></div><h2 id="一、DeepL翻译-全世界最准确的翻译（自称）"><a href="#一、DeepL翻译-全世界最准确的翻译（自称）" class="headerlink" title="一、DeepL翻译:全世界最准确的翻译（自称）"></a><strong>一、DeepL翻译:全世界最准确的翻译（自称）</strong></h2><p><img src="https://picbed.dai2yutou.space/article_img/论文翻译/1.png" alt="Deel"></p><p>网址：<a href="https://www.deepl.com/translator">https://www.deepl.com/translator</a></p><ul><li>优点：在<strong>专有名词</strong>翻译方面很准确，<strong>适合学术论文</strong>,可免费全文件翻译</li><li>缺点：全文件翻译时格式较乱，不过可以用于帮助初步理解美赛论文</li></ul><h2 id="二、翻译狗：媲美人工翻译的文档翻译平台（自称）"><a href="#二、翻译狗：媲美人工翻译的文档翻译平台（自称）" class="headerlink" title="二、翻译狗：媲美人工翻译的文档翻译平台（自称）"></a><strong>二、翻译狗：媲美人工翻译的文档翻译平台（自称）</strong></h2><p><img src="https://picbed.dai2yutou.space/article_img/论文翻译/2.png" alt="翻译狗"></p><p>网址：<a href="https://www.fanyigou.com/">https://www.fanyigou.com/</a></p><ul><li>优点：可全文件翻译且<strong>速度快</strong>，<strong>排版</strong>效果很好，<strong>双语对比并保持同步页面查看</strong></li><li>缺点：多次使用收费，不过产品体验以及售后服务较好</li></ul><h2 id="三、grammarly语法检测：语法纠错和校对"><a href="#三、grammarly语法检测：语法纠错和校对" class="headerlink" title="三、grammarly语法检测：语法纠错和校对"></a><strong>三、grammarly语法检测：语法纠错和校对</strong></h2><p><img src="https://picbed.dai2yutou.space/article_img/论文翻译/3.png" alt="grammarl语法检测"></p><p>网址：<a href="https://app.grammarly.com/">https://app.grammarly.com/</a></p><ul><li>优点：每条批注下面都会配有详细的解释，告诉你哪里错了，为什么要这样修改</li><li>缺点：更细节的检错需要付费，不过大学生一般不需要</li></ul><h2 id="四、注意事项"><a href="#四、注意事项" class="headerlink" title="四、注意事项"></a>四、注意事项</h2><p>竞赛时，不要将写好的完整的英文论文提交到任何翻译或查重/润色的网站，防止被查重从而失去评奖资格。</p><p>写与读英文论文，要始终保持耐心，任何软件都是工具，在根本上还是靠自己。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Echarts社区地址</title>
      <link href="/2023/01/24/Echarts%E7%A4%BE%E5%8C%BA%E5%9C%B0%E5%9D%80/"/>
      <url>/2023/01/24/Echarts%E7%A4%BE%E5%8C%BA%E5%9C%B0%E5%9D%80/</url>
      
        <content type="html"><![CDATA[<p>Echarts官方社区已停止运营，但有许多大佬的开源网站。</p><ul><li><a href="https://www.isqqw.com/#/homepage">www.isqqw.com/#/homepage</a></li><li><a href="http://www.ppchart.com/">http://www.ppchart.com</a></li><li><a href="https://www.makeapie.cn/echarts">www.makeapie.cn/echarts</a>[推荐]</li><li><a href="http://echarts.zhangmuchen.top/">http://echarts.zhangmuchen.top</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据可视化Echarts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo发生error：spawn failed错误的解决方法</title>
      <link href="/2023/01/20/erro_spawn_failed/"/>
      <url>/2023/01/20/erro_spawn_failed/</url>
      
        <content type="html"><![CDATA[<div class="note info simple"><p>Hexo发生error：spawn failed错误的解决方法</p></div><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>当我们将博客通过三连部署到<code>github</code>上时，可能会出现<code>error：spawn</code>的错误。</p><p>这种问题大多数是因为使用<code>git</code>进行<code>push</code>或者使用<code>hexo d</code>的时候改变了一些<code>.deploy_git</code>文件下的内容。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol><li>删除<code>.deploy_git</code>文件夹</li><li>然后，依次执行：<br><code>hexo clean</code><br><code>hexo g</code><br><code>hexo d</code></li></ol><p>此时会发现博客部署成功，问题解决，暴力直接且有效。</p>]]></content>
      
      
      <categories>
          
          <category> bug记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly外挂标签</title>
      <link href="/2023/01/20/Butterfly%E5%A4%96%E6%8C%82%E6%A0%87%E7%AD%BE/"/>
      <url>/2023/01/20/Butterfly%E5%A4%96%E6%8C%82%E6%A0%87%E7%AD%BE/</url>
      
        <content type="html"><![CDATA[<div class="note info flat"><p>参考：<a href="https://www.fomal.cc/posts/2013454d.html">Markdown语法与外挂标签写法汇总 | Fomalhaut🥝</a></p></div><div class="note success simple"><p>提示：使用前需要在站点配置文件或主题配置文件中引入外挂标签</p></div><blockquote><p>hexo博客文章撰写支持markdown语法和外挂标签，我们可以使用Typora等软件编写文章，外挂标签作为加工作用，方便阅读和查看。</p></blockquote><h1 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a>Markdown语法</h1><h2 id="Markdown语法支持的HTML标签如下："><a href="#Markdown语法支持的HTML标签如下：" class="headerlink" title="Markdown语法支持的HTML标签如下："></a>Markdown语法支持的HTML标签如下：</h2><ul><li><code>&lt;p&gt;</code>：表示段落</li><li><code>&lt;strong&gt;</code> 或 <code>&lt;b&gt;</code>：表示加粗</li><li><code>&lt;em&gt;</code> 或 <code>&lt;i&gt;</code>：表示斜体</li><li><code>&lt;del&gt;</code>：表示删除线</li><li><code>&lt;a&gt;</code>：表示链接，可以使用<code>[链接文本](链接地址)</code>的格式进行快捷输入</li><li><code>&lt;img&gt;</code>：表示插入图片，可以使用<code>![图片描述](图片地址)</code>的格式进行快捷输入</li><li><code>&lt;h1&gt;</code> 到 <code>&lt;h6&gt;</code>：表示标题，可以使用<code>#</code>符号进行快捷输入</li><li><code>&lt;ul&gt;</code> 和 <code>&lt;li&gt;</code>：表示无序列表，可以使用<code>-</code>符号进行快捷输入</li><li><code>&lt;ol&gt;</code> 和 <code>&lt;li&gt;</code>：表示有序列表，可以使用数字和<code>.</code>符号进行快捷输入</li><li><code>&lt;blockquote&gt;</code>：表示引用块</li><li><code>&lt;code&gt;</code>：表示行内代码</li><li><code>&lt;pre&gt;</code> 和 <code>&lt;code&gt;</code>：表示代码块，可以使用三个反引号进行快捷输入</li></ul><h1 id="Butterfly外挂标签"><a href="#Butterfly外挂标签" class="headerlink" title="Butterfly外挂标签"></a>Butterfly外挂标签</h1><h2 id="1-行内文本样式-text"><a href="#1-行内文本样式-text" class="headerlink" title="1 行内文本样式 text"></a>1 行内文本样式 text</h2><details class="folding-tag" blue><summary> 行内文本样式 text </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% u 文本内容 %&#125;</span><br><span class="line">&#123;% emp 文本内容 %&#125;</span><br><span class="line">&#123;% wavy 文本内容 %&#125;</span><br><span class="line">&#123;% del 文本内容 %&#125;</span><br><span class="line">&#123;% kbd 文本内容 %&#125;</span><br><span class="line">&#123;% psw 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 带 &#123;% u 下划线 %&#125; 的文本</span><br><span class="line"><span class="bullet">2.</span> 带 &#123;% emp 着重号 %&#125; 的文本</span><br><span class="line"><span class="bullet">3.</span> 带 &#123;% wavy 波浪线 %&#125; 的文本</span><br><span class="line"><span class="bullet">4.</span> 带 &#123;% del 删除线 %&#125; 的文本</span><br><span class="line"><span class="bullet">5.</span> 键盘样式的文本 &#123;% kbd command %&#125; + &#123;% kbd D %&#125;</span><br><span class="line"><span class="bullet">6.</span> 密码样式的文本：&#123;% psw 这里没有验证码 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><ol><li>带 <u>下划线</u> 的文本</li><li>带 <emp>着重号</emp> 的文本</li><li>带 <wavy>波浪线</wavy> 的文本</li><li>带 <del>删除线</del> 的文本</li><li>键盘样式的文本 <kbd>command</kbd> + <kbd>D</kbd></li><li>密码样式的文本：<psw>这里没有验证码</psw></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="2-行内文本-span"><a href="#2-行内文本-span" class="headerlink" title="2 行内文本 span"></a>2 行内文本 span</h2><details class="folding-tag" blue><summary> 行内文本样式 span </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% span 样式参数(参数以空格划分), 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><code>字体：</code>logo，code</li><li><code>颜色：</code>red,yellow,green,cyan,blue,gray</li><li><code>大小：</code>small,h4,h3,h2,h1,large,huge,ultra</li><li><code>对齐方向：</code>left,center,right</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 彩色文字</span><br><span class="line">  在一段话中方便插入各种颜色的标签，包括：&#123;% span red, 红色 %&#125;、&#123;% span yellow, 黄色 %&#125;、&#123;% span green, 绿色 %&#125;、&#123;% span cyan, 青色 %&#125;、&#123;% span blue, 蓝色 %&#125;、&#123;% span gray, 灰色 %&#125;。</span><br><span class="line"><span class="bullet">-</span> 超大号文字</span><br><span class="line">  文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line">  &#123;% span center logo large, Volantis %&#125;</span><br><span class="line">  &#123;% span center small, A Wonderful Theme for Hexo %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><ul><li>彩色文字<br>在一段话中方便插入各种颜色的标签，包括：<span class='p red'>红色</span>、<span class='p yellow'>黄色</span>、<span class='p green'>绿色</span>、<span class='p cyan'>青色</span>、<span class='p blue'>蓝色</span>、<span class='p gray'>灰色</span>。</li><li>超大号文字<br>文档「开始」页面中的标题部分就是超大号文字。<br><span class='p center logo large'>Volantis</span><br><span class='p center small'>A Wonderful Theme for Hexo</span></li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="3-段落文本-p"><a href="#3-段落文本-p" class="headerlink" title="3 段落文本 p"></a>3 段落文本 p</h2><details class="folding-tag" blue><summary> 段落文本 p </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% p 样式参数(参数以空格划分), 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><code>字体：</code>logo，code</li><li><code>颜色：</code>red,yellow,green,cyan,blue,gray</li><li><code>大小：</code>small,h4,h3,h2,h1,large,huge,ultra</li><li><code>对齐方向：</code>left,center,right</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 彩色文字</span><br><span class="line">在一段话中方便插入各种颜色的标签，包括：&#123;% p red, 红色 %&#125;、&#123;% p yellow, 黄色 %&#125;、&#123;% p green, 绿色 %&#125;、&#123;% p cyan, 青色 %&#125;、&#123;% p blue, 蓝色 %&#125;、&#123;% p gray, 灰色 %&#125;。</span><br><span class="line"><span class="bullet">-</span> 超大号文字</span><br><span class="line">文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line">&#123;% p center logo large, Volantis %&#125;</span><br><span class="line">&#123;% p center small, A Wonderful Theme for Hexo %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><ul><li>彩色文字<br>在一段话中方便插入各种颜色的标签，包括：<p class='p red'>红色</p>、<p class='p yellow'>黄色</p>、<p class='p green'>绿色</p>、<p class='p cyan'>青色</p>、<p class='p blue'>蓝色</p>、<p class='p gray'>灰色</p>。</li><li>超大号文字<br>文档「开始」页面中的标题部分就是超大号文字。<p class='p center logo large'>Volantis</p><p class='p center small'>A Wonderful Theme for Hexo</p></li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="4-引用note"><a href="#4-引用note" class="headerlink" title="4 引用note"></a>4 引用note</h2><details class="folding-tag" blue><summary> 引用note </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">通用配置</button></li><li class="tab"><button type="button" data-href="#test-2">语法格式</button></li><li class="tab"><button type="button" data-href="#test-3">参数配置</button></li><li class="tab"><button type="button" data-href="#test-4">示例源码</button></li><li class="tab"><button type="button" data-href="#test-5">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><p>修改主题配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">note:</span></span><br><span class="line">  <span class="comment"># Note tag style values:</span></span><br><span class="line">  <span class="comment">#  - simple    bs-callout old alert style. Default.</span></span><br><span class="line">  <span class="comment">#  - modern    bs-callout new (v2-v3) alert style.</span></span><br><span class="line">  <span class="comment">#  - flat      flat callout style with background, like on Mozilla or StackOverflow.</span></span><br><span class="line">  <span class="comment">#  - disabled  disable all CSS styles import of note tag.</span></span><br><span class="line">  <span class="attr">style:</span> <span class="string">simple</span></span><br><span class="line">  <span class="attr">icons:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">border_radius:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6).</span></span><br><span class="line">  <span class="comment"># Offset also applied to label tag variables. This option can work with disabled note tag.</span></span><br><span class="line">  <span class="attr">light_bg_offset:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><p><strong>1. 自带icon</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note [class] [no-icon] [style] %&#125;</span><br><span class="line">Any content (support inline tags too.io).</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p><strong>2. 外部icon</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note [color] [icon] [style] %&#125;</span><br><span class="line">Any content (support inline tags too.io).</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p><strong>1. 自带icon</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">class</td><td style="text-align:center">【可选】标识，不同的标识有不同的配色 （ default / primary / success / info / warning / danger ）</td></tr><tr><td style="text-align:center">no-icon</td><td style="text-align:center">【可选】不显示 icon</td></tr><tr><td style="text-align:center">style</td><td style="text-align:center">【可选】可以覆盖配置中的 style （simple/modern/flat/disabled）</td></tr></tbody></table></div><p><strong>2. 外部icon</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>参数</strong></th><th style="text-align:center"><strong>用法</strong></th></tr></thead><tbody><tr><td style="text-align:center">class</td><td style="text-align:center">【可选】标识，不同的标识有不同的配色 （ default / blue / pink / red / purple / orange / green ）</td></tr><tr><td style="text-align:center">icon</td><td style="text-align:center">【可选】可配置自定义 icon (只支持 fontawesome 图标, 也可以配置 no-icon )</td></tr><tr><td style="text-align:center">style</td><td style="text-align:center">【可选】可以覆盖配置中的 style （simple/modern/flat/disabled）</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><details class="folding-tag" gray><summary> 1.自带icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note simple %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default simple %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary simple %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success simple %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info simple %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning simple %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger simple %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>2.<code>modern</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note modern %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default modern %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary modern %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success modern %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info modern %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning modern %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger modern %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>3.<code>flat</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note flat %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default flat %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary flat %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success flat %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info flat %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning flat %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger flat %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>4.<code>disabled</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note disabled %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default disabled %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary disabled %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success disabled %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info disabled %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning disabled %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger disabled %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>5.<code>no-icon</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note no-icon %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default no-icon %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary no-icon %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success no-icon %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info no-icon %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning no-icon %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger no-icon %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>              </div>            </details><details class="folding-tag" gray><summary> 2.外部icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; simple %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; simple %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; simple %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; simple%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; simple %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; simple %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; simple %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>2.<code>modern</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; modern %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; modern %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; modern%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; modern %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; modern %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; modern %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>3.<code>flat</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; flat %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; flat %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; flat %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; flat%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; flat %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; flat %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; flat %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>4.<code>disabled</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; disabled %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; disabled %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; disabled %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; disabled %&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; disabled %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; disabled %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; disabled %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>5.<code>no-icon</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note no-icon %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue no-icon %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink no-icon %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red no-icon %&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange no-icon %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple no-icon %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green no-icon %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-5"><details class="folding-tag" gray><summary> 1.自带icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><div class="note simple"><p>默认 提示块标签</p></div><div class="note default simple"><p>default 提示块标签</p></div><div class="note primary simple"><p>primary 提示块标签</p></div><div class="note success simple"><p>success 提示块标签</p></div><div class="note info simple"><p>info 提示块标签</p></div><div class="note warning simple"><p>warning 提示块标签</p></div><div class="note danger simple"><p>danger 提示块标签</p></div><p>2.<code>modern</code>样式</p><div class="note modern"><p>默认 提示块标签</p></div><div class="note default modern"><p>default 提示块标签</p></div><div class="note primary modern"><p>primary 提示块标签</p></div><div class="note success modern"><p>success 提示块标签</p></div><div class="note info modern"><p>info 提示块标签</p></div><div class="note warning modern"><p>warning 提示块标签</p></div><div class="note danger modern"><p>danger 提示块标签</p></div><p>3.<code>flat</code>样式</p><div class="note flat"><p>默认 提示块标签</p></div><div class="note default flat"><p>default 提示块标签</p></div><div class="note primary flat"><p>primary 提示块标签</p></div><div class="note success flat"><p>success 提示块标签</p></div><div class="note info flat"><p>info 提示块标签</p></div><div class="note warning flat"><p>warning 提示块标签</p></div><div class="note danger flat"><p>danger 提示块标签</p></div><p>4.<code>disabled</code>样式</p><div class="note disabled"><p>默认 提示块标签</p></div><div class="note default disabled"><p>default 提示块标签</p></div><div class="note primary disabled"><p>primary 提示块标签</p></div><div class="note success disabled"><p>success 提示块标签</p></div><div class="note info disabled"><p>info 提示块标签</p></div><div class="note warning disabled"><p>warning 提示块标签</p></div><div class="note danger disabled"><p>danger 提示块标签</p></div><p>5.<code>no-icon</code>样式</p><div class="note no-icon flat"><p>默认 提示块标签</p></div><div class="note default no-icon flat"><p>default 提示块标签</p></div><div class="note primary no-icon flat"><p>primary 提示块标签</p></div><div class="note success no-icon flat"><p>success 提示块标签</p></div><div class="note info no-icon flat"><p>info 提示块标签</p></div><div class="note warning no-icon flat"><p>warning 提示块标签</p></div><div class="note danger no-icon flat"><p>danger 提示块标签</p></div>              </div>            </details><details class="folding-tag" gray><summary> 2.外部icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><div class="note icon-padding simple"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding simple"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding simple"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding simple"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding simple"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding simple"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding simple"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>2.<code>modern</code>样式</p><div class="note icon-padding modern"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding modern"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding modern"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding modern"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding modern"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding modern"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>3.<code>flat</code>样式</p><div class="note icon-padding flat"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding flat"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding flat"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding flat"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding flat"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding flat"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>4.<code>disabled</code>样式</p><div class="note icon-padding disabled"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding disabled"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了….</p></div><div class="note pink icon-padding disabled"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding disabled"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding disabled"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding disabled"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding disabled"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>5.<code>no-icon</code>样式</p><div class="note no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue no-icon flat"><p>2021年快到了….</p></div><div class="note pink no-icon flat"><p>小心开车 安全至上</p></div><div class="note red no-icon flat"><p>这是三片呢？还是四片？</p></div><div class="note orange no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple no-icon flat"><p>剪刀石头布</p></div><div class="note green no-icon flat"><p>前端最讨厌的浏览器</p></div>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="5-上标标签-tip"><a href="#5-上标标签-tip" class="headerlink" title="5 上标标签 tip"></a>5 上标标签 tip</h2><details class="folding-tag" blue><summary> 上标标签tip </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip [参数，可选] %&#125;文本内容&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip %&#125;default&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip info %&#125;info&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip success %&#125;success&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip error %&#125;error&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip warning %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip bolt %&#125;bolt&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban %&#125;ban&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip home %&#125;home&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip sync %&#125;sync&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip cogs %&#125;cogs&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip key %&#125;key&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip bell %&#125;bell&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip fa-atom %&#125;自定义font awesome图标&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><div class="tip "><p>default</p></div><div class="tip info"><p>info</p></div><div class="tip success"><p>success</p></div><div class="tip error"><p>error</p></div><div class="tip warning"><p>warning</p></div><div class="tip bolt"><p>bolt</p></div><div class="tip ban"><p>ban</p></div><div class="tip home"><p>home</p></div><div class="tip sync"><p>sync</p></div><div class="tip cogs"><p>cogs</p></div><div class="tip key"><p>key</p></div><div class="tip bell"><p>bell</p></div><div class="tip fa-atom"><p>自定义font awesome图标</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="6-动态标签-anima"><a href="#6-动态标签-anima" class="headerlink" title="6 动态标签 anima"></a>6 动态标签 anima</h2><details class="folding-tag" blue><summary> 动态标签 anima </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip [参数，可选] %&#125;文本内容&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><p>1.On DOM load（当页面加载时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>2.调整动画速度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated faa-fast %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated faa-slow %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>3.On hover（当鼠标悬停时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated-hover %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated-hover %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>4.On parent hover（当鼠标悬停在父级元素时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-parent animated-hover %&#125;<span class="language-xml"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;faa-horizontal&quot;</span>&gt;</span></span>warning<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span>&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-parent animated-hover %&#125;<span class="language-xml"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;faa-flash&quot;</span>&gt;</span></span>ban<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span>&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>1.On DOM load（当页面加载时显示动画）</p><div class="tip warning faa-horizontal animated"><p>warning</p></div><div class="tip ban faa-flash animated"><p>ban</p></div><p><p>2.调整动画速度</p></p><div class="tip warning faa-horizontal animated faa-fast"><p>warning</p></div><div class="tip ban faa-flash animated faa-slow"><p>ban</p></div><p><p>3.On hover（当鼠标悬停时显示动画）</p></p><div class="tip warning faa-horizontal animated-hover"><p>warning</p></div><div class="tip ban faa-flash animated-hover"><p>ban</p></div><p><p>4.On parent hover（当鼠标悬停在父级元素时显示动画）</p></p><div class="tip warning faa-parent animated-hover"><p class="faa-horizontal">warning</p></div><div class="tip ban faa-parent animated-hover"><p class="faa-flash">ban</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="7-复选列表-checkbox"><a href="#7-复选列表-checkbox" class="headerlink" title="7 复选列表 checkbox"></a>7 复选列表 checkbox</h2><details class="folding-tag" blue><summary> 复选列表 checkbox </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% checkbox 样式参数（可选）, 文本（支持简单md） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">样式</td><td style="text-align:center">默认为√，plus, minus, times</td></tr><tr><td style="text-align:center">颜色</td><td style="text-align:center">默认为主题颜色，red,yellow,green,cyan,blue,gray</td></tr><tr><td style="text-align:center">选中状态</td><td style="text-align:center">默认为不选中，checked</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% checkbox 纯文本测试 %&#125;</span><br><span class="line">&#123;% checkbox checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% checkbox red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% checkbox green checked, 绿色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox yellow checked, 黄色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox cyan checked, 青色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox blue checked, 蓝色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox plus green checked, 增加 %&#125;</span><br><span class="line">&#123;% checkbox minus yellow checked, 减少 %&#125;</span><br><span class="line">&#123;% checkbox times red checked, 叉 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><div class='checkbox'><input type="checkbox" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="checkbox" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="checkbox" />            <p>支持自定义颜色</p>            </div><div class='checkbox green checked'><input type="checkbox" checked="checked"/>            <p>绿色 + 默认选中</p>            </div><div class='checkbox yellow checked'><input type="checkbox" checked="checked"/>            <p>黄色 + 默认选中</p>            </div><div class='checkbox cyan checked'><input type="checkbox" checked="checked"/>            <p>青色 + 默认选中</p>            </div><div class='checkbox blue checked'><input type="checkbox" checked="checked"/>            <p>蓝色 + 默认选中</p>            </div><div class='checkbox plus green checked'><input type="checkbox" checked="checked"/>            <p>增加</p>            </div><div class='checkbox minus yellow checked'><input type="checkbox" checked="checked"/>            <p>减少</p>            </div><div class='checkbox times red checked'><input type="checkbox" checked="checked"/>            <p>叉</p>            </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="8-单选列表-radio"><a href="#8-单选列表-radio" class="headerlink" title="8 单选列表 radio"></a>8 单选列表 radio</h2><details class="folding-tag" blue><summary> 单选列表 radio </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% radio 样式参数（可选）, 文本（支持简单md） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center"><code>颜色</code></td><td style="text-align:center">red,yellow,green,cyan,blue,gray</td></tr><tr><td style="text-align:center"><code>选中状态</code></td><td style="text-align:center">checked</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% radio 纯文本测试 %&#125;</span><br><span class="line">&#123;% radio checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% radio red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% radio green, 绿色 %&#125;</span><br><span class="line">&#123;% radio yellow, 黄色 %&#125;</span><br><span class="line">&#123;% radio cyan, 青色 %&#125;</span><br><span class="line">&#123;% radio blue, 蓝色 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><div class='checkbox'><input type="radio" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="radio" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="radio" />            <p>支持自定义颜色</p>            </div><div class='checkbox green'><input type="radio" />            <p>绿色</p>            </div><div class='checkbox yellow'><input type="radio" />            <p>黄色</p>            </div><div class='checkbox cyan'><input type="radio" />            <p>青色</p>            </div><div class='checkbox blue'><input type="radio" />            <p>蓝色</p>            </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="9-时间轴-timeline"><a href="#9-时间轴-timeline" class="headerlink" title="9 时间轴 timeline"></a>9 时间轴 timeline</h2><details class="folding-tag" blue><summary> 时间轴 timeline </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;% timeline 时间线标题（可选）[,color] %&#125;</span><br><span class="line">&lt;!-- timeline 时间节点（标题） --&gt;</span><br><span class="line">正文内容</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&lt;!-- timeline 时间节点（标题） --&gt;</span><br><span class="line">正文内容</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">title</td><td style="text-align:center">标题/时间线</td></tr><tr><td style="text-align:center">color</td><td style="text-align:center"><code>timeline</code>颜色:default(留空) / blue / pink / red / purple / orange / green</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;% timeline 时间轴样式,blue %&#125;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-07-24 [<span class="string">2.6.6 -&gt; 3.0</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases</span>) --&gt;</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 如果有 <span class="code">`hexo-lazyload-image`</span> 插件，需要删除并重新安装最新版本，设置 <span class="code">`lazyload.isSPA: true`</span>。</span><br><span class="line"><span class="bullet">2.</span> 2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <span class="code">`use_cdn: true`</span> 则需要删除。</span><br><span class="line"><span class="bullet">3.</span> 2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</span><br><span class="line"><span class="bullet">4.</span> 2.x 版本的置顶 <span class="code">`top: true`</span> 改为了 <span class="code">`pin: true`</span>，并且同样适用于 <span class="code">`layout: page`</span> 的页面。</span><br><span class="line"><span class="bullet">5.</span> 如果使用了 <span class="code">`hexo-offline`</span> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-05-15 [<span class="string">2.6.3 -&gt; 2.6.6</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6</span>) --&gt;</span><br><span class="line"></span><br><span class="line">不需要额外处理。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-04-20 [<span class="string">2.6.2 -&gt; 2.6.3</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3</span>) --&gt;</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 全局搜索 <span class="code">`seotitle`</span> 并替换为 <span class="code">`seo_title`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的索引规则有变，使用 group 组件的文章内，<span class="code">`group: group_name`</span> 对应的组件名必须是 <span class="code">`group_name`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的列表名优先显示文章的 <span class="code">`short_title`</span> 其次是 <span class="code">`title`</span>。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><div class="timeline blue"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>时间轴样式</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-07-24 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases">2.6.6 -&gt; 3.0</a></p></div></div><div class='timeline-item-content'><ol><li>如果有 <code>hexo-lazyload-image</code> 插件，需要删除并重新安装最新版本，设置 <code>lazyload.isSPA: true</code>。</li><li>2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <code>use_cdn: true</code> 则需要删除。</li><li>2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</li><li>2.x 版本的置顶 <code>top: true</code> 改为了 <code>pin: true</code>，并且同样适用于 <code>layout: page</code> 的页面。</li><li>如果使用了 <code>hexo-offline</code> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-05-15 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6">2.6.3 -&gt; 2.6.6</a></p></div></div><div class='timeline-item-content'><p>不需要额外处理。</p></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-04-20 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3">2.6.2 -&gt; 2.6.3</a></p></div></div><div class='timeline-item-content'><ol><li>全局搜索 <code>seotitle</code> 并替换为 <code>seo_title</code>。</li><li>group 组件的索引规则有变，使用 group 组件的文章内，<code>group: group_name</code> 对应的组件名必须是 <code>group_name</code>。</li><li>group 组件的列表名优先显示文章的 <code>short_title</code> 其次是 <code>title</code>。</li></ol></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="10-链接卡片-link"><a href="#10-链接卡片-link" class="headerlink" title="10 链接卡片 link"></a>10 链接卡片 link</h2><details class="folding-tag" blue><summary> 链接卡片 link </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link 标题, 链接, 图片链接（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link 小漁头记录, https://www.dai2yutou.space/, https://www.dai2yutou.space/img/avatar.jpg %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><div class="tag link"><a class="link-card" title="小漁头记录" href="https://www.dai2yutou.space/"><div class="left"><img src="https://www.dai2yutou.space/img/avatar.jpg"/></div><div class="right"><p class="text">小漁头记录</p><p class="url">https://www.dai2yutou.space/</p></div></a></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="11-按钮-btns"><a href="#11-按钮-btns" class="headerlink" title="11 按钮 btns"></a>11 按钮 btns</h2><details class="folding-tag" blue><summary> 按钮 btns </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns 样式参数 %&#125;</span><br><span class="line">&#123;% cell 标题, 链接, 图片或者图标 %&#125;</span><br><span class="line">&#123;% cell 标题, 链接, 图片或者图标 %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><p>圆角样式：rounded, circle</p></li><li><p>增加文字样式：可以在容器内增加 <code>标题</code>和<code>描述文字</code></p></li><li><p>布局方式：<br>默认为自动宽度，适合视野内只有一两个的情况。</p><p>|  参数  |                  含义                  |<br>| :——: | :——————————————————: |<br>|  wide  |              宽一点的按钮              |<br>|  fill  | 填充布局，自动铺满至少一行，多了会换行 |<br>| center |        居中，按钮之间是固定间距        |<br>| around |                居中分散                |<br>| grid2  |  等宽最多2列，屏幕变窄会适当减少列数   |<br>| grid3  |  等宽最多3列，屏幕变窄会适当减少列数   |<br>| grid4  |  等宽最多4列，屏幕变窄会适当减少列数   |<br>| grid5  |  等宽最多5列，屏幕变窄会适当减少列数   |</p></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>1.如果需要显示类似「团队成员」之类的一组含有头像的链接</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle grid5 %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><p>2.或者含有图标的按钮</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns rounded grid5 %&#125;</span><br><span class="line">&#123;% cell 下载源码, /, fas fa-download %&#125;</span><br><span class="line">&#123;% cell 查看文档, /, fas fa-book-open %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><p>3.圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle center grid5 %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p red, 专业版 %&#125;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_pro.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p green, 免费版 %&#125;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_lite.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>1.如果需要显示类似「团队成员」之类的一组含有头像的链接</p><div class="btns circle grid5">            <a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a>          </div><p><p>2.或者含有图标的按钮</p></p><div class="btns rounded grid5">            <a class="button" href='/' title='下载源码'><i class='fas fa-download'></i>下载源码</a><a class="button" href='/' title='查看文档'><i class='fas fa-book-open'></i>查看文档</a>          </div><p><p>3.圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p></p><div class="btns circle center grid5">            <a href='https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'>  <i class='fab fa-apple'></i> <b>心率管家</b>  <p class='p red'>专业版</p>  <img src='https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_pro.png'></a><a href='https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p green'>免费版</p>  <img src='https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_lite.png'></a>          </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="12-github卡片-ghcard"><a href="#12-github卡片-ghcard" class="headerlink" title="12 github卡片 ghcard"></a>12 github卡片 ghcard</h2><details class="folding-tag" blue><summary> github卡片 ghcard </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="13-github徽标-ghbdage"><a href="#13-github徽标-ghbdage" class="headerlink" title="13 github徽标 ghbdage"></a>13 github徽标 ghbdage</h2><details class="folding-tag" blue><summary> github徽标 ghbdage </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage [right],[left],[logo]||[color],[link],[title]||[option] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">left</td><td style="text-align:center">徽标左边的信息，必选参数。</td></tr><tr><td style="text-align:center">right</td><td style="text-align:center">徽标右边的信息，必选参数，</td></tr><tr><td style="text-align:center">logo</td><td style="text-align:center">徽标图标，图标名称详见<a href="https://simpleicons.org/">simpleicons</a>，可选参数。</td></tr><tr><td style="text-align:center">color</td><td style="text-align:center">徽标右边的颜色，可选参数。</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">指向的链接，可选参数。</td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">徽标的额外信息，可选参数。主要用于优化SEO，但<code>object</code>标签不会像<code>a</code>标签一样在鼠标悬停显示<code>title</code>信息。</td></tr><tr><td style="text-align:center">option</td><td style="text-align:center">自定义参数，支持<a href="https://shields.io/">shields.io</a>的全部API参数支持，具体参数可以参看上文中的拓展写法示例。形式为<code>name1=value2&amp;name2=value2</code>。</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>1.基本参数,定义徽标左右文字和图标</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Theme,Butterfly %&#125;</span><br><span class="line">&#123;% bdage Frame,Hexo,hexo %&#125;</span><br></pre></td></tr></table></figure><p>2.信息参数，定义徽标右侧内容背景色，指向链接</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage CDN,JsDelivr,jsDelivr||abcdef,https://metroui.org.ua/index.html,本站使用JsDelivr为静态资源提供CDN加速 %&#125;</span><br><span class="line">//如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割</span><br><span class="line">&#123;% bdage Source,GitHub,GitHub||,https://github.com/ %&#125;</span><br></pre></td></tr></table></figure><p>3.拓展参数，支持shields的API的全部参数内容</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Hosted,Vercel,Vercel||brightgreen,https://vercel.com/,本站采用双线部署，默认线路托管于Vercel||style=social&amp;logoWidth=20 %&#125;</span><br><span class="line">//如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割</span><br><span class="line">&#123;% bdage Hosted,Vercel,Vercel||||style=social&amp;logoWidth=20&amp;logoColor=violet %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>1.基本参数,定义徽标左右文字和图标</p><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Butterfly-Theme-orange?logo=&color=orange&link=&"></object><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Hexo-Frame-orange?logo=hexo&color=orange&link=&"></object><p>2.信息参数，定义徽标右侧内容背景色，指向链接</p><object class="ghbdage" style="margin-inline:5px" title="本站使用JsDelivr为静态资源提供CDN加速" standby="loading..." data="https://img.shields.io/badge/JsDelivr-CDN-orange?logo=jsDelivr&color=abcdef&link=https://metroui.org.ua/index.html&"></object>//如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割<object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/GitHub-Source-orange?logo=GitHub&color=orange&link=https://github.com/&"></object><p>3.拓展参数，支持shields的API的全部参数内容</p><object class="ghbdage" style="margin-inline:5px" title="本站采用双线部署，默认线路托管于Vercel" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=brightgreen&link=https://vercel.com/&style=social&logoWidth=20"></object>//如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割<object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=orange&link=&style=social&logoWidth=20&logoColor=violet"></object><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="14-网站卡片-sites"><a href="#14-网站卡片-sites" class="headerlink" title="14 网站卡片 sites"></a>14 网站卡片 sites</h2><details class="folding-tag" blue><summary> 网站卡片 sites </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% sitegroup %&#125;</span><br><span class="line">&#123;% site 标题, url=链接, screenshot=截图链接, avatar=头像链接（可选）, description=描述（可选） %&#125;</span><br><span class="line">&#123;% site 标题, url=链接, screenshot=截图链接, avatar=头像链接（可选）, description=描述（可选） %&#125;</span><br><span class="line">&#123;% endsitegroup %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% sitegroup %&#125;</span><br><span class="line">&#123;% site xaoxuu, url=https://xaoxuu.com, screenshot=https://i.loli.net/2020/08/21/VuSwWZ1xAeUHEBC.jpg, avatar=https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/avatar/avatar.png, description=简约风格 %&#125;</span><br><span class="line">&#123;% site inkss, url=https://inkss.cn, screenshot=https://i.loli.net/2020/08/21/Vzbu3i8fXs6Nh5Y.jpg, avatar=https://cdn.jsdelivr.net/gh/inkss/common@master/static/web/avatar.jpg, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site MHuiG, url=https://blog.mhuig.top, screenshot=https://i.loli.net/2020/08/22/d24zpPlhLYWX6D1.png, avatar=https://cdn.jsdelivr.net/gh/MHuiG/imgbed@master/data/p.png, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site Colsrch, url=https://colsrch.top, screenshot=https://i.loli.net/2020/08/22/dFRWXm52OVu8qfK.png, avatar=https://cdn.jsdelivr.net/gh/Colsrch/images/Colsrch/avatar.jpg, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site Linhk1606, url=https://linhk1606.github.io, screenshot=https://i.loli.net/2020/08/21/3PmGLCKicnfow1x.png, avatar=https://i.loli.net/2020/02/09/PN7I5RJfFtA93r2.png, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% endsitegroup %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><div class="site-card-group"><a class="site-card" href="https://xaoxuu.com"><div class="img"><img src="https://i.loli.net/2020/08/21/VuSwWZ1xAeUHEBC.jpg"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/avatar/avatar.png"/><span class="title">xaoxuu</span><span class="desc">简约风格</span></div></a><a class="site-card" href="https://inkss.cn"><div class="img"><img src="https://i.loli.net/2020/08/21/Vzbu3i8fXs6Nh5Y.jpg"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/inkss/common@master/static/web/avatar.jpg"/><span class="title">inkss</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://blog.mhuig.top"><div class="img"><img src="https://i.loli.net/2020/08/22/d24zpPlhLYWX6D1.png"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/MHuiG/imgbed@master/data/p.png"/><span class="title">MHuiG</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://colsrch.top"><div class="img"><img src="https://i.loli.net/2020/08/22/dFRWXm52OVu8qfK.png"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/Colsrch/images/Colsrch/avatar.jpg"/><span class="title">Colsrch</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://linhk1606.github.io"><div class="img"><img src="https://i.loli.net/2020/08/21/3PmGLCKicnfow1x.png"/></div><div class="info"><img src="https://i.loli.net/2020/02/09/PN7I5RJfFtA93r2.png"/><span class="title">Linhk1606</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a> </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="15-行内图片-inlineimage"><a href="#15-行内图片-inlineimage" class="headerlink" title="15 行内图片 inlineimage"></a>15 行内图片 inlineimage</h2><details class="folding-tag" blue><summary> 行内图片 inlineimage </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% inlineimage 图片链接, height=高度（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><code>高度</code>：height=20px</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是 &#123;% inlineimage https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/0000.gif %&#125; 一段话。</span><br><span class="line"></span><br><span class="line">这又是 &#123;% inlineimage https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/5150.gif, height=40px %&#125; 一段话。</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>这是 <img no-lazy class="inline" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/0000.gif" style="height:1.5em"/> 一段话。</p><p>这又是 <img no-lazy class="inline" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/5150.gif" style="height:40px;"/> 一段话。    </p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="16-单张图片-image"><a href="#16-单张图片-image" class="headerlink" title="16 单张图片 image"></a>16 单张图片 image</h2><details class="folding-tag" blue><summary> 单张图片 image </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image 链接, width=宽度（可选）, height=高度（可选）, alt=描述（可选）, bg=占位颜色（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">图片宽度高度</td><td style="text-align:center">width=300px, height=32px</td></tr><tr><td style="text-align:center">图片描述</td><td style="text-align:center">alt=图片描述（butterfly需要在主题配置文件中开启图片描述）</td></tr><tr><td style="text-align:center">占位背景色</td><td style="text-align:center">bg=#f2f2f2</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>1.添加描述</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://www.dai2yutou.space/img/avatar.jpg, alt=每天下课回宿舍的路，没有什么故事。 %&#125;</span><br></pre></td></tr></table></figure><p>2.指定宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://www.dai2yutou.space/img/avatar.jpg, width=400px %&#125;</span><br></pre></td></tr></table></figure><p>3.指定宽度并添加描述</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://www.dai2yutou.space/img/avatar.jpg, width=400px, alt=每天下课回宿舍的路，没有什么故事。 %&#125;</span><br></pre></td></tr></table></figure><p>4.设置占位背景色</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://www.dai2yutou.space/img/avatar.jpg, width=400px, bg=#1D0C04, alt=优化不同宽度浏览的观感 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>1.添加描述</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://www.dai2yutou.space/img/avatar.jpg" alt="每天下课回宿舍的路，没有什么故事。"/></div><span class="image-caption">每天下课回宿舍的路，没有什么故事。</span></div><p><p>2.指定宽度</p></p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://www.dai2yutou.space/img/avatar.jpg" style="width:400px;"/></div></div><p><p>3.指定宽度并添加描述</p></p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://www.dai2yutou.space/img/avatar.jpg" alt="每天下课回宿舍的路，没有什么故事。" style="width:400px;"/></div><span class="image-caption">每天下课回宿舍的路，没有什么故事。</span></div><p><p>4.设置占位背景色</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button>&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</p>              </div>            </details><h2 id="17-音频-audio"><a href="#17-音频-audio" class="headerlink" title="17 音频 audio"></a>17 音频 audio</h2><details class="folding-tag" blue><summary> 音频 audio </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% audio 音频链接 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% audio https://github.com/volantis-x/volantis-docs/releases/download/assets/Lumia1020.mp3 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><div class="audio"><audio controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/Lumia1020.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="18-视频-video"><a href="#18-视频-video" class="headerlink" title="18 视频 video"></a>18 视频 video</h2><details class="folding-tag" blue><summary> 视频 video </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos,列数 %&#125;</span><br><span class="line">&#123;% video 视频链接 %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><code>对齐方向</code>：left, center, right</li><li><code>列数</code>：逗号后面直接写列数，支持 1 ～ 4 列。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><ol><li><p>100%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>50%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 2 %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure></li><li><p>25%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 4 %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>1.100%宽度</p>   <div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><p><p>2.50%宽度</p><br>   <div class="videos" col='2'><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div></p><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div></div><p><p>3.25%宽度</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button>&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</p>              </div>            </details><h2 id="19-相册-gallery"><a href="#19-相册-gallery" class="headerlink" title="19 相册 gallery"></a>19 相册 gallery</h2><details class="folding-tag" blue><summary> 相册 gallery </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><ol><li><p>gallerygroup 相册图库</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;gallery-group-main&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure></li><li><p>gallery 相册</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% gallery %&#125;</span><br><span class="line">markdown 图片格式</span><br><span class="line">&#123;% endgallery %&#125;</span><br></pre></td></tr></table></figure></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ul><li><p>gallerygroup 相册图库</p><p>|    参数     |         用法         |<br>| :————-: | :—————————: |<br>|    name     |       图库名字       |<br>| description |       图库描述       |<br>|    link     | 链接到对应相册的地址 |<br>|   img-url   |       图库封面       |</p></li><li><p>gallery 相册</p><p>区别于旧版的Gallery相册,新的Gallery相册会自动根据图片长度进行排版，书写也更加方便，与markdown格式一样。可根据需要插入到相应的md。无需再自己配置长宽。<strong>建议在粘贴时故意使用长短、大小、横竖不一的图片</strong>，会有更好的效果。（尺寸完全相同的图片只会平铺输出，效果很糟糕）</p></li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><ol><li><p>gallerygroup 相册图库</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;gallery-group-main&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% galleryGroup MC 在Rikkaの六花服务器里留下的足迹 &#x27;/gallery/MC/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg %&#125;</span><br><span class="line">&#123;% galleryGroup Gundam 哦咧哇gundam哒！ &#x27;/gallery/Gundam/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907110508327.png %&#125;</span><br><span class="line">&#123;% galleryGroup I-am-Akilar 某种意义上也算自拍吧 &#x27;/gallery/I-am-Akilar/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907113116651.png %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure></li><li><p>gallery 相册</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% gallery %&#125;</span><br><span class="line">![](<span class="link">https://i.loli.net/2019/12/25/Fze9jchtnyJXMHN.jpg</span>)</span><br><span class="line">![](<span class="link">https://i.loli.net/2019/12/25/ryLVePaqkYm4TEK.jpg</span>)</span><br><span class="line">&#123;% endgallery %&#125;</span><br></pre></td></tr></table></figure></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><ol><li>gallerygroup 相册图库</li></ol><ol><li>gallery 相册</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="20-折叠框-folding"><a href="#20-折叠框-folding" class="headerlink" title="20 折叠框 folding"></a>20 折叠框 folding</h2><details class="folding-tag" blue><summary> 折叠框 folding </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% folding 参数（可选）, 标题 %&#125;</span><br><span class="line">![](<span class="link">https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg</span>)</span><br><span class="line">&#123;% endfolding %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">颜色</td><td style="text-align:center">blue, cyan, green, yellow, red</td></tr><tr><td style="text-align:center">状态</td><td style="text-align:center">状态填写 open 代表默认打开。</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#123;% folding 查看图片测试 %&#125;</span><br><span class="line"></span><br><span class="line">![](<span class="link">https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg</span>)</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding cyan open, 查看默认打开的折叠框 %&#125;</span><br><span class="line"></span><br><span class="line">这是一个默认打开的折叠框。</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding green, 查看代码测试 %&#125;</span><br><span class="line">假装这里有代码块（代码块没法嵌套代码块）</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding yellow, 查看列表测试 %&#125;</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> haha</span><br><span class="line"><span class="bullet">-</span> hehe</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding red, 查看嵌套测试 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding blue, 查看嵌套测试2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding 查看嵌套测试3 %&#125;</span><br><span class="line"></span><br><span class="line">hahaha <span class="language-xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg&#x27;</span> <span class="attr">style</span>=<span class="string">&#x27;height:24px&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><details class="folding-tag" ><summary> 查看图片测试 </summary>              <div class='content'>              <p><img src="https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg" alt=""></p>              </div>            </details><details class="folding-tag" cyan open><summary> 查看默认打开的折叠框 </summary>              <div class='content'>              <p>这是一个默认打开的折叠框。</p>              </div>            </details><details class="folding-tag" green><summary> 查看代码测试 </summary>              <div class='content'>              <p>假装这里有代码块（代码块没法嵌套代码块）</p>              </div>            </details><details class="folding-tag" yellow><summary> 查看列表测试 </summary>              <div class='content'>              <ul><li>haha</li><li>hehe</li></ul>              </div>            </details><details class="folding-tag" red><summary> 查看嵌套测试 </summary>              <div class='content'>              <details class="folding-tag" blue><summary> 查看嵌套测试2 </summary>              <div class='content'>              <details class="folding-tag" ><summary> 查看嵌套测试3 </summary>              <div class='content'>              <p>hahaha <span><img src='https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg' style='height:24px'></span></p>              </div>            </details>              </div>            </details>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="21-分栏-tab"><a href="#21-分栏-tab" class="headerlink" title="21 分栏 tab"></a>21 分栏 tab</h2><details class="folding-tag" blue><summary> 分栏 tab </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs Unique name, [index] %&#125;</span><br><span class="line">&lt;!-- tab [Tab caption] [@icon] --&gt;</span><br><span class="line">Any content (support inline tags too).</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><ol><li><p>Unique name :</p><ul><li>选项卡块标签的唯一名称，不带逗号。</li><li>将在#id中用作每个标签及其索引号的前缀。</li><li>如果名称中包含空格，则对于生成#id，所有空格将由破折号代替。</li><li>仅当前帖子/页面的URL必须是唯一的！</li></ul></li><li><p>[index]:</p><ul><li>活动选项卡的索引号。</li><li>如果未指定，将选择第一个标签（1）。</li><li>如果index为-1，则不会选择任何选项卡。</li><li>可选参数。</li></ul></li><li><p>[Tab caption]:</p><ul><li>当前选项卡的标题。</li><li>如果未指定标题，则带有制表符索引后缀的唯一名称将用作制表符的标题。</li><li>如果未指定标题，但指定了图标，则标题将为空。</li><li>可选参数。</li></ul></li><li><p>[@icon]:</p><ul><li>FontAwesome图标名称（全名，看起来像“ fas fa-font”）</li><li>可以指定带空格或不带空格；</li><li>例如’Tab caption @icon’ 和 ‘Tab caption@icon’.</li><li>可选参数。</li></ul></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><ol><li>Demo 1 - 预设选择第一个【默认】</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><ol><li>Demo 2 - 预设选择tabs</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test2, 3 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><ol><li>Demo 3 - 没有预设值</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test3, -1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><ol><li>Demo 4 - 自定义Tab名 + 只有icon + icon和Tab名</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test4 %&#125;</span><br><span class="line">&lt;!-- tab 第一个Tab --&gt;</span><br><span class="line"><span class="strong">**tab名字为第一个Tab**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab @fab fa-apple-pay --&gt;</span><br><span class="line"><span class="strong">**只有图标 没有Tab名字**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 炸弹@fas fa-bomb --&gt;</span><br><span class="line"><span class="strong">**名字+icon**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><ol><li><p>Demo 1 - 预设选择第一个【默认】</p><div class="tabs" id="test1"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test1-1">test1 1</button></li><li class="tab"><button type="button" data-href="#test1-2">test1 2</button></li><li class="tab"><button type="button" data-href="#test1-3">test1 3</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div></li><li><p>Demo 2 - 预设选择tabs</p><div class="tabs" id="test2"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test2-1">test2 1</button></li><li class="tab"><button type="button" data-href="#test2-2">test2 2</button></li><li class="tab active"><button type="button" data-href="#test2-3">test2 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test2-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test2-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content active" id="test2-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div></li><li><p>Demo 3 - 没有预设值</p><div class="tabs" id="test3"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test3-1">test3 1</button></li><li class="tab"><button type="button" data-href="#test3-2">test3 2</button></li><li class="tab"><button type="button" data-href="#test3-3">test3 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test3-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div></li><li><p>Demo 4 - 自定义Tab名 + 只有icon + icon和Tab名</p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">第一个Tab</button></li><li class="tab"><button type="button" data-href="#test4-2"><i class="fab fa-apple-pay" style="text-align: center;"></i></button></li><li class="tab"><button type="button" data-href="#test4-3"><i class="fas fa-bomb"></i>炸弹</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><p><strong>只有图标 没有Tab名字</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-3"><p><strong>名字+icon</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>&lt;/li&gt;<br>&lt;/ol&gt;<button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button>&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</p>              </div>            </details><h2 id="22-诗词标签-poem"><a href="#22-诗词标签-poem" class="headerlink" title="22 诗词标签 poem"></a>22 诗词标签 poem</h2><details class="folding-tag" blue><summary> 诗词标签 poem </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% poem title,author %&#125;</span><br><span class="line">内容</span><br><span class="line">&#123;% endpoem %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">title</td><td style="text-align:center">诗词标题</td></tr><tr><td style="text-align:center">author</td><td style="text-align:center">作者，可以不写</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;% poem 水调歌头,苏轼 %&#125;</span><br><span class="line">丙辰中秋，欢饮达旦，大醉，作此篇，兼怀子由。</span><br><span class="line">明月几时有？把酒问青天。</span><br><span class="line">不知天上宫阙，今夕是何年？</span><br><span class="line">我欲乘风归去，又恐琼楼玉宇，高处不胜寒。</span><br><span class="line">起舞弄清影，何似在人间？</span><br><span class="line"></span><br><span class="line">转朱阁，低绮户，照无眠。</span><br><span class="line">不应有恨，何事长向别时圆？</span><br><span class="line">人有悲欢离合，月有阴晴圆缺，此事古难全。</span><br><span class="line">但愿人长久，千里共婵娟。</span><br><span class="line">&#123;% endpoem %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><div class='poem'><div class='poem-title'>水调歌头</div><div class='poem-author'>苏轼</div><p>丙辰中秋，欢饮达旦，大醉，作此篇，兼怀子由。<br>明月几时有？把酒问青天。<br>不知天上宫阙，今夕是何年？<br>我欲乘风归去，又恐琼楼玉宇，高处不胜寒。<br>起舞弄清影，何似在人间？</p><p>转朱阁，低绮户，照无眠。<br>不应有恨，何事长向别时圆？<br>人有悲欢离合，月有阴晴圆缺，此事古难全。<br>但愿人长久，千里共婵娟。</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="23-阿里图标-icon"><a href="#23-阿里图标-icon" class="headerlink" title="23 阿里图标 icon"></a>23 阿里图标 icon</h2><details class="folding-tag" blue><summary> 阿里图标 icon </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon [icon-xxxx],[font-size] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">icon-xxxx</td><td style="text-align:center">表示图标<code>font-class</code>,可以在自己的阿里矢量图标库项目的<code>font-class</code>引用方案内查询并复制。</td></tr><tr><td style="text-align:center">font-size</td><td style="text-align:center">表示图标大小，直接填写数字即可，单位为<code>em</code>。图标大小默认值为<code>1em</code>。</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon icon-rat<span class="emphasis">_zi %&#125;&#123;% icon icon-rat,2 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-ox_</span>chou,3 %&#125;&#123;% icon icon-ox,4 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-tiger<span class="emphasis">_yin,5 %&#125;&#123;% icon icon-tiger,6 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-rabbit_</span>mao,1 %&#125;&#123;% icon icon-rabbit,2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-dragon<span class="emphasis">_chen,3 %&#125;&#123;% icon icon-dragon,4 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-snake_</span>si,5 %&#125;&#123;% icon icon-snake,6 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-horse<span class="emphasis">_wu %&#125;&#123;% icon icon-horse,2 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-goat_</span>wei,3 %&#125;&#123;% icon icon-goat,4 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-monkey<span class="emphasis">_shen,5 %&#125;&#123;% icon icon-monkey,6 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-rooster_</span>you %&#125;&#123;% icon icon-rooster,2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-dog<span class="emphasis">_xu,3 %&#125;&#123;% icon icon-dog,4 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-boar_</span>hai,5 %&#125;&#123;% icon icon-boar,6 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rat_zi"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rat"></use></svg><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-ox_chou"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-ox"></use></svg><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-tiger_yin"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-tiger"></use></svg><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rabbit_mao"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rabbit"></use></svg><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-dragon_chen"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-dragon"></use></svg><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-snake_si"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-snake"></use></svg><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-horse_wu"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-horse"></use></svg><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-goat_wei"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-goat"></use></svg><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-monkey_shen"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-monkey"></use></svg><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rooster_you"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rooster"></use></svg><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-dog_xu"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-dog"></use></svg><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-boar_hai"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-boar"></use></svg><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="24-特效标签wow"><a href="#24-特效标签wow" class="headerlink" title="24 特效标签wow"></a>24 特效标签wow</h2><details class="folding-tag" blue><summary> 特效标签wow </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow [animete],[duration],[delay],[offset],[iteration] %&#125;</span><br><span class="line">内容</span><br><span class="line">&#123;% endwow %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">animate</td><td style="text-align:center">动画样式，效果详见<a href="https://animate.style/">animate.css参考文档</a></td></tr><tr><td style="text-align:center">duration</td><td style="text-align:center">选填项，动画持续时间，单位可以是<code>ms</code>也可以是<code>s</code>。例如<code>3s</code>，<code>700ms</code>。</td></tr><tr><td style="text-align:center">delay</td><td style="text-align:center">选填项，动画开始的延迟时间，单位可以是<code>ms</code>也可以是<code>s</code>。例如<code>3s</code>，<code>700ms</code>。</td></tr><tr><td style="text-align:center">offset</td><td style="text-align:center">选填项，开始动画的距离（相对浏览器底部）</td></tr><tr><td style="text-align:center">iteration</td><td style="text-align:center">选填项，动画重复的次数</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>1.flip动画效果。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__fip,5s,5s,100,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`zoomIn`动画效果，持续`5s`，延时`5s`，离底部`100`距离时启动，重复`10`次</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>2.zoomIn动画效果，持续5s，延时5s，离底部100距离时启动，重复10次</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__zoomIn,5s,5s,100,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`zoomIn`动画效果，持续`5s`，延时`5s`，离底部`100`距离时启动，重复`10`次</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>3.slideInRight动画效果，持续5s，延时5s</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__slideInRight,5s,5s %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note orange &#x27;fas fa-car&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`slideInRight`动画效果，持续`5s`，延时`5s`。</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>4.heartBeat动画效果，延时5s，重复10次。此处注意不用的参数位置要留空，用逗号间隔。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__heartBeat,,5s,,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note red &#x27;fas fa-battery-half&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`heartBeat`动画效果，延时`5s`，重复`10`次。</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>1.flip动画效果。</p><div class='wow animate__fip' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset='100'  data-wow-iteration='10' ><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p><code>zoomIn</code>动画效果，持续<code>5s</code>，延时<code>5s</code>，离底部<code>100</code>距离时启动，重复<code>10</code>次</p></div></div><p><p>2.zoomIn动画效果，持续5s，延时5s，离底部100距离时启动，重复10次</p></p><div class='wow animate__zoomIn' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset='100'  data-wow-iteration='10' ><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p><code>zoomIn</code>动画效果，持续<code>5s</code>，延时<code>5s</code>，离底部<code>100</code>距离时启动，重复<code>10</code>次</p></div></div><p><p>3.slideInRight动画效果，持续5s，延时5s</p></p><div class='wow animate__slideInRight' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset=''  data-wow-iteration='' ><div class="note orange icon-padding modern"><i class="note-icon fas fa-car"></i><p><code>slideInRight</code>动画效果，持续<code>5s</code>，延时<code>5s</code>。</p></div></div><p><p>4.heartBeat动画效果，延时5s，重复10次。此处注意不用的参数位置要留空，用逗号间隔。</p></p><div class='wow animate__heartBeat' data-wow-duration='' data-wow-delay='5s' data-wow-offset=''  data-wow-iteration='10' ><div class="note red icon-padding modern"><i class="note-icon fas fa-battery-half"></i><p><code>heartBeat</code>动画效果，延时<code>5s</code>，重复<code>10</code>次。</p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="25-进度条-progress"><a href="#25-进度条-progress" class="headerlink" title="25 进度条 progress"></a>25 进度条 progress</h2><details class="folding-tag" blue><summary> 进度条 progress </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% progress [width] [color] [text] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">width</td><td style="text-align:center">0到100的阿拉伯数字</td></tr><tr><td style="text-align:center">color</td><td style="text-align:center">颜色，取值有red,yellow,green,cyan,blue,gray</td></tr><tr><td style="text-align:center">text</td><td style="text-align:center">进度条上的文字内容</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% progress 10 red 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 30 yellow 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 50 green 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 70 cyan 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 90 blue 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 100 gray 进度条样式预览 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-red"  style="width: 10%" aria-valuenow="10" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-yellow"  style="width: 30%" aria-valuenow="30" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-green"  style="width: 50%" aria-valuenow="50" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-cyan"  style="width: 70%" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-blue"  style="width: 90%" aria-valuenow="90" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-gray"  style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="26-注释-notation"><a href="#26-注释-notation" class="headerlink" title="26 注释 notation"></a>26 注释 notation</h2><details class="folding-tag" blue><summary> 注释 notation </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% nota [label] , [text] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">label</td><td style="text-align:center">注释词汇</td></tr><tr><td style="text-align:center">text</td><td style="text-align:center">悬停显示的内容</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% nota 把鼠标移动到我上面试试 ,可以看到注解内容出现在顶栏 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p><span class='nota' data-nota='可以看到注解内容出现在顶栏'>把鼠标移动到我上面试试</span></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="27-气泡注释-bubble"><a href="#27-气泡注释-bubble" class="headerlink" title="27 气泡注释 bubble"></a>27 气泡注释 bubble</h2><details class="folding-tag" blue><summary> 气泡注释 bubble </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bubble [content] , [notation] ,[background-color] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">content</td><td style="text-align:center">注释词汇</td></tr><tr><td style="text-align:center">notation</td><td style="text-align:center">悬停显示的注释内容</td></tr><tr><td style="text-align:center">background-color</td><td style="text-align:center">可选，气泡背景色，默认为“#71a4e3”</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最近我学到了不少新玩意儿（虽然对很多大佬来说这些已经是旧技术了），比如CSS的&#123;% bubble 兄弟相邻选择器,&quot;例如 h1 + p &#123;margin-top:50px;&#125;&quot; %&#125;，&#123;% bubble flex布局,&quot;Flex 是 Flexible Box 的缩写，意为&quot;弹性布局&quot;，用来为盒状模型提供最大的灵活性&quot;,&quot;#ec5830&quot; %&#125;，&#123;% bubble transform变换,&quot;transform 属性向元素应用 2D 或 3D 转换。该属性允许我们对元素进行旋转、缩放、移动或倾斜。&quot;,&quot;#1db675&quot; %&#125;，animation的&#123;% bubble 贝塞尔速度曲线,&quot;贝塞尔曲线(Bézier curve)，又称贝兹曲线或贝济埃曲线，是应用于二维图形应用程序的数学曲线。一般的矢量图形软件通过它来精确画出曲线，贝兹曲线由线段与节点组成，节点是可拖动的支点，线段像可伸缩的皮筋&quot;,&quot;#de4489&quot; %&#125;写法，还有今天刚看到的&#123;% bubble clip-path,&quot;clip-path属性使用裁剪方式创建元素的可显示区域。区域内的部分显示，区域外的隐藏。&quot;,&quot;#868fd7&quot; %&#125;属性。这些对我来说很新颖的概念狠狠的冲击着我以前积累起来的设计思路。</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>最近我学到了不少新玩意儿（虽然对很多大佬来说这些已经是旧技术了），比如CSS的<span class="bubble-content">兄弟相邻选择器</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#71a4e3;">例如 h1 + p {margin-top:50px;}</span>&lt;/span&gt;，<span class="bubble-content">flex布局</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">Flex 是 Flexible Box 的缩写，意为弹性布局”，用来为盒状模型提供最大的灵活性”</span>&lt;/span&gt;，<span class="bubble-content">transform变换</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#1db675;">transform 属性向元素应用 2D 或 3D 转换。该属性允许我们对元素进行旋转、缩放、移动或倾斜。</span>&lt;/span&gt;，animation的<span class="bubble-content">贝塞尔速度曲线</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#de4489;">贝塞尔曲线(Bézier curve)，又称贝兹曲线或贝济埃曲线，是应用于二维图形应用程序的数学曲线。一般的矢量图形软件通过它来精确画出曲线，贝兹曲线由线段与节点组成，节点是可拖动的支点，线段像可伸缩的皮筋</span>&lt;/span&gt;写法，还有今天刚看到的<span class="bubble-content">clip-path</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#868fd7;">clip-path属性使用裁剪方式创建元素的可显示区域。区域内的部分显示，区域外的隐藏。</span>&lt;/span&gt;属性。这些对我来说很新颖的概念狠狠的冲击着我以前积累起来的设计思路。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="28-PDF展示"><a href="#28-PDF展示" class="headerlink" title="28 PDF展示"></a>28 PDF展示</h2><details class="folding-tag" blue><summary> PDF展示 </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf 文件路径 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">文件路径</td><td style="text-align:center">可以是相对路径或者是在线链接</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 1.本地文件:在md文件路径下创建一个同名文件夹，其内放pdf文件名为xxx.pdf的文件</span></span><br><span class="line">&#123;% pdf xxx.pdf %&#125;</span><br><span class="line"><span class="section"># 2.在线链接</span></span><br><span class="line">&#123;% pdf https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/pdf/小作文讲义.pdf %&#125;</span><br><span class="line"></span><br><span class="line">实际源码：</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;row&quot;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">embed</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/pdf/小作文讲义.pdf&quot;</span> <span class="attr">width</span>=<span class="string">&quot;100%&quot;</span> <span class="attr">height</span>=<span class="string">&quot;550&quot;</span> <span class="attr">type</span>=<span class="string">&quot;application/pdf&quot;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="29-引用文献-reference"><a href="#29-引用文献-reference" class="headerlink" title="29 引用文献 reference"></a>29 引用文献 reference</h2><details class="folding-tag" blue><summary> 引用文献 reference </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">配置参数</button></li><li class="tab"><button type="button" data-href="#test-3">示例源码</button></li><li class="tab"><button type="button" data-href="#test-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># referto 引用上标</span></span><br><span class="line">&#123;% referto [id] , [literature] %&#125;</span><br><span class="line"><span class="section"># referfrom 引用出处</span></span><br><span class="line">&#123;% referfrom [id] , [literature] , [url] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><p>1.referto 引用上标</p><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">id</td><td style="text-align:center">上标序号内容，需与referfrom标签的id对应才能实现跳转</td></tr><tr><td style="text-align:center">literature</td><td style="text-align:center">引用的参考文献名称</td></tr></tbody></table></div><p>2.referfrom 引用出处</p><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td style="text-align:center">id</td><td style="text-align:center">序号内容，需与referto标签的id对应才能实现 跳转</td></tr><tr><td style="text-align:center">literature</td><td style="text-align:center">引用的参考文献名称</td></tr><tr><td style="text-align:center">url</td><td style="text-align:center">引用的参考文献链接，可省略</td></tr></tbody></table></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Akilarの糖果屋(akilar.top)是一个私人性质的博客&#123;% referto &#x27;[1]&#x27;,&#x27;Akilarの糖果屋群聊简介&#x27; %&#125;，从各类教程至生活点滴，无话不谈。建群的目的是提供一个闲聊的场所。博客采用Hexo框架&#123;% referto &#x27;[2]&#x27;,&#x27;Hexo中文文档&#x27; %&#125;，Butterfly主题&#123;% referto &#x27;[3]&#x27;,&#x27;Butterfly 安装文档(一) 快速开始&#x27; %&#125;</span><br><span class="line"></span><br><span class="line">本项目参考了Volantis&#123;% referto &#x27;[4]&#x27;,&#x27;hexo-theme-volantis 标签插件&#x27; %&#125;的标签样式。引入<span class="code">`[tag].js`</span>，并针对<span class="code">`butterfly`</span>主题修改了相应的<span class="code">`[tag].styl`</span>。在此鸣谢<span class="code">`Volantis`</span>主题众开发者。</span><br><span class="line">主要参考内容包括各个volantis的内置标签插件文档&#123;% referto &#x27;[5]&#x27;,&#x27;Volantis文档:内置标签插件&#x27; %&#125;</span><br><span class="line">Butterfly主题的各个衍生魔改&#123;% referto &#x27;[6]&#x27;,&#x27;Butterfly 安装文档:标签外挂（Tag Plugins&#x27; %&#125;&#123;% referto &#x27;[7]&#x27;,&#x27;小弋の生活馆全样式预览&#x27; %&#125;&#123;% referto &#x27;[8]&#x27;,&#x27;l-lin-font-awesome-animation&#x27; %&#125;&#123;% referto &#x27;[9]&#x27;,&#x27;小康的butterfly主题使用文档&#x27; %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% referfrom &#x27;[1]&#x27;,&#x27;Akilarの糖果屋群聊简介&#x27;,&#x27;https://jq.qq.com/?<span class="emphasis">_wv=1027&amp;k=pGLB2C0N&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[2]&#x27;,&#x27;Hexo中文文档&#x27;,&#x27;https://hexo.io/zh-cn/docs/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[3]&#x27;,&#x27;Butterfly 安装文档(一) 快速开始&#x27;,&#x27;https://butterfly.js.org/posts/21cfbf15/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[4]&#x27;,&#x27;hexo-theme-volantis 标签插件&#x27;,&#x27;https://volantis.js.org/v5/tag-plugins/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[5]&#x27;,&#x27;Volantis文档:内置标签插件&#x27;,&#x27;https://volantis.js.org/tag-plugins/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[6]&#x27;,&#x27;Butterfly 安装文档:标签外挂（Tag Plugins&#x27;,&#x27;https://butterfly.js.org/posts/4aa8abbe/#%E6%A8%99%E7%B1%A4%E5%A4%96%E6%8E%9B%EF%BC%88Tag-Plugins%EF%BC%89&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[7]&#x27;,&#x27;小弋の生活馆全样式预览&#x27;,&#x27;https://lovelijunyi.gitee.io/posts/c898.html&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[8]&#x27;,&#x27;l-lin-font-awesome-animation&#x27;,&#x27;https://github.com/l-lin/font-awesome-animation&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[9]&#x27;,&#x27;小康的butterfly主题使用文档&#x27;,&#x27;https://www.antmoe.com/posts/3b43914f/&#x27; %&#125;</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-4"><p>Akilarの糖果屋(akilar.top)是一个私人性质的博客<span class="hidden-anchor" id="referto_[1]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[1]">[1]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Akilarの糖果屋群聊简介</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;，从各类教程至生活点滴，无话不谈。建群的目的是提供一个闲聊的场所。博客采用Hexo框架<span class="hidden-anchor" id="referto_[2]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[2]">[2]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Hexo中文文档</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;，Butterfly主题<span class="hidden-anchor" id="referto_[3]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[3]">[3]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Butterfly 安装文档(一) 快速开始</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;</p><p>本项目参考了Volantis<span class="hidden-anchor" id="referto_[4]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[4]">[4]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">hexo-theme-volantis 标签插件</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;的标签样式。引入<code>[tag].js</code>，并针对<code>butterfly</code>主题修改了相应的<code>[tag].styl</code>。在此鸣谢<code>Volantis</code>主题众开发者。<br>主要参考内容包括各个volantis的内置标签插件文档<span class="hidden-anchor" id="referto_[5]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[5]">[5]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Volantis文档:内置标签插件</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;<br>Butterfly主题的各个衍生魔改<span class="hidden-anchor" id="referto_[6]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[6]">[6]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Butterfly 安装文档:标签外挂（Tag Plugins</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;<span class="hidden-anchor" id="referto_[7]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[7]">[7]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">小弋の生活馆全样式预览</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;<span class="hidden-anchor" id="referto_[8]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[8]">[8]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">l-lin-font-awesome-animation</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;<span class="hidden-anchor" id="referto_[9]">&lt;/span&gt;<sup class="reference"><a href="#referfrom_[9]">[9]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">小康的butterfly主题使用文档</span><span class="reference-title">参考资料</span>&lt;/span&gt;&lt;/span&gt;</p><div class="reference-source"><span class="hidden-anchor" id="referfrom_[1]"></span><a class="reference-anchor" href="#referto_[1]">[1]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://jq.qq.com/?_wv=1027&k=pGLB2C0N">Akilarの糖果屋群聊简介</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[2]"></span><a class="reference-anchor" href="#referto_[2]">[2]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://hexo.io/zh-cn/docs/">Hexo中文文档</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[3]"></span><a class="reference-anchor" href="#referto_[3]">[3]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://butterfly.js.org/posts/21cfbf15/">Butterfly 安装文档(一) 快速开始</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[4]"></span><a class="reference-anchor" href="#referto_[4]">[4]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://volantis.js.org/v5/tag-plugins/">hexo-theme-volantis 标签插件</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[5]"></span><a class="reference-anchor" href="#referto_[5]">[5]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://volantis.js.org/tag-plugins/">Volantis文档:内置标签插件</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[6]"></span><a class="reference-anchor" href="#referto_[6]">[6]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://butterfly.js.org/posts/4aa8abbe/#%E6%A8%99%E7%B1%A4%E5%A4%96%E6%8E%9B%EF%BC%88Tag-Plugins%EF%BC%89">Butterfly 安装文档:标签外挂（Tag Plugins</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[7]"></span><a class="reference-anchor" href="#referto_[7]">[7]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://lovelijunyi.gitee.io/posts/c898.html">小弋の生活馆全样式预览</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[8]"></span><a class="reference-anchor" href="#referto_[8]">[8]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://github.com/l-lin/font-awesome-animation">l-lin-font-awesome-animation</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[9]"></span><a class="reference-anchor" href="#referto_[9]">[9]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://www.antmoe.com/posts/3b43914f/">小康的butterfly主题使用文档</a></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="30-Hexo-tag-map-插件"><a href="#30-Hexo-tag-map-插件" class="headerlink" title="30 Hexo-tag-map 插件"></a>30 Hexo-tag-map 插件</h2><details class="folding-tag" blue><summary> Hexo-tag-map 插件 </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% + 标签值 + 经度 + 纬度 + 文本 + 缩放等级 + 宽 + 高 + 默认图层 + %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% map 120.101101,30.239119 %&#125;</span><br><span class="line">&#123;% googleMap 120.101101,30.239119, 这里是西湖灵隐寺，据说求姻缘很灵验哦！ %&#125;</span><br><span class="line">&#123;% geoqMap 120.101101,30.239119, 这里是西湖灵隐寺，据说求姻缘很灵验哦！, 13, 90%, 320px, 3 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><p>渲染好像出点了问题，暂时还没有调试~不急</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details><h2 id="31-隐藏块"><a href="#31-隐藏块" class="headerlink" title="31 隐藏块"></a>31 隐藏块</h2><details class="folding-tag" blue><summary> 隐藏块 </summary>              <div class='content'>              <div class="tabs" id="test"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test-1">标签语法</button></li><li class="tab"><button type="button" data-href="#test-2">示例源码</button></li><li class="tab"><button type="button" data-href="#test-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% hideBlock display,bg,color %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endhideBlock %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% hideBlock 点我预览, blue %&#125;</span><br><span class="line">这里有张图片：</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://picbed.dai2yutou.space/web_img/web_background2.jpg&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;image (1)&quot;</span> <span class="attr">style</span>=<span class="string">&quot;zoom:67%;&quot;</span> /&gt;</span></span></span><br><span class="line">&#123;% endhideBlock %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test-3"><div class="hide-block"><button type="button" class="hide-button" style="background-color:  blue;">点我预览    </button><div class="hide-content"><p>这里有张图片：<br><img src="https://picbed.dai2yutou.space/web_img/web_background2.jpg" alt="image (1)" style="zoom:67%;" /></p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> butterfly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>停车场管理模拟系统</title>
      <link href="/2023/01/07/%E5%81%9C%E8%BD%A6%E5%9C%BA%E7%AE%A1%E7%90%86%E6%A8%A1%E6%8B%9F%E7%B3%BB%E7%BB%9F/"/>
      <url>/2023/01/07/%E5%81%9C%E8%BD%A6%E5%9C%BA%E7%AE%A1%E7%90%86%E6%A8%A1%E6%8B%9F%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<details class="folding-tag" ><summary> 博主心得🙄 </summary>              <div class='content'>              <p>本次实训我们组被分配的是<strong>停车场模拟系统</strong>，感觉相比其他组的任务来说，我们的还是相对容易的哈哈哈哈。</p><p><strong>其他组的有：</strong></p><ul><li>基于A*算法的迷宫小游戏开发</li><li>欢乐连连看游戏设计</li><li>山东理工大学新生校园导游程序</li><li>景区信息管理系统</li><li>电梯模拟</li><li>机票管理系统</li><li>张店区市内公交查询系统</li><li>图书管理系统</li></ul><p>竟然还有开发游戏的，这不摆等什么的【开个玩笑啦😂】</p><p>本次实训历时四周左右，但我们组真正意义上也就花了十天左右，代码大部分都是我一个人开发出来的，因为我是组长所以相对来说责任比较大（当组长难啊呜呜呜😭）。</p><p>答辩的时候也出了点意外，9点需要进腾讯会议，然后开始一组接着一组开始答辩，在答辩前一天晚上，我在和我的女朋友打着电话（💏），睡觉也没有挂，一直打的，我想这订了四五个闹铃应该能起来吧【我晚上一般睡的比较晚，都12点1、2点睡觉，然后白天睡到自然醒十点十一点那样的】，结果手机忘记连着充电器了，导致第二天迟到了二十几分钟，最后还是我老妈叫我起来吃饭我才醒的。结果让答辩老师知道了，我们组的另一个成员说老师点名就叫了两个人，全是我们组的，可怕的是我两都迟到了，导致我们答辩的时候效果特别差，老师很生气，我作为组长还向老师和所有同学道了歉（好丢人😰），希望老师能给我们一个好的成绩😭。</p><p>不多叭叭，下面看代码+实训报告~</p>              </div>            </details><h1 id="一、题目要求"><a href="#一、题目要求" class="headerlink" title="一、题目要求"></a>一、题目要求</h1><h2 id="1-1-问题描述"><a href="#1-1-问题描述" class="headerlink" title="1.1 问题描述"></a>1.1 问题描述</h2><p>模拟一个停车场系统。停车场根据停车的占地面积进行收费。我们假设一个停车场存在大、中、小三种车型的停车位（不同类型的停车位收费标准不同，且上一级别的车不能在下一级别的停车位停车）。当有车辆进入停车场时，登记车的型号，车牌以及时间，并将车停放在停车场内。当取车时，输入车牌号，取出相应的车，并记录现在时间，根据停车时间及车位类型收取费用。当停车场满时，在停车场外的一个长度为N的候车道等待进入停车场停车。</p><p>设停车场是一个可停放n辆汽车的狭长通道，且只有一个大门可供汽车进出。汽车在停车场内按车辆到达时间的先后顺序，依次由北向南排列（大门在最南端，最先到达的第一辆车停放在车场的最北端），若车场内已停满n辆汽车，则后来的汽车只能在门外的便道上等候，一旦有车开走，则排在便道上的第一辆车即可开入；当停车场内某辆车要离开时，在它之后开入的车辆必须先退出车场为它让路，待该辆车开出大门外，其它车辆再按原次序进入车场。</p><h2 id="1-2-基本要求"><a href="#1-2-基本要求" class="headerlink" title="1.2 基本要求"></a>1.2 基本要求</h2><p><strong>（1）前提要求</strong></p><p>​        以栈模拟停车场，以队列模拟车场外的便道，按照从终端读入的输入数据序列进行模拟管理。每一组输入数据包括三个数据项：汽车“到达”或“离去”信息、汽车牌照号码及到达或离去的时刻，对每一组输入数据进行操作后的输出数据为：若是车辆到达，则输出汽车在停车场内或便道上的停车位置；若是车离去；则输出汽车在停车场内停留的时间和应交纳的费用（在便道上停留的时间不收费）。栈以顺序结构实现，队列以链表实现。</p><p><strong>(2) 测试数据</strong></p><p>​        设n=2,输入数据为：（‘A’，1，5），（‘A’，2，10），（‘D’，1，15），（‘A’，3， 20）， （‘A’，4，25），（‘A’，5，30），（‘D’，2，35），（‘D’，4，40），（‘E’，0，0）。每一组输入数据包括三个数据项：汽车“到达”或“离去”信息、汽车牌照号码及到达或离去的时刻，其中，‘A’表示到达；‘D’表示离去，‘E’表示输入结束。</p><p><strong>(3) 实现提示</strong></p><p>​        需另设一个栈，临时停放为给要离去的汽车让路而从停车场退出来的汽车，也用顺序存储结构实现。输入数据按到达或离去的时刻有序。栈中每个元素表示一辆汽车，包含两个数据项：汽车的牌照号码和进入停车场的时刻。</p><p><strong>(4) 功能要求：</strong></p><ul><li>用文件保存停车场和候车道的基本数据</li><li>停车：可选择车位，输入车牌号，将车的基本信息存入车位中 </li><li>取车：输入车牌，获取时间，计算费用 </li><li>查询：在停车时，可以查询停车场的整体情况</li><li>空位提示：一旦取车后停车场出现空位，需要将信息传递给候车道。候车道队列出列 </li><li>满位提示：一旦停车厂位满，则需要再候车道停车，即候车道入队列。 </li></ul><h2 id="1-3-选做内容"><a href="#1-3-选做内容" class="headerlink" title="1.3 选做内容"></a>1.3 选做内容</h2><p>有余力的同学可以考虑用图形界面实现停车场的状态显示</p><h1 id="二、实训报告"><a href="#二、实训报告" class="headerlink" title="二、实训报告"></a>二、实训报告</h1><details class="folding-tag" ><summary> 实训报告 </summary>              <div class='content'>              <div class="row">   <embed src="https://picbed.dai2yutou.space/PDF/程序设计与数据结构停车场模拟管理实训报告.pdf" width="100%" height="550" type="application/pdf"></div>              </div>            </details><h1 id="三、代码展示"><a href="#三、代码展示" class="headerlink" title="三、代码展示"></a>三、代码展示</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//******头文件******</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;windows.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 停车场模拟系统</span></span><br><span class="line"><span class="comment">// 本停车场可停大、中、小型三种型号的车辆</span></span><br><span class="line"><span class="comment">// 收费规则：按分钟收费。大【0.2】 中【0.1】 小【0.05】</span></span><br><span class="line"><span class="comment">// 本停车场停放的车辆最多为MAXSIZE</span></span><br><span class="line"><span class="comment">// 停车场的车位数限制，但便道可无限停车</span></span><br><span class="line"><span class="comment">// 车牌号规定位汉字+大写字母+5个阿拉伯数字</span></span><br><span class="line"><span class="comment">//系统语句颜色：3 湖蓝色 加亮</span></span><br><span class="line"><span class="comment">//提示语句：4 红色 加亮</span></span><br><span class="line"><span class="comment">//结果语句：9 淡浅绿色 加亮</span></span><br><span class="line"><span class="comment">//输入语句：7 白色 加亮</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//******宏定义，规定停车场的车位数量******</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXSIZE 6<span class="comment">//停车场最多可停留汽车辆数</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//******全局变量*****</span></span><br><span class="line"><span class="comment">//下面是每分钟停车费用，我们按分钟收费,分大中小三种车型</span></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> price_1 = <span class="number">0.2</span>;<span class="comment">//大</span></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> price_2 = <span class="number">0.1</span>;<span class="comment">//中</span></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> price_3 = <span class="number">0.05</span>;<span class="comment">//小</span></span><br><span class="line">string Car_Type;<span class="comment">//汽车的型号</span></span><br><span class="line"><span class="type">int</span> i;<span class="comment">//汽车在便道中的位置</span></span><br><span class="line"><span class="type">int</span> number=<span class="number">0</span>; <span class="comment">//计数器：记录停车时的总输入量</span></span><br><span class="line"><span class="type">int</span> number_SeqList=<span class="number">0</span>; <span class="comment">//计数器：记录当前停车场已有的车辆数</span></span><br><span class="line"><span class="type">int</span> number_LQueue=<span class="number">0</span>;  <span class="comment">//计数器：记录当前便道上已有的停车数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//*******定义结构体存储信息******</span></span><br><span class="line"><span class="comment">//时间</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">time</span> &#123;</span><br><span class="line"><span class="type">int</span> h, m;<span class="comment">//小时 分钟</span></span><br><span class="line">&#125;Time;</span><br><span class="line"></span><br><span class="line"><span class="comment">//汽车</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">car</span> &#123;</span><br><span class="line">Time t1, t2;<span class="comment">//到达时间和离开时间</span></span><br><span class="line">string num;<span class="comment">//车牌号</span></span><br><span class="line"><span class="type">int</span> p1, p2;<span class="comment">//汽车在停车场或者便道停留位置</span></span><br><span class="line">string type;    <span class="comment">//汽车的型号</span></span><br><span class="line">&#125;Car;</span><br><span class="line"></span><br><span class="line"><span class="comment">//停车场</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">Car data[MAXSIZE];</span><br><span class="line"><span class="type">int</span> top;</span><br><span class="line">&#125;SeqList;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 停车场车位</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">int</span> space;</span><br><span class="line">    <span class="type">int</span> flag;</span><br><span class="line">&#125;Car_space;</span><br><span class="line">Car_space Car_Space[MAXSIZE];</span><br><span class="line"></span><br><span class="line"><span class="comment">//便道</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">qnode</span> &#123;</span><br><span class="line">Car data;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">qnode</span>* next;</span><br><span class="line">&#125;QNode;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">lqueue</span> &#123;</span><br><span class="line">QNode* front, * rear;</span><br><span class="line">&#125;LQueue;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//******函数声明******</span></span><br><span class="line"><span class="comment">//初始化操作</span></span><br><span class="line"><span class="function">SeqList* <span class="title">Init_SeqList</span><span class="params">()</span></span>;<span class="comment">//初始化停车场</span></span><br><span class="line"><span class="function">LQueue* <span class="title">Init_LQueue</span><span class="params">()</span></span>;<span class="comment">//初始化便道</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Init_Car_Space</span><span class="params">()</span></span>;  <span class="comment">//初始化停车场车位</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//停车问题函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_In</span><span class="params">(SeqList* p, LQueue* q)</span></span>;<span class="comment">//汽车到达</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Push_SeqList</span><span class="params">(SeqList* s, Car x)</span></span>;<span class="comment">//汽车进入停车场</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">In_LQueue</span><span class="params">(LQueue* q, Car x)</span></span>;<span class="comment">//停车场满的时候进入便道</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Traverse_LQueue</span><span class="params">(LQueue* q)</span></span>;    <span class="comment">//便道第一辆车进入停车场后，对后面车辆前移</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//取车问题函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_Out</span><span class="params">(SeqList* p, LQueue* q)</span></span>;<span class="comment">//汽车离去</span></span><br><span class="line"><span class="function">Car <span class="title">Pop_SeqList</span><span class="params">(SeqList* s)</span></span>;<span class="comment">//汽车离开停车场</span></span><br><span class="line"><span class="function">Car <span class="title">Out_LQueue</span><span class="params">(LQueue* q)</span></span>;<span class="comment">//停车场有车离开有空位时便道上的车辆按照到达顺序依次离开便道并进入停车场</span></span><br><span class="line"><span class="function">Car <span class="title">Traverse_SeqList</span><span class="params">(SeqList *q,<span class="type">int</span> top,string num)</span></span>;     <span class="comment">////栈的遍历用来寻找出栈的车辆</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//查询有关函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Display</span><span class="params">(SeqList* p)</span></span>;<span class="comment">//显示停车场内车辆信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//中间衔接函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_Condition</span><span class="params">(SeqList* p, LQueue* q)</span></span>;<span class="comment">//获取车辆的型号和进入/离开停车场的信息</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Str_strcmp</span><span class="params">(string str1,string str2)</span></span>;  <span class="comment">//字符串比较函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//检验操作函数</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Car_if</span><span class="params">(SeqList* p, Car c)</span></span>;<span class="comment">//检验停车场内是否有该车辆</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkNum</span><span class="params">(string s)</span></span>;        <span class="comment">//检查车辆的车牌号格式是否正确</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkTime</span><span class="params">(<span class="type">int</span> h,<span class="type">int</span> m)</span></span>;    <span class="comment">//检查车辆的时间是否正确</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Check_TimeOut</span><span class="params">(<span class="type">int</span> h1,<span class="type">int</span> h2,<span class="type">int</span> m1,<span class="type">int</span> m2)</span></span>;        <span class="comment">//检查汽车离开出栈的时间是否合法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//文件操作函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LQueue_Input</span><span class="params">(string Car_Type, Car car)</span></span>;<span class="comment">//向便道文件写入车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LQueue_Delete</span><span class="params">(Car car)</span></span>;<span class="comment">//删除便道文件中离开便道的车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">SeqList_Input</span><span class="params">(string Car_Type, Car car)</span></span>;<span class="comment">//向停车场写入车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">SeqList_Delete</span><span class="params">(Car car)</span></span>;<span class="comment">//删除停车场车辆数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//页面可视化操作函数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">color</span><span class="params">(<span class="type">int</span> m)</span></span>;   <span class="comment">//修改字体颜色</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">menu_load</span><span class="params">()</span></span>;<span class="comment">//程序刚开始运行时的加载过程</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">menu</span><span class="params">()</span></span>;<span class="comment">//系统主菜单页面</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_1</span><span class="params">()</span></span>;<span class="comment">//加载百分比可视化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_2</span><span class="params">()</span></span>;<span class="comment">//系统退出百分比可视化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_3</span><span class="params">()</span></span>;<span class="comment">//进入系统加载界面</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_4</span><span class="params">()</span></span>;<span class="comment">//进入停车场加载界面</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//******主函数******</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//首先清空上次停车场停入的汽车和便道中的汽车</span></span><br><span class="line">FILE *file;</span><br><span class="line">file = <span class="built_in">fopen</span>(<span class="string">&quot;SeqList.txt&quot;</span>,<span class="string">&quot;w&quot;</span>);</span><br><span class="line"><span class="built_in">fclose</span>(file);</span><br><span class="line">file = <span class="built_in">fopen</span>(<span class="string">&quot;LQueue.txt&quot;</span>,<span class="string">&quot;w&quot;</span>);</span><br><span class="line"><span class="built_in">fclose</span>(file);</span><br><span class="line">file = <span class="built_in">fopen</span>(<span class="string">&quot;temp.txt&quot;</span>,<span class="string">&quot;w&quot;</span>);</span><br><span class="line"><span class="built_in">fclose</span>(file);</span><br><span class="line"><span class="built_in">Init_Car_Space</span>();   <span class="comment">//初始化停车场车位</span></span><br><span class="line"><span class="comment">//开始加载并进入停车管理系统系统</span></span><br><span class="line">    <span class="built_in">menu_load</span>();</span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//******全部函数******</span></span><br><span class="line"><span class="comment">//初始化停车场</span></span><br><span class="line"><span class="function">SeqList* <span class="title">Init_SeqList</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">SeqList* s;</span><br><span class="line">s = <span class="keyword">new</span> SeqList;</span><br><span class="line">s-&gt;top = <span class="number">-1</span>;</span><br><span class="line"><span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化停车场车位</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Init_Car_Space</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;MAXSIZE;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        Car_Space[j].space = j+<span class="number">1</span>;</span><br><span class="line">        Car_Space[j].flag = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化便道</span></span><br><span class="line"><span class="function">LQueue* <span class="title">Init_LQueue</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">LQueue* q;</span><br><span class="line">QNode* p;</span><br><span class="line">q = <span class="keyword">new</span> lqueue;</span><br><span class="line">p = <span class="keyword">new</span> qnode;</span><br><span class="line">p-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">q-&gt;front = p;</span><br><span class="line">q-&gt;rear = p;</span><br><span class="line"><span class="keyword">return</span> q;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//向便道文件写入车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LQueue_Input</span><span class="params">(string Car_Type, Car car)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">FILE* fp;</span><br><span class="line"><span class="keyword">if</span> ((fp = <span class="built_in">fopen</span>(<span class="string">&quot;LQueue.txt&quot;</span>, <span class="string">&quot;at+&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout&lt;&lt;<span class="string">&quot;不能打开文件&quot;</span>&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">string st1 = <span class="string">&quot;车牌号: &quot;</span>+ car.num + <span class="string">&quot;; 型号: &quot;</span>+car.type + <span class="string">&quot;; &quot;</span>;</span><br><span class="line"><span class="type">int</span> h = car.t1.h;</span><br><span class="line"><span class="type">int</span> m = car.t1.m;</span><br><span class="line"><span class="type">int</span> locate = car.p2;</span><br><span class="line">    <span class="type">char</span> st2[st1.<span class="built_in">length</span>()];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;st1.<span class="built_in">length</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        st2[j] = st1[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fprintf</span>(fp,<span class="string">&quot;%s便道车位00%d; 停入便道时间: %d时%d分。\n&quot;</span>,st2,locate,h,m);</span><br><span class="line">    <span class="comment">//fputs(st2,fp);</span></span><br><span class="line"><span class="built_in">fclose</span>(fp);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除便道文件中离开便道的车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LQueue_Delete</span><span class="params">(Car car)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE *fp1,*fp2;</span><br><span class="line">    <span class="type">char</span> strLine[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">if</span> ((fp1 = <span class="built_in">fopen</span>(<span class="string">&quot;LQueue.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ((fp2 = <span class="built_in">fopen</span>(<span class="string">&quot;temp.txt&quot;</span>, <span class="string">&quot;wt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将车牌号字符串转换为字符数组</span></span><br><span class="line">    string st1 = car.num;</span><br><span class="line">    <span class="type">char</span> st2[st1.<span class="built_in">length</span>()];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;st1.<span class="built_in">length</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        st2[j] = st1[j];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">fscanf</span>(fp1,<span class="string">&quot;%[^\n]&quot;</span>,strLine)!=EOF)<span class="comment">//循环读取每一行，直到文件尾</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">fgetc</span>(fp1);</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">strstr</span>(strLine,st2)==<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">fprintf</span>(fp2,<span class="string">&quot;%s\n&quot;</span>,strLine);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">fclose</span>(fp1);</span><br><span class="line"><span class="built_in">fclose</span>(fp2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((fp1 = <span class="built_in">fopen</span>(<span class="string">&quot;temp.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ((fp2 = <span class="built_in">fopen</span>(<span class="string">&quot;LQueue.txt&quot;</span>, <span class="string">&quot;wt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">fscanf</span>(fp1,<span class="string">&quot;%[^\n]&quot;</span>,strLine)!=EOF)<span class="comment">//循环读取每一行，直到文件尾</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">fgetc</span>(fp1);</span><br><span class="line">        <span class="built_in">fprintf</span>(fp2,<span class="string">&quot;%s\n&quot;</span>,strLine);</span><br><span class="line">&#125;</span><br><span class="line">    <span class="built_in">fclose</span>(fp1);</span><br><span class="line"><span class="built_in">fclose</span>(fp2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//向停车场写入车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">SeqList_Input</span><span class="params">(string Car_Type, Car car)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">FILE* fp;</span><br><span class="line"><span class="keyword">if</span> ((fp = <span class="built_in">fopen</span>(<span class="string">&quot;SeqList.txt&quot;</span>, <span class="string">&quot;at+&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">string st1 = <span class="string">&quot;车牌号: &quot;</span>+ car.num + <span class="string">&quot;; 型号: &quot;</span>+car.type + <span class="string">&quot;; &quot;</span>;</span><br><span class="line"><span class="type">int</span> h = car.t1.h;</span><br><span class="line"><span class="type">int</span> m = car.t1.m;</span><br><span class="line"><span class="type">int</span> locate = car.p1;</span><br><span class="line">    <span class="type">char</span> st2[st1.<span class="built_in">length</span>()];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;st1.<span class="built_in">length</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        st2[j] = st1[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fprintf</span>(fp,<span class="string">&quot;%s停车场车位00%d; 停车时间: %d时%d分。\n&quot;</span>,st2,locate,h,m);</span><br><span class="line"><span class="comment">//fputs(st2, fp);</span></span><br><span class="line"><span class="built_in">fclose</span>(fp);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除停车场车辆数据</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">SeqList_Delete</span><span class="params">(Car car)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE *fp1,*fp2;</span><br><span class="line">    <span class="type">char</span> strLine[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">if</span> ((fp1 = <span class="built_in">fopen</span>(<span class="string">&quot;SeqList.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ((fp2 = <span class="built_in">fopen</span>(<span class="string">&quot;temp.txt&quot;</span>, <span class="string">&quot;wt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将车牌号字符串转换为字符数组</span></span><br><span class="line">    string st1 = car.num;</span><br><span class="line">    <span class="type">char</span> st2[st1.<span class="built_in">length</span>()];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;st1.<span class="built_in">length</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        st2[j] = st1[j];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">fscanf</span>(fp1,<span class="string">&quot;%[^\n]&quot;</span>,strLine)!=EOF)   <span class="comment">//循环读取每一行，直到文件尾</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">fgetc</span>(fp1);    <span class="comment">//将fp所指向的文件一行内容读到strLine缓冲区</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">strstr</span>(strLine,st2)==<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">fprintf</span>(fp2,<span class="string">&quot;%s\n&quot;</span>,strLine);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">fclose</span>(fp1);</span><br><span class="line"><span class="built_in">fclose</span>(fp2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((fp1 = <span class="built_in">fopen</span>(<span class="string">&quot;temp.txt&quot;</span>, <span class="string">&quot;rt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ((fp2 = <span class="built_in">fopen</span>(<span class="string">&quot;SeqList.txt&quot;</span>, <span class="string">&quot;wt&quot;</span>)) == <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;不能打开文件&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">fscanf</span>(fp1,<span class="string">&quot;%[^\n]&quot;</span>,strLine)!=EOF)<span class="comment">//循环读取每一行，直到文件尾</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">fgetc</span>(fp1);    <span class="comment">//将fp所指向的文件一行内容读到strLine缓冲区</span></span><br><span class="line">        <span class="built_in">fprintf</span>(fp2,<span class="string">&quot;%s\n&quot;</span>,strLine);</span><br><span class="line">&#125;</span><br><span class="line">    <span class="built_in">fclose</span>(fp1);</span><br><span class="line"><span class="built_in">fclose</span>(fp2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//汽车进入停车场</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Push_SeqList</span><span class="params">(SeqList* s, Car x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">s-&gt;top++;</span><br><span class="line">s-&gt;data[s-&gt;top] = x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//汽车离开停车场</span></span><br><span class="line"><span class="function">Car <span class="title">Pop_SeqList</span><span class="params">(SeqList* s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Car x = s-&gt;data[s-&gt;top];</span><br><span class="line">s-&gt;top--;</span><br><span class="line"><span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//停车场满的时候进入便道</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">In_LQueue</span><span class="params">(LQueue* q, Car x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">QNode* p;</span><br><span class="line">p = <span class="keyword">new</span> qnode;</span><br><span class="line">p-&gt;data = x;</span><br><span class="line">p-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">q-&gt;rear-&gt;next = p;</span><br><span class="line">q-&gt;rear = p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//便道第一辆车进入停车场后，对后面车辆前移</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Traverse_LQueue</span><span class="params">(LQueue* q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(q-&gt;front!=q-&gt;rear)</span><br><span class="line">    &#123;</span><br><span class="line">        q-&gt;front = q-&gt;front-&gt;next;</span><br><span class="line">        q-&gt;front-&gt;data.p2--;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//停车场有车离开有空位时便道上的车辆按照到达顺序依次离开便道并进入停车场</span></span><br><span class="line"><span class="function">Car <span class="title">Out_LQueue</span><span class="params">(LQueue* q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Car x;</span><br><span class="line">QNode* p;</span><br><span class="line">p = q-&gt;front-&gt;next;</span><br><span class="line">q-&gt;front-&gt;next = p-&gt;next;</span><br><span class="line">x = p-&gt;data;</span><br><span class="line"><span class="keyword">delete</span> p;</span><br><span class="line"><span class="keyword">if</span> (q-&gt;front-&gt;next == <span class="literal">NULL</span>)<span class="comment">//只有一个元素时，出队后队空，此时还要修改队尾指针</span></span><br><span class="line">&#123;</span><br><span class="line">q-&gt;front-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">q-&gt;rear = q-&gt;front;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//检验停车场内是否有该车辆</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Car_if</span><span class="params">(SeqList* p, Car c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> t=p-&gt;top;</span><br><span class="line"><span class="keyword">while</span> (!<span class="built_in">Str_strcmp</span>(c.num,p-&gt;data[t].num))</span><br><span class="line">&#123;</span><br><span class="line">        t--;</span><br><span class="line"><span class="keyword">if</span>(t==<span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (t == <span class="number">-1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//汽车到达</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_In</span><span class="params">(SeqList* p, LQueue* q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Car c;</span><br><span class="line">    string num; <span class="comment">//临时储存车牌号</span></span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;请输入汽车的型号[small/mid/large]:\n&quot;</span>;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; Car_Type;</span><br><span class="line">c.type = Car_Type;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请输入汽车车牌号:\n&quot;</span>;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; num;</span><br><span class="line"><span class="keyword">while</span>(!<span class="built_in">checkNum</span>(num))  <span class="comment">//检验车牌号的格式,如果输入错误则需要重新输入</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-----------------------------------------------------------------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;| 注意：车牌号格式为普通车牌号格式【第一位为省份，第二位为发证机关A~H|J~N|P~Y，第三~七位为号码0~9|A~H|J~N|P~Z】 |&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-----------------------------------------------------------------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;请重新输入汽车车牌号:\n&quot;</span>;</span><br><span class="line">        cin &gt;&gt; num;</span><br><span class="line">    &#125;</span><br><span class="line">    c.num = num;</span><br><span class="line">    <span class="keyword">if</span>(p-&gt;top!=<span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">Car_if</span>(p,c))     <span class="comment">//判断车牌号是否重复</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;你输入的车牌号已经在车库当中了，请检查你的车牌号是否正确：\n&quot;</span>;</span><br><span class="line">            <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">            cin &gt;&gt; c.num;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请输入汽车的到达时间（格式：小时+空格+分钟）:\n&quot;</span>;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; c.t1.h &gt;&gt; c.t1.m;</span><br><span class="line"><span class="keyword">while</span>(!<span class="built_in">checkTime</span>(c.t1.h,c.t1.m))    <span class="comment">//判断输入的到达时间是否符合要求</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;---------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;| 注意：我们的时间要求是24小时的标准时间,注意中间的空格 |&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;---------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;请重新输入汽车的到达时间（格式：小时+空格+分钟）:\n&quot;</span>;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">        cin &gt;&gt; c.t1.h &gt;&gt; c.t1.m;</span><br><span class="line">    &#125;</span><br><span class="line">number++;   <span class="comment">//到此输入的信息+1</span></span><br><span class="line"><span class="keyword">if</span> (p-&gt;top == MAXSIZE - <span class="number">1</span>)  <span class="comment">//当停车场满时</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">//进入便道</span></span><br><span class="line">c.p2 = ++i;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;生意火爆，停车场已满，进入便道等待，位置为：00&quot;</span> &lt;&lt; c.p2 &lt;&lt; endl;</span><br><span class="line">number_LQueue++;    <span class="comment">//便道上等待的车辆+1</span></span><br><span class="line"><span class="built_in">In_LQueue</span>(q, c);</span><br><span class="line"><span class="built_in">LQueue_Input</span>(Car_Type,c);<span class="comment">//向便道文件写入车辆数据</span></span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">2000</span>);</span><br><span class="line">        <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">//进入停车场</span></span><br><span class="line">        <span class="type">int</span> flag = <span class="number">1</span>;   <span class="comment">//标记输入车位是否错误，输入正确置为0</span></span><br><span class="line">        <span class="type">int</span> arr[MAXSIZE];   <span class="comment">//临时记录停车场的空位</span></span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;停车场未满，正在进入停车场~&quot;</span> &lt;&lt;endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;当前空闲车位有:&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;MAXSIZE;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(Car_Space[j].flag == <span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot; &quot;</span>&lt;&lt; Car_Space[j].space;     <span class="comment">//输出停车场的空位</span></span><br><span class="line">                arr[j] = Car_Space[j].space;    <span class="comment">//将空位信息存到arr数组中，方便后续检查</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请选择车位~  &quot;</span>&lt;&lt;endl;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; c.p1;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;MAXSIZE;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(c.p1==arr[j] &amp;&amp; c.p1!=<span class="number">0</span>)  flag = <span class="number">0</span>;   <span class="comment">//判断输入车位是否空闲或者存在，符合要求，flag置为0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(flag)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;输入有误，请重新选择车位~  &quot;</span>&lt;&lt;endl;</span><br><span class="line">            <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">            cin &gt;&gt; c.p1;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;MAXSIZE;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(c.p1==arr[j] &amp;&amp; c.p1!=<span class="number">0</span>)  flag = <span class="number">0</span>;   <span class="comment">//判断输入车位是否空闲或者存在，符合要求，flag置为0</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;正在停车，位置为：00&quot;</span> &lt;&lt; c.p1 &lt;&lt; endl; <span class="comment">//输入正确进入停车场</span></span><br><span class="line"><span class="built_in">color</span>(<span class="number">3</span>);</span><br><span class="line"><span class="built_in">load_4</span>();   <span class="comment">//停车加载动画</span></span><br><span class="line">Car_Space[c.p1<span class="number">-1</span>].flag = <span class="number">0</span>; <span class="comment">//此位置已停车，flag置为0</span></span><br><span class="line">number_SeqList++;   <span class="comment">//停车场已经停的车辆+1</span></span><br><span class="line"><span class="built_in">Push_SeqList</span>(p, c);</span><br><span class="line"><span class="built_in">SeqList_Input</span>(Car_Type,c);<span class="comment">//向停车场文件写入车辆数据</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Car_Condition</span>(p,q); <span class="comment">//再进行下一次的输入判断</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//检查车辆的车牌号是否正确</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkNum</span><span class="params">(string S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">char</span> s[S.<span class="built_in">length</span>()];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;S.<span class="built_in">length</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        s[j] = S[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> n = S.<span class="built_in">length</span>(); <span class="comment">//车牌的长度</span></span><br><span class="line">    <span class="type">char</span> *ch; <span class="comment">//用来放车牌号的汉字</span></span><br><span class="line">    <span class="built_in">strncpy</span>(ch,s,<span class="number">2</span>);    <span class="comment">//截取字符串的前两个字符，汉字占两个字节</span></span><br><span class="line">    <span class="keyword">if</span>(n!=<span class="number">8</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>((<span class="built_in">strcmp</span>(ch,<span class="string">&quot;京&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;津&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;晋&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;冀&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;蒙&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;辽&quot;</span>)!=<span class="number">0</span>)&amp;&amp;</span><br><span class="line">       (<span class="built_in">strcmp</span>(ch,<span class="string">&quot;吉&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;黑&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;沪&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;苏&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;浙&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;皖&quot;</span>)!=<span class="number">0</span>)&amp;&amp;</span><br><span class="line">       (<span class="built_in">strcmp</span>(ch,<span class="string">&quot;闽&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;赣&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;鲁&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;豫&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;鄂&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;湘&quot;</span>)!=<span class="number">0</span>)&amp;&amp;</span><br><span class="line">       (<span class="built_in">strcmp</span>(ch,<span class="string">&quot;粤&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;桂&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;琼&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;渝&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;川&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;贵&quot;</span>)!=<span class="number">0</span>)&amp;&amp;</span><br><span class="line">       (<span class="built_in">strcmp</span>(ch,<span class="string">&quot;云&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;藏&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;陕&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;甘&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;青&quot;</span>)!=<span class="number">0</span>)&amp;&amp;(<span class="built_in">strcmp</span>(ch,<span class="string">&quot;宁&quot;</span>)!=<span class="number">0</span>)&amp;&amp;</span><br><span class="line">       (<span class="built_in">strcmp</span>(ch,<span class="string">&quot;新&quot;</span>)!=<span class="number">0</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(s[<span class="number">2</span>]&lt;<span class="string">&#x27;A&#x27;</span> || s[<span class="number">2</span>]&gt;<span class="string">&#x27;Z&#x27;</span> || s[<span class="number">2</span>]==<span class="string">&#x27;I&#x27;</span> || s[<span class="number">2</span>]==<span class="string">&#x27;O&#x27;</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(!(s[<span class="number">3</span>]&gt;=<span class="string">&#x27;0&#x27;</span> &amp;&amp; s[<span class="number">3</span>]&lt;=<span class="string">&#x27;9&#x27;</span>) &amp;&amp; !(s[<span class="number">3</span>]&gt;=<span class="string">&#x27;A&#x27;</span> &amp;&amp; s[<span class="number">3</span>]&lt;=<span class="string">&#x27;H&#x27;</span>) &amp;&amp; !(s[<span class="number">3</span>]&gt;=<span class="string">&#x27;J&#x27;</span> &amp;&amp; s[<span class="number">3</span>]&lt;=<span class="string">&#x27;N&#x27;</span>) &amp;&amp; !(s[<span class="number">3</span>]&gt;=<span class="string">&#x27;P&#x27;</span> &amp;&amp; s[<span class="number">3</span>]&lt;=<span class="string">&#x27;Z&#x27;</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(!(s[<span class="number">4</span>]&gt;=<span class="string">&#x27;0&#x27;</span> &amp;&amp; s[<span class="number">4</span>]&lt;=<span class="string">&#x27;9&#x27;</span>) &amp;&amp; !(s[<span class="number">4</span>]&gt;=<span class="string">&#x27;A&#x27;</span> &amp;&amp; s[<span class="number">4</span>]&lt;=<span class="string">&#x27;H&#x27;</span>) &amp;&amp; !(s[<span class="number">4</span>]&gt;=<span class="string">&#x27;J&#x27;</span> &amp;&amp; s[<span class="number">4</span>]&lt;=<span class="string">&#x27;N&#x27;</span>) &amp;&amp; !(s[<span class="number">4</span>]&gt;=<span class="string">&#x27;P&#x27;</span> &amp;&amp; s[<span class="number">4</span>]&lt;=<span class="string">&#x27;Z&#x27;</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(!(s[<span class="number">5</span>]&gt;=<span class="string">&#x27;0&#x27;</span> &amp;&amp; s[<span class="number">5</span>]&lt;=<span class="string">&#x27;9&#x27;</span>) &amp;&amp; !(s[<span class="number">5</span>]&gt;=<span class="string">&#x27;A&#x27;</span> &amp;&amp; s[<span class="number">5</span>]&lt;=<span class="string">&#x27;H&#x27;</span>) &amp;&amp; !(s[<span class="number">5</span>]&gt;=<span class="string">&#x27;J&#x27;</span> &amp;&amp; s[<span class="number">5</span>]&lt;=<span class="string">&#x27;N&#x27;</span>) &amp;&amp; !(s[<span class="number">5</span>]&gt;=<span class="string">&#x27;P&#x27;</span> &amp;&amp; s[<span class="number">5</span>]&lt;=<span class="string">&#x27;Z&#x27;</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span>(!(s[<span class="number">6</span>]&gt;=<span class="string">&#x27;0&#x27;</span> &amp;&amp; s[<span class="number">6</span>]&lt;=<span class="string">&#x27;9&#x27;</span>) &amp;&amp; !(s[<span class="number">6</span>]&gt;=<span class="string">&#x27;A&#x27;</span> &amp;&amp; s[<span class="number">6</span>]&lt;=<span class="string">&#x27;H&#x27;</span>) &amp;&amp; !(s[<span class="number">6</span>]&gt;=<span class="string">&#x27;J&#x27;</span> &amp;&amp; s[<span class="number">6</span>]&lt;=<span class="string">&#x27;N&#x27;</span>) &amp;&amp; !(s[<span class="number">6</span>]&gt;=<span class="string">&#x27;P&#x27;</span> &amp;&amp; s[<span class="number">6</span>]&lt;=<span class="string">&#x27;Z&#x27;</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//检查车辆的时间是否正确</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkTime</span><span class="params">(<span class="type">int</span> h,<span class="type">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">0</span>&lt;=h&amp;&amp;h&lt;<span class="number">24</span> &amp;&amp; <span class="number">0</span>&lt;=m&amp;&amp;m&lt;<span class="number">60</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//汽车离去</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_Out</span><span class="params">(SeqList* p, LQueue* q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Car c, x1, first, x2 , car;</span><br><span class="line"><span class="type">int</span> flag = <span class="number">0</span>;   <span class="comment">//标记作用：用来标记便道中是否有车【0表示没有，1表示有】</span></span><br><span class="line">string num;    <span class="comment">//临时存储汽车车牌号</span></span><br><span class="line"><span class="keyword">if</span> (p-&gt;top == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">//停车场无车</span></span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前停车场无车~&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p-&gt;top == MAXSIZE - <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">//当停车库满了的时候判断便道是否有车</span></span><br><span class="line"><span class="keyword">if</span> (q-&gt;front == q-&gt;rear) <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">first = <span class="built_in">Out_LQueue</span>(q);<span class="comment">//便道内第一辆车</span></span><br><span class="line">flag = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">SeqList* temp = <span class="built_in">Init_SeqList</span>();<span class="comment">//临时栈，保留有车离开让车时从停车场退出的车辆</span></span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请输入汽车车牌号:\n&quot;</span>;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; num;</span><br><span class="line"><span class="keyword">while</span>(!<span class="built_in">checkNum</span>(num))  <span class="comment">//检验车牌号的格式,如果输入错误则需要重新输入</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-----------------------------------------------------------------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;| 注意：车牌号格式为普通车牌号格式【第一位为省份，第二位为发证机关A~H|J~N|P~Y，第三~七位为号码0~9|A~H|J~N|P~Z】 |&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;-----------------------------------------------------------------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;请重新输入汽车车牌号:\n&quot;</span>;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">        cin &gt;&gt; num;</span><br><span class="line">    &#125;</span><br><span class="line">    c.num = num;</span><br><span class="line"><span class="comment">//判断停车场内是否有该车辆</span></span><br><span class="line"><span class="keyword">if</span>(!<span class="built_in">Car_if</span>(p,c))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;该停车场内没有该车辆~&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">Car_Condition</span>(p,q); <span class="comment">//再进行下一次的输入判断</span></span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请输入汽车的离开时间（格式：小时+空格+分钟）:\n&quot;</span>;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; c.t2.h &gt;&gt; c.t2.m;</span><br><span class="line"><span class="keyword">while</span>(!<span class="built_in">checkTime</span>(c.t2.h,c.t2.m))    <span class="comment">//判断输入的离开时间是否符合要求</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;---------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;| 注意：我们的时间要求是24小时的标准时间,注意中间的空格 |&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;---------------------------------------------------------&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;请重新输入汽车的到达时间（格式：小时+空格+分钟）:\n&quot;</span>;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">        cin &gt;&gt; c.t2.h &gt;&gt; c.t2.m;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> location = p-&gt;top;</span><br><span class="line">    car = <span class="built_in">Traverse_SeqList</span>(p,location,num);     <span class="comment">//得到目标车量</span></span><br><span class="line">    <span class="keyword">while</span>(!<span class="built_in">Check_TimeOut</span>(car.t1.h,c.t2.h,car.t1.m,c.t2.m))     <span class="comment">//判断目标车辆的离开时间是否合法</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//不合法需要重新输入</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;时间不合法，请重新输入:\n&quot;</span>;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">        cin &gt;&gt; c.t2.h &gt;&gt; c.t2.m;</span><br><span class="line">    &#125;</span><br><span class="line">    number++;   <span class="comment">//车辆离开信息输入成功，输入数据+1</span></span><br><span class="line"><span class="keyword">while</span> (c.num != p-&gt;data[p-&gt;top].num)</span><br><span class="line">&#123;</span><br><span class="line">x1 = <span class="built_in">Pop_SeqList</span>(p);</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;车牌号为&quot;</span> &lt;&lt; x1.num &lt;&lt; <span class="string">&quot;的车让路&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">Push_SeqList</span>(temp, x1);</span><br><span class="line">&#125;</span><br><span class="line">x1 = <span class="built_in">Pop_SeqList</span>(p);<span class="comment">//目标车辆,即准备开出的车辆</span></span><br><span class="line">x1.t2.h = c.t2.h, x1.t2.m = c.t2.m;</span><br><span class="line"><span class="type">int</span> t = (x1.t2.h - x1.t1.h) * <span class="number">60</span> + x1.t2.m - x1.t1.m; <span class="comment">//总停留分钟</span></span><br><span class="line"><span class="type">int</span> t1, t2;</span><br><span class="line"><span class="keyword">if</span> (x1.t2.m &gt;= x1.t1.m)</span><br><span class="line">&#123;</span><br><span class="line">t2 = x1.t2.m - x1.t1.m;</span><br><span class="line">t1 = x1.t2.h - x1.t1.h;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">t2 = x1.t2.m + <span class="number">60</span> - x1.t1.m;</span><br><span class="line">t1 = x1.t2.h - x1.t1.h - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;车牌号为&quot;</span> &lt;&lt; x1.num &lt;&lt; <span class="string">&quot;的车开出&quot;</span> &lt;&lt; endl;</span><br><span class="line">Car_Space[x1.p1<span class="number">-1</span>].flag = <span class="number">1</span>;    <span class="comment">//车辆离开，停车场的车位空出，flag置为1。</span></span><br><span class="line">number_SeqList--;   <span class="comment">//停车场的车辆数目减少</span></span><br><span class="line"><span class="built_in">SeqList_Delete</span>(x1);<span class="comment">//停车场文件删除此车辆</span></span><br><span class="line"><span class="comment">//刚才让出的车辆按照原来的顺序依次开入停车场</span></span><br><span class="line"><span class="keyword">while</span> (temp-&gt;top != <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">x2 = <span class="built_in">Pop_SeqList</span>(temp);</span><br><span class="line"><span class="comment">//x.p1 = p-&gt;top + 2;</span></span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;车牌号为&quot;</span> &lt;&lt; x2.num &lt;&lt; <span class="string">&quot;的车开入&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">Push_SeqList</span>(p, x2);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//若便道有车则让便道上第一辆车进入停车场</span></span><br><span class="line"><span class="keyword">if</span> (flag)</span><br><span class="line">&#123;</span><br><span class="line">first.p1 = x1.p1;    <span class="comment">//将离开的车位给便道上开入的汽车</span></span><br><span class="line">Car_Space[x1.p1<span class="number">-1</span>].flag = <span class="number">0</span>;    <span class="comment">//便道的上车开入，停车场的车位被占用，flag置为0</span></span><br><span class="line">first.t1.h = x1.t2.h;    <span class="comment">//更新便道上开入的汽车的开始停车的时间【便道上停车的时间不收费】</span></span><br><span class="line">first.t1.m = x1.t2.m;</span><br><span class="line"><span class="built_in">Push_SeqList</span>(p, first);</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;车牌号为&quot;</span> &lt;&lt; first.num &lt;&lt; <span class="string">&quot;的车开入&quot;</span> &lt;&lt; endl;</span><br><span class="line">i--;    <span class="comment">//对便道上的车位标签-1</span></span><br><span class="line"><span class="built_in">Traverse_LQueue</span>(q);     <span class="comment">//便道第一辆车进入停车场后，对后面车辆前移</span></span><br><span class="line">number_SeqList++;       <span class="comment">//便道上的车开入停车场，停车场的车辆数目+1</span></span><br><span class="line">number_LQueue--;        <span class="comment">//便道上的车开入停车场，便道上的车辆数目-1</span></span><br><span class="line"><span class="built_in">LQueue_Delete</span>(first);<span class="comment">//删除便道文件的第一辆车</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//计算车费</span></span><br><span class="line">Car_Type = x1.type;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line"><span class="keyword">if</span> (!Car_Type.<span class="built_in">compare</span>(<span class="string">&quot;large&quot;</span>))</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;您的车辆型号符合大车型，我们对你进行0.2￥/分钟的收费标准进行收费&quot;</span>&lt;&lt;endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;停留时间：&quot;</span> &lt;&lt; t1 &lt;&lt; <span class="string">&quot;小时&quot;</span> &lt;&lt; t2 &lt;&lt; <span class="string">&quot;分钟&quot;</span> &lt;&lt; <span class="string">&quot;    收费：&quot;</span> &lt;&lt; t * price_1 &lt;&lt; <span class="string">&quot;元\n&quot;</span> &lt;&lt; <span class="string">&quot;欢迎下次光临~&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!Car_Type.<span class="built_in">compare</span>(<span class="string">&quot;mid&quot;</span>))</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;您的车辆型号符合中车型，我们对你进行0.1￥/分钟的收费标准进行收费&quot;</span>&lt;&lt;endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;停留时间：&quot;</span> &lt;&lt; t1 &lt;&lt; <span class="string">&quot;小时&quot;</span> &lt;&lt; t2 &lt;&lt; <span class="string">&quot;分钟&quot;</span> &lt;&lt; <span class="string">&quot;    收费：&quot;</span> &lt;&lt; t * price_2 &lt;&lt; <span class="string">&quot;元\n&quot;</span> &lt;&lt; <span class="string">&quot;欢迎下次光临~&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!Car_Type.<span class="built_in">compare</span>(<span class="string">&quot;small&quot;</span>))</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;您的车辆型号符合大车型，我们对你进行0.05￥/分钟的收费标准进行收费&quot;</span>&lt;&lt;endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;停留时间：&quot;</span> &lt;&lt; t1 &lt;&lt; <span class="string">&quot;小时&quot;</span> &lt;&lt; t2 &lt;&lt; <span class="string">&quot;分钟&quot;</span> &lt;&lt; <span class="string">&quot;    收费：&quot;</span> &lt;&lt; t * price_3 &lt;&lt; <span class="string">&quot;元\n&quot;</span> &lt;&lt; <span class="string">&quot;欢迎下次光临~&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;您的车型不符合我们的规范我们将对你按0.5￥/分钟的收费标准进行收费&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;停留时间：&quot;</span> &lt;&lt; t1 &lt;&lt; <span class="string">&quot;小时&quot;</span> &lt;&lt; t2 &lt;&lt; <span class="string">&quot;分钟&quot;</span> &lt;&lt; <span class="string">&quot;    收费：&quot;</span> &lt;&lt; t * <span class="number">0.5</span> &lt;&lt; <span class="string">&quot;元\n&quot;</span> &lt;&lt; <span class="string">&quot;建议换个停车场！&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">500</span>);</span><br><span class="line"><span class="built_in">Car_Condition</span>(p,q); <span class="comment">//再进行下一次的输入判断</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//显示停车场内车辆信息</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Display</span><span class="params">(SeqList* p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Car x;</span><br><span class="line">SeqList* temp = <span class="built_in">Init_SeqList</span>();</span><br><span class="line"><span class="keyword">if</span> (p-&gt;top == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前停车场无车，生意惨淡TvT~&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">2000</span>);</span><br><span class="line">        <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">while</span> (p-&gt;top != <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">x = <span class="built_in">Pop_SeqList</span>(p);</span><br><span class="line"><span class="built_in">Push_SeqList</span>(temp, x);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前停放的车辆信息:&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">while</span> (temp-&gt;top != <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">x = <span class="built_in">Pop_SeqList</span>(temp);</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;停车位置为：00&quot;</span> &lt;&lt; x.p1 &lt;&lt; <span class="string">&quot;  车牌号码:&quot;</span> &lt;&lt; x.num &lt;&lt; <span class="string">&quot;  停车时间：&quot;</span> &lt;&lt; x.t1.h &lt;&lt; <span class="string">&quot;点&quot;</span> &lt;&lt; x.t1.m &lt;&lt; <span class="string">&quot;分&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">Push_SeqList</span>(p, x);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Sleep</span>(<span class="number">4000</span>);</span><br><span class="line"><span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取车辆进入/离开停车场的信息</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Car_Condition</span><span class="params">(SeqList* p, LQueue* q)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">string x;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">9</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前已经输入&quot;</span>&lt;&lt;number&lt;&lt;<span class="string">&quot;组数据&quot;</span> &lt;&lt; <span class="string">&quot;这是第&quot;</span>&lt;&lt;number+<span class="number">1</span>&lt;&lt;<span class="string">&quot;次输入信息&quot;</span>&lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前停车场已停有&quot;</span>&lt;&lt;number_SeqList&lt;&lt;<span class="string">&quot;辆车&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;当前便道上已有&quot;</span>&lt;&lt;number_LQueue&lt;&lt;<span class="string">&quot;辆车在等待&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;请输入车辆进入或离开的信息[A表示进入停车场/D表示离开停车场/E表示终止输入]:\n&quot;</span>;\</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; x;</span><br><span class="line"><span class="keyword">if</span>(!x.<span class="built_in">compare</span>(<span class="string">&quot;A&quot;</span>))</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">Car_In</span>(p, q);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(!x.<span class="built_in">compare</span>(<span class="string">&quot;D&quot;</span>))</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">Car_Out</span>(p, q);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(!x.<span class="built_in">compare</span>(<span class="string">&quot;E&quot;</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;输入终止&quot;</span> &lt;&lt;endl;</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">1000</span>);</span><br><span class="line">        <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;输入信息有误，请重新输入！！！&quot;</span>&lt;&lt;endl;</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">300</span>);</span><br><span class="line">        <span class="built_in">Car_Condition</span>(p,q);     <span class="comment">//递归调用Car_Condition函数，重新输入信息</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//修改字体颜色</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">color</span><span class="params">(<span class="type">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">SetConsoleTextAttribute</span>(<span class="built_in">GetStdHandle</span>(STD_OUTPUT_HANDLE),m|<span class="number">8</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//程序刚开始运行时的加载过程</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">menu_load</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">color</span>(<span class="number">3</span>);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;\n\n\n\t\t\t\t================欢迎使用停车场===================&quot;</span> &lt;&lt; endl;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;\t\t\t\t*\t\t\t\t\t\t&quot;</span>&lt;&lt;<span class="string">&quot;*&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;\t\t\t\t*\t  当前正在加载，请耐心等待\t\t&quot;</span>&lt;&lt;<span class="string">&quot;*&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;\t\t\t\t*\t\t\t\t\t\t&quot;</span>&lt;&lt;<span class="string">&quot;*&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;\t\t\t\t=================================================&quot;</span> &lt;&lt; endl&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">500</span>);</span><br><span class="line">    <span class="built_in">load_3</span>();</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">1000</span>);</span><br><span class="line">    <span class="built_in">load_1</span>();</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">500</span>);</span><br><span class="line">    <span class="built_in">menu</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//系统主菜单页面</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">menu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> flag = <span class="number">1</span>;</span><br><span class="line">    SeqList* p = <span class="built_in">Init_SeqList</span>();</span><br><span class="line">LQueue* q = <span class="built_in">Init_LQueue</span>();</span><br><span class="line">    <span class="keyword">while</span> (flag)</span><br><span class="line">&#123;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">3</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\n\n\n\t\t\t=========================欢迎使用停车场===========================&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t\t\t\t\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t   本停车场最多可停放&quot;</span> &lt;&lt; MAXSIZE &lt;&lt; <span class="string">&quot;辆汽车&quot;</span> &lt;&lt; <span class="string">&quot;\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t\t\t\t\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||   本停车场可停大[large]、中[mid]、小[small]三种型号的车辆&quot;</span> &lt;&lt; <span class="string">&quot;    ||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t\t\t\t\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||  收费标准：大[0.2￥/分钟]，大[0.1￥/分钟]，小[0.05￥/分钟]&quot;</span> &lt;&lt; <span class="string">&quot;   ||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t\t\t\t\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t   如果您不幸不符合我们的要求，按每分钟0.5￥收费&quot;</span>&lt;&lt; <span class="string">&quot;\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t||\t\t\t\t\t\t\t\t||&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t==================================================================&quot;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;\t\t\t    **********1.输入汽车信息   2.查看车库  3.退出系统**********&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="built_in">color</span>(<span class="number">4</span>);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;请进行选择[输入序号即可]:\n&quot;</span>;</span><br><span class="line"><span class="type">char</span> i;</span><br><span class="line"><span class="built_in">color</span>(<span class="number">7</span>);</span><br><span class="line">cin &gt;&gt; i;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">getchar</span>() != <span class="string">&#x27;\n&#x27;</span>);<span class="comment">//清除缓冲区</span></span><br><span class="line"><span class="keyword">switch</span> (i)</span><br><span class="line">&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&#x27;1&#x27;</span>:number=<span class="number">0</span>;<span class="built_in">Car_Condition</span>(p, q); <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&#x27;2&#x27;</span>:<span class="built_in">Display</span>(p); <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&#x27;3&#x27;</span>:flag = <span class="number">0</span>; <span class="built_in">color</span>(<span class="number">3</span>);<span class="built_in">load_2</span>(); <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:cout &lt;&lt; <span class="string">&quot;输入错误啦~请重新输入数字1--3~&quot;</span> &lt;&lt; endl; <span class="built_in">Sleep</span>(<span class="number">1000</span>); <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>); <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//加载百分比可视化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_1</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\n请稍后...... &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">100</span>; i++)  <span class="comment">// 打印百分比</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cout.<span class="built_in">width</span>(<span class="number">3</span>);  <span class="comment">// i的输出为3位宽</span></span><br><span class="line">        cout &lt;&lt; i &lt;&lt; <span class="string">&quot;%&quot;</span>;</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">15</span>);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;\b\b\b\b&quot;</span>;  <span class="comment">//回删三个字符，使数字在原地变化</span></span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\t\t\t\t\t\t\t\t&quot;</span> &lt;&lt; <span class="string">&quot;\n\n&quot;</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;加载完成！&quot;</span>;</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">1000</span>);</span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//系统退出百分比可视化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_2</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\n请稍后...... &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">100</span>; i++)  <span class="comment">// 打印百分比</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cout.<span class="built_in">width</span>(<span class="number">3</span>);  <span class="comment">// i的输出为3位宽</span></span><br><span class="line">        cout &lt;&lt; i &lt;&lt; <span class="string">&quot;%&quot;</span>;</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">15</span>);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;\b\b\b\b&quot;</span>;  <span class="comment">//回删三个字符，使数字在原地变化</span></span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\t\t\t\t\t\t\t\t&quot;</span> &lt;&lt; <span class="string">&quot;\n\n&quot;</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;系统退出成功！&quot;</span>;</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">1000</span>);</span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//进入系统加载界面</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_3</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string reading=<span class="string">&quot;正在为您进入系统~~&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;reading.<span class="built_in">length</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;reading[i];</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//停车加载百分比可视化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_4</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\n请稍后...... &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">100</span>; i++)  <span class="comment">// 打印百分比</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cout.<span class="built_in">width</span>(<span class="number">3</span>);  <span class="comment">// i的输出为3位宽</span></span><br><span class="line">        cout &lt;&lt; i &lt;&lt; <span class="string">&quot;%&quot;</span>;</span><br><span class="line">        <span class="built_in">Sleep</span>(<span class="number">15</span>);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;\b\b\b\b&quot;</span>;  <span class="comment">//回删三个字符，使数字在原地变化</span></span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\t\t\t\t\t\t\t\t&quot;</span> &lt;&lt; <span class="string">&quot;\n\n&quot;</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;停车成功！&quot;</span>;</span><br><span class="line">    <span class="built_in">Sleep</span>(<span class="number">2000</span>);</span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;cls&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//检查汽车离开出栈的时间是否合法</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Check_TimeOut</span><span class="params">(<span class="type">int</span> h1,<span class="type">int</span> h2,<span class="type">int</span> m1,<span class="type">int</span> m2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(((h2*<span class="number">60</span>+m2)-(h1*<span class="number">60</span>+m1))&gt;<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈的遍历用来寻找出栈的车辆</span></span><br><span class="line"><span class="function">Car <span class="title">Traverse_SeqList</span><span class="params">(SeqList *p,<span class="type">int</span> top,string num)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(num!=p-&gt;data[top].num)</span><br><span class="line">    &#123;</span><br><span class="line">        top--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p-&gt;data[top];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//字符串比较函数</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Str_strcmp</span><span class="params">(string str1,string str2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!str1.<span class="built_in">compare</span>(str2)) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>开源地址：<a href="https://github.com/xiaoyutoua/Parking-lot-management-simulation-system">https://github.com/xiaoyutoua/Parking-lot-management-simulation-system</a></p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构实训 </tag>
            
            <tag> 算法 </tag>
            
            <tag> C/C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客备份与恢复</title>
      <link href="/2023/01/06/Hexo%E5%8D%9A%E5%AE%A2%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"/>
      <url>/2023/01/06/Hexo%E5%8D%9A%E5%AE%A2%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<div class="tip key"><p>本文旨在解决在不同电脑上都能维护博客或配置、发布的内容丢失可恢复的问题。</p></div><details class="folding-tag" ><summary> 参考博客© </summary>              <div class='content'>              <p>本文参考博客如下，因为备份对博客来说比较重要，所以记录下来，防止原作者删除。</p><div class="tag link"><a class="link-card" title="Mupceet" href="https://mupceet.com/2019/09/backup-hexo-blog/"><div class="left"><img src="https://mupceet.com/images/avatar_me.png"/></div><div class="right"><p class="text">Mupceet</p><p class="url">https://mupceet.com/2019/09/backup-hexo-blog/</p></div></a></div>              </div>            </details><p>观察部署到仓库的内容，我们可以看到上传的内容是 <code>public</code> 文件夹下的所有内容。事实上 <code>hexo-deploy-git</code> 插件是通过拷贝 <code>public</code> 文件夹内容到 <code>.deploy_git</code> 文件夹下，然后提交推送到远程分支上实现了网站文件的部署。</p><p>那我们的备份思路也可以这样，上传目录下的其他所有文件就可以了，同时我们不能修改博客的发布分支，因此考虑备份其他所有文件到新分支中。</p><p>最简单直接的方法就是在仓库创建一个新的分支，把本地所有的内容都上传到该分支上。但这样会备份一部分不必要的文件，例如 <code>public</code> 文件夹内容，它可以再次生成，就没有必要备份。</p><p>那具体要备份哪些文件呢？</p><h1 id="cd-备份"><a href="#cd-备份" class="headerlink" title=":cd:备份"></a>:cd:备份</h1><h2 id="1、备份的文件列表"><a href="#1、备份的文件列表" class="headerlink" title="1、备份的文件列表"></a>1、备份的文件列表</h2><p>我们先看下，现在博客文件夹都有什么内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── db.json</span><br><span class="line">├── node_modules</span><br><span class="line">├── package.json</span><br><span class="line">├── package-lock.json</span><br><span class="line">├── public</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">└── themes</span><br><span class="line">    ├── next</span><br><span class="line">    └── landscape</span><br></pre></td></tr></table></figure><p>这几个文件或文件夹的内容分别是：</p><ol><li><code>_config.yml</code> 文件：站点配置文件，很多功能、插件需要修改该配置文件应用生效。</li><li><code>node_modules</code> 文件夹：包含依赖的模块。</li><li><code>package.json</code> 文件：依赖的模块列表。说明见：<a href="https://www.npmjs.cn/files/package.json/">package.json:Specifics of npm’s package.json handling</a></li><li><code>package-lock.json</code> 文件：依赖的模块安装记录。说明见：<a href="https://www.npmjs.cn/files/package-locks/">npm-package-locks:An explanation of npm lockfiles</a></li><li><code>public</code> 文件夹：包含生成的网页静态文件。</li><li><code>scaffolds</code> 文件夹：包含创建的文章、分类、标签界面的模板。博客的定制修改会对模板进行修改。</li><li><code>source</code> 文件夹：包含生成网页所需要的源文件，包括包含我们心血的 Markdown 文稿，这也是最重要的内容。</li><li><code>themes</code> 文件夹：其中 <code>landscape</code> 是默认的主题，其他文件夹是克隆下来时的主题。</li></ol><p>我们可以参考 Hexo 初始化使用的仓库的备份列表，它的仓库是 <a href="https://github.com/hexojs/hexo-starter">hexojs/hexo-starter</a>。我们看下它备份了哪些内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scaffolds</span><br><span class="line">source</span><br><span class="line">themes</span><br><span class="line">.gitignore</span><br><span class="line">.gitmodules</span><br><span class="line">_config.yml</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><p>比对一下，它抛弃了：</p><ol><li><code>node_modules</code> &amp; <code>package-lock.json</code>：这两部分内容，只要保留 <code>package.json</code>，执行 <code>npm install</code> 就可以下载、生成。</li><li><code>public</code>：执行 <code>hexo g</code> 即可根据源文件生成网页内容。</li></ol><p>这些可重新生成的文件都可以不上传，因为它们只要使用特定的命令操作即可恢复。</p><p>它增加了 <code>.gitmodules</code>，那它的作用又是什么呢？其实 <code>hexojs/hexo-starter</code> 是通过 Git 的 Submodule 功能来下载主题模块，本身仓库并不备份主题文件。考虑下我们需要如何备份主题文件目录，有两个方案：</p><ol><li>一个方案是将其内容全部上传进行备份，这样可以保证原主题的更新不会影响你原先配置的效果。</li><li>另一个方案是像 <code>hexo-starter</code> 仓库一样通过 Git 的 Submodule 功能来管理，这样可以进行主题的更新。</li></ol><p>这里我选择和 Hexo 初始化仓库一样使用 Git 子模块的方式进行主题的备份处理。不过通过子模块管理的方式我们恢复时仅会同步到 Next 主题的原文件，没法直接同步我们对主题配置文件或其他文件的修改，因为我们没有权限提交修改到 Next 仓库中。因此我创建了了个 <code>themes_custom</code> 文件夹来存放对应主题的修改的备份，这样子我们同步之后，只需要对比一下内容手动把这些配置应用过去就可以快速完成对主题的配置。</p><p>最终备份文件列表如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scaffolds</span><br><span class="line">source</span><br><span class="line">themes</span><br><span class="line">themes_custom/next</span><br><span class="line">.gitignore</span><br><span class="line">.gitmodules</span><br><span class="line">_config.yml</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><h2 id="2、具体操作"><a href="#2、具体操作" class="headerlink" title="2、具体操作"></a>2、具体操作</h2><p>有了方案之后，我们备份的具体操作如下：</p><ol><li><p>先修改 <code>.gitignore</code> 文件，查看之后由于原文件已经忽略了 <code>public</code> 和 <code>node_modules</code> 文件夹，因此仅需要添加 <code>package-lock.json</code> 到忽略清单中。</p></li><li><p>我们可以删除不使用的主题 <code>landscape</code> 或者把主题路径添加到忽略列表中。</p></li><li><p>创建 <code>themes_custom/next</code> 文件夹，将对主题进行的配件修改的文件拷贝一份到这里</p></li><li><p>执行以下命令，在本地创建备份仓库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> blog</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git init</span></span><br><span class="line">已初始化空的 Git 仓库于 blog/.git/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git submodule add https://github.com/theme-next/hexo-theme-next.git themes/next</span></span><br><span class="line">添加位于 &#x27;themes/next&#x27; 的现存仓库到索引</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git add .</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git commit -m <span class="string">&quot;init blog backup&quot;</span></span></span><br></pre></td></tr></table></figure></li><li><p>将备份内容 push 到远程仓库的备份分支 <code>hexo</code> 上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git branch -m master hexo</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git remote add origin https://github.com/Mupceet/mupceet.github.io.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git push -u origin hexo:hexo</span></span><br></pre></td></tr></table></figure><p>这里，我对分支进行了重命名以减少记忆负担。</p></li></ol><p>经过以上步骤，我们就备份了所有必要的文件。后续更新博客也要及时地将这些文件进行提交并上传完成备份。</p><h1 id="dvd-恢复"><a href="#dvd-恢复" class="headerlink" title=":dvd:恢复"></a>:dvd:恢复</h1><p>有了备份之后，在另一台电脑上创建博客，或者是恢复备份时，就可以直接使用我们备份的内容进行操作。</p><ol><li><p>环境准备</p><p>具体见<a href="https://www.fomal.cc/posts/e593433d.html">Hexo博客搭建基础教程(一) | Fomalhaut🥝</a></p></li><li><p>克隆备份的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> --recursive -b hexo https://github.com/Mupceet/mupceet.github.io.git blog</span></span><br></pre></td></tr></table></figure></li><li><p>下载 npm 依赖模块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> blog</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">npm install</span></span><br></pre></td></tr></table></figure></li><li><p>恢复主题配置</p><p>将 <code>themes_custom</code> 文件夹中对主题的配置的修改恢复到对应的主题文件夹中，这里建议使用对比的方式对其进行修改，而不是直接覆盖，这样就完成了主题的配置。</p></li><li><p>克隆原博客内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> blog</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/Mupceet/mupceet.github.io.git .deploy_git</span></span><br></pre></td></tr></table></figure></li><li><p>正常更新博客</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">hexo g</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> hexo s</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">hexo d</span></span><br></pre></td></tr></table></figure></li></ol><p>可以看到，使用备份文件恢复博客的环境是非常简单的，强烈建议大家搭建好博客之后增加一个备份操作。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客备份与恢复 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习7.2-基于U-Net的KITTI道路分割</title>
      <link href="/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-2-%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84KITTI%E9%81%93%E8%B7%AF%E5%88%86%E5%89%B2/"/>
      <url>/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-2-%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84KITTI%E9%81%93%E8%B7%AF%E5%88%86%E5%89%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="基于U-Net的KITTI道路分割"><a href="#基于U-Net的KITTI道路分割" class="headerlink" title="基于U-Net的KITTI道路分割"></a>基于U-Net的KITTI道路分割</h1><p><a href="https://www.cvlibs.net/datasets/kitti/eval_odometry.php">KITTI数据集</a>是一个著名的自动驾驶数据集。它包括一批图片，和语义分割、实例分割等多种任务的标签。下图展示了KITTI数据集的典型样本，分为 ’Road’, ’City’, ’Residential’, ’Campus’ 和’Person’五类。原始数据采集于2011年的5天，共有180GB数据。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f6e515987621426a8acdc6f67baefe8376044e25e03347159d44d3e74adcd73d" width = "600"  div align=center" width = "800"></center><center><br>图1：KITTI数据集样本</br></center><p>KITTI数据集由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合创办，是目前国际上最大的自动驾驶场景下的计算机视觉算法评测数据集。该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。KITTI包含市区、乡村和高速公路等场景采集的真实图像数据，每张图像中最多达15辆车和30个行人，还有各种程度的遮挡与截断。</p><h2 id="一、数据处理"><a href="#一、数据处理" class="headerlink" title="一、数据处理"></a>一、数据处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="keyword">as</span> PilImage</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><h3 id="1）读入数据"><a href="#1）读入数据" class="headerlink" title="1）读入数据"></a>1）读入数据</h3><p>下面分别将图像分割所需的原始图像与标签图像读入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压数据集</span></span><br><span class="line">!unzip -q -d ./work/data_kitti /home/aistudio/data/data170768/data_kitti.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据解压完成&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>数据解压完成</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取一个训练图片和对应的标签图像展示一下</span></span><br><span class="line">image = np.array(PilImage.<span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/training/image_2/000163_10.png&#x27;</span>))</span><br><span class="line">label = np.array(PilImage.<span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/training/semantic_road/000163_10.png&#x27;</span>))</span><br><span class="line">rgb_ = np.array(PilImage.<span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/training/semantic_rgb/000163_10.png&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># subplot 在同一画面中创建和控制多个图形位置</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.imshow(label, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.imshow(rgb_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/79.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_images_path = <span class="string">&quot;work/data_kitti/training/image_2/&quot;</span></span><br><span class="line">label_images_path = <span class="string">&quot;work/data_kitti/training/semantic_road/&quot;</span></span><br><span class="line">image_count = <span class="built_in">len</span>([</span><br><span class="line">                    os.path.join(train_images_path, image_name)</span><br><span class="line">                    <span class="keyword">for</span> image_name <span class="keyword">in</span> os.listdir(train_images_path) <span class="comment"># listdir 访问路径下所有文件</span></span><br><span class="line">                    <span class="keyword">if</span> image_name.endswith(<span class="string">&#x27;.png&#x27;</span>) <span class="comment"># endswith 返回后缀</span></span><br><span class="line">                    ])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像样本的总数量是:&quot;</span>, image_count)</span><br></pre></td></tr></table></figure><pre><code>图像样本的总数量是: 200</code></pre><h3 id="2）划分数据集"><a href="#2）划分数据集" class="headerlink" title="2）划分数据集"></a>2）划分数据集</h3><p>对数据集进行处理，划分训练集、测试集。</p><p>我们第一步是对原始的数据集进行整理，得到<strong>数据集和标签两个数组，分别一一对应</strong>，在这里是用了一个非常简单的方法，按照文件名称进行排序，这样可以在使用的时候能够很方便的找到原始数据和标签的对应关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_sort_images</span>(<span class="params">image_dir, image_type</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对文件夹内的图像进行按照文件名排序</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    files = []</span><br><span class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> os.listdir(image_dir):</span><br><span class="line">        <span class="keyword">if</span> image_name.endswith(<span class="string">&#x27;.&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(image_type)) <span class="keyword">and</span> <span class="keyword">not</span> image_name.startswith(<span class="string">&#x27;.&#x27;</span>):</span><br><span class="line">            files.append(os.path.join(image_dir, image_name))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(files)</span><br><span class="line"></span><br><span class="line">images = _sort_images(train_images_path, <span class="string">&#x27;png&#x27;</span>)</span><br><span class="line">labels = _sort_images(label_images_path, <span class="string">&#x27;png&#x27;</span>)</span><br></pre></td></tr></table></figure><p>接着，我们将图片和标签一一对应写入文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eval_num = <span class="built_in">int</span>(image_count * <span class="number">0.15</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_file</span>(<span class="params">mode, images, labels</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/&#123;&#125;.txt&#x27;</span>.<span class="built_in">format</span>(mode), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(images)):</span><br><span class="line">            f.write(<span class="string">&#x27;&#123;&#125;\t&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(images[i], labels[i]))</span><br><span class="line"></span><br><span class="line">write_file(<span class="string">&#x27;train&#x27;</span>, images[:-eval_num], labels[:-eval_num])</span><br><span class="line">write_file(<span class="string">&#x27;test&#x27;</span>, images[-eval_num:], labels[-eval_num:])</span><br><span class="line">write_file(<span class="string">&#x27;predict&#x27;</span>, images[-eval_num:], labels[-eval_num:])</span><br></pre></td></tr></table></figure><p>我们通过继承高级API提供的Dataset类，自定义了一个分割数据集类PetDataset。 </p><ul><li>通过实现_load_img函数，使用统一的图像处理接口封装图像处理策略，用于规整图像大小和通道；</li><li>通过实现<strong>getitem</strong>函数，分别读入训练图片和标签，同时对输入图像的RGB三个通道的值分别做标准化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PetDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    数据集定义</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.image_size = IMAGE_SIZE</span><br><span class="line">        self.mode = mode.lower()</span><br><span class="line">        <span class="keyword">assert</span> self.mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;predict&#x27;</span>], \</span><br><span class="line">        <span class="string">&quot;mode should be &#x27;train&#x27; or &#x27;test&#x27; or &#x27;predict&#x27;, but got &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.mode)</span><br><span class="line">        self.train_images = []</span><br><span class="line">        self.label_images = []</span><br><span class="line">        <span class="comment"># 读入训练图片和训练标签</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/&#123;&#125;.txt&#x27;</span>.<span class="built_in">format</span>(self.mode), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                image, label = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">                self.train_images.append(image)</span><br><span class="line">                self.label_images.append(label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_img</span>(<span class="params">self, path, color_mode=<span class="string">&#x27;rgb&#x27;</span>, transforms=[]</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        统一的图像处理接口封装，用于规整图像大小和通道</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            img = PilImage.<span class="built_in">open</span>(io.BytesIO(f.read()))</span><br><span class="line">            <span class="comment"># img.convert 用以指定一种色彩模式</span></span><br><span class="line">            <span class="keyword">if</span> color_mode == <span class="string">&#x27;grayscale&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> img.mode <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;I;16&#x27;</span>, <span class="string">&#x27;I&#x27;</span>):</span><br><span class="line">                    img = img.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> color_mode == <span class="string">&#x27;rgba&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> img.mode != <span class="string">&#x27;RGBA&#x27;</span>:</span><br><span class="line">                    img = img.convert(<span class="string">&#x27;RGBA&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> color_mode == <span class="string">&#x27;rgb&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> img.mode != <span class="string">&#x27;RGB&#x27;</span>:</span><br><span class="line">                    img = img.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&#x27;color_mode must be &quot;grayscale&quot;, &quot;rgb&quot;, or &quot;rgba&quot;&#x27;</span>)</span><br><span class="line">            <span class="comment"># 返回组合的图像处理策略</span></span><br><span class="line">            <span class="keyword">return</span> T.Compose([T.Resize(self.image_size)] + transforms)(img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        返回 image, label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        train_image = self._load_img(</span><br><span class="line">                            self.train_images[idx], </span><br><span class="line">                            transforms=[T.Transpose(), T.Normalize(mean=<span class="number">127.5</span>, std=<span class="number">127.5</span>)]</span><br><span class="line">                            ) <span class="comment"># 调整维度，RGB标准化</span></span><br><span class="line">        label_image = self._load_img(</span><br><span class="line">                            self.label_images[idx], </span><br><span class="line">                            color_mode=<span class="string">&#x27;grayscale&#x27;</span>,</span><br><span class="line">                            transforms=[T.Grayscale()]</span><br><span class="line">                            ) <span class="comment"># 指定一种色彩模式，将图像转换为灰度</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 返回image, label</span></span><br><span class="line">        train_image = np.array(train_image, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        label_image = np.array(label_image, dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(label_image)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_image, label_image</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        返回数据集总数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.train_images)</span><br></pre></td></tr></table></figure><h2 id="二、U-Net"><a href="#二、U-Net" class="headerlink" title="二、U-Net"></a>二、U-Net</h2><p>U-Net提出的初衷是为了解决医学图像分割的问题，它以一种U型的网络结构来获取上下文的信息和位置信息。<br>U-Net可以说是最常用、最简单的一种分割模型了，它简单、高效、易懂、容易构建、可以从小数据集中训练。在U-Net之前，则是更老的FCN（全卷积神经网络）网络，不过FCN网络的准确度较低，不比U-Net好用。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/82715fe9c1f242a08b16439f910c188bd5a1cf845d0847579635075d606eddc4" width = "600"  div align=center" width = "800"></center><center><br>图2：U-Net</br></center><p>我们可以整理一下U-Net网络的结构：</p><ol><li>先对图片进行卷积和池化，在U-Net论文中是池化4次，比方说一开始的图片是256x256的，那么就会变成 128x128，64x64，32x32，16x16 四个不同尺寸的特征；</li><li>然后我们对16x16的特征图做上采样或者反卷积，得到32x32的特征图，这个32x32的特征图与之前的32x32的特征图进行通道上的拼接concat；</li><li>然后再对拼接之后的特征图做卷积和上采样，得到64x64的特征图，再与之前的64x64的特征拼接，卷积，再上采样；</li><li>经过四次上采样可以得到一个与输入图像尺寸相同的256x256的预测结果。</li></ol><p>U-Net有效的缓解了FCN带来的问题：通过反卷积得到的更大的尺寸的特征图的边缘，是缺少信息的，毕竟每一次下采样提炼特征的同时，也必然会损失一些边缘特征，而失去的特征并不能从上采样中找回，因此通过特征的拼接，来实现边缘特征的一个找回。</p><h3 id="1）建立模型"><a href="#1）建立模型" class="headerlink" title="1）建立模型"></a><strong>1）建立模型</strong></h3><p>接下来我们定义训练数据所需要的网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">paddle.set_device(<span class="string">&#x27;gpu&#x27;</span>)</span><br><span class="line">paddle.__version__</span><br></pre></td></tr></table></figure><pre><code>&#39;2.3.2&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleConv</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    两次卷积模块:</span></span><br><span class="line"><span class="string">    (convolution =&gt; [BN] =&gt; ReLU) * 2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DoubleConv, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.double_conv = paddle.nn.Sequential(</span><br><span class="line">            paddle.nn.Conv2D(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            paddle.nn.BatchNorm2D(out_channels),</span><br><span class="line">            paddle.nn.ReLU(),</span><br><span class="line">            paddle.nn.Conv2D(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            paddle.nn.BatchNorm2D(out_channels),</span><br><span class="line">            paddle.nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.double_conv(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Down</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    下采样:</span></span><br><span class="line"><span class="string">    maxpool + DoubleConv</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(Down, self).__init__()</span><br><span class="line">        self.maxpool_conv = paddle.nn.Sequential(</span><br><span class="line">            paddle.nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.maxpool_conv(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Up</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    上采样:</span></span><br><span class="line"><span class="string">    Upsample + DoubleConv</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, bilinear=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Up, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> bilinear: <span class="comment"># 双线性插值</span></span><br><span class="line">            <span class="comment"># paddle.nn.Upsample：用于调整一个 batch 中图片的大小</span></span><br><span class="line">            self.up = paddle.nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 二维转置卷积</span></span><br><span class="line">            self.up = paddle.nn.Conv2DTranspose(in_channels // <span class="number">2</span>, in_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x1, x2</span>):</span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        <span class="comment"># print(x2.shape, x1.shape)</span></span><br><span class="line">        <span class="comment"># print(x2.shape[2] - x1.shape[2])</span></span><br><span class="line">        <span class="comment"># 求原图和上采样图的差值</span></span><br><span class="line">        diffY = paddle.to_tensor([x2.shape[<span class="number">2</span>] - x1.shape[<span class="number">2</span>]])</span><br><span class="line">        diffX = paddle.to_tensor([x2.shape[<span class="number">3</span>] - x1.shape[<span class="number">3</span>]])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># paddle.vision.transforms.Pad：使用求解的差值//2，对输入图像进行填充</span></span><br><span class="line">        x1 = F.pad(x1, [diffX // <span class="number">2</span>, diffX - diffX // <span class="number">2</span>, diffY // <span class="number">2</span>, diffY - diffY // <span class="number">2</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对修改完尺寸的图片进行联结</span></span><br><span class="line">        x = paddle.concat([x2, x1], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure><p>我们可以查看一下模型的整体结构，同时打印模型各个层的输出参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">整体结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">U_Net</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, bilinear=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(U_Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line">        self.inc = DoubleConv(<span class="number">3</span>, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">256</span>, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">128</span>, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.output_conv = paddle.nn.Conv2D(<span class="number">64</span>, num_classes, kernel_size=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x1 = self.inc(inputs)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        y = self.output_conv(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">打印模型参数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">network = U_Net(num_classes)</span><br><span class="line">model = paddle.Model(network)</span><br><span class="line"><span class="comment"># 模型可视化</span></span><br><span class="line">IMAGE_SIZE = (<span class="number">160</span>, <span class="number">160</span>)</span><br><span class="line">model.summary((<span class="number">1</span>, <span class="number">3</span>, <span class="number">160</span>, <span class="number">160</span>))</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;total_params&#39;: 13403330, &#39;trainable_params&#39;: 13387458&#125;</code></pre><h3 id="2）训练模型"><a href="#2）训练模型" class="headerlink" title="2）训练模型"></a><strong>2）训练模型</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = PetDataset(mode=<span class="string">&#x27;train&#x27;</span>) <span class="comment"># 训练数据集</span></span><br><span class="line">val_dataset = PetDataset(mode=<span class="string">&#x27;test&#x27;</span>) <span class="comment"># 验证数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方梯度的移动平均值</span></span><br><span class="line">optim = paddle.optimizer.RMSProp(</span><br><span class="line">                                learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                rho=<span class="number">0.9</span>, </span><br><span class="line">                                momentum=<span class="number">0.0</span>, </span><br><span class="line">                                epsilon=<span class="number">1e-07</span>, </span><br><span class="line">                                parameters=model.parameters()</span><br><span class="line">                                )</span><br><span class="line"></span><br><span class="line">model.prepare(optim, paddle.nn.CrossEntropyLoss(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">        train_dataset, </span><br><span class="line">        val_dataset, </span><br><span class="line">        epochs=<span class="number">5</span>, </span><br><span class="line">        batch_size=<span class="number">16</span>, </span><br><span class="line">        verbose=<span class="number">1</span>, <span class="comment"># 日志信息</span></span><br><span class="line">        save_dir=<span class="string">&#x27;./model/UNet_1&#x27;</span>, </span><br><span class="line">        save_freq=<span class="number">1</span> <span class="comment"># 保存每一步模型的参数</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">model_path = <span class="string">&quot;./model/UNet_1&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load</span></span><br><span class="line"><span class="comment"># layer_state_dict = paddle.load(&quot;linear_net.pdparams&quot;)</span></span><br><span class="line"><span class="comment"># opt_state_dict = paddle.load(&quot;opt.pdopt&quot;)</span></span><br><span class="line"><span class="comment"># layer.set_state_dict(layer_state_dict)</span></span><br><span class="line"><span class="comment"># opt.set_state_dict(opt_state_dict)</span></span><br></pre></td></tr></table></figure><h3 id="3）评估模型"><a href="#3）评估模型" class="headerlink" title="3）评估模型"></a><strong>3）评估模型</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(val_dataset, batch_size=<span class="number">64</span>, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;loss&#39;: [0.1808225]&#125;</code></pre><h3 id="4）模型预测与可视化"><a href="#4）模型预测与可视化" class="headerlink" title="4）模型预测与可视化"></a><strong>4）模型预测与可视化</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diceCoeff</span>(<span class="params">preds, gts, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    图像分割的Dice损失：</span></span><br><span class="line"><span class="string">    dice = (2 * (pred ∩ gt)) / (pred ∪ gt)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将预测结果与真实结果分别遍历出来，并在批量维上联结</span></span><br><span class="line">    pred = np.concatenate([np.expand_dims(pred,<span class="number">0</span>) <span class="keyword">for</span> pred <span class="keyword">in</span> preds], axis=<span class="number">0</span>)</span><br><span class="line">    gt = np.concatenate([np.expand_dims(gt,<span class="number">0</span>) <span class="keyword">for</span> gt <span class="keyword">in</span> gts], axis=<span class="number">0</span>)</span><br><span class="line">    c = data.shape[<span class="number">2</span>] <span class="comment"># 通道数</span></span><br><span class="line">    num = pred.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 改变Tensor的形状</span></span><br><span class="line">    pred_flat = pred.reshape(num, -<span class="number">1</span>)</span><br><span class="line">    gt_flat = gt.reshape(num, -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    eval_results = <span class="number">0</span>    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, c):</span><br><span class="line">        unionset = np.<span class="built_in">sum</span>(pred_flat==i, <span class="number">1</span>) + np.<span class="built_in">sum</span>(gt_flat==i, <span class="number">1</span>) <span class="comment"># 并集</span></span><br><span class="line">        pred_tmp, gt_tmp = pred_flat, gt_flat</span><br><span class="line">        pred_tmp[pred_tmp==i]=<span class="number">1</span></span><br><span class="line">        pred_tmp[pred_tmp!=i]=<span class="number">0</span></span><br><span class="line">        gt_tmp[gt_tmp==i]=<span class="number">1</span></span><br><span class="line">        gt_tmp[gt_tmp!=i]=<span class="number">0</span></span><br><span class="line">        intersection = np.<span class="built_in">sum</span>(pred_tmp * gt_tmp, <span class="number">1</span>) <span class="comment"># 交集--&gt; 计数比较结果为1的</span></span><br><span class="line">        eval_results +=  (<span class="number">2</span> * intersection + eps) / (unionset + eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> eval_results/(c-<span class="number">1</span>)/num</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">predict_dataset = PetDataset(mode=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">predict_results = model.predict(predict_dataset)</span><br><span class="line"><span class="comment"># 读取1000个预测结果中的第一张图片的分割信息</span></span><br><span class="line"><span class="built_in">print</span>(predict_results[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">mask_idx = <span class="number">0</span></span><br><span class="line">pred_list = []</span><br><span class="line">gt_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取测试数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;work/data_kitti/predict.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        image_path, label_path = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        resize_t = T.Compose([ T.Resize(IMAGE_SIZE)])</span><br><span class="line">        image = resize_t(PilImage.<span class="built_in">open</span>(image_path))</span><br><span class="line">        label = resize_t(PilImage.<span class="built_in">open</span>(label_path))</span><br><span class="line">        image = np.array(image).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">        label = np.array(label).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i &gt; <span class="number">8</span>: </span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型只有一个输出，所以我们通过predict_results[0]来取出1000个预测的结果</span></span><br><span class="line">        <span class="comment"># 映射原始图片的index来取出预测结果，提取mask进行展示</span></span><br><span class="line">        <span class="comment"># (2, 160, 160)--&gt;(160, 160, 2)</span></span><br><span class="line">        data = predict_results[<span class="number">0</span>][mask_idx][<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">        mask = np.argmax(data, axis=-<span class="number">1</span>)</span><br><span class="line">        pred_list.append(mask)</span><br><span class="line">        gt_list.append(label)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 越接近1，效果越好</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;dice指标结果：&#x27;</span>, diceCoeff(pred_list, gt_list))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># subplot(行数，列数，位数)</span></span><br><span class="line">        <span class="comment"># 绘制原图</span></span><br><span class="line">        plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">        plt.title(<span class="string">&#x27;Input Image&#x27;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">        <span class="comment"># 绘制标签</span></span><br><span class="line">        plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">2</span>)</span><br><span class="line">        plt.imshow(label, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&#x27;Label&#x27;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">        <span class="comment"># 绘制预测图</span></span><br><span class="line">        plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">3</span>)</span><br><span class="line">        plt.imshow(mask.astype(<span class="string">&#x27;uint8&#x27;</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&#x27;Predict&#x27;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">        i += <span class="number">3</span></span><br><span class="line">        mask_idx += <span class="number">1</span></span><br><span class="line">        pred_list=[]</span><br><span class="line">        gt_list=[]</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Predict begin...step 30/30 [==============================] - 25ms/step        Predict samples: 30(2, 160, 160)dice指标结果： [0.79124088]dice指标结果： [0.70802263]dice指标结果： [0.73505523]</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/80.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习7.1-图像分割任务的实现</title>
      <link href="/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
      <url>/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、图像分割"><a href="#一、图像分割" class="headerlink" title="一、图像分割"></a>一、图像分割</h1><p>:label:<code>sec_semantic_segmentation</code></p><p><strong>图像分割（image segmentation）</strong> 任务的定义是：根据某些规则将图片分成若干个特定的、具有独特性质的区域，并提出感兴趣目标的技术和过程。目前图像分割任务发展出了以下几个子领域：<strong>语义分割（semantic segmentation）</strong>、<strong>实例分割（instance segmentation）</strong> 以及2018年兴起的新领域<strong>全景分割（panoptic segmentation）</strong>。</p><p>而想要理清三个子领域的区别就不得不提到关于图像分割中 things 和 stuff 的区别：图像中的内容可以按照是否有固定形状分为 things 类别和 stuff 类别，其中，人，车等有固定形状的物体属于 things 类别（可数名词通常属于 things）；天空，草地等没有固定形状的物体属于 stuff 类别（不可数名词属于 stuff）。</p><p>语义分割更注重「类别之间的区分」，而实例分割更注重「个体之间的区分」，如下图所示，语义分割会重点将前景里的人群和背景里树木、天空和草地分割开，但是它不区分人群的单独个体，如图中的人全部标记为红色，导致右边黄色框中的人无法辨别是一个人还是不同的人；而实例分割会重点将人群里的每一个人分割开，但是不在乎草地、树木和天空的分割。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/dab09d5c8bec4632801d73dfde33b2f34aa54a394edd43b2b9003d7463af1ee2" width = "800"  div align=center" width = "800"></center><center><br>图1：图像分割不同的领域</br></center><p><br></br></p><p>全景分割可以说是语义分割和实例分割的结合，每个 stuff 类别与 things 类别都被分割开，可以看到，things 类别的不同个体也被彼此分割开了。</p><h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a><strong>语义分割</strong></h2><p>在之前讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨语义分割问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中每一个<strong>像素</strong>的内容：其语义区域的标注和预测是像素级的。</p><p>语义分割提供有关道路上自由空间的信息，以及检测车道标记和交通标志等信息。 </p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/320c215536fe4f7489796ea3adffd3f1910171e2ff1b422eb24382d36e5e43b2" width = "600"  div align=center" width = "800"></center><center><br>图2：驾驶场景分割</br></center><p><br></br></p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/076d232f7da04178a4d4644602919462eeeff9f9a0614ea6948c46f7998fe432" width = "600"  div align=center" width = "800"></center><center><br>图3：人像分割</br></center><p><br></br></p><p> 面部的语义分割通常涉及诸如皮肤、头发、眼睛、鼻子、嘴巴和背景等的分类。 面部分割在计算机视觉的许多面部应用中是有用的，例如性别、表情、年龄和种族的估计。</p><h2 id="Pascal-VOC2012-语义分割数据集"><a href="#Pascal-VOC2012-语义分割数据集" class="headerlink" title="Pascal VOC2012 语义分割数据集"></a><strong>Pascal VOC2012 语义分割数据集</strong></h2><p>最重要的语义分割数据集之一是<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC2012</a>。下面我们深入了解一下这个数据集。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9921bc85efee4507a62bc2e34df1fd8260a04eb408ed4fdcb8163c6af3900ce7" width = "800"  div align=center" width = "800"></center><center><br>图4：VOC2012 语义分割数据集</br></center><hr><p>VOC2012数据集有多种用途，里面的数据有些可以用于分割、动作识别等。共有二十个类别：</p><p>Person:person</p><p>Animal: bird, cat, cow, dog, horse, sheep</p><p>Vehicle:aeroplane, bicycle, boat, bus, car, motorbike, train</p><p>Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor</p><p>主要有四个大类别，分别是人、常见动物、交通车辆、室内家具用品。主要为图像分类、对象检测识别、图像分割三类任务服务。其中：</p><p>（1）<strong>Annotations文件夹</strong>：里面是图像对应的XML标注信息。每张图像对应一个XML文件。XML文件声明了图像数据的来源、大小等信息。</p><p>（2）<strong>ImageSets文件夹</strong>：</p><ul><li>Action：存放人的行为动作的图像信息；</li><li>Layout：存放的是具有人体部位的数据（人的head、hand、feet等等）；</li><li>Main：存放的是目标检测的数据，总共分为20类；</li><li>Segmentation：是可以用于分割的图像数据，包含val.txt、train.txt等。</li></ul><p>（3）<strong>JPEGImages</strong>：所有的原始图像文件，格式必须是JPG格式。</p><p>（4）<strong>SegmentationClass</strong>：类别分割label图。注意Class的意思就是对所有图像中的相同类别使用一个颜色，比如，飞机使用枣红色，那么所有图像中飞机的分割label标注就是枣红色。</p><p>（5）<strong>SegmentationObject</strong>：实例分割label图。一种图中识别出所有的object之后，对object使用不同的颜色进行填充。</p><hr><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a><strong>加载数据</strong></h3><p>进入路径<code>VOCdevkit/VOC2012</code>之后，我们可以看到数据集的不同组件。<code>ImageSets/Segmentation</code>路径包含用于训练和测试样本的文本文件，而<code>JPEGImages</code>和<code>SegmentationClass</code>路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。<br>下面将<code>read_voc_images</code>函数定义为将所有输入的图像和标签读入内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tar -xf /home/aistudio/data/data150715/VOCtrainval_11-May-<span class="number">2012.</span>tar</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">voc_dir = <span class="string">&quot;VOCdevkit/VOC2012&quot;</span><span class="comment"># 数据集解压后的目录</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, </span><br><span class="line">                            <span class="string">&#x27;ImageSets&#x27;</span>, </span><br><span class="line">                            <span class="string">&#x27;Segmentation&#x27;</span>, </span><br><span class="line">                            <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="comment"># 读入图片 &#x27;JPEGImages&#x27;</span></span><br><span class="line">        feature_load = np.asarray(vision.image_load(os.path.join(voc_dir, </span><br><span class="line">                                                <span class="string">&#x27;JPEGImages&#x27;</span>, </span><br><span class="line">                                                <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.jpg&#x27;</span>))).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        features.append(feature_load)</span><br><span class="line">        <span class="comment"># 把像素标签也存成图片 &#x27;SegmentationClass&#x27;</span></span><br><span class="line">        label_load = np.asarray(vision.image_load(os.path.join(voc_dir, </span><br><span class="line">                                <span class="string">&#x27;SegmentationClass&#x27;</span>, </span><br><span class="line">                                <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.png&#x27;</span>)).convert(<span class="string">&#x27;RGB&#x27;</span>)).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        labels.append(label_load)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = train_features[<span class="number">0</span>:n] + train_labels[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment"># 参数img的格式为(channels,imagesize,imagesize)，</span></span><br><span class="line"><span class="comment"># show_images在现实的时候输入的是(imagesize,imagesize,channels)</span></span><br><span class="line">imgs = [img.transpose((<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">ppl.show_images(imgs, <span class="number">2</span>, n, scale=<span class="number">2.5</span>);</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/75.png" alt="png"></p><p>接下来，我们列举RGB颜色值和类名。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>, <span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure><p>通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了<code>voc_colormap2label</code>函数来构建从RGB到VOC类别索引的映射，而<code>voc_label_indices</code>函数将VOC标签中的RGB值映射到它们的类别索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = paddle.zeros((<span class="number">256</span> ** <span class="number">3</span>, ), dtype=paddle.int64) <span class="comment"># 占位</span></span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        <span class="comment"># 把RGB换算成一个整数（使与类别的整数值对应）</span></span><br><span class="line">        idx = (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]</span><br><span class="line">        colormap2label[idx] = i<span class="comment"># i为RGB类别索引，idx为RGB类别转换为的一个数</span></span><br><span class="line">        <span class="comment">#print(idx, i)</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label <span class="comment"># 返回一个字典</span></span><br><span class="line">voc_colormap2label()</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[16777216], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [0, 0, 0, ..., 0, 0, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colormap = train_labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="comment">#print(colormap)</span></span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span></span><br><span class="line">           + colormap[:, :, <span class="number">2</span>]) <span class="comment"># 将RGB值换成一个下标idx,将idx直接给字典即每个像素对应的标号找到。</span></span><br><span class="line">    <span class="comment">#print(idx)</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure><p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用了idx来代替RGB</span></span><br><span class="line">colormap2label = voc_colormap2label()</span><br><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], colormap2label)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[10, 10], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]) aeroplane</code></pre><h3 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h3><p>在之前的实验，我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。</p><p>为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># rect为随机裁剪后的一个的框，get_param返回指定模型或模块对象的指定参数的名称或值</span></span><br><span class="line">    rect = vision.transforms.RandomCrop(</span><br><span class="line">            size=(height, width))._get_param(img=feature,output_size=(height, width))</span><br><span class="line">    <span class="comment"># crop 在图像分割时，假设对图片裁剪后对标号也要做相应裁剪，才能使标号还能对应住图片。</span></span><br><span class="line">    feature = vision.transforms.functional.crop(feature, *rect)</span><br><span class="line">    label = vision.transforms.functional.crop(label, *rect)</span><br><span class="line">    <span class="comment"># print(feature.shape, label.shape)</span></span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = []</span><br><span class="line"><span class="built_in">print</span>(train_features[<span class="number">0</span>].shape) <span class="comment"># 图像尺寸不一致</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), </span><br><span class="line">                            train_labels[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line">imgs = [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">ppl.show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n, scale=<span class="number">2.5</span>);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>, <span class="number">281</span>, <span class="number">500</span>)<span class="comment"># 第一张图片的尺寸大小</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/76.png" alt="png"></p><h3 id="自定义语义分割数据集类"><a href="#自定义语义分割数据集类" class="headerlink" title="自定义语义分割数据集类"></a><strong>自定义语义分割数据集类</strong></h3><p>我们通过继承高级API提供的<code>Dataset</code>类，自定义了一个语义分割数据集类<code>VOCSegDataset</code>。通过实现<code>__getitem__</code>函数，我们可以任意访问数据集中索引为<code>idx</code>的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的<code>filter</code>函数移除掉。此外，我们还定义了<code>normalize_image</code>函数，从而对输入图像的RGB三个通道的值分别做标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VOCSegDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span><br><span class="line">        self.transform = vision.transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        self.features = [self.normalize_image(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> self.<span class="built_in">filter</span>(features)]</span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        self.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入图像的RGB三个通道的值分别做标准化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize_image</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transform(img / <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 比较图像的尺寸和随机裁剪所指定的输出尺寸，如小于则剔除。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.shape[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">            img.shape[<span class="number">2</span>] &gt;= self.crop_size[<span class="number">1</span>])]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机裁剪特征和标签图像，返回索引为idx的输入图像及其每个像素的类别索引。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        feature, label = voc_rand_crop(</span><br><span class="line">            self.features[idx].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), </span><br><span class="line">            self.labels[idx].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)),</span><br><span class="line">            *self.crop_size</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)), self.colormap2label))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a><strong>读取数据集</strong></h3><p>我们通过自定义的<code>VOCSegDataset</code>类来分别创建训练集和测试集的实例。假设我们指定随机裁剪的输出图像的形状为$320\times 480$，下面我们可以查看训练集和测试集所保留的样本个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line"><span class="comment">#由于数据集过大，整体运行会超显存，请在运行到这一步的时候重启内核。</span></span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir)<span class="comment"># true表示开始训练</span></span><br></pre></td></tr></table></figure><pre><code>read 1114 examples</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure><pre><code>read 1078 examples</code></pre><p>设批量大小为64，我们定义训练集的迭代器。<br>打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签<code>Y</code>是一个三维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter = paddle.io.DataLoader(voc_train, </span><br><span class="line">                                    batch_size=batch_size, </span><br><span class="line">                                    shuffle=<span class="literal">True</span>, </span><br><span class="line">                                    drop_last=<span class="literal">True</span>) <span class="comment"># 扔掉不足batch的样本</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[64, 320, 480, 3]64批量大小 329×480像素 3的通道数</span></span><br><span class="line"><span class="string">[64, 320, 480]#由于y每一个像素点位置对应的是类别的取值，而原图对应的是RGB3个数值，所以这里是单通道</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="整合所有组件"><a href="#整合所有组件" class="headerlink" title="[整合所有组件]"></a>[<strong>整合所有组件</strong>]</h3><p>最后，我们定义以下<code>load_data_voc</code>函数来下载并读取Pascal VOC2012语义分割数据集。它返回训练集和测试集的数据迭代器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_voc</span>(<span class="params">batch_size, crop_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class="line">    voc_dir =  <span class="string">&quot;VOCdevkit/VOC2012&quot;</span></span><br><span class="line">    train_iter = paddle.io.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), </span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    test_iter = paddle.io.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir), </span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></figure><h1 id="二、转置卷积"><a href="#二、转置卷积" class="headerlink" title="二、转置卷积"></a>二、转置卷积</h1><p>:label:<code>sec_transposed_conv</code></p><p>到目前为止，我们所见到的卷积神经网络层，例如卷积层（ :numref:<code>sec_conv_layer</code>）和池化层（ :numref:<code>sec_pooling</code>），通常会减少下采样输入图像的空间维度（高和宽）。然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。】</p><p>例如，<strong>输出像素所处的通道维可以保有输入像素在同一位置上的分类结果</strong>。为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增加<strong>上采样</strong>中间层特征图的空间维度。</p><p>在算法中常用的上采样方法就是双线性插值以及转置卷积。在本节中，我们将介绍<strong>转置卷积（transposed convolution）</strong> ，用于逆转下采样导致的空间尺寸减小，使空间尺寸放大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> ppl</span><br></pre></td></tr></table></figure><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h2><p>让我们暂时忽略通道，从基本的转置卷积开始，设步幅为1且没有填充。假设我们有一个$n_h \times n_w$的输入张量和一个$k_h \times k_w$的卷积核。以步幅为1滑动卷积核窗口，每行    $n_w$次，每列$n_h$次，共产生$n_h n_w$个中间结果。每个中间结果都是一个$(n_h + k_h - 1) \times (n_w + k_w - 1)$的张量，初始化为0。为了计算每个中间张量，输入张量中的每个元素都要乘以卷积核，从而使所得的$k_h \times k_w$张量替换中间张量的一部分。请注意，每个中间张量被替换部分的位置与输入张量中元素的位置相对应。最后，所有中间结果相加以获得最终结果。</p><p>下图解释了如何为$2\times 2$的输入张量计算卷积核为$2\times 2$的转置卷积：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4d0c75a3740d49b4b795d856fb4e652d021d20fcf72e495a8d2b7f12deb74b4b" width = "800"  div align=center" width = "800"></center><center><br>图5：转置卷积</br></center><p>这将是一个一对多（one-to-many）的映射关系。我们可以对输入矩阵<code>X</code>和卷积核矩阵<code>K</code>(<strong>实现基本的转置卷积运算</strong>)<code>trans_conv</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trans_conv</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = paddle.zeros((X.shape[<span class="number">0</span>] + h - <span class="number">1</span>, X.shape[<span class="number">1</span>] + w - <span class="number">1</span>)) <span class="comment"># 输出形状公式</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i:i+h, j:j+w] += X[i,j] * K </span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>与通过卷积核“减少”输入元素的常规卷积（在 :numref:<code>sec_conv_layer</code>中）相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出。此实现是基本的二维转置卷积运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">K = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">trans_conv(X, K)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[0. , 0. , 1. ],        [0. , 4. , 6. ],        [4. , 12., 9. ]])</code></pre><p>或者，当输入<code>X</code>和卷积核<code>K</code>都是四维张量时，我们可以使用高级API获得相同的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X, K = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)), K.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">class paddle.nn.Conv2DTranspose(</span></span><br><span class="line"><span class="string">                                in_channels, #输入通道数</span></span><br><span class="line"><span class="string">                                out_channels, #输出通道数</span></span><br><span class="line"><span class="string">                                kernel_size, #卷积核的大小</span></span><br><span class="line"><span class="string">                                stride=1, </span></span><br><span class="line"><span class="string">                                padding=0, </span></span><br><span class="line"><span class="string">                                output_padding=0, # 输出形状上一侧额外添加的大小. 默认值: 0</span></span><br><span class="line"><span class="string">                                groups=1, # 二维卷积层的组数</span></span><br><span class="line"><span class="string">                                dilation=1, # 空洞大小</span></span><br><span class="line"><span class="string">                                weight_attr=None, </span></span><br><span class="line"><span class="string">                                bias_attr=None, </span></span><br><span class="line"><span class="string">                                data_format=&#x27;NCHW&#x27;</span></span><br><span class="line"><span class="string">                                )</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=False,       [[[[0. , 0. , 1. ],          [0. , 4. , 6. ],          [4. , 12., 9. ]]]])</code></pre><h2 id="填充、步幅和多通道"><a href="#填充、步幅和多通道" class="headerlink" title="填充、步幅和多通道"></a><strong>填充、步幅和多通道</strong></h2><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a><strong>填充</strong></h3><p>与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。（你加我就减）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入了填充实际上使得输出尺寸变小</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 1, 1], dtype=float32, place=Place(gpu:0), stop_gradient=False,       [[[[4.]]]])</code></pre><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a><strong>步幅</strong></h3><p>在转置卷积中，步幅被指定为中间结果（输出），而不是输入。使用上图中相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，具体会体现在输出图像上。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/1bfa009a5c01420da9747628011a0ec961986b86336f43618daf48b205c824ae" width = "600"  div align=center" width = "800"></center><center><br>图6：步幅为2下的转置卷积</br></center><p>以下代码可以验证步幅为2的转置卷积的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入了步幅实际上使得输出尺寸变大</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 4, 4], dtype=float32, place=Place(gpu:0), stop_gradient=False,       [[[[0., 0., 0., 1.],          [0., 0., 2., 3.],          [0., 2., 0., 3.],          [4., 6., 6., 9.]]]])</code></pre><p>对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\times k_w$的卷积核张量。当指定多个输出通道时，每个输出通道将有一个$c_i\times k_h\times k_w$的卷积核。</p><p>常规卷积与转置卷积可以相互转化。如果我们将$\mathsf{X}$代入卷积层$f$来输出$\mathsf{Y}=f(\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\mathsf{X}$中通道数的转置卷积层$g$，那$g(Y)$的形状将与$\mathsf{X}$相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.rand(shape=(<span class="number">1</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">conv = nn.Conv2D(</span><br><span class="line">    <span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">tconv = nn.Conv2DTranspose(</span><br><span class="line">    <span class="number">20</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">tconv(conv(X)).shape == X.shape</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h1 id="三、全卷积网络"><a href="#三、全卷积网络" class="headerlink" title="三、全卷积网络"></a>三、全卷积网络</h1><p>:label:<code>sec_fcn</code></p><p>语义分割是对图像中的每个像素分类。<strong>全卷积网络（fully convolutional network，FCN）</strong> 采用卷积神经网络实现了从图像像素到像素类别的变换。与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过转置卷积实现的。</p><p>因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h2 id="1-构造模型"><a href="#1-构造模型" class="headerlink" title="1. 构造模型"></a>1. 构造模型</h2><p>下面我们了解一下全卷积网络模型最基本的设计。全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/036bdd31b81147f99d2e06665bf99de0dd6b41ed67e44fdd898a1c7f66f2be52" width = "800"  div align=center" width = "800"></center><center><br>图7：全卷积网络</br></center><p>下面，我们使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征，并将该网络记为<code>pretrained_net</code>。ResNet-18模型的最后几层包括全局平均池化层和全连接层，然而全卷积网络中不需要它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = vision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(pretrained_net.children())</span><br></pre></td></tr></table></figure><p>接下来，我们创建一个全卷积网络<code>net</code>。它复制了ResNet-18中大部分的预训练层，除了最后的全局平均池化层和最接近输出的全连接层。<br>给定高度为320和宽度为480的输入，<code>net</code>的前向传播将输入的高和宽减小至原来的$1/32$，即10和15。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*<span class="built_in">list</span>(pretrained_net.children())[:-<span class="number">2</span>])</span><br><span class="line">X = paddle.rand(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 512, 10, 15]</code></pre><p>接下来，我们使用$1\times1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）。最后，我们需要将特征图的高度和宽度增加32倍，从而将其变回输入图像的高和宽。</p><p>回想一下 ​​<code>sec_padding</code>中卷积层输出形状的计算方法：由于$(320-64+16\times2+32)/32=10$且$(480-64+16\times2+32)/32=15$，我们构造一个步幅为$32$的转置卷积层，并将卷积核的高和宽设为$64$，填充为$16$。我们可以看到如果步幅为$s$，填充为$s/2$（假设$s/2$是整数）且卷积核的高和宽为$2s$，转置卷积核会将输入的高和宽分别放大$s$倍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line"><span class="comment"># 添加一个子层 add_sublayer</span></span><br><span class="line">net.add_sublayer(<span class="string">&#x27;final_conv&#x27;</span>, nn.Conv2D(<span class="number">512</span>, num_classes, kernel_size=<span class="number">1</span>))</span><br><span class="line">net.add_sublayer(<span class="string">&#x27;transpose_conv&#x27;</span>, </span><br><span class="line">                nn.Conv2DTranspose(</span><br><span class="line">                                num_classes, </span><br><span class="line">                                num_classes, </span><br><span class="line">                                kernel_size=<span class="number">64</span>, </span><br><span class="line">                                padding=<span class="number">16</span>, </span><br><span class="line">                                stride=<span class="number">32</span></span><br><span class="line">                                )</span><br><span class="line">                )</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 21, 320, 480]</code></pre><h2 id="2-初始化转置卷积层"><a href="#2-初始化转置卷积层" class="headerlink" title="2. 初始化转置卷积层"></a>2. 初始化转置卷积层</h2><p>在图像处理中，我们有时需要将图像放大，即<strong>上采样（upsampling）</strong>。<strong>双线性插值（bilinear interpolation）</strong> 是常用的上采样方法之一，它也经常用于初始化转置卷积层。</p><p>为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。首先，将输出图像的坐标$(x,y)$映射到输入图像的坐标$(x’,y’)$上。例如，根据输入与输出的尺寸之比来映射。请注意，映射后的$x′$和$y′$是实数。然后，在输入图像上找到离坐标$(x’,y’)$最近的4个像素。最后，输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x’,y’)$的相对距离来计算。双线性插值的上采样可以通过转置卷积层实现，内核由以下<code>bilinear_kernel</code>函数构造，这里我们在<strong>中心位置</strong>插入了一个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;利用双线性插值来初始化卷积核参数&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_kernel</span>(<span class="params">in_channels, out_channels, kernel_size</span>):</span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="comment"># 填充个数</span></span><br><span class="line">    <span class="built_in">print</span>(factor)</span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 中心位置</span></span><br><span class="line">    <span class="built_in">print</span>(center)</span><br><span class="line">    <span class="comment"># 表示出kernel_size长度的列向量和行向量</span></span><br><span class="line">    og = (paddle.arange(kernel_size).reshape((-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    paddle.arange(kernel_size).reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;og&quot;</span>, og)</span><br><span class="line">    <span class="comment"># 使用坐标及相对距离计算</span></span><br><span class="line">    filt = (<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor) * (<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    </span><br><span class="line">    weight = paddle.zeros((in_channels, out_channels, kernel_size, kernel_size))</span><br><span class="line">    weight[<span class="built_in">range</span>(in_channels), <span class="built_in">range</span>(out_channels), :, :] = filt</span><br><span class="line">    <span class="built_in">print</span>(weight.shape)</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure><p>让我们用双线性插值的上采样实验它由转置卷积层实现。我们构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.Conv2DTranspose(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">conv_trans.weight.set_value(bilinear_kernel(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># [3, 3, 4, 4]--&gt; [通道，通道，核大小，核大小]--&gt; 4x4就是权重的数量</span></span><br></pre></td></tr></table></figure><pre><code>21.5og (Tensor(shape=[4, 1], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [[0],        [1],        [2],        [3]]), Tensor(shape=[1, 4], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [[0, 1, 2, 3]]))Tensor(shape=[4, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[0.25000000],        [0.75000000],        [0.75000000],        [0.25000000]])Tensor(shape=[1, 4], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[0.25000000, 0.75000000, 0.75000000, 0.25000000]])[3, 3, 4, 4]</code></pre><p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>。为了打印图像，我们需要调整通道维的位置。<br>可以看到，转置卷积层将图像的高和宽分别放大了2倍。除了坐标刻度不同，双线性插值放大的图像和在 :numref:<code>sec_bbox</code>中打印出的原图看上去没什么两样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">img = vision.transforms.ToTensor()(ppl.Image.<span class="built_in">open</span>(<span class="string">&#x27;work/catdog.jpg&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># [通道，高，宽]</span></span><br><span class="line">X = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line">Y = conv_trans(X) <span class="comment"># 双线性插值后输出Y--&gt; 高宽放大2倍</span></span><br><span class="line">out_img = Y[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).detach()</span><br><span class="line"><span class="built_in">print</span>(out_img.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;********************&quot;</span>)</span><br><span class="line"></span><br><span class="line">ppl.set_figsize()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input image shape:&#x27;</span>, img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).shape)</span><br><span class="line">ppl.plt.imshow(img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output image shape:&#x27;</span>, out_img.shape)</span><br><span class="line">ppl.plt.imshow(out_img)</span><br></pre></td></tr></table></figure><pre><code>[3, 561, 728][1, 3, 561, 728][1122, 1456, 3]********************input image shape: [561, 728, 3]output image shape: [1122, 1456, 3]</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/77.png" alt="svg"></p><p>在全卷积网络中，我们用双线性插值的上采样初始化转置卷积层。对于$1\times 1$卷积层，我们使用Xavier初始化参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)</span><br><span class="line">net.transpose_conv.weight.set_value(W)</span><br></pre></td></tr></table></figure><h2 id="3-读取数据集"><a href="#3-读取数据集" class="headerlink" title="3. 读取数据集"></a>3. 读取数据集</h2><p>我们读取语义分割读取数据集。指定随机裁剪的输出图像的形状为$320\times 480$：高和宽都可以被$32$整除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = <span class="number">8</span>, (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">train_iter, test_iter = ppl.load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></table></figure><pre><code>read 1114 examplesread 1078 examples</code></pre><h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><p>现在我们可以训练全卷积网络了。这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为我们使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维。此外，模型基于每个像素的预测类别是否正确来计算准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    <span class="comment"># 先求高的平均，再求宽的平均</span></span><br><span class="line">    <span class="keyword">return</span> F.cross_entropy(inputs, targets, reduction=<span class="string">&#x27;none&#x27;</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = ppl.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            X = paddle.cast(X.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)), dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            pred = net(X)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(pred.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> pred.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                y_hat = ppl.argmax(pred, axis=<span class="number">1</span>)</span><br><span class="line">            cmp = paddle.cast(y_hat, y.dtype) == y</span><br><span class="line">            metric.add(<span class="built_in">float</span>(cmp.<span class="built_in">sum</span>()), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch_ch13</span>(<span class="params">net, X, y, loss, trainer</span>):</span><br><span class="line">    net.train()</span><br><span class="line">    trainer.clear_grad()</span><br><span class="line">    pred = net(X)</span><br><span class="line">    l = loss(pred.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)), y)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> pred.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 将one-hot转换为数值的形式</span></span><br><span class="line">        y_hat = ppl.argmax(pred, axis=<span class="number">1</span>)</span><br><span class="line">    cmp = paddle.cast(y_hat, y.dtype) == y</span><br><span class="line">    train_acc_sum = <span class="built_in">float</span>(cmp.<span class="built_in">sum</span>())</span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs</span>):</span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = ppl.Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            <span class="comment"># 数据维度需要修改为[N, C, H, W]，以符合模型的输入维度。</span></span><br><span class="line">            <span class="comment"># 模型的输入数据类型为float32</span></span><br><span class="line">            features = paddle.cast(features.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)), dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            l, acc = train_batch_ch13(net, features, labels, loss, trainer)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span> &#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, wd = <span class="number">2</span>, <span class="number">0.001</span>, <span class="number">1e-3</span></span><br><span class="line">trainer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters(), weight_decay=wd)</span><br><span class="line">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs)</span><br></pre></td></tr></table></figure><pre><code>loss 0.492,  train acc 0.852,  test acc 0.844 </code></pre><h2 id="5-预测"><a href="#5-预测" class="headerlink" title="5. 预测"></a>5. 预测</h2><p>在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">img</span>): <span class="comment"># 返回一个跟原图高宽等同的矩阵：[320, 480]</span></span><br><span class="line">    <span class="comment"># expand_dims 增加一个维度</span></span><br><span class="line">    X = np.expand_dims(test_iter.dataset.normalize_image(img), <span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(X.shape) # [1, 3, 320, 480]</span></span><br><span class="line">    X = paddle.to_tensor(X, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    pred = net(X).argmax(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(pred.shape) # [1, 320, 480]</span></span><br><span class="line">    <span class="keyword">return</span> pred.reshape((pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure><p>为了<strong>可视化预测的类别</strong>给每个像素，我们将预测类别映射回它们在数据集中的标注颜色。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label2image</span>(<span class="params">pred</span>):</span><br><span class="line">    colormap = paddle.to_tensor(ppl.VOC_COLORMAP)</span><br><span class="line">    <span class="comment"># print(colormap.shape)</span></span><br><span class="line">    X = paddle.cast(pred, dtype=<span class="string">&#x27;int64&#x27;</span>)  <span class="comment"># 转换数据类型</span></span><br><span class="line">    <span class="comment"># print(&quot;label2image: &quot;, X.shape)</span></span><br><span class="line">    <span class="keyword">return</span> colormap[X]</span><br></pre></td></tr></table></figure><p>测试数据集中的图像大小和形状各异。由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。</p><p>为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播。请注意，这些区域的并集需要完整覆盖输入图像。当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为<code>softmax</code>运算的输入，从而预测类别。</p><p>为简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320\times480$的区域用于预测。对于这些测试图像，我们逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">voc_dir = <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span></span><br><span class="line">test_images, test_labels = ppl.read_voc_images(voc_dir, <span class="literal">False</span>)</span><br><span class="line">n, imgs = <span class="number">4</span>, []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    crop_rect = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">    crop_rect2 = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">320</span>, <span class="number">480</span>) <span class="comment"># 从左上角开始截取形状</span></span><br><span class="line">    rect = paddle.vision.transforms.RandomCrop(size=crop_rect)._get_param(</span><br><span class="line">                            img=test_images[i].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)),</span><br><span class="line">                            output_size=crop_rect</span><br><span class="line">                            )</span><br><span class="line">    X = paddle.vision.transforms.functional.crop(test_images[i].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), *crop_rect2)</span><br><span class="line">    X = X.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    pred = label2image(predict(X)) <span class="comment"># 预测图</span></span><br><span class="line">    croped = paddle.vision.transforms.crop(test_labels[i], *rect)</span><br><span class="line">    croped = croped.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># 真实标签</span></span><br><span class="line">    X = X.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># 原图</span></span><br><span class="line">    imgs += [X, pred, paddle.vision.transforms.crop(test_labels[i], *crop_rect2).transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))]</span><br><span class="line"><span class="comment"># 第一行是真实图片，第二行是预测结果，第三行是标签图片</span></span><br><span class="line">ppl.show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n, scale=<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/78.png" alt="svg"></p><h1 id="四、小结"><a href="#四、小结" class="headerlink" title="四、小结"></a>四、小结</h1><ul><li>语义分割通过将图像划分为属于不同语义类别的区域，来识别并理解图像中像素级别的内容。</li><li>由于语义分割的输入图像和标签在像素上一一对应，输入图像会被随机裁剪为固定尺寸而不是缩放。</li><li>与通过卷积核减少输入元素的常规卷积相反，转置卷积通过卷积核广播输入元素，从而产生形状大于输入的输出。</li><li>全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。</li><li>在全卷积网络中，我们可以将转置卷积层初始化为双线性插值的上采样。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习6.4-其他目标检测模型概述</title>
      <link href="/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.4-%E5%85%B6%E4%BB%96%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"/>
      <url>/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.4-%E5%85%B6%E4%BB%96%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="其他目标检测模型概述"><a href="#其他目标检测模型概述" class="headerlink" title="其他目标检测模型概述"></a><strong>其他目标检测模型概述</strong></h1><p>目标检测是计算机视觉领域中的一个重要研究方向，也是其他复杂视觉任务的基础。 给定一张图像，我们可以通过目标检测来对图像中物体的位置进行检测，同时判断物体的类别。</p><p>根据算法的流程可以将目标检测算法分为两种流派，一种是以Faster R-CNN为代表的Two-Stage算法，另一种是以SSD、YOLO为代表的One-Stage算法。</p><ul><li>Two-Stage算法：先进行<strong>区域生成（Region Proposal,RP）</strong>(一个可能包含待检测物体的预选框)，再通过卷积神经网络进行样本分类。任务：特征提取-&gt;生成RP-&gt;分类/定位回归。</li><li>One-Stage算法：不用RP，直接在网络中提取特征来预测物体分类和位置。任务：特征提取-&gt;分类/定位回归。</li></ul><p>两种方式各有各的特别，Two-Stage很明显检测的精度要高一点，但是检测速度慢；One-Stage放弃了高精度，但是换来了速度，速度比Two-Stage算法快很多。</p><h2 id="一、传统目标检测算法"><a href="#一、传统目标检测算法" class="headerlink" title="一、传统目标检测算法"></a>一、传统目标检测算法</h2><p>传统目标检测主要分为以下几个步骤，如下图所示：</p><p>（1）输入一张图片；生成一系列候选框；</p><p>（2）对候选框进行特征提取；</p><p>（3）使用分类器对框内物体类别判断；</p><p>（4）使用NMS非极大值抑制来筛选最优的框</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/83d9a3d0d07b442489766dcd3a3dff750f70cff2020041f09c785d7fb1adce92" width = "800"></center><center><br>图1：传统目标检测算法检测示意图</br></center><p><br></br></p><h2 id="二、Two-stage-目标检测算法"><a href="#二、Two-stage-目标检测算法" class="headerlink" title="二、Two stage 目标检测算法"></a>二、Two stage 目标检测算法</h2><h3 id="1）R-CNN"><a href="#1）R-CNN" class="headerlink" title="1）R-CNN"></a><strong>1）R-CNN</strong></h3><p>R-CNN（Region with CNN Feature）2014年提出，在此之前都是传统的目标检测算法，人为定义特征进行检测，进入了瓶颈期，进步缓慢，R-CNN出来之后将目标检测领域的准确率至少提高了30%，是第一个成功将深度学习应用到目标检测上的算法。R-CNN基于卷积神经网络(CNN)，线性回归，和<a href="https://shao12138.blog.csdn.net/article/details/121164645?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-121164645-blog-125703635.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-121164645-blog-125703635.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=1">支持向量机(SVM)</a>等算法，实现目标检测技术。</p><p>R-CNN作为R-CNN系列的第⼀代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。R-CNN算法流程可分为四个步骤：</p><ol><li>一张图像生成1k~2k个候选区域（Selective Search方法）；</li><li>对每个候选区域，使用深度网络提取特征；</li><li>特征送入每一类的SVM分类器，判断是否属于该类；</li><li>使用回归器精细修正候选框位置。例如对于每个框，回归器预测其与真实框的距离差距(左上角与右下角点坐标)。<br><br></br></li></ol><center><img src="https://ai-studio-static-online.cdn.bcebos.com/02e51c65e5f945a1b02a9447790273192f71fab6e5c04c2b8b4cdbeedc74ed2e" width = "800"></center><center><br>图2：R-CNN</br></center><p><br></br></p><hr><p><strong>选择性搜索算法（Selective Search,SS）</strong>：算法原理如下：首先将每个像素作为一组。然后，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的 ROI。与寻找几乎个区域比起来，这种方法要高效的多。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/329f7fc53a3e48dcbbf863965a5826435db95b4dd2ba4b3588760dca6d509d52" width = "800"></center><center><br>图3：Selective Search算法应用</br></center><p><br></br></p><hr><p>R-CNN存在以下问题：</p><ul><li>测试速度慢，测试一张图片约53s（CPU），用SS算法提取候选框用时约2秒（相对较快），一张图片内候选框之间存在大量重叠，提取特征操作冗余。</li><li>训练速度慢，过程繁琐。</li><li>训练所需空间大，需要从每个图像中的每个目标候选框提取特征，并写入磁盘，对于非常深的网络，提取特征需要数百GB的存储空间。</li></ul><h3 id="2）Fast-R-CNN"><a href="#2）Fast-R-CNN" class="headerlink" title="2）Fast R-CNN"></a><strong>2）Fast R-CNN</strong></h3><p>R-CNN 需要非常多的候选区域以提升准确度，但其实有很多区域是彼此重叠的。如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们可能需要重复提取很多次特征。因此 R-CNN 的训练和预测速度非常慢。</p><p>我们知道，CNN 中的特征图以一种密集的方式表征空间特征，那么我们能直接使用特征图代替原图来检测目标吗？答案是肯定的。</p><p>Fast R-CNN 使用CNN网络先提取整个图像的特征，而不是对每个图像块提取多次。然后，我们可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 来生成 ROI区域在对应的特征图上的映射特征图块，并用于目标检测任务中。我们使用 ROI 池化将特征图块转换为固定的大小，并送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，所以它能显著地减少处理时间。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/06f66759af814e7e81a495e5a47e1a2f4eb9880d25f34deca82d393238bb18c3" width = "800"></center><center><br>图4：Fast R-CNN</br></center><p><br></br></p><hr><p><strong>感兴趣区域池化（ROI pooling）</strong>：</p><p>因为 Fast R-CNN 使用全连接层，所以我们应用感兴趣区域池化将不同大小的 ROI 转换为固定大小。感兴趣区域池化从具有多个卷积和最大池层的深度卷积网络获得的固定大小的特征映射。比如我们将 8×8 特征图转换为预定义的 2×2 大小：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8ab4b968df8249a28fb6cd32cb9a3d4e0e1d2b69da984c5390a5d361df1df908" width = "600"></center><center><br>图5：ROI pooling的应用</br></center><p><br></br></p><ul><li>左上角：输入特征图；</li><li>右上角：将 ROI（蓝色区域即候选区域）与特征图重叠；</li><li>左下角：将 ROI 拆分为目标维度。例如，对于 2×2 目标，我们将 ROI 分割为 4 个大小相似或相等的部分；</li><li>右下角：找到每个部分的最大值，得到变换后的特征图。</li></ul><p>按上述步骤得到一个 2×2 的特征图块，可以送至分类器和边界框回归器中。使用softmax损失进行分类；使用回归损失校正预测框。总损失是两部分的和，然后反向传播进行训练。</p><hr><p>Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。同时Fast R-CNN重新使用卷积网络中的特征映射，显着加快训练和测试效率。</p><h3 id="3）Faster-R-CNN"><a href="#3）Faster-R-CNN" class="headerlink" title="3）Faster R-CNN"></a><strong>3）Faster R-CNN</strong></h3><p>Fast R-CNN 依赖于外部候选区域方法，如选择性搜索。但这些算法在 CPU 上运行且速度很慢。</p><p>与其使用固定的算法得到候选区域，不如让网络自己学习自己的候选区域应该是什么。因此，Faster R-CNN 采用与 Fast R-CNN 相同的设计，只是它用<strong>区域生成网络（Region Proposal Network，RPN）</strong> 代替了候选区域方法。新的候选区域网络（RPN）在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。RPN的引入，可以说是真正意义上把目标检测整个流程融入到一个神经网络中。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/98e9c780a50a4194ab988a16f0f5c0196b0a118f775447edbecbfdaebe45fc17" width = "800"></center><center><br>图6：Faster R-CNN</br></center><p><br></br></p><hr><p><strong>区域生成网络（RPN）</strong>:</p><p>区域生成网络用于生成候选区域（proposals），该层通过一组固定的尺寸和比例得到一组锚点（anchors）,通过softmax判断锚点属于前景或背景，再利用区域回归修正锚点从而获得精确的候选区域。包括以下部分：</p><ul><li>生成 anchor boxes</li><li>判断每个 anchor box 为 foreground(包含物体) 或者 background(背景) ，二分类</li><li>边界框回归(bounding box regression) 对 anchor box 进行微调，使得预测框和真实框更加接近</li></ul><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f118bdb7042d4a579048542299d6434a0287731b7c8c434aa22eda60bc04dea8" width = "800"></center><center><br>图7：区域生成网络的应用</br></center><p><br></br></p><hr><p>Faster RCNN 与 SSD 的 anchors 区别在于：</p><p>（1）前者在一个特征图上预设了 anchor，先进行初步修正与筛选，之后再进行分类与回归</p><p>（2）后者在多个特征图上预设了 anchor（多尺度），并直接在所有这些anchor上进行分类与回归（单阶段）</p><h2 id="三、One-Stage-目标检测算法"><a href="#三、One-Stage-目标检测算法" class="headerlink" title="三、One-Stage 目标检测算法"></a>三、One-Stage 目标检测算法</h2><p>SSD（Single-Shot MultiBox Detector），使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器，即将提取物体位置和判断物体类别融合在一起进行，其最主要的特点是识别器用于判断物体的特征不仅仅来自于神经网络的输出，还来自于神经网络的中间结果。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a54cb5d12e3e45d49047359dd6cee691322e77af07cf4e208ac66230341534c2" width = "800"></center><center><br>图8：SSD</br></center><p><br></br></p><p>SSD是YOLO之后又一个引人注目的目标检测结构，它沿用了YOLO中直接回归 bbox和分类概率的方法，同时又参考了Faster R-CNN，大量使用anchor来提升识别准确度。通过把这两种结构相结合，SSD保持了很高的识别速度，还能把mAP提升到较高的水平。</p><h2 id="四、小结"><a href="#四、小结" class="headerlink" title="四、小结"></a>四、小结</h2><ul><li>R-CNN 对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框。</li><li>Fast R-CNN 对 R-CNN 的一个主要改进：只对整个图像做卷积神经网络的前向传播。它还引入了兴趣区域池化层，从而为具有不同形状的兴趣区域抽取相同形状的特征。</li><li>Faster R-CNN 将 Fast R-CNN 中使用的选择性搜索替换为参与训练的区域生成网络（RPN），这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度。</li><li>单发多框检测（SSD）是一种多尺度目标检测模型。基于基础网络块和各个多尺度特征块，单发多框检测生成不同数量和不同大小的锚框，并通过预测这些锚框的类别和偏移量检测不同大小的目标。<a href="https://aistudio.baidu.com/aistudio/projectdetail/4880934">https://aistudio.baidu.com/aistudio/projectdetail/4880934</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习6.3-YOLOv3实现AI识虫（下）</title>
      <link href="/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.3-YOLOv3%E5%AE%9E%E7%8E%B0AI%E8%AF%86%E8%99%AB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
      <url>/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.3-YOLOv3%E5%AE%9E%E7%8E%B0AI%E8%AF%86%E8%99%AB%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="一、基于YOLOv3的AI识虫实验"><a href="#一、基于YOLOv3的AI识虫实验" class="headerlink" title="一、基于YOLOv3的AI识虫实验"></a>一、基于YOLOv3的AI识虫实验</h1><p>基于YOLOv3的AI识虫实验流程如下图所示，包含如下9个步骤：</p><p><strong>1.数据处理</strong>：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取，同时，对于训练数据，使用数据增广策略来提升模型泛化性能；</p><p><strong>2.模型构建</strong>：设计深度神经网络结构（模型的假设空间）；</p><p><strong>3.模型后处理</strong>：通过模型预测得到的概率图，经过一系列后处理操作得到真实的输出值；</p><p><strong>4.损失函数定义</strong>：根据预测值和真实值构建损失函数，神经网络通过最小化损失函数使得网络的输出值更接近真实值；</p><p><strong>5.训练配置</strong>：实例化模型，加载模型参数，指定模型采用的寻解算法（优化器）；</p><p><strong>6.模型训练</strong>：执行多轮训练不断调整参数，以达到较好的效果；</p><p><strong>7.模型保存</strong>：将模型参数保存到指定位置，便于后续推理或继续训练使用；</p><p><strong>8.模型评估</strong>：对训练好的模型进行评估测试，观察准确率和Loss；</p><p><strong>9.模型推理及可视化</strong>：使用一张真实图片来验证模型识别的效果，并可视化推理结果。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/fc58e8f1fa1943448d5077d7bff25afa94c4d41925274f3985592cc935f253c9" width = "900"></center><blockquote><p>上述不是文本检测是目标检测，打错了！！！</p></blockquote><p><br></br></p><hr><blockquote><p><strong>说明：</strong></p><p>不同的深度学习任务，使用深度学习框架的代码结构基本相似。大家掌握了一个任务的实现方法，便很容易在此基础上举一反三。使用深度学习框架可以屏蔽底层实现，用户只需关注模型的逻辑结构。同时，简化了计算，降低了深度学习入门门槛。</p></blockquote><hr><h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>本实验支持在飞桨实训平台或本地环境操作，建议您使用飞桨实训平台。</p><ul><li><strong>飞桨实训平台</strong>：实训平台集成了实验必须的大部分相关环境，代码可在线运行，同时还提供了免费算力，即使实践复杂模型也无算力之忧。</li><li><strong>本地环境</strong>：如果您选择在本地环境上操作，需要安装Python3.X、飞桨开源框架2.X 等实验必须的环境，具体请参见<a href="https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html">《飞桨开始使用-安装》</a>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压数据脚本，将文件解压到work目录下</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始解压......&#x27;</span>)</span><br><span class="line">!unzip  -o -q -d  /home/aistudio/work /home/aistudio/data/data170339/insects.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;解压完成&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageEnhance</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">INSECT_NAMES = [<span class="string">&#x27;Boerner&#x27;</span>, <span class="string">&#x27;Leconte&#x27;</span>, <span class="string">&#x27;Linnaeus&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;acuminatus&#x27;</span>, <span class="string">&#x27;armandi&#x27;</span>, <span class="string">&#x27;coleoptera&#x27;</span>, <span class="string">&#x27;linnaeus&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_insect_names</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    return a dict, as following,</span></span><br><span class="line"><span class="string">        &#123;&#x27;Boerner&#x27;: 0,</span></span><br><span class="line"><span class="string">         &#x27;Leconte&#x27;: 1,</span></span><br><span class="line"><span class="string">         &#x27;Linnaeus&#x27;: 2, </span></span><br><span class="line"><span class="string">         &#x27;acuminatus&#x27;: 3,</span></span><br><span class="line"><span class="string">         &#x27;armandi&#x27;: 4,</span></span><br><span class="line"><span class="string">         &#x27;coleoptera&#x27;: 5,</span></span><br><span class="line"><span class="string">         &#x27;linnaeus&#x27;: 6</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    It can map the insect name into an integer label.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    insect_category2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(INSECT_NAMES):</span><br><span class="line">        insect_category2id[item] = i <span class="comment"># 构建键值对</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> insect_category2id</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_annotations</span>(<span class="params">cname2cid, datadir</span>):</span><br><span class="line">    filenames = os.listdir(os.path.join(datadir, <span class="string">&#x27;annotations&#x27;</span>, <span class="string">&#x27;xmls&#x27;</span>))</span><br><span class="line">    records = []</span><br><span class="line">    ct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> filenames:</span><br><span class="line">        fid = fname.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>] <span class="comment"># 序号</span></span><br><span class="line">        fpath = os.path.join(datadir, <span class="string">&#x27;annotations&#x27;</span>, <span class="string">&#x27;xmls&#x27;</span>, fname) <span class="comment"># 路径</span></span><br><span class="line">        img_file = os.path.join(datadir, <span class="string">&#x27;images&#x27;</span>, fid + <span class="string">&#x27;.jpeg&#x27;</span>) <span class="comment"># 序号--&gt;图片</span></span><br><span class="line">        tree = ET.parse(fpath) <span class="comment"># 读取xml文件</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tree.find(<span class="string">&#x27;id&#x27;</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            im_id = np.array([ct])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            im_id = np.array([<span class="built_in">int</span>(tree.find(<span class="string">&#x27;id&#x27;</span>).text)])</span><br><span class="line"></span><br><span class="line">        objs = tree.findall(<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">        im_w = <span class="built_in">float</span>(tree.find(<span class="string">&#x27;size&#x27;</span>).find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">        im_h = <span class="built_in">float</span>(tree.find(<span class="string">&#x27;size&#x27;</span>).find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line">        gt_bbox = np.zeros((<span class="built_in">len</span>(objs), <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        gt_class = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        is_crowd = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        difficult = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, obj <span class="keyword">in</span> <span class="built_in">enumerate</span>(objs):</span><br><span class="line">            cname = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">            gt_class[i] = cname2cid[cname]</span><br><span class="line">            _difficult = <span class="built_in">int</span>(obj.find(<span class="string">&#x27;difficult&#x27;</span>).text)</span><br><span class="line">            x1 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;xmin&#x27;</span>).text)</span><br><span class="line">            y1 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;ymin&#x27;</span>).text)</span><br><span class="line">            x2 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;xmax&#x27;</span>).text)</span><br><span class="line">            y2 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;ymax&#x27;</span>).text)</span><br><span class="line">            x1 = <span class="built_in">max</span>(<span class="number">0</span>, x1)</span><br><span class="line">            y1 = <span class="built_in">max</span>(<span class="number">0</span>, y1)</span><br><span class="line">            x2 = <span class="built_in">min</span>(im_w - <span class="number">1</span>, x2)</span><br><span class="line">            y2 = <span class="built_in">min</span>(im_h - <span class="number">1</span>, y2)</span><br><span class="line">            <span class="comment"># 这里将原二点表示转换为xywh格式，使用xywh格式来表示目标物体真实框</span></span><br><span class="line">            gt_bbox[i] = [(x1+x2)/<span class="number">2.0</span> , (y1+y2)/<span class="number">2.0</span>, x2-x1+<span class="number">1.</span>, y2-y1+<span class="number">1.</span>]</span><br><span class="line">            is_crowd[i] = <span class="number">0</span></span><br><span class="line">            difficult[i] = _difficult</span><br><span class="line"></span><br><span class="line">        voc_rec = &#123;</span><br><span class="line">            <span class="string">&#x27;im_file&#x27;</span>: img_file,</span><br><span class="line">            <span class="string">&#x27;im_id&#x27;</span>: im_id,</span><br><span class="line">            <span class="string">&#x27;h&#x27;</span>: im_h,</span><br><span class="line">            <span class="string">&#x27;w&#x27;</span>: im_w,</span><br><span class="line">            <span class="string">&#x27;is_crowd&#x27;</span>: is_crowd,</span><br><span class="line">            <span class="string">&#x27;gt_class&#x27;</span>: gt_class,</span><br><span class="line">            <span class="string">&#x27;gt_bbox&#x27;</span>: gt_bbox,</span><br><span class="line">            <span class="string">&#x27;gt_poly&#x27;</span>: [],</span><br><span class="line">            <span class="string">&#x27;difficult&#x27;</span>: difficult</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(objs) != <span class="number">0</span>:</span><br><span class="line">            records.append(voc_rec)</span><br><span class="line">        ct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> records</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_bbox</span>(<span class="params">gt_bbox, gt_class</span>):</span><br><span class="line">    <span class="comment"># 对于一般的检测任务来说，一张图片上往往会有多个目标物体</span></span><br><span class="line">    <span class="comment"># 设置参数MAX_NUM = 50， 即一张图片最多取50个真实框；</span></span><br><span class="line">    <span class="comment"># 如果真实框的数目少于50个，则将不足部分的gt_bbox, gt_class和gt_score的各项数值全设置为0</span></span><br><span class="line">    MAX_NUM = <span class="number">50</span></span><br><span class="line">    gt_bbox2 = np.zeros((MAX_NUM, <span class="number">4</span>))</span><br><span class="line">    gt_class2 = np.zeros((MAX_NUM,))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gt_bbox)):</span><br><span class="line">        gt_bbox2[i, :] = gt_bbox[i, :]</span><br><span class="line">        gt_class2[i] = gt_class[i]</span><br><span class="line">        <span class="keyword">if</span> i &gt;= MAX_NUM:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> gt_bbox2, gt_class2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_data_from_file</span>(<span class="params">record</span>):</span><br><span class="line">    im_file = record[<span class="string">&#x27;im_file&#x27;</span>]</span><br><span class="line">    h = record[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">    w = record[<span class="string">&#x27;w&#x27;</span>]</span><br><span class="line">    is_crowd = record[<span class="string">&#x27;is_crowd&#x27;</span>]</span><br><span class="line">    gt_class = record[<span class="string">&#x27;gt_class&#x27;</span>]</span><br><span class="line">    gt_bbox = record[<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">    difficult = record[<span class="string">&#x27;difficult&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    img = cv2.imread(im_file)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 机器校验</span></span><br><span class="line">    <span class="keyword">assert</span> img.shape[<span class="number">0</span>] == <span class="built_in">int</span>(h), \</span><br><span class="line">             <span class="string">&quot;image height of &#123;&#125; inconsistent in record(&#123;&#125;) and img file(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">               im_file, h, img.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> img.shape[<span class="number">1</span>] == <span class="built_in">int</span>(w), \</span><br><span class="line">             <span class="string">&quot;image width of &#123;&#125; inconsistent in record(&#123;&#125;) and img file(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">               im_file, w, img.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    gt_boxes, gt_labels = get_bbox(gt_bbox, gt_class)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gt_bbox 用相对值</span></span><br><span class="line">    gt_boxes[:, <span class="number">0</span>] = gt_boxes[:, <span class="number">0</span>] / <span class="built_in">float</span>(w)</span><br><span class="line">    gt_boxes[:, <span class="number">1</span>] = gt_boxes[:, <span class="number">1</span>] / <span class="built_in">float</span>(h)</span><br><span class="line">    gt_boxes[:, <span class="number">2</span>] = gt_boxes[:, <span class="number">2</span>] / <span class="built_in">float</span>(w)</span><br><span class="line">    gt_boxes[:, <span class="number">3</span>] = gt_boxes[:, <span class="number">3</span>] / <span class="built_in">float</span>(h)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> img, gt_boxes, gt_labels, (h, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义可视化函数，用于对比原图和图像增强的效果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize</span>(<span class="params">srcimg, img_enhance</span>):</span><br><span class="line">    plt.figure(num=<span class="number">2</span>, figsize=(<span class="number">6</span>,<span class="number">12</span>))</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Src Image&#x27;</span>, color=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 不显示坐标轴</span></span><br><span class="line">    plt.imshow(srcimg) <span class="comment"># 显示原图片</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对原图做随机改变亮暗、对比度和颜色等数据增强</span></span><br><span class="line">    srcimg_gtbox = records[<span class="number">0</span>][<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">    srcimg_label = records[<span class="number">0</span>][<span class="string">&#x27;gt_class&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Enhance Image&#x27;</span>, color=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 不显示坐标轴</span></span><br><span class="line">    plt.imshow(img_enhance/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机改变亮暗、对比度和颜色等</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_distort</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="comment"># 随机改变亮度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_brightness</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Brightness(img).enhance(e)</span><br><span class="line">    <span class="comment"># 随机改变对比度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_contrast</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Contrast(img).enhance(e)</span><br><span class="line">    <span class="comment"># 随机改变颜色</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_color</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Color(img).enhance(e)</span><br><span class="line"></span><br><span class="line">    ops = [random_brightness, random_contrast, random_color]</span><br><span class="line">    np.random.shuffle(ops)</span><br><span class="line"></span><br><span class="line">    img = Image.fromarray(img)</span><br><span class="line">    img = ops[<span class="number">0</span>](img)</span><br><span class="line">    img = ops[<span class="number">1</span>](img)</span><br><span class="line">    img = ops[<span class="number">2</span>](img)</span><br><span class="line">    img = np.asarray(img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机填充</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_expand</span>(<span class="params">img,</span></span><br><span class="line"><span class="params">                  gtboxes,</span></span><br><span class="line"><span class="params">                  max_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">                  fill=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                  keep_ratio=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                  thresh=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &gt; thresh:</span><br><span class="line">        <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_ratio &lt; <span class="number">1.0</span>:</span><br><span class="line">        <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line">    h, w, c = img.shape</span><br><span class="line">    ratio_x = random.uniform(<span class="number">1</span>, max_ratio)</span><br><span class="line">    <span class="keyword">if</span> keep_ratio:</span><br><span class="line">        ratio_y = ratio_x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ratio_y = random.uniform(<span class="number">1</span>, max_ratio)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过缩放与平移实现填充的目的</span></span><br><span class="line">    oh = <span class="built_in">int</span>(h * ratio_y)</span><br><span class="line">    ow = <span class="built_in">int</span>(w * ratio_x)</span><br><span class="line">    off_x = random.randint(<span class="number">0</span>, ow - w)</span><br><span class="line">    off_y = random.randint(<span class="number">0</span>, oh - h)</span><br><span class="line"></span><br><span class="line">    out_img = np.zeros((oh, ow, c))</span><br><span class="line">    <span class="keyword">if</span> fill <span class="keyword">and</span> <span class="built_in">len</span>(fill) == c:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(c):</span><br><span class="line">            out_img[:, :, i] = fill[i] * <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">    out_img[off_y:off_y + h, off_x:off_x + w, :] = img</span><br><span class="line">    gtboxes[:, <span class="number">0</span>] = ((gtboxes[:, <span class="number">0</span>] * w) + off_x) / <span class="built_in">float</span>(ow)</span><br><span class="line">    gtboxes[:, <span class="number">1</span>] = ((gtboxes[:, <span class="number">1</span>] * h) + off_y) / <span class="built_in">float</span>(oh)</span><br><span class="line">    gtboxes[:, <span class="number">2</span>] = gtboxes[:, <span class="number">2</span>] / ratio_x</span><br><span class="line">    gtboxes[:, <span class="number">3</span>] = gtboxes[:, <span class="number">3</span>] / ratio_y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_img.astype(<span class="string">&#x27;uint8&#x27;</span>), gtboxes <span class="comment"># 返回填充后的图片与边界框</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机翻转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_flip</span>(<span class="params">img, gtboxes, thresh=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &gt; thresh:</span><br><span class="line">        <span class="comment"># 调整步长</span></span><br><span class="line">        img = img[:, ::-<span class="number">1</span>, :]</span><br><span class="line">        gtboxes[:, <span class="number">0</span>] = <span class="number">1.0</span> - gtboxes[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱真实框排列顺序</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffle_gtbox</span>(<span class="params">gtbox, gtlabel</span>):</span><br><span class="line">    gt = np.concatenate([gtbox, gtlabel[:, np.newaxis]], axis=<span class="number">1</span>)</span><br><span class="line">    idx = np.arange(gt.shape[<span class="number">0</span>])</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line">    gt = gt[idx, :]</span><br><span class="line">    <span class="keyword">return</span> gt[:, :<span class="number">4</span>], gt[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机缩放</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_interp</span>(<span class="params">img, size, interp=<span class="literal">None</span></span>):</span><br><span class="line">    interp_method = [cv2.INTER_NEAREST,   <span class="comment"># 最近邻插值</span></span><br><span class="line">                    cv2.INTER_LINEAR,     <span class="comment"># 线性插值</span></span><br><span class="line">                    cv2.INTER_AREA,       <span class="comment"># 区域插值</span></span><br><span class="line">                    cv2.INTER_CUBIC,      <span class="comment"># 三次样条插值</span></span><br><span class="line">                    cv2.INTER_LANCZOS4]   <span class="comment"># LANCZOS4插值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机选择缩放时的插值方法</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> interp <span class="keyword">or</span> interp <span class="keyword">not</span> <span class="keyword">in</span> interp_method:</span><br><span class="line">        interp = interp_method[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(interp_method) - <span class="number">1</span>)]</span><br><span class="line">    h, w, _ = img.shape</span><br><span class="line">    im_scale_x = size / <span class="built_in">float</span>(w)</span><br><span class="line">    im_scale_y = size / <span class="built_in">float</span>(h)</span><br><span class="line">    <span class="comment"># 调整图像大小</span></span><br><span class="line">    img = cv2.resize(img, <span class="literal">None</span>, <span class="literal">None</span>, fx=im_scale_x, fy=im_scale_y, interpolation=interp)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像增广方法汇总</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_augment</span>(<span class="params">img, gtboxes, gtlabels, size, means=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 随机改变亮暗、对比度和颜色等</span></span><br><span class="line">    img = random_distort(img)</span><br><span class="line">    <span class="comment"># 随机填充</span></span><br><span class="line">    img, gtboxes = random_expand(img, gtboxes, fill=means)</span><br><span class="line">    <span class="comment"># 随机缩放</span></span><br><span class="line">    img = random_interp(img, size)</span><br><span class="line">    <span class="comment"># 随机翻转</span></span><br><span class="line">    img, gtboxes = random_flip(img, gtboxes)</span><br><span class="line">    <span class="comment"># 随机打乱真实框排列顺序</span></span><br><span class="line">    gtboxes, gtlabels = shuffle_gtbox(gtboxes, gtlabels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img.astype(<span class="string">&#x27;float32&#x27;</span>), gtboxes.astype(<span class="string">&#x27;float32&#x27;</span>), gtlabels.astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_data</span>(<span class="params">record, size=<span class="number">640</span></span>):</span><br><span class="line">    img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)</span><br><span class="line">    img, gt_boxes, gt_labels = image_augment(img, gt_boxes, gt_labels, size)</span><br><span class="line">    mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">    std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">    img = img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> img, gt_boxes, gt_labels, scales</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一个批次内样本随机缩放的尺寸</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_size</span>(<span class="params">mode</span>):</span><br><span class="line">    <span class="keyword">if</span> (mode == <span class="string">&#x27;train&#x27;</span>) <span class="keyword">or</span> (mode == <span class="string">&#x27;valid&#x27;</span>):</span><br><span class="line">        inds = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">        ii = np.random.choice(inds)</span><br><span class="line">        img_size = <span class="number">320</span> + ii * <span class="number">32</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_size = <span class="number">608</span></span><br><span class="line">    <span class="keyword">return</span> img_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将list形式的batch数据转化成多个array构成的tuple</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_array</span>(<span class="params">batch_data</span>):</span><br><span class="line">    img_array = np.array([item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    gt_box_array = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    gt_labels_array = np.array([item[<span class="number">2</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    img_scale = np.array([item[<span class="number">3</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> img_array, gt_box_array, gt_labels_array, img_scale</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据读取类，继承Paddle.io.Dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, datadir, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        self.datadir = datadir</span><br><span class="line">        cname2cid = get_insect_names()</span><br><span class="line">        self.records = get_annotations(cname2cid, datadir)</span><br><span class="line">        self.img_size = <span class="number">640</span>  <span class="comment"># get_img_size(mode)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        record = self.records[idx]</span><br><span class="line">        <span class="comment"># print(&quot;print: &quot;, record)</span></span><br><span class="line">        img, gt_bbox, gt_labels, im_shape = get_img_data(record, size=self.img_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, gt_bbox, gt_labels, np.array(im_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.records)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 list形式的batch数据 转化成多个array构成的tuple</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_test_array</span>(<span class="params">batch_data</span>):</span><br><span class="line">    img_name_array = np.array([item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data])</span><br><span class="line">    img_data_array = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    img_scale_array = np.array([item[<span class="number">2</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> img_name_array, img_data_array, img_scale_array</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据读取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_data_loader</span>(<span class="params">datadir, batch_size= <span class="number">10</span>, test_image_size=<span class="number">608</span>, mode=<span class="string">&#x27;test&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载测试用的图片，测试数据没有groundtruth标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image_names = os.listdir(datadir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        batch_data = []</span><br><span class="line">        img_size = test_image_size</span><br><span class="line">        <span class="keyword">for</span> image_name <span class="keyword">in</span> image_names:</span><br><span class="line">            file_path = os.path.join(datadir, image_name)</span><br><span class="line">            img = cv2.imread(file_path)</span><br><span class="line">            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">            H = img.shape[<span class="number">0</span>]</span><br><span class="line">            W = img.shape[<span class="number">1</span>]</span><br><span class="line">            img = cv2.resize(img, (img_size, img_size))</span><br><span class="line"></span><br><span class="line">            mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">            std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">            mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">            std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">            out_img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">            out_img = out_img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            img = out_img <span class="comment">#np.transpose(out_img, (2,0,1))</span></span><br><span class="line">            im_shape = [H, W]</span><br><span class="line"></span><br><span class="line">            batch_data.append((image_name.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>], img, im_shape))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_data) == batch_size:</span><br><span class="line">                <span class="keyword">yield</span> make_test_array(batch_data)</span><br><span class="line">                batch_data = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_data) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">yield</span> make_test_array(batch_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><pre><code>开始解压......解压完成</code></pre><h1 id="二、单阶段目标检测模型YOLOv3"><a href="#二、单阶段目标检测模型YOLOv3" class="headerlink" title="二、单阶段目标检测模型YOLOv3"></a>二、单阶段目标检测模型YOLOv3</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><strong>1. 概述</strong></h2><p>经典的R-CNN系列算法也被称为<strong>两阶段目标检测算法</strong>，由于这种方法需要<strong>先产生候选区域，再对候选区域做分类和位置坐标的预测</strong>，因此算法速度非常慢，这类算法被称为两阶段目标检测算法。与此对应的是以YOLO算法为代表的<strong>单阶段检测算法</strong>，只需要一个网络即可<strong>同时产生候选区域并预测出物体的类别和位置坐标</strong>。</p><p>与R-CNN系列算法不同，YOLOv3使用单个网络结构，在产生候选区域的同时即可预测出物体类别和位置，不需要分成两阶段来完成检测任务。另外，YOLOv3算法产生的预测框数目比Faster R-CNN少很多。Faster R-CNN中每个真实框可能对应多个标签为正的候选区域，而YOLOv3里面每个真实框只对应一个正的候选区域。这些特性使得YOLOv3算法具有更快的速度，能到达实时响应的水平。</p><p>Joseph Redmon等人在2015年提出YOLO（You Only Look Once，YOLO）算法，通常也被称为YOLOv1；2016年，他们对算法进行改进，又提出YOLOv2版本；2018年发展出YOLOv3版本。<a href="https://zhuanlan.zhihu.com/p/539932517">[YOLO家族进化史（v1-v7）]</a></p><h2 id="2-YOLOv3模型设计思想"><a href="#2-YOLOv3模型设计思想" class="headerlink" title="2. YOLOv3模型设计思想"></a><strong>2. YOLOv3模型设计思想</strong></h2><p>YOLOv3算法的基本思想可以分成两部分：</p><p><strong>在训练阶段</strong>：</p><ol><li>按一定规则在图片上产生一系列的候选区域，然后根据这些候选区域与图片上物体真实框之间的位置关系对候选区域进行标注。</li></ol><ul><li>跟真实框足够接近的那些候选区域会被标注为正样本，同时将真实框的位置作为正样本的位置目标。</li><li>偏离真实框较大的那些候选区域则会被标注为负样本，负样本不需要预测位置或者类别。</li></ul><ol><li>使用卷积神经网络提取图片特征并对候选区域的位置和类别进行预测。这样每个预测框就可以看成是一个样本，根据真实框相对它的位置和类别进行了标注而获得标签值，通过网络模型预测其位置和类别，将网络预测值和标签值进行比较，就可以建立起损失函数。</li></ol><p><strong>在预测阶段</strong>：计算预测框得分和位置，然后使用非极大值抑制消除重合较大的框，得到最终结果。</p><p>YOLOv3的算法流程如 <strong>图1</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/46fbb2f9d1734a10a79d342dd443a4a0ec9c362af417484a972d7ee9525d0c8c" width = "900"></center><center><br>图1：YOLOv3的算法流程图</br></center><p><br></br></p><h2 id="3-YOLOv3模型训练"><a href="#3-YOLOv3模型训练" class="headerlink" title="3. YOLOv3模型训练"></a><strong>3. YOLOv3模型训练</strong></h2><p>拆分来看，YOLOv3算法的训练流程可以分成两部分，如 <strong>图2</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/362b1599a9bf403c982ccf76b6f35d23ead0a520921b4e84a6952181d6692d40" width = "800"></center><center><br>图2：YOLOv3算法训练流程图 </br></center><p><br></br></p><ul><li><strong>图2</strong> 左边是输入图片，上半部分所示的过程是使用卷积神经网络对图片提取特征，随着网络不断向前传播，特征图的尺寸越来越小，每个像素点会代表更加抽象的特征模式，直到输出特征图，其尺寸减小为原图的$\frac{1}{32}$。</li><li><strong>图2</strong> 下半部分描述了生成候选区域的过程，首先将原图划分成多个小方块，每个小方块的大小是$32 \times 32$，然后以每个小方块为中心分别生成一系列锚框，整张图片都会被锚框覆盖到。在每个锚框的基础上产生一个与之对应的预测框，根据锚框和预测框与图片上物体真实框之间的位置关系，对这些预测框进行标注。</li><li>将上方支路中输出的特征图与下方支路中产生的预测框标签建立关联，创建损失函数，开启端到端的训练过程。</li></ul><p>我们可以依据上面的图进一步对训练流程进行拆解，总结出实现方案：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/0321f37cc92940888349662ad4526c95c3498bfa4a094f44a80650fe4147c5de" width = "800"></center><center><br>图3：训练流程问题拆解 </br></center><p><br></br></p><p>接下来具体介绍流程中各节点的算法原理。</p><h3 id="3-1-产生候选区域"><a href="#3-1-产生候选区域" class="headerlink" title="3.1 产生候选区域"></a><strong>3.1 产生候选区域</strong></h3><p>如何产生候选区域，是检测模型的核心设计方案。目前大多数基于卷积神经网络的模型所采用的方式大体如下：</p><ol><li>按一定的规则在图片上生成一系列位置固定的锚框，将这些锚框看作是可能的候选区域。</li><li>对锚框是否包含目标物体进行预测，如果包含目标物体，还需要预测所包含物体的类别，以及预测框相对于锚框位置需要调整的幅度。</li></ol><p><strong>1）生成锚框</strong></p><p>将原始图片划分成$m\times n$个区域，如 <strong>图3</strong> 所示，原始图片高度$H=640$, 宽度$W=480$，如果我们选择小块区域的尺寸为$32 \times 32$，则$m$和$n$分别为：</p><script type="math/tex; mode=display">m = \frac{640}{32} = 20</script><script type="math/tex; mode=display">n = \frac{480}{32} = 15</script><p>也就是说，我们将原始图像分成了20行15列小方块区域。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/2dd1cbeb53644552a8cb38f3f834dbdda5046a489465454d93cdc88d1ce65ca5" width = "400"></center><center><br>图4：将图片划分成多个32x32的小方块 </br></center><p><br></br></p><p>YOLOv3算法会在每个区域的中心，生成一系列锚框。为了展示方便，我们仅在图中第十行第四列的小方块位置附近画出生成的锚框，如 <strong>图5</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/b9eca2257fc0432d9f59bdecc8d54d144dd590e990f54c6d82f60a1a1531915b" width = "400"></center><center><br>图5：在第10行第4列的小方块区域生成3个锚框 </br></center><p><br></br></p><hr><blockquote><p><strong>说明：</strong></p><p>这里为了跟程序中的编号对应，最上面的行号是第0行，最左边的列号是第0列。</p></blockquote><hr><p><strong>图6</strong> 展示在每个区域附近都生成3个锚框，很多锚框堆叠在一起可能不太容易看清楚，但过程跟上面类似，只是需要以每个区域的中心点为中心，分别生成3个锚框。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/0880c3b5ec2d40edb476f4fcbadd87aa9f37059cd24d4a1a9d37c627ce5f618a" width = "400"></center><center><br>图6：在每个小方块区域生成3个锚框 </br></center><p><br></br></p><p>在每个小方块区域都生成三个锚框，这会覆盖整张图片，在这张图片上一共生成了900个锚框。</p><p><strong>2）生成预测框</strong></p><p>在前面已经指出，锚框的位置都是固定好的，不可能刚好跟物体边界框重合，需要<strong>在锚框的基础上进行位置的微调以生成预测框</strong>。预测框相对于锚框会有不同的中心位置和大小，采用什么方式能得到预测框呢？我们先来考虑如何生成其中心位置坐标。</p><p>比如上面图中在第10行第4列的小方块区域中心生成的一个锚框，如绿色虚线框所示。以小方格的宽度为单位长度，</p><p>此小方块区域左上角的位置坐标是：</p><script type="math/tex; mode=display">c_x = 4\\c_y = 10</script><p>此锚框的区域中心坐标是：</p><script type="math/tex; mode=display">center\_x = c_x + 0.5 = 4.5\\center\_y = c_y + 0.5 = 10.5</script><p>可以通过下面的方式生成预测框的中心坐标：</p><script type="math/tex; mode=display">b_x = c_x + \sigma(t_x)</script><script type="math/tex; mode=display">b_y = c_y + \sigma(t_y)</script><p>其中$t_x$和$t_y$为实数，$\sigma(x)$是我们之前学过的Sigmoid函数，其定义如下：</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1 + exp(-x)}</script><p>由于Sigmoid的函数值在$0 \thicksim 1$之间，因此由上面公式计算出来的预测框的中心点总是落在第十行第四列的小区域内部。</p><p>当$t_x=t_y=0$时，$b_x = c_x + 0.5$，$b_y = c_y + 0.5$，预测框中心与锚框中心重合，都是小区域的中心。</p><p>锚框的大小是预先设定好的，在模型中可以当作是超参数，下图中画出的锚框尺寸是</p><script type="math/tex; mode=display">p_h = 350</script><script type="math/tex; mode=display">p_w = 250</script><p>通过下面的公式生成预测框的大小：</p><script type="math/tex; mode=display">b_h = p_h e^{t_h}</script><script type="math/tex; mode=display">b_w = p_w e^{t_w}</script><p>如果$t_x=t_y=0, t_h=t_w=0$，则预测框跟锚框重合。</p><p>如果给$t_x, t_y, t_h, t_w$随机赋值如下：</p><script type="math/tex; mode=display">t_x = 0.2,  t_y = 0.3, t_w = 0.1, t_h = -0.12</script><p>则可以得到预测框的坐标是(154.98, 357.44, 276.29, 310.42)，如 <strong>图7</strong> 中蓝色框所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/28ae0fbc087e42449cdea6c48ad53f64cffdf66216a646d884ae778f7c341149" width = "400"></center><center><br>图7：生成预测框 </br></center><p><br></br></p><hr><blockquote><p><strong>说明：</strong><br>这里坐标采用$xywh$的格式。预测框是在锚框的基础上进行的微调。</p></blockquote><hr><p>这里我们会问：当$t_x, t_y, t_w, t_h$取值为多少的时候，预测框能够跟真实框重合？为了回答问题，只需要将上面预测框坐标中的$b_x, b_y, b_h, b_w$设置为真实框的位置，即可求解出$t$的数值。</p><p>令：</p><script type="math/tex; mode=display">\sigma(t^*_x) + c_x = gt_x</script><script type="math/tex; mode=display">\sigma(t^*_y) + c_y = gt_y</script><script type="math/tex; mode=display">p_w e^{t^*_w} = gt_h</script><script type="math/tex; mode=display">p_h e^{t^*_h} = gt_w</script><p>可以求解出：$(t^<em>_x, t^</em>_y, t^<em>_w, t^</em>_h)$</p><p>如果$t$是网络预测的输出值，将$t^<em>$作为目标值，以他们之间的差距作为损失函数，则可以建立起一个回归问题，通过学习网络参数，使得$t$足够接近$t^</em>$，从而能够求解出预测框的位置坐标和大小。</p><p>预测框可以看作是在锚框基础上的一个微调，每个锚框会有一个跟它对应的预测框，我们需要确定上面计算式中的$t_x, t_y, t_w, t_h$，从而计算出与锚框对应的预测框的位置和形状。</p><p><strong>3）标注候选区域</strong></p><p>在YOLOv3中，每个区域会产生3种不同形状的锚框，每个锚框都是一个可能的候选区域，对这些候选区域我们需要了解如下几件事情：</p><ul><li>锚框是否包含物体，这可以看成是一个二分类问题，使用标签<code>objectness</code>来表示。</li></ul><p>当锚框包含了物体时，<code>objectness=1</code>，表示预测框属于正类；当锚框不包含物体时，设置<code>objectness=0</code>，表示锚框属于负类；</p><p>还有一种情况，有些预测框跟真实框之间的IoU很大，但并不是最大的那个，那么直接将其objectness标签设置为0当作负样本，可能并不妥当，为了避免这种情况，YOLOv3算法设置了一个IoU阈值<code>iou_threshold</code>，当预测框的objectness不为1，但是其与某个真实框的IoU大于iou_threshold时，就将其objectness标签设置为-1，不参与损失函数的计算。</p><ul><li><p>如果锚框包含了物体，那么就需要计算对应的预测框中心位置和大小应该是多少，或者说上文中的$t_x, t_y, t_w, t_h$应该是多少，使用<code>location</code>标签。</p></li><li><p>如果锚框包含了物体，那么就需要计算具体类别是什么，这里使用变量<code>label</code>来表示其所属类别的标签。</p></li></ul><p>总结起来，如 <strong>图8</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/3b914be0c6274916bc7abe4922d4d0fb75be340172764f7096af5be0c2737c57" width = "700"></center><center><br>图8：标注流程示意图 </br></center><p><br></br></p><p>选取任意一个锚框对它进行标注，也就是需要确定其对应的<code>objectness</code>，<code>(t_x, t_y, t_w, t_h)</code>和<code>label</code>，下面将分别讲述如何确定这三个标签的值。</p><p><br></p><p><strong>①标注锚框是否包含物体（objectness）</strong></p><p>如 <strong>图9</strong> 所示，这里一共有3个目标，以最左边的人像为例，其真实框是$(133.96, 328.42, 186.06, 374.63)$。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f21679e68d2b496698ed788a16d4ea2e5bc6f82b253a44ef9508b6a4fc9b6be4" width = "600"></center><center><br>图9：选出与真实框中心位于同一区域的锚框 </br></center><p><br></br></p><p>真实框的中心点坐标是：</p><script type="math/tex; mode=display">center\_x = 133.96\\center\_y = 328.42\\i = 133.96 / 32 = 4.18625\\j = 328.42 / 32 = 10.263125</script><p>它落在了第10行第4列的小方块内，如<strong>图10</strong>所示。此小方块区域可以生成3个不同形状的锚框，其在图上的编号和大小分别是$A_1(116, 90), A_2(156, 198), A_3(373, 326)$。</p><p>用这3个不同形状的锚框跟真实框计算IoU，选出IoU最大的锚框。这里为了简化计算，只考虑锚框的形状，不考虑其跟真实框中心之间的偏移，具体计算结果如 <strong>图14</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/3008337ea66c44068042c670db54368edc56b1e43ced4b6b811bdc95b64ca3d5" width = "400"></center><center><br>图10：选出与真实框与锚框的IoU </br></center><p><br></br></p><p>其中跟真实框IoU最大的是锚框$A_3$，形状是$(373, 326)$，将它所对应的预测框的objectness标签设置为1，其所包括的物体类别就是真实框里面的物体所属类别。</p><p>依次可以找出其他几个真实框对应的IoU最大的锚框，然后将它们的预测框的objectness标签也都设置为1。这里一共有$20 \times 15 \times 3 = 900$个锚框，只有3个预测框会被标注为正。</p><p>由于每个真实框只对应一个objectness标签为正的预测框，如果有些预测框跟真实框之间的IoU很大，但并不是最大的那个，那么直接将其objectness标签设置为0当作负样本，可能并不妥当。为了避免这种情况，YOLOv3算法设置了一个IoU阈值iou_threshold，当预测框的objectness不为1，但是其与某个真实框的IoU大于iou_threshold时，就将其objectness标签设置为-1，不参与损失函数的计算。</p><p>所有其他的预测框，其objectness标签均设置为0，表示负类。</p><p>对于objectness=1的预测框，需要进一步确定其位置和包含物体的具体分类标签，但是对于objectness=0或者-1的预测框，则不用管他们的位置和类别。</p><p><br></p><p><strong>②标注预测框的位置坐标标签（location）</strong></p><p>当锚框objectness=1时，需要确定预测框位置相对于它微调的幅度，也就是锚框的位置标签。</p><p>在前面我们已经问过这样一个问题：当$t_x, t_y, t_w, t_h$取值为多少的时候，预测框能够跟真实框重合？其做法是将预测框坐标中的$b_x, b_y, b_h, b_w$设置为真实框的坐标，即可求解出$t$的数值。</p><p>令：</p><script type="math/tex; mode=display">\sigma(t^*_x) + c_x = gt_x\\\sigma(t^*_y) + c_y = gt_y\\p_w e^{t^*_w} = gt_w\\p_h e^{t^*_h} = gt_h</script><p>对于$t_x^<em>$和$t_y^</em>$，由于Sigmoid的反函数不好计算，我们直接使用$\sigma(t^<em>_x)$和$\sigma(t^</em>_y)$作为回归的目标。</p><script type="math/tex; mode=display">d_x^* = \sigma(t^*_x) = gt_x - c_x\\d_y^* = \sigma(t^*_y) = gt_y - c_y\\t^*_w = log(\frac{gt_w}{p_w})\\t^*_h = log(\frac{gt_h}{p_h})</script><p>如果$(t_x, t_y, t_h, t_w)$是网络预测的输出值，将$(d_x^<em>, d_y^</em>, t_w^<em>, t_h^</em>)$作为$(\sigma(t_x), \sigma(t_y), t_h, t_w)$的目标值，以它们之间的差距作为损失函数，则可以建立起一个回归问题，通过学习网络参数，使得$t$足够接近$t^*$，从而能够求解出预测框的位置。</p><p><br></p><p><strong>③标注锚框包含物体类别的标签（label）</strong></p><p>对于objectness=1的锚框，需要确定其具体类别。正如上面所说，objectness标注为1的锚框，会有一个真实框跟它对应，该锚框所属物体类别，即是其所对应的真实框包含的物体类别。这里使用one-hot向量来表示类别标签label。比如一共有10个分类，而真实框里面包含的物体类别是第2类，则label为$(0,1,0,0,0,0,0,0,0,0)$</p><p><strong>4）标注锚框的具体程序</strong></p><p>最后，通过这种方式，我们在每个小方块区域都生成了一系列的锚框作为候选区域，并且根据图片上真实物体的位置，标注出了每个候选区域对应的objectness标签、位置需要调整的幅度以及包含的物体所属的类别。位置需要调整的幅度由4个变量描述$(t_x, t_y, t_w, t_h)$，objectness标签需要用一个变量描述$obj$，描述所属类别的变量长度等于类别数C。</p><p>对于每个锚框，模型需要预测输出$(t_x, t_y, t_w, t_h, P_{obj}, P_1, P_2,… , P_C)$，其中$P_{obj}$是锚框是否包含物体的概率，$P_1, P_2,… , P_C$则是锚框包含的物体属于每个类别的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标注预测框的objectness</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_objectness_label</span>(<span class="params">img, gt_boxes, gt_labels, iou_threshold = <span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                         anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span></span><br><span class="line"><span class="params">                         num_classes=<span class="number">7</span>, downsample=<span class="number">32</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    img：是输入的图像数据，形状是[N, C, H, W]</span></span><br><span class="line"><span class="string">    gt_boxes：真实框，维度是[N, 50, 4]，其中50是真实框数目的上限，真实框坐标格式是xywh，这里使用相对值</span></span><br><span class="line"><span class="string">    gt_labels：真实框所属类别，维度是[N, 50]</span></span><br><span class="line"><span class="string">    iou_threshold：当预测框与真实框的iou大于iou_threshold时不将其看作是负样本</span></span><br><span class="line"><span class="string">    anchors：锚框可选的尺寸</span></span><br><span class="line"><span class="string">    anchor_masks：通过与anchors一起确定本层级的特征图应该选用多大尺寸的锚框</span></span><br><span class="line"><span class="string">    num_classes：类别数目</span></span><br><span class="line"><span class="string">    downsample：特征图相对于输入网络的图片尺寸变化的比例</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    img_shape = img.shape</span><br><span class="line">    batchsize = img_shape[<span class="number">0</span>]</span><br><span class="line">    num_anchors = <span class="built_in">len</span>(anchors) // <span class="number">2</span><span class="comment"># 锚框的数目</span></span><br><span class="line">    input_h = img_shape[<span class="number">2</span>]</span><br><span class="line">    input_w = img_shape[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 将输入图片划分成num_rows x num_cols个小方块区域，每个小方块的边长是 downsample</span></span><br><span class="line">    <span class="comment"># 计算一共有多少行小方块</span></span><br><span class="line">    num_rows = input_h // downsample</span><br><span class="line">    <span class="comment"># 计算一共有多少列小方块</span></span><br><span class="line">    num_cols = input_w // downsample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对象标签 3个</span></span><br><span class="line">    label_objectness = np.zeros([batchsize, num_anchors, num_rows, num_cols])</span><br><span class="line">    <span class="comment"># 类别标签 每个锚框7个类别</span></span><br><span class="line">    label_classification = np.zeros([batchsize, num_anchors, num_classes, num_rows, num_cols])</span><br><span class="line">    <span class="comment"># 位置坐标标签 每个锚框对应4个坐标值</span></span><br><span class="line">    label_location = np.zeros([batchsize, num_anchors, <span class="number">4</span>, num_rows, num_cols])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用来调节不同尺寸的锚框对损失函数的贡献，作为加权系数和位置损失函数相乘</span></span><br><span class="line">    scale_location = np.ones([batchsize, num_anchors, num_rows, num_cols])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对batchsize进行循环，依次处理每张图片</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(batchsize):</span><br><span class="line">        <span class="comment"># 对图片上的真实框进行循环，依次找出跟真实框形状最匹配的锚框</span></span><br><span class="line">        <span class="keyword">for</span> n_gt <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gt_boxes[n])):</span><br><span class="line">            gt = gt_boxes[n][n_gt] <span class="comment"># 真实框</span></span><br><span class="line">            gt_cls = gt_labels[n][n_gt] <span class="comment"># 真实框所属类别</span></span><br><span class="line">            gt_center_x = gt[<span class="number">0</span>]</span><br><span class="line">            gt_center_y = gt[<span class="number">1</span>]</span><br><span class="line">            gt_width = gt[<span class="number">2</span>]</span><br><span class="line">            gt_height = gt[<span class="number">3</span>]</span><br><span class="line">            <span class="keyword">if</span> (gt_width &lt; <span class="number">1e-3</span>) <span class="keyword">or</span> (gt_height &lt; <span class="number">1e-3</span>):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            i = <span class="built_in">int</span>(gt_center_y * num_rows)</span><br><span class="line">            j = <span class="built_in">int</span>(gt_center_x * num_cols)</span><br><span class="line">            ious = []</span><br><span class="line">            <span class="keyword">for</span> ka <span class="keyword">in</span> <span class="built_in">range</span>(num_anchors):</span><br><span class="line">                bbox1 = [<span class="number">0.</span>, <span class="number">0.</span>, <span class="built_in">float</span>(gt_width), <span class="built_in">float</span>(gt_height)] <span class="comment"># 真实框</span></span><br><span class="line">                anchor_w = anchors[ka * <span class="number">2</span>]</span><br><span class="line">                anchor_h = anchors[ka * <span class="number">2</span> + <span class="number">1</span>]</span><br><span class="line">                bbox2 = [<span class="number">0.</span>, <span class="number">0.</span>, anchor_w/<span class="built_in">float</span>(input_w), anchor_h/<span class="built_in">float</span>(input_h)] <span class="comment"># 锚框</span></span><br><span class="line">                <span class="comment"># 计算iou</span></span><br><span class="line">                iou = box_iou_xywh(bbox1, bbox2)</span><br><span class="line">                ious.append(iou)</span><br><span class="line">            ious = np.array(ious)</span><br><span class="line">            inds = np.argsort(ious) <span class="comment"># 生成把ious从小到大排序后的索引</span></span><br><span class="line">            k = inds[-<span class="number">1</span>]</span><br><span class="line">            label_objectness[n, k, i, j] = <span class="number">1</span></span><br><span class="line">            c = gt_cls</span><br><span class="line">            label_classification[n, k, c, i, j] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># for those prediction bbox with objectness =1, set label of location</span></span><br><span class="line">            dx_label = gt_center_x * num_cols - j</span><br><span class="line">            dy_label = gt_center_y * num_rows - i</span><br><span class="line">            dw_label = np.log(gt_width * input_w / anchors[k*<span class="number">2</span>])</span><br><span class="line">            dh_label = np.log(gt_height * input_h / anchors[k*<span class="number">2</span> + <span class="number">1</span>])</span><br><span class="line">            label_location[n, k, <span class="number">0</span>, i, j] = dx_label</span><br><span class="line">            label_location[n, k, <span class="number">1</span>, i, j] = dy_label</span><br><span class="line">            label_location[n, k, <span class="number">2</span>, i, j] = dw_label</span><br><span class="line">            label_location[n, k, <span class="number">3</span>, i, j] = dh_label</span><br><span class="line">            <span class="comment"># scale_location用来调节不同尺寸的锚框对损失函数的贡献，作为加权系数和位置损失函数相乘</span></span><br><span class="line">            scale_location[n, k, i, j] = <span class="number">2.0</span> - gt_width * gt_height <span class="comment"># scales--&gt;(h, w)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 目前根据每张图片上所有出现过的gt_box，都标注出了objectness为正的预测框，剩下的预测框则默认objectness为0</span></span><br><span class="line">    <span class="comment"># 对于objectness为1的预测框，标出了他们所包含的物体类别，以及位置回归的目标</span></span><br><span class="line">    <span class="keyword">return</span> label_objectness.astype(<span class="string">&#x27;float32&#x27;</span>), \</span><br><span class="line">            label_location.astype(<span class="string">&#x27;float32&#x27;</span>), \</span><br><span class="line">            label_classification.astype(<span class="string">&#x27;float32&#x27;</span>),\</span><br><span class="line">            scale_location.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算IoU，矩形框的坐标形式为xywh</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou_xywh</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    x1min, y1min = box1[<span class="number">0</span>] - box1[<span class="number">2</span>]/<span class="number">2.0</span>, box1[<span class="number">1</span>] - box1[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    x1max, y1max = box1[<span class="number">0</span>] + box1[<span class="number">2</span>]/<span class="number">2.0</span>, box1[<span class="number">1</span>] + box1[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    s1 = box1[<span class="number">2</span>] * box1[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    x2min, y2min = box2[<span class="number">0</span>] - box2[<span class="number">2</span>]/<span class="number">2.0</span>, box2[<span class="number">1</span>] - box2[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    x2max, y2max = box2[<span class="number">0</span>] + box2[<span class="number">2</span>]/<span class="number">2.0</span>, box2[<span class="number">1</span>] + box2[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    s2 = box2[<span class="number">2</span>] * box2[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    xmin = np.maximum(x1min, x2min)</span><br><span class="line">    ymin = np.maximum(y1min, y2min)</span><br><span class="line">    xmax = np.minimum(x1max, x2max)</span><br><span class="line">    ymax = np.minimum(y1max, y2max)</span><br><span class="line">    inter_h = np.maximum(ymax - ymin, <span class="number">0.</span>)</span><br><span class="line">    inter_w = np.maximum(xmax - xmin, <span class="number">0.</span>)</span><br><span class="line">    intersection = inter_h * inter_w</span><br><span class="line"></span><br><span class="line">    union = s1 + s2 - intersection</span><br><span class="line">    iou = intersection / union</span><br><span class="line">    <span class="keyword">return</span> iou </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TRAINDIR = <span class="string">&#x27;/home/aistudio/work/insects/train&#x27;</span></span><br><span class="line">TESTDIR = <span class="string">&#x27;/home/aistudio/work/insects/test&#x27;</span></span><br><span class="line">VALIDDIR = <span class="string">&#x27;/home/aistudio/work/insects/val&#x27;</span></span><br><span class="line">train_dataset = TrainDataset(TRAINDIR, mode=<span class="string">&#x27;train&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line">reader = paddle.io.DataLoader(</span><br><span class="line">                            train_dataset, </span><br><span class="line">                            batch_size=<span class="number">2</span>, </span><br><span class="line">                            shuffle=<span class="literal">True</span>, </span><br><span class="line">                            num_workers=<span class="number">1</span>, </span><br><span class="line">                            drop_last=<span class="literal">True</span>)</span><br><span class="line">img, gt_boxes, gt_labels, im_shape = <span class="built_in">next</span>(reader())</span><br><span class="line">img, gt_boxes, gt_labels, im_shape = img.numpy(), gt_boxes.numpy(), gt_labels.numpy(), im_shape.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算出锚框对应的标签</span></span><br><span class="line">label_objectness, \</span><br><span class="line">label_location, \</span><br><span class="line">label_classification, \</span><br><span class="line">scale_location \</span><br><span class="line">= get_objectness_label(img, gt_boxes, gt_labels, </span><br><span class="line">                        iou_threshold = <span class="number">0.7</span>,</span><br><span class="line">                        anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span><br><span class="line">                        num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img.shape, gt_boxes.shape, gt_labels.shape, im_shape.shape</span><br></pre></td></tr></table></figure><pre><code>((2, 3, 640, 640), (2, 50, 4), (2, 50), (2, 2))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (批量，锚框数，偏移量/类别数，h，w)</span></span><br><span class="line">label_objectness.shape, label_location.shape, label_classification.shape, scale_location.shape</span><br></pre></td></tr></table></figure><pre><code>((2, 3, 20, 20), (2, 3, 4, 20, 20), (2, 3, 7, 20, 20), (2, 3, 20, 20))</code></pre><p>上面的程序实现了对锚框进行标注，对于每个真实框，选出了与它形状最匹配的锚框，将其objectness标注为1，并且将$[d_x^<em>, d_y^</em>, t_h^<em>, t_w^</em>]$作为正样本位置的标签，真实框包含的物体类别作为锚框的类别。而其余的锚框，objectness将被标注为0，无需标注出位置和类别的标签。</p><p><strong>注意</strong>：这里还遗留一个小问题，前面我们说了对于与真实框IoU较大的那些锚框，需要将其objectness标注为-1，不参与损失函数的计算。我们先将这个问题放一放，等到后面建立损失函数的时候再补上。</p><h3 id="3-2-卷积网络提取特征"><a href="#3-2-卷积网络提取特征" class="headerlink" title="3.2 卷积网络提取特征"></a><strong>3.2 卷积网络提取特征</strong></h3><p>在上一节图像分类的课程中，我们已经学习过了通过卷积神经网络提取图像特征。通过连续使用多层卷积和池化等操作，能得到语义含义更加丰富的特征图。在检测问题中，也使用卷积神经网络逐层提取图像特征，通过最终的输出特征图来表征物体位置和类别等信息。</p><p><strong>骨干网络[backbone]</strong></p><p>YOLOv3算法使用的骨干网络是<a href="https://blog.csdn.net/weixin_48167570/article/details/120688156">Darknet53</a>。Darknet53网络的具体结构如 <strong>图11</strong> 所示：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d5cb1e88d3f44259be1427a90ee454a57738ee8083ad40269f5485988526f30d" width = "400"></center><center><br>图11：Darknet53网络结构 </br></center><p><br></br></p><p>在检测任务中，将图中C0后面的平均池化、全连接层和Softmax去掉，保留从输入到C0部分的网络结构，作为检测模型的基础网络结构，也称为骨干网络。YOLOv3模型会在骨干网络的基础上，再添加检测相关的网络模块。</p><hr><blockquote><ul><li><strong>名词解释</strong>：特征图的步幅</li></ul><p>在提取特征的过程中通常会使用步幅大于1的卷积或者池化，导致后面的特征图尺寸越来越小，<strong>特征图的步幅等于输入图片尺寸除以特征图尺寸</strong>。</p><p>例如：C0的尺寸是$20\times20$，原图尺寸是$640\times640$，则C0的步幅是$\frac{640}{20}=32$。同理，C1的步幅是16，C2的步幅是8。</p></blockquote><hr><p>下面的程序是Darknet53骨干网络的实现代码，这里将上图中C0、C1、C2所表示的输出数据取出，并查看它们的形状分别是，$C0 [1, 1024, 20, 20]$，$C1 [1, 512, 40, 40]$，$C2 [1, 256, 80, 80]$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将卷积和批归一化封装为ConvBNLayer，方便后续复用</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNLayer</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span>, padding=<span class="number">0</span>, act=<span class="string">&quot;leaky&quot;</span></span>):</span><br><span class="line">        <span class="comment"># act为激活函数</span></span><br><span class="line">        <span class="comment"># kernel_size=3 表示使用了3×3的卷积核</span></span><br><span class="line">        <span class="comment"># stride=1 表示步幅为1</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 创建卷积层</span></span><br><span class="line">        self.conv = paddle.nn.Conv2D(</span><br><span class="line">            in_channels=ch_in,</span><br><span class="line">            out_channels=ch_out,</span><br><span class="line">            kernel_size=kernel_size,</span><br><span class="line">            stride=stride,</span><br><span class="line">            padding=padding,</span><br><span class="line">            groups=groups,</span><br><span class="line">            <span class="comment"># 随机正态（高斯）分布初始化函数</span></span><br><span class="line">            weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Normal(<span class="number">0.</span>, <span class="number">0.02</span>)),</span><br><span class="line">            bias_attr=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 创建批归一化层</span></span><br><span class="line">        self.batch_norm = paddle.nn.BatchNorm2D(</span><br><span class="line">            num_features=ch_out,</span><br><span class="line">            weight_attr=paddle.ParamAttr(</span><br><span class="line">                <span class="comment"># 将参数随机初始化</span></span><br><span class="line">                initializer=paddle.nn.initializer.Normal(<span class="number">0.</span>, <span class="number">0.02</span>),</span><br><span class="line">                <span class="comment"># L2 权重衰减正则化，默认正则化系数为0.</span></span><br><span class="line">                regularizer=paddle.regularizer.L2Decay(<span class="number">0.</span>)),</span><br><span class="line">            bias_attr=paddle.ParamAttr(</span><br><span class="line">                initializer=paddle.nn.initializer.Constant(<span class="number">0.0</span>),</span><br><span class="line">                regularizer=paddle.regularizer.L2Decay(<span class="number">0.</span>)))</span><br><span class="line">        self.act = act</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):<span class="comment"># 输入传入</span></span><br><span class="line">        out = self.conv(inputs)<span class="comment"># 卷积</span></span><br><span class="line">        out = self.batch_norm(out)<span class="comment"># 批量归一化</span></span><br><span class="line">        <span class="keyword">if</span> self.act == <span class="string">&#x27;leaky&#x27;</span>:</span><br><span class="line">            out = F.leaky_relu(x=out, negative_slope=<span class="number">0.1</span>)<span class="comment"># 放入激活函数</span></span><br><span class="line">        <span class="keyword">return</span> out<span class="comment"># 输出结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义下采样模块，使图片尺寸减半，每次运行完后高宽减半</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DownSample</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 ch_in,</span></span><br><span class="line"><span class="params">                 ch_out,</span></span><br><span class="line"><span class="params">                 kernel_size=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">                 padding=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(DownSample, self).__init__()</span><br><span class="line">        <span class="comment"># 使用 stride=2 的卷积，可以使图片尺寸减半</span></span><br><span class="line">        self.conv_bn_layer = ConvBNLayer(</span><br><span class="line">            ch_in=ch_in,</span><br><span class="line">            ch_out=ch_out,</span><br><span class="line">            kernel_size=kernel_size,</span><br><span class="line">            stride=stride,</span><br><span class="line">            padding=padding)</span><br><span class="line">        self.ch_out = ch_out</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        out = self.conv_bn_layer(inputs)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义残差块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基本残差块的定义，输入x经过两层卷积，然后接第二层卷积的输出和输入x相加</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch_in, ch_out</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 第一个卷积层</span></span><br><span class="line">        self.conv1 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_in,</span><br><span class="line">            ch_out=ch_out,</span><br><span class="line">            kernel_size=<span class="number">1</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">0</span></span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 第二个卷积层</span></span><br><span class="line">        self.conv2 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out,</span><br><span class="line">            ch_out=ch_out*<span class="number">2</span>,<span class="comment"># 输出通道×2</span></span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        conv1 = self.conv1(inputs)</span><br><span class="line">        conv2 = self.conv2(conv1)</span><br><span class="line">        <span class="comment"># 将第二个卷积层的输出和最初的输入值按元素对应位置相加（前提：形状不变）</span></span><br><span class="line">        out = paddle.add(x=inputs, y=conv2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多个残差块封装为一个层级，方便后续复用     </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerWarp</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    添加多层（count）残差块，组成Darknet53网络的一个层级</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch_in, ch_out, count, is_test=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerWarp,self).__init__()</span><br><span class="line">        self.basicblock0 = BasicBlock(ch_in, ch_out)</span><br><span class="line">        self.res_out_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, count):</span><br><span class="line">            <span class="comment"># 使用add_sublayer添加子层</span></span><br><span class="line">            res_out = self.add_sublayer(<span class="string">&quot;basic_block_%d&quot;</span> % (i),</span><br><span class="line">                BasicBlock(ch_out*<span class="number">2</span>, ch_out))</span><br><span class="line">            self.res_out_list.append(res_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        y = self.basicblock0(inputs)</span><br><span class="line">        <span class="keyword">for</span> basic_block_i <span class="keyword">in</span> self.res_out_list:</span><br><span class="line">            y = basic_block_i(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># DarkNet 每组残差块的个数，来自DarkNet的网络结构图</span></span><br><span class="line">DarkNet_cfg = &#123;<span class="number">53</span>: ([<span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">4</span>])&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DarkNet53骨干网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DarkNet53_conv_body</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DarkNet53_conv_body, self).__init__()</span><br><span class="line">        self.stages = DarkNet_cfg[<span class="number">53</span>]</span><br><span class="line">        self.stages = self.stages[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层卷积</span></span><br><span class="line">        self.conv0 = ConvBNLayer(</span><br><span class="line">            ch_in=<span class="number">3</span>,</span><br><span class="line">            ch_out=<span class="number">32</span>,</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下采样，使用stride=2的卷积来实现</span></span><br><span class="line">        self.downsample0 = DownSample(</span><br><span class="line">            ch_in=<span class="number">32</span>,</span><br><span class="line">            ch_out=<span class="number">32</span> * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加各个层级的实现</span></span><br><span class="line">        self.darknet53_conv_block_list = []</span><br><span class="line">        self.downsample_list = []</span><br><span class="line">        <span class="keyword">for</span> i, stage <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.stages):</span><br><span class="line">            conv_block = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;stage_%d&quot;</span> % (i),</span><br><span class="line">                LayerWarp(<span class="number">32</span>*(<span class="number">2</span>**(i+<span class="number">1</span>)),</span><br><span class="line">                <span class="number">32</span>*(<span class="number">2</span>**i),</span><br><span class="line">                stage))</span><br><span class="line">            self.darknet53_conv_block_list.append(conv_block)</span><br><span class="line">        <span class="comment"># 两个层级之间使用DownSample将尺寸减半</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.stages) - <span class="number">1</span>):</span><br><span class="line">            downsample = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;stage_%d_downsample&quot;</span> % i,</span><br><span class="line">                DownSample(ch_in=<span class="number">32</span>*(<span class="number">2</span>**(i+<span class="number">1</span>)),</span><br><span class="line">                    ch_out=<span class="number">32</span>*(<span class="number">2</span>**(i+<span class="number">2</span>))))</span><br><span class="line">            self.downsample_list.append(downsample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        out = self.conv0(inputs)</span><br><span class="line">        <span class="comment">#print(&quot;conv1:&quot;,out.numpy())</span></span><br><span class="line">        out = self.downsample0(out)</span><br><span class="line">        <span class="comment">#print(&quot;downsample:&quot;,out.numpy())</span></span><br><span class="line">        blocks = []</span><br><span class="line">        <span class="comment"># 依次将各个层级作用在输入上面</span></span><br><span class="line">        <span class="keyword">for</span> i, conv_block_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.darknet53_conv_block_list):</span><br><span class="line">            out = conv_block_i(out)</span><br><span class="line">            blocks.append(out)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(self.stages) - <span class="number">1</span>:</span><br><span class="line">                out = self.downsample_list[i](out)</span><br><span class="line">        <span class="comment"># 将C0, C1, C2作为返回值</span></span><br><span class="line">        <span class="keyword">return</span> blocks[-<span class="number">1</span>:-<span class="number">4</span>:-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>下面这段示例代码，指定输入数据的形状是$(1, 3, 640, 640)$，则3个层级的输出特征图的形状分别是$C0 (1, 1024, 20, 20)$，$C1 (1, 512, 40, 40)$和$C2 (1, 256, 80, 80)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看Darknet53网络输出特征图</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line"><span class="built_in">print</span>(C0.shape, C1.shape, C2.shape)</span><br></pre></td></tr></table></figure><pre><code>[1, 1024, 20, 20] [1, 512, 40, 40] [1, 256, 80, 80]</code></pre><h2 id="4-根据输出特征图计算预测框位置和类别"><a href="#4-根据输出特征图计算预测框位置和类别" class="headerlink" title="4.根据输出特征图计算预测框位置和类别"></a>4.根据输出特征图计算预测框位置和类别</h2><p>YOLOv3中对每个预测框计算逻辑如下：</p><ul><li><p>预测框是否包含物体。也可理解为objectness=1的概率是多少，可以用网络输出一个实数$x$，可以用$Sigmoid(x)$表示objectness为正的概率$P_{obj}$</p></li><li><p>预测物体位置和形状。物体位置和形状$t_x, t_y, t_w, t_h$可以用网络输出4个实数来表示$t_x, t_y, t_w, t_h$</p></li><li><p>预测物体类别。预测图像中物体的具体类别是什么，或者说其属于每个类别的概率分别是多少。总的类别数为C，需要预测物体属于每个类别的概率$(P_1, P_2, …, P_C)$，可以用网络输出C个实数$(x_1, x_2, …, x_C)$，对每个实数分别求Sigmoid函数，让$P_i = Sigmoid(x_i)$，则可以表示出物体属于每个类别的概率。</p></li></ul><p>对于一个预测框，网络需要输出$(5 + C)$个实数来表征它是否包含物体、位置和形状尺寸以及属于每个类别的概率。由于我们在每个小方块区域都生成了K个预测框，则所有预测框一共需要网络输出的预测值数目是：</p><script type="math/tex; mode=display">[K(5 + C)] \times m \times n</script><p>还有更重要的一点是网络输出必须要能区分出小方块区域的位置来，不能直接将特征图连接一个输出大小为$[K(5 + C)] \times m \times n$的全连接层。</p><h3 id="4-1建立输出特征图与预测框之间的关联"><a href="#4-1建立输出特征图与预测框之间的关联" class="headerlink" title="4.1建立输出特征图与预测框之间的关联"></a><strong>4.1建立输出特征图与预测框之间的关联</strong></h3><p>现在观察特征图，经过多次卷积核池化之后，其步幅stride=32，$640 \times 480$大小的输入图片变成了$20\times15$的特征图；而小方块区域的数目正好是$20\times15$，也就是说可以让特征图上每个像素点分别跟原图上一个小方块区域对应。这也是为什么我们最开始将小方块区域的尺寸设置为32的原因，这样可以巧妙的将小方块区域跟特征图上的像素点对应起来，解决了空间位置的对应关系。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/59bd2592dd9f4f4dada8333307198888e667b15969ce434eb52c0232d9608a62" width = "600"></center><center><br>图12：特征图C0与小方块区域形状对比 </br></center><p><br></br></p><p>下面需要将像素点$(i,j)$与第i行第j列的小方块区域所需要的预测值关联起来，每个小方块区域产生K个预测框，每个预测框需要$(5 + C)$个实数预测值，则每个像素点相对应的要有$K(5 + C)$个实数。为了解决这一问题，对特征图进行多次卷积，并将最终的输出通道数设置为$K(5 + C)$，即可将生成的特征图与每个预测框所需要的预测值巧妙的对应起来。当然，这种对应是为了将骨干网络提取的特征对接输出层来形成Loss。实际中，这几个尺寸可以随着任务数据分布的不同而调整，只要保证特征图输出尺寸（控制卷积核和下采样）和输出层尺寸（控制小方块区域的大小）相同即可。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f1d445d77cd84c60897ade92cc24f951a7672a72dc6f43b8aeb6932b6db778d6" width = "600"></center><center><br>图13：特征图C0与小方块区域一一对应 </br></center><p><br></br></p><p>骨干网络的输出特征图是C0，下面的程序是对C0进行多次卷积以得到跟预测框相关的特征图P0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 YOLOv3 后续连接的网络层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YoloDetectionBlock</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="comment"># 使用多层卷积和BN提取特征</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,ch_in,ch_out,is_test=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YoloDetectionBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> ch_out % <span class="number">2</span> == <span class="number">0</span>, \</span><br><span class="line">            <span class="string">&quot;channel &#123;&#125; cannot be divided by 2&quot;</span>.<span class="built_in">format</span>(ch_out)</span><br><span class="line"></span><br><span class="line">        self.conv0 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_in,</span><br><span class="line">            ch_out=ch_out,</span><br><span class="line">            kernel_size=<span class="number">1</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">0</span>)</span><br><span class="line">        self.conv1 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out,</span><br><span class="line">            ch_out=ch_out*<span class="number">2</span>,</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out*<span class="number">2</span>,</span><br><span class="line">            ch_out=ch_out,</span><br><span class="line">            kernel_size=<span class="number">1</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">0</span>)</span><br><span class="line">        self.conv3 = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out,</span><br><span class="line">            ch_out=ch_out*<span class="number">2</span>,</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线性层</span></span><br><span class="line">        self.route = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out*<span class="number">2</span>,</span><br><span class="line">            ch_out=ch_out,</span><br><span class="line">            kernel_size=<span class="number">1</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 最末层</span></span><br><span class="line">        self.tip = ConvBNLayer(</span><br><span class="line">            ch_in=ch_out,</span><br><span class="line">            ch_out=ch_out*<span class="number">2</span>,</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        out = self.conv0(inputs)</span><br><span class="line">        out = self.conv1(out)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        route = self.route(out)</span><br><span class="line">        tip = self.tip(route)</span><br><span class="line">        <span class="keyword">return</span> route, tip</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">NUM_ANCHORS = <span class="number">3</span> <span class="comment"># 锚框数</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span> <span class="comment"># 类别数</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(P0.shape)</span><br></pre></td></tr></table></figure><pre><code>[1, 36, 20, 20]填充的存在大小不会发生变化</code></pre><p>如上面的代码所示，可以由特征图C0生成特征图P0，P0的形状是$[1, 36, 20, 20]$。每个小方块区域生成的锚框或者预测框的数量是3，物体类别数目是7，每个区域需要的预测值个数是$3 \times (5 + 7) = 36$，正好等于P0的输出通道数。</p><p>将$P0[t, 0:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框所需要的12个预测值对应，$P0[t, 12:24, i, j]$与输入的第t张图片上小方块区域$(i, j)$第2个预测框所需要的12个预测值对应，$P0[t, 24:36, i, j]$与输入的第t张图片上小方块区域$(i, j)$第3个预测框所需要的12个预测值对应。</p><p>$P0[t, 0:4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 5:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的类别对应。</p><p>如 <strong>图14</strong> 所示，通过这种方式可以巧妙的将网络输出特征图，与每个小方块区域生成的预测框对应起来了。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9ea44b2c11f74f1484ab2bdc93be4008008cdee0b8d34dcb97bc9f89af935d1c" width = "800"></center><center><br>图14：特征图P0与候选区域的关联 </br></center><p><br></br></p><h3 id="4-2计算预测框是否包含物体的概率"><a href="#4-2计算预测框是否包含物体的概率" class="headerlink" title="4.2计算预测框是否包含物体的概率"></a><strong>4.2计算预测框是否包含物体的概率</strong></h3><p>根据前面的分析，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 4+12, i, j]$与第2个预测框的objectness对应，…，则可以使用下面的程序将objectness相关的预测取出，并使用<code>paddle.nn.functional.sigmoid</code>计算输出概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line">reshaped_p0 = paddle.reshape(P0, [-<span class="number">1</span>, NUM_ANCHORS, NUM_CLASSES + <span class="number">5</span>, P0.shape[<span class="number">2</span>], P0.shape[<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(P0.shape, reshaped_p0.shape)</span><br><span class="line">pred_objectness = reshaped_p0[:, :, <span class="number">4</span>, :, :]</span><br><span class="line">pred_objectness_probability = F.sigmoid(pred_objectness)</span><br><span class="line"><span class="built_in">print</span>(pred_objectness.shape, pred_objectness_probability.shape)</span><br></pre></td></tr></table></figure><pre><code>[1, 36, 20, 20] [1, 3, 12, 20, 20][1, 3, 20, 20] [1, 3, 20, 20]</code></pre><p>上面的输出程序显示，预测框是否包含物体的概率<code>pred_objectness_probability</code>，其数据形状是[1, 3, 20, 20]，与我们上面提到的预测框个数一致，数据大小在0～1之间，表示预测框为正样本的概率。</p><h3 id="4-3计算预测框位置坐标"><a href="#4-3计算预测框位置坐标" class="headerlink" title="4.3计算预测框位置坐标"></a><strong>4.3计算预测框位置坐标</strong></h3><p>$P0[t, 0:4, i, j]$与输入的第$t$张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 12:16, i, j]$与第2个预测框的位置对应，依此类推，则使用下面的程序可以从$P0$中取出跟预测框位置相关的预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred =  paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reshaped_p0 = paddle.reshape(P0, [-<span class="number">1</span>, NUM_ANCHORS, NUM_CLASSES + <span class="number">5</span>, P0.shape[<span class="number">2</span>], P0.shape[<span class="number">3</span>]])</span><br><span class="line">pred_objectness = reshaped_p0[:, :, <span class="number">4</span>, :, :]</span><br><span class="line">pred_objectness_probability = F.sigmoid(pred_objectness)</span><br><span class="line"></span><br><span class="line">pred_location = reshaped_p0[:, :, <span class="number">0</span>:<span class="number">4</span>, :, :]</span><br><span class="line"><span class="built_in">print</span>(pred_location.shape)</span><br></pre></td></tr></table></figure><pre><code>[1, 3, 4, 20, 20]</code></pre><p>网络输出值是$(t_x, t_y, t_w, t_h)$，还需要将其转化为$(x_1, y_1, x_2, y_2)$这种形式的坐标表示。我们使用Numpy来实现这一过程。在后面的代码中使用飞桨<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/ops/yolo_box_cn.html#yolo-box">paddle.vision.ops.yolo_box</a> API可以直接计算出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义Sigmoid函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1.0</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络特征图输出的[tx, ty, th, tw]转化成预测框的坐标[x1, y1, x2, y2]</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_yolo_box_xxyy</span>(<span class="params">pred, anchors, num_classes, downsample</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    pred是网络输出特征图转化成的numpy.ndarray</span></span><br><span class="line"><span class="string">    anchors 是一个list。表示锚框的大小，</span></span><br><span class="line"><span class="string">                例如 anchors = [116, 90, 156, 198, 373, 326]，表示有三个锚框，</span></span><br><span class="line"><span class="string">                第一个锚框大小[w, h]是[116, 90]，第二个锚框大小是[156, 198]，第三个锚框大小是[373, 326]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batchsize = pred.shape[<span class="number">0</span>]</span><br><span class="line">    num_rows = pred.shape[-<span class="number">2</span>]</span><br><span class="line">    num_cols = pred.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    input_h = num_rows * downsample</span><br><span class="line">    input_w = num_cols * downsample</span><br><span class="line"></span><br><span class="line">    num_anchors = <span class="built_in">len</span>(anchors) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pred的形状是[N, C, H, W]，其中C = NUM_ANCHORS * (5 + NUM_CLASSES)</span></span><br><span class="line">    <span class="comment"># 对pred进行reshape</span></span><br><span class="line">    pred = pred.reshape([-<span class="number">1</span>, num_anchors, <span class="number">5</span>+num_classes, num_rows, num_cols])</span><br><span class="line">    pred_location = pred[:, :, <span class="number">0</span>:<span class="number">4</span>, :, :]</span><br><span class="line">    pred_location = np.transpose(pred_location, (<span class="number">0</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># [N, H, W, NUM_ANCHORS, 4]</span></span><br><span class="line">    anchors_this = [] <span class="comment"># 输出锚框</span></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(num_anchors):</span><br><span class="line">        anchors_this.append([anchors[ind*<span class="number">2</span>], anchors[ind*<span class="number">2</span>+<span class="number">1</span>]])</span><br><span class="line">    anchors_this = np.array(anchors_this).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 最终输出数据保存在pred_box中，其形状是[N, H, W, NUM_ANCHORS, 4]，</span></span><br><span class="line">    <span class="comment"># 其中最后一个维度4代表位置的4个坐标</span></span><br><span class="line">    pred_box = np.zeros(pred_location.shape)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(batchsize):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_rows):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_cols):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_anchors):</span><br><span class="line">                    pred_box[n, i, j, k, <span class="number">0</span>] = j</span><br><span class="line">                    pred_box[n, i, j, k, <span class="number">1</span>] = i</span><br><span class="line">                    pred_box[n, i, j, k, <span class="number">2</span>] = anchors_this[k][<span class="number">0</span>]</span><br><span class="line">                    pred_box[n, i, j, k, <span class="number">3</span>] = anchors_this[k][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里使用相对坐标，pred_box的输出元素数值在0.~1.0之间</span></span><br><span class="line">    pred_box[:, :, :, :, <span class="number">0</span>] = (sigmoid(pred_location[:, :, :, :, <span class="number">0</span>]) + pred_box[:, :, :, :, <span class="number">0</span>]) / num_cols</span><br><span class="line">    pred_box[:, :, :, :, <span class="number">1</span>] = (sigmoid(pred_location[:, :, :, :, <span class="number">1</span>]) + pred_box[:, :, :, :, <span class="number">1</span>]) / num_rows</span><br><span class="line">    pred_box[:, :, :, :, <span class="number">2</span>] = np.exp(pred_location[:, :, :, :, <span class="number">2</span>]) * pred_box[:, :, :, :, <span class="number">2</span>] / input_w</span><br><span class="line">    pred_box[:, :, :, :, <span class="number">3</span>] = np.exp(pred_location[:, :, :, :, <span class="number">3</span>]) * pred_box[:, :, :, :, <span class="number">3</span>] / input_h</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将坐标从xywh转化成xyxy</span></span><br><span class="line">    pred_box[:, :, :, :, <span class="number">0</span>] = pred_box[:, :, :, :, <span class="number">0</span>] - pred_box[:, :, :, :, <span class="number">2</span>] / <span class="number">2.</span></span><br><span class="line">    pred_box[:, :, :, :, <span class="number">1</span>] = pred_box[:, :, :, :, <span class="number">1</span>] - pred_box[:, :, :, :, <span class="number">3</span>] / <span class="number">2.</span></span><br><span class="line">    pred_box[:, :, :, :, <span class="number">2</span>] = pred_box[:, :, :, :, <span class="number">0</span>] + pred_box[:, :, :, :, <span class="number">2</span>]</span><br><span class="line">    pred_box[:, :, :, :, <span class="number">3</span>] = pred_box[:, :, :, :, <span class="number">1</span>] + pred_box[:, :, :, :, <span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 截取 np.clip(a, a_min, a_max, out=None)</span></span><br><span class="line">    pred_box = np.clip(pred_box, <span class="number">0.</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred_box</span><br></pre></td></tr></table></figure><p>通过调用上面定义的<code>get_yolo_box_xxyy</code>函数，可以从$P0$计算出预测框坐标来，具体程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line">reshaped_p0 = paddle.reshape(P0, [-<span class="number">1</span>, NUM_ANCHORS, NUM_CLASSES + <span class="number">5</span>, P0.shape[<span class="number">2</span>], P0.shape[<span class="number">3</span>]])</span><br><span class="line">pred_objectness = reshaped_p0[:, :, <span class="number">4</span>, :, :]</span><br><span class="line">pred_objectness_probability = F.sigmoid(pred_objectness)</span><br><span class="line"></span><br><span class="line">pred_location = reshaped_p0[:, :, <span class="number">0</span>:<span class="number">4</span>, :, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># anchors包含了预先设定好的锚框尺寸</span></span><br><span class="line">anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line"><span class="comment"># downsample 是特征图P0的步幅</span></span><br><span class="line"><span class="comment"># 由输出特征图P0计算预测框位置坐标</span></span><br><span class="line">pred_boxes = get_yolo_box_xxyy(P0.numpy(), anchors, num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)</span><br><span class="line"><span class="built_in">print</span>(pred_location.shape, pred_boxes.shape)</span><br><span class="line"><span class="built_in">print</span>(pred_boxes[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>].shape, pred_boxes[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># 一个中心三个锚框 location</span></span><br></pre></td></tr></table></figure><pre><code>[1, 3, 4, 20, 20] (1, 20, 20, 3, 4)(3, 4) [[0.         0.         0.11592992 0.09357996] [0.         0.         0.14583146 0.17757062] [0.         0.         0.32789061 0.26856107]]</code></pre><h3 id="4-4计算物体属于每个类别概率"><a href="#4-4计算物体属于每个类别概率" class="headerlink" title="4.4计算物体属于每个类别概率"></a><strong>4.4计算物体属于每个类别概率</strong></h3><p>$P0[t, 5:12, i, j]$与输入的第$t$张图片上小方块区域$(i, j)$第1个预测框包含物体的类别对应，$P0[t, 17:24, i, j]$与第2个预测框的类别对应，依此类推，则使用下面的程序可以从$P0$中取出那些跟预测框类别相关的预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line">reshaped_p0 = paddle.reshape(P0, [-<span class="number">1</span>, NUM_ANCHORS, NUM_CLASSES + <span class="number">5</span>, P0.shape[<span class="number">2</span>], P0.shape[<span class="number">3</span>]])</span><br><span class="line"><span class="comment"># 取出与objectness相关的预测值</span></span><br><span class="line">pred_objectness = reshaped_p0[:, :, <span class="number">4</span>, :, :]</span><br><span class="line">pred_objectness_probability = F.sigmoid(pred_objectness)</span><br><span class="line"><span class="comment"># 取出与位置相关的预测值</span></span><br><span class="line">pred_location = reshaped_p0[:, :, <span class="number">0</span>:<span class="number">4</span>, :, :]</span><br><span class="line"><span class="comment"># 取出与类别相关的预测值</span></span><br><span class="line">pred_classification = reshaped_p0[:, :, <span class="number">5</span>:<span class="number">5</span>+NUM_CLASSES, :, :]</span><br><span class="line">pred_classification_probability = F.sigmoid(pred_classification)</span><br><span class="line"><span class="built_in">print</span>(pred_objectness_probability.shape, pred_location.shape, pred_classification.shape)</span><br></pre></td></tr></table></figure><pre><code>[1, 3, 20, 20] [1, 3, 4, 20, 20] [1, 3, 7, 20, 20]</code></pre><p>上面的程序通过$P0$计算出了预测框包含的物体所属类别的概率，<code>pred_classification_probability</code>的形状是$[1, 3, 7, 20, 20]$，数值在0~1之间。</p><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5.损失函数"></a><strong>5.损失函数</strong></h2><p>上面从概念上将输出特征图上的像素点与预测框关联起来了，那么要对神经网络进行求解，还必须从数学上将网络输出和预测框关联起来，也就是要建立起损失函数跟网络输出之间的关系。下面讨论如何建立起YOLOv3的损失函数。</p><p>对于每个预测框，YOLOv3模型会建立三种类型的损失函数：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/fc7bbc5436d84fb0bb18b817125947cbd441dbdef7ae4365b55da5188ea161b8" width = "800"></center><center><br>图15：损失函数的建立思路 </br></center><p><br></br></p><ul><li><p>表征是否包含目标物体的损失函数，通过pred_objectness和label_objectness计算。</p><pre><code>loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_objectness, label_objectness)</code></pre></li><li><p>表征物体位置的损失函数，通过pred_location和label_location计算。</p><pre><code>pred_location_x = pred_location[:, :, 0, :, :]pred_location_y = pred_location[:, :, 1, :, :]pred_location_w = pred_location[:, :, 2, :, :]pred_location_h = pred_location[:, :, 3, :, :]loss_location_x = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_x, label_location_x)loss_location_y = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_y, label_location_y)loss_location_w = paddle.abs(pred_location_w - label_location_w)loss_location_h = paddle.abs(pred_location_h - label_location_h)loss_location = loss_location_x + loss_location_y + loss_location_w + loss_location_h</code></pre></li><li><p>表征物体类别的损失函数，通过pred_classification和label_classification计算。</p><pre><code>loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_classification, label_classification)</code></pre></li></ul><p>我们已经知道怎么计算这些预测值和标签了，但是遗留了一个小问题，就是没有标注出哪些锚框的objectness为-1。为了完成这一步，我们需要计算出所有预测框跟真实框之间的IoU，然后把那些IoU大于阈值的真实框挑选出来。实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 挑选出跟真实框IoU大于阈值的预测框</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_iou_above_thresh_inds</span>(<span class="params">pred_box, gt_boxes, iou_threshold</span>):</span><br><span class="line">    batchsize = pred_box.shape[<span class="number">0</span>]</span><br><span class="line">    num_rows = pred_box.shape[<span class="number">1</span>]</span><br><span class="line">    num_cols = pred_box.shape[<span class="number">2</span>]</span><br><span class="line">    num_anchors = pred_box.shape[<span class="number">3</span>]</span><br><span class="line">    ret_inds = np.zeros([batchsize, num_rows, num_cols, num_anchors])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batchsize):</span><br><span class="line">        pred_box_i = pred_box[i]</span><br><span class="line">        gt_boxes_i = gt_boxes[i]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gt_boxes_i)):</span><br><span class="line">            <span class="comment"># 计算iou</span></span><br><span class="line">            gt = gt_boxes_i[k]</span><br><span class="line">            gtx_min = gt[<span class="number">0</span>] - gt[<span class="number">2</span>] / <span class="number">2.</span></span><br><span class="line">            gty_min = gt[<span class="number">1</span>] - gt[<span class="number">3</span>] / <span class="number">2.</span></span><br><span class="line">            gtx_max = gt[<span class="number">0</span>] + gt[<span class="number">2</span>] / <span class="number">2.</span></span><br><span class="line">            gty_max = gt[<span class="number">1</span>] + gt[<span class="number">3</span>] / <span class="number">2.</span></span><br><span class="line">            <span class="keyword">if</span> (gtx_max - gtx_min &lt; <span class="number">1e-3</span>) <span class="keyword">or</span> (gty_max - gty_min &lt; <span class="number">1e-3</span>):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            x1 = np.maximum(pred_box_i[:, :, :, <span class="number">0</span>], gtx_min)</span><br><span class="line">            y1 = np.maximum(pred_box_i[:, :, :, <span class="number">1</span>], gty_min)</span><br><span class="line">            x2 = np.minimum(pred_box_i[:, :, :, <span class="number">2</span>], gtx_max)</span><br><span class="line">            y2 = np.minimum(pred_box_i[:, :, :, <span class="number">3</span>], gty_max)</span><br><span class="line">            intersection = np.maximum(x2 - x1, <span class="number">0.</span>) * np.maximum(y2 - y1, <span class="number">0.</span>)</span><br><span class="line">            s1 = (gty_max - gty_min) * (gtx_max - gtx_min)</span><br><span class="line">            s2 = (pred_box_i[:, :, :, <span class="number">2</span>] - pred_box_i[:, :, :, <span class="number">0</span>]) * (pred_box_i[:, :, :, <span class="number">3</span>] - pred_box_i[:, :, :, <span class="number">1</span>])</span><br><span class="line">            union = s2 + s1 - intersection</span><br><span class="line">            iou = intersection / union</span><br><span class="line">            <span class="comment"># np.where(condition) 判断条件成立时返回索引坐标</span></span><br><span class="line">            above_inds = np.where(iou &gt; iou_threshold)</span><br><span class="line">            ret_inds[i][above_inds] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># [batchsize, num_rows, num_cols, num_anchors]--&gt;[batchsize, num_anchors, num_rows, num_cols]</span></span><br><span class="line">    ret_inds = np.transpose(ret_inds, (<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> ret_inds.astype(<span class="string">&#x27;bool&#x27;</span>)</span><br></pre></td></tr></table></figure><p>上面的函数可以得到哪些锚框的objectness需要被标注为-1，通过下面的程序，对label_objectness进行处理，将IoU大于阈值，但又不是正样本的锚框标注为-1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label_objectness_ignore</span>(<span class="params">label_objectness, iou_above_thresh_indices</span>):</span><br><span class="line">    <span class="comment"># 注意：这里不能简单的使用 label_objectness[iou_above_thresh_indices] = -1，</span></span><br><span class="line">    <span class="comment">#         这样可能会造成label_objectness为1的点被设置为-1了</span></span><br><span class="line">    <span class="comment">#         只有将那些被标注为0，且与真实框IoU超过阈值的预测框才被标注为-1</span></span><br><span class="line">    negative_indices = (label_objectness &lt; <span class="number">0.5</span>)</span><br><span class="line">    ignore_indices = negative_indices * iou_above_thresh_indices</span><br><span class="line">    label_objectness[ignore_indices] = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> label_objectness</span><br></pre></td></tr></table></figure><p>下面通过调用这两个函数，实现如何将部分预测框的label_objectness设置为-1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">reader = paddle.io.DataLoader(train_dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">img, gt_boxes, gt_labels, im_shape = <span class="built_in">next</span>(reader())</span><br><span class="line">img, gt_boxes, gt_labels, im_shape = img.numpy(), gt_boxes.numpy(), gt_labels.numpy(), im_shape.numpy()</span><br><span class="line"><span class="comment"># 计算出锚框对应的标签</span></span><br><span class="line">label_objectness, label_location, label_classification, scale_location = get_objectness_label(img,</span><br><span class="line">                                                                                              gt_boxes, gt_labels, </span><br><span class="line">                                                                                              iou_threshold = <span class="number">0.7</span>,</span><br><span class="line">                                                                                              anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span><br><span class="line">                                                                                              num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)</span><br><span class="line">                                               </span><br><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = paddle.nn.Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = paddle.to_tensor(img)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"></span><br><span class="line"><span class="comment"># anchors包含了预先设定好的锚框尺寸</span></span><br><span class="line">anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line"><span class="comment"># downsample是特征图P0的步幅</span></span><br><span class="line">pred_boxes = get_yolo_box_xxyy(P0.numpy(), anchors, num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)</span><br><span class="line">iou_above_thresh_indices = get_iou_above_thresh_inds(pred_boxes, gt_boxes, iou_threshold=<span class="number">0.7</span>)</span><br><span class="line">label_objectness = label_objectness_ignore(label_objectness, iou_above_thresh_indices)</span><br></pre></td></tr></table></figure><p>使用这种方式，就可以将那些没有被标注为正样本，但又与真实框IoU比较大的样本objectness标签设置为-1了，不计算其对任何一种损失函数的贡献。计算总的损失函数的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_loss</span>(<span class="params">output, label_objectness, label_location, label_classification, scales, num_anchors=<span class="number">3</span>, num_classes=<span class="number">7</span></span>):</span><br><span class="line">    <span class="comment"># 将output从[N, C, H, W]变形为[N, NUM_ANCHORS, NUM_CLASSES + 5, H, W]</span></span><br><span class="line">    reshaped_output = paddle.reshape(output, [-<span class="number">1</span>, num_anchors, num_classes + <span class="number">5</span>, output.shape[<span class="number">2</span>], output.shape[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从output中取出跟objectness相关的预测值</span></span><br><span class="line">    pred_objectness = reshaped_output[:, :, <span class="number">4</span>, :, :]</span><br><span class="line">    loss_objectness = F.binary_cross_entropy_with_logits(pred_objectness, label_objectness, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># pos_samples 只有在正样本的地方取值为1.，其它地方取值全为0.</span></span><br><span class="line">    pos_objectness = label_objectness &gt; <span class="number">0</span> <span class="comment"># 正类样本</span></span><br><span class="line">    <span class="comment"># paddle.cast(x, dtype) 将 x 的数据类型转换为 dtype</span></span><br><span class="line">    pos_samples = paddle.cast(pos_objectness, <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    pos_samples.stop_gradient=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从output中取出所有跟位置相关的预测值</span></span><br><span class="line">    tx = reshaped_output[:, :, <span class="number">0</span>, :, :]</span><br><span class="line">    ty = reshaped_output[:, :, <span class="number">1</span>, :, :]</span><br><span class="line">    tw = reshaped_output[:, :, <span class="number">2</span>, :, :]</span><br><span class="line">    th = reshaped_output[:, :, <span class="number">3</span>, :, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从label_location中取出各个位置坐标的标签</span></span><br><span class="line">    dx_label = label_location[:, :, <span class="number">0</span>, :, :]</span><br><span class="line">    dy_label = label_location[:, :, <span class="number">1</span>, :, :]</span><br><span class="line">    tw_label = label_location[:, :, <span class="number">2</span>, :, :]</span><br><span class="line">    th_label = label_location[:, :, <span class="number">3</span>, :, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建损失函数</span></span><br><span class="line">    loss_location_x = F.binary_cross_entropy_with_logits(tx, dx_label, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    loss_location_y = F.binary_cross_entropy_with_logits(ty, dy_label, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    loss_location_w = paddle.<span class="built_in">abs</span>(tw - tw_label)</span><br><span class="line">    loss_location_h = paddle.<span class="built_in">abs</span>(th - th_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算总的位置损失函数</span></span><br><span class="line">    loss_location = loss_location_x + loss_location_y + loss_location_h + loss_location_w</span><br><span class="line">    <span class="comment"># 乘以scales</span></span><br><span class="line">    loss_location = loss_location * scales <span class="comment"># scales--&gt;(h, w)</span></span><br><span class="line">    <span class="comment"># 只计算正样本的位置损失函数</span></span><br><span class="line">    loss_location = loss_location * pos_samples</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从output取出所有跟物体类别相关的像素点</span></span><br><span class="line">    pred_classification = reshaped_output[:, :, <span class="number">5</span>:<span class="number">5</span>+num_classes, :, :]</span><br><span class="line">    <span class="comment"># 计算分类相关的损失函数</span></span><br><span class="line">    loss_classification = F.binary_cross_entropy_with_logits(pred_classification, label_classification, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    <span class="comment"># 将损失求和</span></span><br><span class="line">    loss_classification = paddle.<span class="built_in">sum</span>(loss_classification, axis=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 只计算objectness为正的样本的分类损失函数</span></span><br><span class="line">    loss_classification = loss_classification * pos_samples</span><br><span class="line">    </span><br><span class="line">    total_loss = loss_objectness + loss_location + loss_classification</span><br><span class="line">    <span class="comment"># 对所有预测框的loss进行求和</span></span><br><span class="line">    total_loss = paddle.<span class="built_in">sum</span>(total_loss, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="comment"># 对所有样本求平均</span></span><br><span class="line">    total_loss = paddle.mean(total_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算出锚框对应的标签</span></span><br><span class="line">label_objectness, label_location, label_classification, scale_location = get_objectness_label(img,</span><br><span class="line">                                                                                              gt_boxes, gt_labels, </span><br><span class="line">                                                                                              iou_threshold = <span class="number">0.7</span>,</span><br><span class="line">                                                                                              anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span><br><span class="line">                                                                                              num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)                                                           </span><br><span class="line"></span><br><span class="line">NUM_ANCHORS = <span class="number">3</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line">num_filters=NUM_ANCHORS * (NUM_CLASSES + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">backbone = DarkNet53_conv_body()</span><br><span class="line">detection = YoloDetectionBlock(ch_in=<span class="number">1024</span>, ch_out=<span class="number">512</span>)</span><br><span class="line">conv2d_pred = Conv2D(in_channels=<span class="number">1024</span>, out_channels=num_filters,  kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = paddle.to_tensor(img)</span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line">route, tip = detection(C0)</span><br><span class="line">P0 = conv2d_pred(tip)</span><br><span class="line"><span class="comment"># anchors包含了预先设定好的锚框尺寸</span></span><br><span class="line">anchors = [<span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line"><span class="comment"># downsample是特征图P0的步幅</span></span><br><span class="line">pred_boxes = get_yolo_box_xxyy(P0.numpy(), anchors, num_classes=<span class="number">7</span>, downsample=<span class="number">32</span>)</span><br><span class="line">iou_above_thresh_indices = get_iou_above_thresh_inds(pred_boxes, gt_boxes, iou_threshold=<span class="number">0.7</span>)</span><br><span class="line">label_objectness = label_objectness_ignore(label_objectness, iou_above_thresh_indices)</span><br><span class="line"></span><br><span class="line">label_objectness = paddle.to_tensor(label_objectness)</span><br><span class="line">label_location = paddle.to_tensor(label_location)</span><br><span class="line">label_classification = paddle.to_tensor(label_classification)</span><br><span class="line">scales = paddle.to_tensor(scale_location)</span><br><span class="line">label_objectness.stop_gradient=<span class="literal">True</span></span><br><span class="line">label_location.stop_gradient=<span class="literal">True</span></span><br><span class="line">label_classification.stop_gradient=<span class="literal">True</span></span><br><span class="line">scales.stop_gradient=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">total_loss = get_loss(P0, label_objectness, label_location, label_classification, scales,</span><br><span class="line">                          num_anchors=NUM_ANCHORS, num_classes=NUM_CLASSES)</span><br><span class="line">total_loss_data = total_loss.numpy()</span><br><span class="line"><span class="built_in">print</span>(total_loss_data)</span><br></pre></td></tr></table></figure><pre><code>[896.6715]</code></pre><p>上面的程序计算出了总的损失函数，看到这里，读者已经了解到了YOLOv3算法的大部分内容，包括如何生成锚框、给锚框打上标签、通过卷积神经网络提取特征、将输出特征图跟预测框相关联、建立起损失函数。</p><h2 id="6-多尺度检测"><a href="#6-多尺度检测" class="headerlink" title="6.多尺度检测"></a><strong>6.多尺度检测</strong></h2><p>目前我们计算损失函数是在特征图P0的基础上进行的，它的步幅stride=32。特征图的尺寸比较小，像素点数目比较少，每个像素点的感受野很大，具有非常丰富的高层级语义信息，可能比较容易检测到较大的目标。为了能够检测到尺寸较小的那些目标，需要在尺寸较大的特征图上面建立预测输出。</p><p>如果我们在C2或者C1这种层级的特征图上直接产生预测输出，可能面临新的问题，它们没有经过充分的特征提取，像素点包含的语义信息不够丰富，有可能难以提取到有效的特征模式。在目标检测中，解决这一问题的方式是，<strong>将高层级的特征图尺寸放大之后跟低层级的特征图进行融合</strong>，得到的新特征图既能包含丰富的语义信息，又具有较多的像素点，能够描述更加精细的结构。</p><p>具体的网络实现方式如 <strong>图16</strong> 所示：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/b6d3b425644342e48bd0a50ebde90d882fd10717e0e44a53a44e98225bbb6df8" width = "800"></center><center><br>图16：生成多层级的输出特征图P0、P1、P2 </br></center><p><br></br></p><p>YOLOv3在每个区域的中心位置产生3个锚框，在3个层级的特征图上产生锚框的大小分别为P2 [(10×13),(16×30),(33×23)]，P1 [(30×61),(62×45),(59× 119)]，P0[(116 × 90), (156 × 198), (373 × 326]。越往后的特征图上用到的锚框尺寸也越大，能捕捉到大尺寸目标的信息；越往前的特征图上锚框尺寸越小，能捕捉到小尺寸目标的信息。</p><p>在完整网络定义时，我们需要使用 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/ops/yolo_loss_cn.html#yolo-loss">paddle.vision.ops.yolo_loss</a> API来计算损失函数，该API 将上述候选区域的标注以及多尺度的损失函数计算统一地进行了封装。</p><blockquote><p>paddle.vision.ops.yolo_loss(x, gt_box, gt_label, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, gt_score=None, use_label_smooth=True, name=None, scale_x_y=1.0)</p></blockquote><p>关键参数说明如下：</p><ul><li>x: 输出特征图。</li><li>gt_box: 真实框。</li><li>gt_label: 真实框标签。</li><li>ignore_thresh，预测框与真实框IoU阈值超过ignore_thresh时，不作为负样本，YOLOv3模型里设置为0.7。</li><li>downsample_ratio，特征图P0的下采样比例，使用Darknet53骨干网络时为32。</li><li>gt_score，真实框的置信度，在使用了mixup（混类增强）技巧时用到。</li><li>use_label_smooth，一种训练技巧，如不使用，设置为False。</li><li>name，该层的名字，比如’yolov3_loss’，默认值为None，一般无需设置。</li></ul><p>对于使用了多层级特征图产生预测框的方法，其具体实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义上采样模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Upsample</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Upsample,self).__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># 获取上采样过程中的动态尺寸</span></span><br><span class="line">        shape_nchw = paddle.shape(inputs)</span><br><span class="line">        <span class="comment"># paddle.slice(input, axes, starts, ends)：沿多个轴生成 input 的切片</span></span><br><span class="line">        shape_hw = paddle.<span class="built_in">slice</span>(shape_nchw, axes=[<span class="number">0</span>], starts=[<span class="number">2</span>], ends=[<span class="number">4</span>])</span><br><span class="line">        shape_hw.stop_gradient = <span class="literal">True</span></span><br><span class="line">        in_shape = paddle.cast(shape_hw, dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">        out_shape = in_shape * self.scale</span><br><span class="line">        out_shape.stop_gradient = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整一个 batch 中图片的大小：NEAREST 最近邻插值</span></span><br><span class="line">        out = paddle.nn.functional.interpolate(x=inputs, scale_factor=self.scale, mode=<span class="string">&quot;NEAREST&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YOLOv3</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YOLOv3,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 提取图像特征的骨干代码</span></span><br><span class="line">        self.block = DarkNet53_conv_body()</span><br><span class="line">        self.block_outputs = []</span><br><span class="line">        self.yolo_blocks = []</span><br><span class="line">        self.route_blocks_2 = []</span><br><span class="line">        <span class="comment"># 生成3个层级的特征图P0, P1, P2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="comment"># 添加从ci生成ri和ti的模块</span></span><br><span class="line">            yolo_block = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;yolo_detecton_block_%d&quot;</span> % (i),</span><br><span class="line">                YoloDetectionBlock(</span><br><span class="line">                                   ch_in=<span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span> <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> <span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span> + <span class="number">512</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                   ch_out = <span class="number">512</span>//(<span class="number">2</span>**i)))</span><br><span class="line">            self.yolo_blocks.append(yolo_block)</span><br><span class="line"></span><br><span class="line">            num_filters = <span class="number">3</span> * (self.num_classes + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加从ti生成pi的模块，这是一个Conv2D操作，输出通道数为3 * (num_classes + 5)</span></span><br><span class="line">            block_out = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;block_out_%d&quot;</span> % (i),</span><br><span class="line">                paddle.nn.Conv2D(in_channels=<span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span>,</span><br><span class="line">                       out_channels=num_filters,</span><br><span class="line">                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                       stride=<span class="number">1</span>,</span><br><span class="line">                       padding=<span class="number">0</span>,</span><br><span class="line">                       weight_attr=paddle.ParamAttr(</span><br><span class="line">                           initializer=paddle.nn.initializer.Normal(<span class="number">0.</span>, <span class="number">0.02</span>)),</span><br><span class="line">                       bias_attr=paddle.ParamAttr(</span><br><span class="line">                           initializer=paddle.nn.initializer.Constant(<span class="number">0.0</span>),</span><br><span class="line">                           regularizer=paddle.regularizer.L2Decay(<span class="number">0.</span>))))</span><br><span class="line">            self.block_outputs.append(block_out)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 对ri进行卷积</span></span><br><span class="line">                route = self.add_sublayer(<span class="string">&quot;route2_%d&quot;</span>%i,</span><br><span class="line">                                          ConvBNLayer(ch_in=<span class="number">512</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                                      ch_out=<span class="number">256</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                                      kernel_size=<span class="number">1</span>,</span><br><span class="line">                                                      stride=<span class="number">1</span>,</span><br><span class="line">                                                      padding=<span class="number">0</span>))</span><br><span class="line">                self.route_blocks_2.append(route)</span><br><span class="line">            <span class="comment"># 将ri放大以便跟c_&#123;i+1&#125;保持同样的尺寸</span></span><br><span class="line">            self.upsample = Upsample()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = []</span><br><span class="line">        blocks = self.block(inputs)</span><br><span class="line">        <span class="keyword">for</span> i, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(blocks):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将r_&#123;i-1&#125;经过卷积和上采样之后得到特征图，与这一级的ci进行拼接</span></span><br><span class="line">                block = paddle.concat([route, block], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 从ci生成ti和ri</span></span><br><span class="line">            route, tip = self.yolo_blocks[i](block)</span><br><span class="line">            <span class="comment"># 从ti生成pi</span></span><br><span class="line">            block_out = self.block_outputs[i](tip)</span><br><span class="line">            <span class="comment"># 将pi放入列表</span></span><br><span class="line">            outputs.append(block_out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 对ri进行卷积调整通道数</span></span><br><span class="line">                route = self.route_blocks_2[i](route)</span><br><span class="line">                <span class="comment"># 对ri进行放大，使其尺寸和c_&#123;i+1&#125;保持一致</span></span><br><span class="line">                route = self.upsample(route)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_loss</span>(<span class="params">self, outputs, gtbox, gtlabel, gtscore=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 anchors = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span></span><br><span class="line"><span class="params">                 anchor_masks = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</span></span><br><span class="line"><span class="params">                 ignore_thresh=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                 use_label_smooth=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用 paddle.vision.ops.yolo_loss，直接计算损失函数，过程更简洁，速度也更快</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.losses = []</span><br><span class="line">        downsample = <span class="number">32</span></span><br><span class="line">        <span class="keyword">for</span> i, out <span class="keyword">in</span> <span class="built_in">enumerate</span>(outputs): <span class="comment"># 对三个层级分别求损失函数</span></span><br><span class="line">            anchor_mask_i = anchor_masks[i]</span><br><span class="line">            loss = paddle.vision.ops.yolo_loss(</span><br><span class="line">                    x=out,  <span class="comment"># out是P0, P1, P2中的一个</span></span><br><span class="line">                    gt_box=gtbox,  <span class="comment"># 真实框坐标</span></span><br><span class="line">                    gt_label=gtlabel,  <span class="comment"># 真实框类别</span></span><br><span class="line">                    gt_score=gtscore,  <span class="comment"># 真实框得分，使用mixup训练技巧时需要，不使用该技巧时直接设置为1，形状与gtlabel相同</span></span><br><span class="line">                    anchors=anchors,   <span class="comment"># 锚框尺寸，包含[w0, h0, w1, h1, ..., w8, h8]共9个锚框的尺寸</span></span><br><span class="line">                    anchor_mask=anchor_mask_i, <span class="comment"># 筛选锚框的mask，例如anchor_mask_i=[3, 4, 5]，将anchors中第3、4、5个锚框挑选出来给该层级使用</span></span><br><span class="line">                    class_num=self.num_classes, <span class="comment"># 分类类别数</span></span><br><span class="line">                    ignore_thresh=ignore_thresh, <span class="comment"># 当预测框与真实框IoU &gt; ignore_thresh，标注objectness = -1</span></span><br><span class="line">                    downsample_ratio=downsample, <span class="comment"># 特征图相对于原图缩小的倍数，例如P0是32， P1是16，P2是8</span></span><br><span class="line">                    use_label_smooth=<span class="literal">False</span>)      <span class="comment"># 使用label_smooth训练技巧时会用到，这里没用此技巧，直接设置为False</span></span><br><span class="line">            self.losses.append(paddle.mean(loss))  <span class="comment">#mean对每张图片求和</span></span><br><span class="line">            downsample = downsample // <span class="number">2</span> <span class="comment"># 下一级特征图的缩放倍数会减半</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.losses) <span class="comment"># 对每个层级求和</span></span><br></pre></td></tr></table></figure><h2 id="7-开启端到端训练"><a href="#7-开启端到端训练" class="headerlink" title="7.开启端到端训练"></a><strong>7.开启端到端训练</strong></h2><p>训练过程如 <strong>图17</strong> 所示，输入图片经过特征提取得到三个层级的输出特征图P0(stride=32)、P1(stride=16)和P2(stride=8)，相应的分别使用不同大小的小方块区域去生成对应的锚框和预测框，并对这些锚框进行标注。</p><ul><li><p>P0层级特征图，对应着使用$32\times32$大小的小方块，在每个区域中心生成大小分别为$[116, 90]$, $[156, 198]$, $[373, 326]$的三种锚框。</p></li><li><p>P1层级特征图，对应着使用$16\times16$大小的小方块，在每个区域中心生成大小分别为$[30, 61]$, $[62, 45]$, $[59, 119]$的三种锚框。</p></li><li><p>P2层级特征图，对应着使用$8\times8$大小的小方块，在每个区域中心生成大小分别为$[10, 13]$, $[16, 30]$, $[33, 23]$的三种锚框。</p></li></ul><p>将三个层级的特征图与对应锚框之间的标签关联起来，并建立损失函数，总的损失函数等于三个层级的损失函数相加。通过极小化损失函数，可以开启端到端的训练过程。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/736da9cd3a4f4a1c98187a6cdf1a0334af73470b6d7c4b2fbaa9660d4bd20621" width = "600"></center><center><br>图17：端到端训练流程 </br></center><p><br></br></p><p>训练过程的具体实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############# 这段代码运行前：请重启项目环境（GPU）#######################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">ANCHORS = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line">ANCHOR_MASKS = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line">IGNORE_THRESH = <span class="number">.7</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr</span>(<span class="params">base_lr = <span class="number">0.0001</span>, lr_decay = <span class="number">0.1</span></span>):</span><br><span class="line">    bd = [<span class="number">10000</span>, <span class="number">20000</span>]</span><br><span class="line">    lr = [base_lr, base_lr * lr_decay, base_lr * lr_decay * lr_decay]</span><br><span class="line">    learning_rate = paddle.optimizer.lr.PiecewiseDecay(boundaries=bd, values=lr)</span><br><span class="line">    <span class="keyword">return</span> learning_rate</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    TRAINDIR = <span class="string">&#x27;/home/aistudio/work/insects/train&#x27;</span></span><br><span class="line">    TESTDIR = <span class="string">&#x27;/home/aistudio/work/insects/test&#x27;</span></span><br><span class="line">    VALIDDIR = <span class="string">&#x27;/home/aistudio/work/insects/val&#x27;</span></span><br><span class="line">    paddle.set_device(<span class="string">&quot;gpu:0&quot;</span>)</span><br><span class="line">    <span class="comment"># 创建数据读取类</span></span><br><span class="line">    train_dataset = TrainDataset(TRAINDIR, mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    valid_dataset = TrainDataset(VALIDDIR, mode=<span class="string">&#x27;valid&#x27;</span>)</span><br><span class="line">    test_dataset = TrainDataset(VALIDDIR, mode=<span class="string">&#x27;valid&#x27;</span>)</span><br><span class="line">    <span class="comment"># 使用paddle.io.DataLoader创建数据读取器，并设置batchsize，进程数量num_workers等参数</span></span><br><span class="line">    train_loader = paddle.io.DataLoader(train_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>, use_shared_memory=<span class="literal">False</span>)</span><br><span class="line">    valid_loader = paddle.io.DataLoader(valid_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>, use_shared_memory=<span class="literal">False</span>)</span><br><span class="line">    model = YOLOv3(num_classes = NUM_CLASSES)    <span class="comment"># 创建模型</span></span><br><span class="line">    learning_rate = get_lr()</span><br><span class="line">    opt = paddle.optimizer.Momentum(</span><br><span class="line">                 learning_rate=learning_rate,</span><br><span class="line">                 momentum=<span class="number">0.9</span>,</span><br><span class="line">                 weight_decay=paddle.regularizer.L2Decay(<span class="number">0.0005</span>),</span><br><span class="line">                 parameters=model.parameters())  <span class="comment"># 创建优化器</span></span><br><span class="line">    <span class="comment"># opt = paddle.optimizer.Adam(learning_rate=learning_rate, weight_decay=paddle.regularizer.L2Decay(0.0005), parameters=model.parameters())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">####迭代周期数自行调整######</span></span><br><span class="line"></span><br><span class="line">    MAX_EPOCH = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            img, gt_boxes, gt_labels, img_scale = data</span><br><span class="line">            gt_scores = np.ones(gt_labels.shape).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            gt_scores = paddle.to_tensor(gt_scores)</span><br><span class="line">            img = paddle.to_tensor(img)</span><br><span class="line">            gt_boxes = paddle.to_tensor(gt_boxes)</span><br><span class="line">            gt_labels = paddle.to_tensor(gt_labels)</span><br><span class="line">            outputs = model(img)  <span class="comment"># 前向传播，输出[P0, P1, P2]</span></span><br><span class="line">            loss = model.get_loss(outputs, gt_boxes, gt_labels, gtscore=gt_scores,</span><br><span class="line">                                  anchors = ANCHORS,</span><br><span class="line">                                  anchor_masks = ANCHOR_MASKS,</span><br><span class="line">                                  ignore_thresh=IGNORE_THRESH,</span><br><span class="line">                                  use_label_smooth=<span class="literal">False</span>)        <span class="comment"># 计算损失函数</span></span><br><span class="line"></span><br><span class="line">            loss.backward()    <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            opt.step()         <span class="comment"># 更新参数</span></span><br><span class="line">            opt.clear_grad()</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                timestring = time.strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>,time.localtime(time.time()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;[TRAIN]epoch &#123;&#125;, iter &#123;&#125;, output loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(timestring, epoch, i, loss.numpy()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># save params of model</span></span><br><span class="line">        <span class="keyword">if</span> (epoch % <span class="number">5</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == MAX_EPOCH -<span class="number">1</span>):</span><br><span class="line">            paddle.save(model.state_dict(), <span class="string">&#x27;yolo_epoch&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch结束之后在验证集上进行测试</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_loader()):</span><br><span class="line">            img, gt_boxes, gt_labels, img_scale = data</span><br><span class="line">            gt_scores = np.ones(gt_labels.shape).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            gt_scores = paddle.to_tensor(gt_scores)</span><br><span class="line">            img = paddle.to_tensor(img)</span><br><span class="line">            gt_boxes = paddle.to_tensor(gt_boxes)</span><br><span class="line">            gt_labels = paddle.to_tensor(gt_labels)</span><br><span class="line">            outputs = model(img)</span><br><span class="line">            loss = model.get_loss(outputs, gt_boxes, gt_labels, gtscore=gt_scores,</span><br><span class="line">                                  anchors = ANCHORS,</span><br><span class="line">                                  anchor_masks = ANCHOR_MASKS,</span><br><span class="line">                                  ignore_thresh=IGNORE_THRESH,</span><br><span class="line">                                  use_label_smooth=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                timestring = time.strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>,time.localtime(time.time()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;[VALID]epoch &#123;&#125;, iter &#123;&#125;, output loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(timestring, epoch, i, loss.numpy()))</span><br><span class="line">        model.train()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">paddle.save(model.state_dict(), <span class="string">&quot;linear_net.pdparams&quot;</span>)</span><br><span class="line">paddle.save(opt.state_dict(), <span class="string">&quot;opt.pdopt&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load</span></span><br><span class="line"><span class="comment"># layer_state_dict = paddle.load(&quot;linear_net.pdparams&quot;)</span></span><br><span class="line"><span class="comment"># opt_state_dict = paddle.load(&quot;opt.pdopt&quot;)</span></span><br><span class="line"><span class="comment"># layer.set_state_dict(layer_state_dict)</span></span><br><span class="line"><span class="comment"># opt.set_state_dict(opt_state_dict)</span></span><br></pre></td></tr></table></figure><h2 id="8-预测"><a href="#8-预测" class="headerlink" title="8.预测"></a><strong>8.预测</strong></h2><p>预测过程流程 <strong>图18</strong> 如下所示：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/15c140b1844d419cbe237b1a70f4099266aa168c05dc413f8e232f688050fa75" width = "400"></center><center><br>图18：预测流程 </br></center><p><br></br></p><p><strong>预测过程可以分为两步</strong>：</p><ol><li>通过网络输出计算出预测框位置和所属类别的得分。 </li><li>使用非极大值抑制来消除重叠较大的预测框。</li></ol><p>对于第1步，前面我们已经讲过如何通过网络输出值计算pred_objectness_probability, pred_boxes以及pred_classification_probability，这里推荐大家直接使用<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/ops/yolo_box_cn.html#yolo-box">paddle.vision.ops.yolo_box</a>，关键参数含义如下：</p><p><code>paddle.vision.ops.yolo_box(x, img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox=True, name=None, scale_x_y=1.0)</code></p><ul><li>x，网络输出特征图，例如上面提到的P0或者P1、P2。</li><li>img_size，输入图片尺寸。</li><li>anchors，使用到的anchor的尺寸，如[10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]</li><li>class_num，物体类别数。</li><li>conf_thresh, 置信度阈值，得分低于该阈值的预测框位置数值不用计算直接设置为0.0。</li><li>downsample_ratio, 特征图的下采样比例，例如P0是32，P1是16，P2是8。</li><li>name=None，名字，例如’yolo_box’，一般无需设置，默认值为None。</li></ul><p>返回值包括两项，<code>boxes</code>和<code>scores</code>，其中<code>boxes</code>是所有预测框的坐标值，<code>scores</code>是所有预测框的得分。</p><p>预测框得分的定义是所属类别的概率乘以其预测框是否包含目标物体的objectness概率，即</p><script type="math/tex; mode=display">score = P_{obj} \cdot P_{classification}</script><p>在上面定义的类YOLOv3下面添加函数<code>get_pred</code>，通过调用<code>paddle.vision.ops.yolo_box</code>获得P0、P1、P2三个层级的特征图对应的预测框和得分，并将他们拼接在一块，即可得到所有的预测框及其属于各个类别的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义YOLOv3模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YOLOv3</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YOLOv3,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 提取图像特征的骨干代码</span></span><br><span class="line">        self.block = DarkNet53_conv_body()</span><br><span class="line">        self.block_outputs = []</span><br><span class="line">        self.yolo_blocks = []</span><br><span class="line">        self.route_blocks_2 = []</span><br><span class="line">        <span class="comment"># 生成3个层级的特征图P0, P1, P2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="comment"># 添加从ci生成ri和ti的模块</span></span><br><span class="line">            yolo_block = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;yolo_detecton_block_%d&quot;</span> % (i),</span><br><span class="line">                YoloDetectionBlock(</span><br><span class="line">                                   ch_in=<span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span> <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> <span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span> + <span class="number">512</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                   ch_out = <span class="number">512</span>//(<span class="number">2</span>**i)))</span><br><span class="line">            self.yolo_blocks.append(yolo_block)</span><br><span class="line"></span><br><span class="line">            num_filters = <span class="number">3</span> * (self.num_classes + <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加从ti生成pi的模块，这是一个Conv2D操作，输出通道数为3 * (num_classes + 5)</span></span><br><span class="line">            block_out = self.add_sublayer(</span><br><span class="line">                <span class="string">&quot;block_out_%d&quot;</span> % (i),</span><br><span class="line">                paddle.nn.Conv2D(in_channels=<span class="number">512</span>//(<span class="number">2</span>**i)*<span class="number">2</span>,</span><br><span class="line">                       out_channels=num_filters,</span><br><span class="line">                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                       stride=<span class="number">1</span>,</span><br><span class="line">                       padding=<span class="number">0</span>,</span><br><span class="line">                       weight_attr=paddle.ParamAttr(</span><br><span class="line">                           initializer=paddle.nn.initializer.Normal(<span class="number">0.</span>, <span class="number">0.02</span>)),</span><br><span class="line">                       bias_attr=paddle.ParamAttr(</span><br><span class="line">                           initializer=paddle.nn.initializer.Constant(<span class="number">0.0</span>),</span><br><span class="line">                           regularizer=paddle.regularizer.L2Decay(<span class="number">0.</span>))))</span><br><span class="line">            self.block_outputs.append(block_out)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 对ri进行卷积</span></span><br><span class="line">                route = self.add_sublayer(<span class="string">&quot;route2_%d&quot;</span>%i,</span><br><span class="line">                                          ConvBNLayer(ch_in=<span class="number">512</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                                      ch_out=<span class="number">256</span>//(<span class="number">2</span>**i),</span><br><span class="line">                                                      kernel_size=<span class="number">1</span>,</span><br><span class="line">                                                      stride=<span class="number">1</span>,</span><br><span class="line">                                                      padding=<span class="number">0</span>))</span><br><span class="line">                self.route_blocks_2.append(route)</span><br><span class="line">            <span class="comment"># 将ri放大以便跟c_&#123;i+1&#125;保持同样的尺寸</span></span><br><span class="line">            self.upsample = Upsample()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = []</span><br><span class="line">        blocks = self.block(inputs)</span><br><span class="line">        <span class="keyword">for</span> i, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(blocks):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将r_&#123;i-1&#125;经过卷积和上采样之后得到特征图，与这一级的ci进行拼接</span></span><br><span class="line">                block = paddle.concat([route, block], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 从ci生成ti和ri</span></span><br><span class="line">            route, tip = self.yolo_blocks[i](block)</span><br><span class="line">            <span class="comment"># 从ti生成pi</span></span><br><span class="line">            block_out = self.block_outputs[i](tip)</span><br><span class="line">            <span class="comment"># 将pi放入列表</span></span><br><span class="line">            outputs.append(block_out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 对ri进行卷积调整通道数</span></span><br><span class="line">                route = self.route_blocks_2[i](route)</span><br><span class="line">                <span class="comment"># 对ri进行放大，使其尺寸和c_&#123;i+1&#125;保持一致</span></span><br><span class="line">                route = self.upsample(route)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_loss</span>(<span class="params">self, outputs, gtbox, gtlabel, gtscore=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 anchors = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span></span><br><span class="line"><span class="params">                 anchor_masks = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</span></span><br><span class="line"><span class="params">                 ignore_thresh=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                 use_label_smooth=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用paddle.vision.ops.yolo_loss，直接计算损失函数，过程更简洁，速度也更快</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.losses = []</span><br><span class="line">        downsample = <span class="number">32</span></span><br><span class="line">        <span class="keyword">for</span> i, out <span class="keyword">in</span> <span class="built_in">enumerate</span>(outputs): <span class="comment"># 对三个层级分别求损失函数</span></span><br><span class="line">            anchor_mask_i = anchor_masks[i]</span><br><span class="line">            loss = paddle.vision.ops.yolo_loss(</span><br><span class="line">                    x=out,  <span class="comment"># out是P0, P1, P2中的一个</span></span><br><span class="line">                    gt_box=gtbox,  <span class="comment"># 真实框坐标</span></span><br><span class="line">                    gt_label=gtlabel,  <span class="comment"># 真实框类别</span></span><br><span class="line">                    gt_score=gtscore,  <span class="comment"># 真实框得分，使用mixup训练技巧时需要，不使用该技巧时直接设置为1，形状与gtlabel相同</span></span><br><span class="line">                    anchors=anchors,   <span class="comment"># 锚框尺寸，包含[w0, h0, w1, h1, ..., w8, h8]共9个锚框的尺寸</span></span><br><span class="line">                    anchor_mask=anchor_mask_i, <span class="comment"># 筛选锚框的mask，例如anchor_mask_i=[3, 4, 5]，将anchors中第3、4、5个锚框挑选出来给该层级使用</span></span><br><span class="line">                    class_num=self.num_classes, <span class="comment"># 分类类别数</span></span><br><span class="line">                    ignore_thresh=ignore_thresh, <span class="comment"># 当预测框与真实框IoU &gt; ignore_thresh，标注objectness = -1</span></span><br><span class="line">                    downsample_ratio=downsample, <span class="comment"># 特征图相对于原图缩小的倍数，例如P0是32， P1是16，P2是8</span></span><br><span class="line">                    use_label_smooth=<span class="literal">False</span>)      <span class="comment"># 使用label_smooth训练技巧时会用到，这里没用此技巧，直接设置为False</span></span><br><span class="line">            self.losses.append(paddle.mean(loss))  <span class="comment">#mean对每张图片求和</span></span><br><span class="line">            downsample = downsample // <span class="number">2</span> <span class="comment"># 下一级特征图的缩放倍数会减半</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.losses) <span class="comment"># 对每个层级求和</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_pred</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 outputs,</span></span><br><span class="line"><span class="params">                 im_shape=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 anchors = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>],</span></span><br><span class="line"><span class="params">                 anchor_masks = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</span></span><br><span class="line"><span class="params">                 valid_thresh = <span class="number">0.01</span></span>):</span><br><span class="line">        downsample = <span class="number">32</span></span><br><span class="line">        total_boxes = []</span><br><span class="line">        total_scores = []</span><br><span class="line">        <span class="keyword">for</span> i, out <span class="keyword">in</span> <span class="built_in">enumerate</span>(outputs):</span><br><span class="line">            anchor_mask = anchor_masks[i]</span><br><span class="line">            anchors_this_level = []</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> anchor_mask:</span><br><span class="line">                anchors_this_level.append(anchors[<span class="number">2</span> * m])</span><br><span class="line">                anchors_this_level.append(anchors[<span class="number">2</span> * m + <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            boxes, scores = paddle.vision.ops.yolo_box(</span><br><span class="line">                   x=out,</span><br><span class="line">                   img_size=im_shape,</span><br><span class="line">                   anchors=anchors_this_level,</span><br><span class="line">                   class_num=self.num_classes,</span><br><span class="line">                   conf_thresh=valid_thresh,</span><br><span class="line">                   downsample_ratio=downsample,</span><br><span class="line">                   name=<span class="string">&quot;yolo_box&quot;</span> + <span class="built_in">str</span>(i))</span><br><span class="line">            total_boxes.append(boxes)</span><br><span class="line">            total_scores.append(</span><br><span class="line">                        paddle.transpose(</span><br><span class="line">                        scores, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">            downsample = downsample // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        yolo_boxes = paddle.concat(total_boxes, axis=<span class="number">1</span>)</span><br><span class="line">        yolo_scores = paddle.concat(total_scores, axis=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> yolo_boxes, yolo_scores</span><br></pre></td></tr></table></figure><p>第1步的计算结果会在每个小方块区域上生成多个预测框，而这些预测框中很多都有较大的重合度，因此需要消除重叠较大的冗余检测框。</p><p>下面示例代码中的预测框是使用模型对图片预测之后输出的，这里一共选出了11个预测框，在图上画出预测框如下所示。在每个人像周围，都出现了多个预测框，需要消除冗余的预测框以得到最终的预测结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画图展示目标物体边界框</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画矩形框的程序    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_rectangle</span>(<span class="params">currentAxis, bbox, edgecolor = <span class="string">&#x27;k&#x27;</span>, facecolor = <span class="string">&#x27;y&#x27;</span>, fill=<span class="literal">False</span>, linestyle=<span class="string">&#x27;-&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># currentAxis，坐标轴，通过plt.gca()获取</span></span><br><span class="line">    <span class="comment"># bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]</span></span><br><span class="line">    <span class="comment"># edgecolor，边框线条颜色</span></span><br><span class="line">    <span class="comment"># facecolor，填充颜色</span></span><br><span class="line">    <span class="comment"># fill, 是否填充</span></span><br><span class="line">    <span class="comment"># linestype，边框线型</span></span><br><span class="line">    <span class="comment"># patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数</span></span><br><span class="line">    rect=patches.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>]+<span class="number">1</span>, bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]+<span class="number">1</span>, linewidth=<span class="number">1</span>,</span><br><span class="line">                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)</span><br><span class="line">    currentAxis.add_patch(rect)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;/home/aistudio/000000086956.jpg&#x27;</span></span><br><span class="line">im = imread(filename)</span><br><span class="line">plt.imshow(im)</span><br><span class="line"></span><br><span class="line">currentAxis=plt.gca()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测框位置</span></span><br><span class="line">boxes = np.array([[<span class="number">4.21716537e+01</span>, <span class="number">1.28230896e+02</span>, <span class="number">2.26547668e+02</span>, <span class="number">6.00434631e+02</span>],</span><br><span class="line">       [<span class="number">3.18562988e+02</span>, <span class="number">1.23168472e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.05688416e+02</span>],</span><br><span class="line">       [<span class="number">2.62704697e+01</span>, <span class="number">1.39430557e+02</span>, <span class="number">2.20587097e+02</span>, <span class="number">6.38959656e+02</span>],</span><br><span class="line">       [<span class="number">4.24965363e+01</span>, <span class="number">1.42706665e+02</span>, <span class="number">2.25955185e+02</span>, <span class="number">6.35671204e+02</span>],</span><br><span class="line">       [<span class="number">2.37462646e+02</span>, <span class="number">1.35731537e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.31451294e+02</span>],</span><br><span class="line">       [<span class="number">3.19390472e+02</span>, <span class="number">1.29295090e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.33003845e+02</span>],</span><br><span class="line">       [<span class="number">3.28933838e+02</span>, <span class="number">1.22736115e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.39000000e+02</span>],</span><br><span class="line">       [<span class="number">4.44292603e+01</span>, <span class="number">1.70438187e+02</span>, <span class="number">2.26841858e+02</span>, <span class="number">6.39000000e+02</span>],</span><br><span class="line">       [<span class="number">2.17988785e+02</span>, <span class="number">3.02472412e+02</span>, <span class="number">4.06062927e+02</span>, <span class="number">6.29106628e+02</span>],</span><br><span class="line">       [<span class="number">2.00241089e+02</span>, <span class="number">3.23755096e+02</span>, <span class="number">3.96929321e+02</span>, <span class="number">6.36386108e+02</span>],</span><br><span class="line">       [<span class="number">2.14310303e+02</span>, <span class="number">3.23443665e+02</span>, <span class="number">4.06732849e+02</span>, <span class="number">6.35775269e+02</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测框得分</span></span><br><span class="line">scores = np.array([<span class="number">0.5247661</span> , <span class="number">0.51759845</span>, <span class="number">0.86075854</span>, <span class="number">0.9910175</span> , <span class="number">0.39170712</span>,</span><br><span class="line">       <span class="number">0.9297706</span> , <span class="number">0.5115228</span> , <span class="number">0.270992</span>  , <span class="number">0.19087596</span>, <span class="number">0.64201415</span>, <span class="number">0.879036</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出所有预测框</span></span><br><span class="line"><span class="keyword">for</span> box <span class="keyword">in</span> boxes:</span><br><span class="line">    draw_rectangle(currentAxis, box)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/72.png" alt="png"></p><p><br></p><p><strong>非极大值抑制（non-maximum suppression, nms）</strong></p><p>这里使用非极大值抑制来消除冗余框。基本思想是，如果有多个预测框都对应同一个物体，则只选出得分最高的那个预测框，剩下的预测框被丢弃掉。如何判断两个预测框对应的是同一个物体呢，标准该怎么设置？<strong>如果两个预测框的类别一样，而且他们的位置重合度比较大，则可以认为他们是在预测同一个目标</strong>。</p><p><code>非极大值抑制的做法是，选出某个类别得分最高的预测框，然后看哪些预测框跟它的IoU大于阈值，就把这些预测框给丢弃掉。这里IoU的阈值是超参数，需要提前设置，YOLOv3模型里面设置的是0.5</code>。</p><p>比如在上面的程序中，boxes里面一共对应11个预测框，scores给出了它们预测”人”这一类别的得分。</p><ul><li>Step0：创建选中列表，keep_list = []</li><li>Step1：对得分进行排序，remain_list = [ 3,  5, 10,  2,  9,  0,  1,  6,  4,  7,  8]， </li><li>Step2：选出boxes[3]，此时keep_list为空，不需要计算IoU，直接将其放入keep_list，keep_list = [3]， remain_list=[5, 10,  2,  9,  0,  1,  6,  4,  7,  8]</li><li>Step3：选出boxes[5]，此时keep_list中已经存在boxes[3]，计算出IoU(boxes[3], boxes[5]) = 0.0，显然小于阈值，则keep_list=[3, 5], remain_list = [10,  2,  9,  0,  1,  6,  4,  7,  8]</li><li>Step4：选出boxes[10]，此时keep_list=[3, 5]，计算IoU(boxes[3], boxes[10])=0.0268，IoU(boxes[5], boxes[10])=0.0268 = 0.24，都小于阈值，则keep_list = [3, 5, 10]，remain_list=[2,  9,  0,  1,  6,  4,  7,  8]</li><li>Step5：选出boxes[2]，此时keep_list = [3, 5, 10]，计算IoU(boxes[3], boxes[2]) = 0.88，超过了阈值，直接将boxes[2]丢弃，keep_list=[3, 5, 10]，remain_list=[9,  0,  1,  6,  4,  7,  8]</li><li>Step6：选出boxes[9]，此时keep_list = [3, 5, 10]，计算IoU(boxes[3], boxes[9]) = 0.0577，IoU(boxes[5], boxes[9]) = 0.205，IoU(boxes[10], boxes[9]) = 0.88，超过了阈值，将boxes[9]丢弃掉。keep_list=[3, 5, 10]，remain_list=[0,  1,  6,  4,  7,  8]</li><li>Step7：重复上述Step6直到remain_list为空。</li></ul><p>最终得到keep_list=[3, 5, 10]，也就是预测框3、5、10被最终挑选出来了，如下图所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画图展示目标物体边界框</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画矩形框的程序    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_rectangle</span>(<span class="params">currentAxis, bbox, edgecolor = <span class="string">&#x27;k&#x27;</span>, facecolor = <span class="string">&#x27;y&#x27;</span>, fill=<span class="literal">False</span>, linestyle=<span class="string">&#x27;-&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># currentAxis，坐标轴，通过plt.gca()获取</span></span><br><span class="line">    <span class="comment"># bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]</span></span><br><span class="line">    <span class="comment"># edgecolor，边框线条颜色</span></span><br><span class="line">    <span class="comment"># facecolor，填充颜色</span></span><br><span class="line">    <span class="comment"># fill, 是否填充</span></span><br><span class="line">    <span class="comment"># linestype，边框线型</span></span><br><span class="line">    <span class="comment"># patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数</span></span><br><span class="line">    rect=patches.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>]+<span class="number">1</span>, bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]+<span class="number">1</span>, linewidth=<span class="number">1</span>,</span><br><span class="line">                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)</span><br><span class="line">    currentAxis.add_patch(rect)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;000000086956.jpg&#x27;</span></span><br><span class="line">im = imread(filename)</span><br><span class="line">plt.imshow(im)</span><br><span class="line"></span><br><span class="line">currentAxis=plt.gca()</span><br><span class="line"></span><br><span class="line">boxes = np.array([[<span class="number">4.21716537e+01</span>, <span class="number">1.28230896e+02</span>, <span class="number">2.26547668e+02</span>, <span class="number">6.00434631e+02</span>],</span><br><span class="line">       [<span class="number">3.18562988e+02</span>, <span class="number">1.23168472e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.05688416e+02</span>],</span><br><span class="line">       [<span class="number">2.62704697e+01</span>, <span class="number">1.39430557e+02</span>, <span class="number">2.20587097e+02</span>, <span class="number">6.38959656e+02</span>],</span><br><span class="line">       [<span class="number">4.24965363e+01</span>, <span class="number">1.42706665e+02</span>, <span class="number">2.25955185e+02</span>, <span class="number">6.35671204e+02</span>],</span><br><span class="line">       [<span class="number">2.37462646e+02</span>, <span class="number">1.35731537e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.31451294e+02</span>],</span><br><span class="line">       [<span class="number">3.19390472e+02</span>, <span class="number">1.29295090e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.33003845e+02</span>],</span><br><span class="line">       [<span class="number">3.28933838e+02</span>, <span class="number">1.22736115e+02</span>, <span class="number">4.79000000e+02</span>, <span class="number">6.39000000e+02</span>],</span><br><span class="line">       [<span class="number">4.44292603e+01</span>, <span class="number">1.70438187e+02</span>, <span class="number">2.26841858e+02</span>, <span class="number">6.39000000e+02</span>],</span><br><span class="line">       [<span class="number">2.17988785e+02</span>, <span class="number">3.02472412e+02</span>, <span class="number">4.06062927e+02</span>, <span class="number">6.29106628e+02</span>],</span><br><span class="line">       [<span class="number">2.00241089e+02</span>, <span class="number">3.23755096e+02</span>, <span class="number">3.96929321e+02</span>, <span class="number">6.36386108e+02</span>],</span><br><span class="line">       [<span class="number">2.14310303e+02</span>, <span class="number">3.23443665e+02</span>, <span class="number">4.06732849e+02</span>, <span class="number">6.35775269e+02</span>]])</span><br><span class="line"> </span><br><span class="line">scores = np.array([<span class="number">0.5247661</span> , <span class="number">0.51759845</span>, <span class="number">0.86075854</span>, <span class="number">0.9910175</span> , <span class="number">0.39170712</span>,</span><br><span class="line">       <span class="number">0.9297706</span> , <span class="number">0.5115228</span> , <span class="number">0.270992</span>  , <span class="number">0.19087596</span>, <span class="number">0.64201415</span>, <span class="number">0.879036</span>])</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出最终保留的预测框</span></span><br><span class="line">inds = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    box = boxes[inds[i]]</span><br><span class="line">    draw_rectangle(currentAxis, box, edgecolor=colors[i])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/73.png" alt="png"></p><p>非极大值抑制的具体实现代码如下面的<code>nms</code>函数的定义，需要说明的是数据集中含有多个类别的物体，所以这里需要做多分类非极大值抑制，其实现原理与非极大值抑制相同，区别在于需要对每个类别都做非极大值抑制，实现代码如下面的<code>multiclass_nms</code>所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非极大值抑制</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">bboxes, scores, score_thresh, nms_thresh, pre_nms_topk, i=<span class="number">0</span>, c=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    nms</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从小到大排列取出索引</span></span><br><span class="line">    inds = np.argsort(scores)</span><br><span class="line">    inds = inds[::-<span class="number">1</span>]</span><br><span class="line">    keep_inds = []</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(inds) &gt; <span class="number">0</span>):</span><br><span class="line">        cur_ind = inds[<span class="number">0</span>]</span><br><span class="line">        cur_score = scores[cur_ind]</span><br><span class="line">        <span class="comment"># if score of the box is less than score_thresh, just drop it</span></span><br><span class="line">        <span class="keyword">if</span> cur_score &lt; score_thresh:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        keep = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> ind <span class="keyword">in</span> keep_inds:</span><br><span class="line">            current_box = bboxes[cur_ind]</span><br><span class="line">            remain_box = bboxes[ind]</span><br><span class="line">            iou = box_iou_xyxy(current_box, remain_box)</span><br><span class="line">            <span class="keyword">if</span> iou &gt; nms_thresh:</span><br><span class="line">                keep = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#if i == 0 and c == 4 and cur_ind == 951:</span></span><br><span class="line">            <span class="comment">#print(&#x27;suppressed, &#x27;, keep, i, c, cur_ind, ind, iou)</span></span><br><span class="line">        <span class="keyword">if</span> keep:</span><br><span class="line">            keep_inds.append(cur_ind)</span><br><span class="line">        inds = inds[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(keep_inds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多分类非极大值抑制</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiclass_nms</span>(<span class="params">bboxes, scores, score_thresh=<span class="number">0.01</span>, nms_thresh=<span class="number">0.45</span>, pre_nms_topk=<span class="number">1000</span>, pos_nms_topk=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This is for multiclass_nms</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size = bboxes.shape[<span class="number">0</span>]</span><br><span class="line">    class_num = scores.shape[<span class="number">1</span>]</span><br><span class="line">    rets = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        bboxes_i = bboxes[i]</span><br><span class="line">        scores_i = scores[i]</span><br><span class="line">        ret = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(class_num):</span><br><span class="line">            scores_i_c = scores_i[c]</span><br><span class="line">            keep_inds = nms(bboxes_i, scores_i_c, score_thresh, nms_thresh, pre_nms_topk, i=i, c=c)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(keep_inds) &lt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            keep_bboxes = bboxes_i[keep_inds]</span><br><span class="line">            keep_scores = scores_i_c[keep_inds]</span><br><span class="line">            keep_results = np.zeros([keep_scores.shape[<span class="number">0</span>], <span class="number">6</span>])</span><br><span class="line">            keep_results[:, <span class="number">0</span>] = c</span><br><span class="line">            keep_results[:, <span class="number">1</span>] = keep_scores[:]</span><br><span class="line">            keep_results[:, <span class="number">2</span>:<span class="number">6</span>] = keep_bboxes[:, :]</span><br><span class="line">            ret.append(keep_results)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(ret) &lt; <span class="number">1</span>:</span><br><span class="line">            rets.append(ret)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        ret_i = np.concatenate(ret, axis=<span class="number">0</span>)</span><br><span class="line">        scores_i = ret_i[:, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(scores_i) &gt; pos_nms_topk:</span><br><span class="line">            inds = np.argsort(scores_i)[::-<span class="number">1</span>]</span><br><span class="line">            inds = inds[:pos_nms_topk]</span><br><span class="line">            ret_i = ret_i[inds]</span><br><span class="line"></span><br><span class="line">        rets.append(ret_i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rets</span><br></pre></td></tr></table></figure><p>下面是完整的测试程序，在测试数据集上的输出结果将会被保存在<code>pred_results.json</code>文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算IoU，矩形框的坐标形式为xyxy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou_xyxy</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    <span class="comment"># 获取box1左上角和右下角的坐标</span></span><br><span class="line">    x1min, y1min, x1max, y1max = box1[<span class="number">0</span>], box1[<span class="number">1</span>], box1[<span class="number">2</span>], box1[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 计算box1的面积</span></span><br><span class="line">    s1 = (y1max - y1min + <span class="number">1.</span>) * (x1max - x1min + <span class="number">1.</span>)</span><br><span class="line">    <span class="comment"># 获取box2左上角和右下角的坐标</span></span><br><span class="line">    x2min, y2min, x2max, y2max = box2[<span class="number">0</span>], box2[<span class="number">1</span>], box2[<span class="number">2</span>], box2[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 计算box2的面积</span></span><br><span class="line">    s2 = (y2max - y2min + <span class="number">1.</span>) * (x2max - x2min + <span class="number">1.</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算相交矩形框的坐标</span></span><br><span class="line">    xmin = np.maximum(x1min, x2min)</span><br><span class="line">    ymin = np.maximum(y1min, y2min)</span><br><span class="line">    xmax = np.minimum(x1max, x2max)</span><br><span class="line">    ymax = np.minimum(y1max, y2max)</span><br><span class="line">    <span class="comment"># 计算相交矩形行的高度、宽度、面积</span></span><br><span class="line">    inter_h = np.maximum(ymax - ymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">    inter_w = np.maximum(xmax - xmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">    intersection = inter_h * inter_w</span><br><span class="line">    <span class="comment"># 计算相并面积</span></span><br><span class="line">    union = s1 + s2 - intersection</span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = intersection / union</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 yolo_epoch50 数据脚本</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始解压......&#x27;</span>)</span><br><span class="line">!unzip  -o -q -d  /home/aistudio /home/aistudio/data/data170339/yolo_epoch50.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;解压完成&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>开始解压......解压完成</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">ANCHORS = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line">ANCHOR_MASKS = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line">VALID_THRESH = <span class="number">0.01</span></span><br><span class="line">NMS_TOPK = <span class="number">400</span></span><br><span class="line">NMS_POSK = <span class="number">100</span></span><br><span class="line">NMS_THRESH = <span class="number">0.45</span></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    TRAINDIR = <span class="string">&#x27;/home/aistudio/work/insects/train/images&#x27;</span></span><br><span class="line">    TESTDIR = <span class="string">&#x27;/home/aistudio/work/insects/test/images&#x27;</span></span><br><span class="line">    VALIDDIR = <span class="string">&#x27;/home/aistudio/work/insects/val&#x27;</span></span><br><span class="line"></span><br><span class="line">    model = YOLOv3(num_classes=NUM_CLASSES)</span><br><span class="line">    params_file_path = <span class="string">&#x27;/home/aistudio/yolo_epoch50.pdparams&#x27;</span></span><br><span class="line">    model_state_dict = paddle.load(params_file_path)</span><br><span class="line">    model.load_dict(model_state_dict)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    total_results = []</span><br><span class="line">    test_loader = test_data_loader(TESTDIR, batch_size= <span class="number">1</span>, mode=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader()):</span><br><span class="line">        img_name, img_data, img_scale_data = data</span><br><span class="line">        img = paddle.to_tensor(img_data)</span><br><span class="line">        img_scale = paddle.to_tensor(img_scale_data)</span><br><span class="line"></span><br><span class="line">        outputs = model.forward(img)</span><br><span class="line">        bboxes, scores = model.get_pred(outputs,</span><br><span class="line">                                 im_shape=img_scale,</span><br><span class="line">                                 anchors=ANCHORS,</span><br><span class="line">                                 anchor_masks=ANCHOR_MASKS,</span><br><span class="line">                                 valid_thresh = VALID_THRESH)</span><br><span class="line"></span><br><span class="line">        bboxes_data = bboxes.numpy()</span><br><span class="line">        scores_data = scores.numpy()</span><br><span class="line">        result = multiclass_nms(bboxes_data, scores_data,</span><br><span class="line">                      score_thresh=VALID_THRESH, </span><br><span class="line">                      nms_thresh=NMS_THRESH, </span><br><span class="line">                      pre_nms_topk=NMS_TOPK, </span><br><span class="line">                      pos_nms_topk=NMS_POSK)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)):</span><br><span class="line">            result_j = result[j]</span><br><span class="line">            img_name_j = img_name[j]</span><br><span class="line">            total_results.append([img_name_j, result_j.tolist()])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;processed &#123;&#125; pictures&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(total_results)))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    json.dump(total_results, <span class="built_in">open</span>(<span class="string">&#x27;pred_results.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>))</span><br></pre></td></tr></table></figure><p>json文件中保存着测试结果，是包含所有图片预测结果的list，其构成如下：</p><pre><code>[[img_name, [[label, score, x1, y1, x2, y2], ..., [label, score, x1, y1, x2, y2]]],  [img_name, [[label, score, x1, y1, x2, y2], ..., [label, score, x1, y1, x2, y2]]],  ... [img_name, [[label, score, x1, y1, x2, y2],..., [label, score, x1, y1, x2, y2]]]]</code></pre><p>list中的每一个元素是一张图片的预测结果，list的总长度等于图片的数目，每张图片预测结果的格式是：</p><pre><code> [img_name, [[label, score, x1, y1, x2, y2],..., [label, score, x1, y1, x2, y2]]]</code></pre><p>其中第一个元素是图片名称image_name，第二个元素是包含该图片所有预测框的list， 预测框列表：</p><pre><code> [[label, score, x1, x2, y1, y2],..., [label, score, x1, y1, x2, y2]]</code></pre><p>预测框列表中每个元素[label, score, x1, y1, x2, y2]描述了一个预测框，label是预测框所属类别标签，score是预测框的得分；x1, y1, x2, y2对应预测框左上角坐标(x1, y1)，右下角坐标(x2, y2)。每张图片可能有很多个预测框，则将其全部放在预测框列表中。</p><h2 id="9-模型效果及可视化展示"><a href="#9-模型效果及可视化展示" class="headerlink" title="9.模型效果及可视化展示"></a><strong>9.模型效果及可视化展示</strong></h2><p>上面的程序展示了如何读取测试数据集的图片，并将最终结果保存在json格式的文件中。为了更直观的给读者展示模型效果，下面的程序添加了如何读取单张图片，并画出其产生的预测框。</p><p><strong>1. 创建数据读取器以读取单张图片的数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取单张测试图片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">single_image_data_loader</span>(<span class="params">filename, test_image_size=<span class="number">608</span>, mode=<span class="string">&#x27;test&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载测试用的图片，测试数据没有groundtruth标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size= <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        batch_data = []</span><br><span class="line">        img_size = test_image_size</span><br><span class="line">        file_path = os.path.join(filename)</span><br><span class="line">        img = cv2.imread(file_path)</span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">        H = img.shape[<span class="number">0</span>]</span><br><span class="line">        W = img.shape[<span class="number">1</span>]</span><br><span class="line">        img = cv2.resize(img, (img_size, img_size))</span><br><span class="line"></span><br><span class="line">        mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">        std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">        mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        out_img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">        out_img = out_img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        img = out_img <span class="comment">#np.transpose(out_img, (2,0,1))</span></span><br><span class="line">        im_shape = [H, W]</span><br><span class="line"></span><br><span class="line">        batch_data.append((image_name.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>], img, im_shape))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_data) == batch_size:</span><br><span class="line">            <span class="keyword">yield</span> make_test_array(batch_data)</span><br><span class="line">            batch_data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><p><strong>2. 定义绘制预测框的画图函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义画图函数</span></span><br><span class="line">INSECT_NAMES = [<span class="string">&#x27;Boerner&#x27;</span>, <span class="string">&#x27;Leconte&#x27;</span>, <span class="string">&#x27;Linnaeus&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;acuminatus&#x27;</span>, <span class="string">&#x27;armandi&#x27;</span>, <span class="string">&#x27;coleoptera&#x27;</span>, <span class="string">&#x27;linnaeus&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画矩形框的函数 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_rectangle</span>(<span class="params">currentAxis, bbox, edgecolor = <span class="string">&#x27;k&#x27;</span>, facecolor = <span class="string">&#x27;y&#x27;</span>, fill=<span class="literal">False</span>, linestyle=<span class="string">&#x27;-&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># currentAxis，坐标轴，通过plt.gca()获取</span></span><br><span class="line">    <span class="comment"># bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]</span></span><br><span class="line">    <span class="comment"># edgecolor，边框线条颜色</span></span><br><span class="line">    <span class="comment"># facecolor，填充颜色</span></span><br><span class="line">    <span class="comment"># fill, 是否填充</span></span><br><span class="line">    <span class="comment"># linestype，边框线型</span></span><br><span class="line">    <span class="comment"># patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数</span></span><br><span class="line">    rect=patches.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>]+<span class="number">1</span>, bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]+<span class="number">1</span>, linewidth=<span class="number">1</span>,</span><br><span class="line">                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)</span><br><span class="line">    currentAxis.add_patch(rect)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义绘制预测结果的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_results</span>(<span class="params">result, filename, draw_thresh=<span class="number">0.5</span></span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    im = imread(filename)</span><br><span class="line">    plt.imshow(im)</span><br><span class="line">    currentAxis=plt.gca()</span><br><span class="line">    colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;purple&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> result:</span><br><span class="line">        box = item[<span class="number">2</span>:<span class="number">6</span>]</span><br><span class="line">        label = <span class="built_in">int</span>(item[<span class="number">0</span>])</span><br><span class="line">        name = INSECT_NAMES[label]</span><br><span class="line">        <span class="keyword">if</span> item[<span class="number">1</span>] &gt; draw_thresh:</span><br><span class="line">            draw_rectangle(currentAxis, box, edgecolor = colors[label])</span><br><span class="line">            plt.text(box[<span class="number">0</span>], box[<span class="number">1</span>], name, fontsize=<span class="number">12</span>, color=colors[label])</span><br></pre></td></tr></table></figure><p><strong>3. 读取与预测可视化</strong></p><p>使用上面定义的single_image_data_loader函数读取指定的图片，输入网络并计算出预测框和得分，然后使用多分类非极大值抑制消除冗余的框。将最终结果画图展示出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">ANCHORS = [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">23</span>, <span class="number">30</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">45</span>, <span class="number">59</span>, <span class="number">119</span>, <span class="number">116</span>, <span class="number">90</span>, <span class="number">156</span>, <span class="number">198</span>, <span class="number">373</span>, <span class="number">326</span>]</span><br><span class="line">ANCHOR_MASKS = [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line">VALID_THRESH = <span class="number">0.01</span></span><br><span class="line">NMS_TOPK = <span class="number">400</span></span><br><span class="line">NMS_POSK = <span class="number">100</span></span><br><span class="line">NMS_THRESH = <span class="number">0.45</span></span><br><span class="line"></span><br><span class="line">NUM_CLASSES = <span class="number">7</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    image_name = <span class="string">&#x27;/home/aistudio/work/insects/test/images/1880.jpeg&#x27;</span></span><br><span class="line">    params_file_path = <span class="string">&#x27;/home/aistudio/yolo_epoch50.pdparams&#x27;</span></span><br><span class="line"></span><br><span class="line">    model = YOLOv3(num_classes=NUM_CLASSES)</span><br><span class="line">    model_state_dict = paddle.load(params_file_path)</span><br><span class="line">    model.load_dict(model_state_dict)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    total_results = []</span><br><span class="line">    test_loader = single_image_data_loader(image_name, mode=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader()):</span><br><span class="line">        img_name, img_data, img_scale_data = data</span><br><span class="line">        img = paddle.to_tensor(img_data)</span><br><span class="line">        img_scale = paddle.to_tensor(img_scale_data)</span><br><span class="line"></span><br><span class="line">        outputs = model.forward(img)</span><br><span class="line">        bboxes, scores = model.get_pred(outputs,</span><br><span class="line">                                 im_shape=img_scale,</span><br><span class="line">                                 anchors=ANCHORS,</span><br><span class="line">                                 anchor_masks=ANCHOR_MASKS,</span><br><span class="line">                                 valid_thresh = VALID_THRESH)</span><br><span class="line"></span><br><span class="line">        bboxes_data = bboxes.numpy()</span><br><span class="line">        scores_data = scores.numpy()</span><br><span class="line">        results = multiclass_nms(bboxes_data, scores_data,</span><br><span class="line">                      score_thresh=VALID_THRESH, </span><br><span class="line">                      nms_thresh=NMS_THRESH, </span><br><span class="line">                      pre_nms_topk=NMS_TOPK, </span><br><span class="line">                      pos_nms_topk=NMS_POSK)</span><br><span class="line"></span><br><span class="line">result = results[<span class="number">0</span>]</span><br><span class="line">draw_results(result, image_name, draw_thresh=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/74.png" alt="png"></p><p>通过上面的程序，清晰的给读者展示了如何使用训练好的权重，对图片进行预测并将结果可视化。最终输出的图片上，检测出了每个昆虫，标出了它们的边界框和具体类别。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习6.2-YOLOv3实现AI识虫（上）</title>
      <link href="/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.2-YOLOv3%E5%AE%9E%E7%8E%B0AI%E8%AF%86%E8%99%AB%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
      <url>/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.2-YOLOv3%E5%AE%9E%E7%8E%B0AI%E8%AF%86%E8%99%AB%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="林业病虫害数据集和数据预处理方法介绍"><a href="#林业病虫害数据集和数据预处理方法介绍" class="headerlink" title="林业病虫害数据集和数据预处理方法介绍"></a><strong>林业病虫害数据集和数据预处理方法介绍</strong></h1><p>在本节中，将使用百度与林业大学合作开发的林业病虫害防治项目中用到昆虫数据集。<a href="https://mp.weixin.qq.com/s?__biz=Mzg2OTEzODA5MA%3D%3D&amp;idx=1&amp;mid=2247486145&amp;scene=21&amp;sn=e51b1cb266b74bbf900e0d63d1dbb24f#wechat_redirect">[人工智能浪潮中的“虫虫特工队”]</a><br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/516ed89a765b497b9c338077652b96392cd5079e2f764235990c2173ee7c37bf" width = "600"></center><center><br>图1：项目数据集预览 </br></center><p><br></br></p><h2 id="1-读取AI识虫数据集标注信息"><a href="#1-读取AI识虫数据集标注信息" class="headerlink" title="1 读取AI识虫数据集标注信息"></a><strong>1 读取AI识虫数据集标注信息</strong></h2><p>AI识虫数据集结构如下：</p><ul><li>该数据集提供了<code>2183</code>张图片，其中训练集<code>1693</code>张，验证集<code>245</code>张，测试集<code>245</code>张；</li><li>包含7种昆虫，分别是<code>Boerner、Leconte、Linnaeus、acuminatus、armandi、coleoptera</code>和<code>linnaeus</code>；</li><li>包含了<code>图片（images）</code>和<code>标注（Annotations/xmls）</code>，接下来先将数据解压，将解压后的数据存放在 <code>insects</code> 目录下。</li></ul><h3 id="1）解压数据集文件"><a href="#1）解压数据集文件" class="headerlink" title="1）解压数据集文件"></a><strong>1）解压数据集文件</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压数据脚本，将文件解压到work目录下</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始解压......&#x27;</span>)</span><br><span class="line">!unzip  -o -q -d  /home/aistudio/work /home/aistudio/data/data170339/insects.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;解压完成&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>开始解压......解压完成</code></pre><p>将数据解压之后，可以看到insects目录下的结构如下所示。</p><pre><code>    insects        |---train        |         |---annotations        |         |         |---xmls        |         |                  |---100.xml        |         |                  |---101.xml        |         |                  |---...        |         |        |         |---images        |                   |---100.jpeg        |                   |---101.jpeg        |                   |---...        |        |---val        |        |---annotations        |        |         |---xmls        |        |                  |---1221.xml        |        |                  |---1277.xml        |        |                  |---...        |        |        |        |---images        |                  |---1221.jpeg        |                  |---1277.jpeg        |                  |---...        |        |---test                 |---images                           |---1833.jpeg                           |---1838.jpeg                           |---...</code></pre><hr><p>insects包含train、val和test三个文件夹。<code>train/annotations/xmls</code>目录下存放着图片的标注。每个 xml 文件是对一张图片的说明，包括图片尺寸、包含的昆虫名称、在图片上出现的位置等信息。</p><p>下面是每一个<code>xml</code>文件中的具体内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">        &lt;folder&gt;刘霏霏&lt;/folder&gt;</span><br><span class="line">        &lt;filename&gt;100.jpeg&lt;/filename&gt;                   # 文件名</span><br><span class="line">        &lt;path&gt;/home/fion/桌面/刘霏霏/100.jpeg&lt;/path&gt;</span><br><span class="line">        &lt;source&gt;                                        # 图像来源（不重要）</span><br><span class="line">                &lt;database&gt;Unknown&lt;/database&gt;</span><br><span class="line">        &lt;/source&gt;</span><br><span class="line">        &lt;size&gt;                                          # 图像尺寸（长宽以及通道数）</span><br><span class="line">                &lt;width&gt;1336&lt;/width&gt;</span><br><span class="line">                &lt;height&gt;1336&lt;/height&gt;</span><br><span class="line">                &lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">        &lt;/size&gt;</span><br><span class="line">        &lt;segmented&gt;0&lt;/segmented&gt;                        # 是否用于分割</span><br><span class="line">        &lt;object&gt;                                        # 检测到的物体</span><br><span class="line">                &lt;name&gt;Boerner&lt;/name&gt;                    # 物体类别</span><br><span class="line">                &lt;pose&gt;Unspecified&lt;/pose&gt;                # 拍摄角度</span><br><span class="line">                &lt;truncated&gt;0&lt;/truncated&gt;                # 是否被截断（0表示完整）</span><br><span class="line">                &lt;difficult&gt;0&lt;/difficult&gt;                # 目标是否难以识别（0表示容易识别）</span><br><span class="line">                &lt;bndbox&gt;                                # bounding-box（包含左下角和右上角xy坐标）</span><br><span class="line">                        &lt;xmin&gt;500&lt;/xmin&gt;</span><br><span class="line">                        &lt;ymin&gt;893&lt;/ymin&gt;</span><br><span class="line">                        &lt;xmax&gt;656&lt;/xmax&gt;</span><br><span class="line">                        &lt;ymax&gt;966&lt;/ymax&gt;</span><br><span class="line">                &lt;/bndbox&gt;</span><br><span class="line">        &lt;/object&gt;</span><br><span class="line">        &lt;object&gt;                                        # 检测到多个物体</span><br><span class="line">                &lt;name&gt;Leconte&lt;/name&gt;</span><br><span class="line">                &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">                &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">                &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">                &lt;bndbox&gt;</span><br><span class="line">                        &lt;xmin&gt;622&lt;/xmin&gt;</span><br><span class="line">                        &lt;ymin&gt;490&lt;/ymin&gt;</span><br><span class="line">                        &lt;xmax&gt;756&lt;/xmax&gt;</span><br><span class="line">                        &lt;ymax&gt;610&lt;/ymax&gt;</span><br><span class="line">                &lt;/bndbox&gt;</span><br><span class="line">        &lt;/object&gt;</span><br><span class="line">        &lt;object&gt;</span><br><span class="line">                &lt;name&gt;armandi&lt;/name&gt;</span><br><span class="line">                &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">                &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">                &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">                &lt;bndbox&gt;</span><br><span class="line">                        &lt;xmin&gt;432&lt;/xmin&gt;</span><br><span class="line">                        &lt;ymin&gt;663&lt;/ymin&gt;</span><br><span class="line">                        &lt;xmax&gt;517&lt;/xmax&gt;</span><br><span class="line">                        &lt;ymax&gt;729&lt;/ymax&gt;</span><br><span class="line">                &lt;/bndbox&gt;</span><br><span class="line">        &lt;/object&gt;</span><br><span class="line">        &lt;object&gt;</span><br><span class="line">                &lt;name&gt;coleoptera&lt;/name&gt;</span><br><span class="line">                &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">                &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">                &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">                &lt;bndbox&gt;</span><br><span class="line">                        &lt;xmin&gt;624&lt;/xmin&gt;</span><br><span class="line">                        &lt;ymin&gt;685&lt;/ymin&gt;</span><br><span class="line">                        &lt;xmax&gt;697&lt;/xmax&gt;</span><br><span class="line">                        &lt;ymax&gt;771&lt;/ymax&gt;</span><br><span class="line">                &lt;/bndbox&gt;</span><br><span class="line">        &lt;/object&gt;</span><br><span class="line">        &lt;object&gt;</span><br><span class="line">                &lt;name&gt;linnaeus&lt;/name&gt;</span><br><span class="line">                &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">                &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">                &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">                &lt;bndbox&gt;</span><br><span class="line">                        &lt;xmin&gt;783&lt;/xmin&gt;</span><br><span class="line">                        &lt;ymin&gt;700&lt;/ymin&gt;</span><br><span class="line">                        &lt;xmax&gt;856&lt;/xmax&gt;</span><br><span class="line">                        &lt;ymax&gt;802&lt;/ymax&gt;</span><br><span class="line">                &lt;/bndbox&gt;</span><br><span class="line">        &lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PIL库读取图片，并转为numpy.array的格式</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;work/insects/train/images/922.jpeg&#x27;</span>)</span><br><span class="line">img = np.array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出读取的图片</span></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/64.png" alt="png"></p><p>上面列出的xml文件中的主要参数说明如下：</p><ul><li><p><code>size</code>：图片尺寸。</p></li><li><p><code>object</code>：图片中包含的物体，一张图片可能中包含多个物体。</p></li><li><p><code>name</code>：昆虫名称；</p></li><li><p><code>bndbox</code>：物体真实框。</p></li></ul><h3 id="2）转化数字标签"><a href="#2）转化数字标签" class="headerlink" title="2）转化数字标签"></a><strong>2）转化数字标签</strong></h3><p>下面我们将从数据集中读取xml文件，将每张图片的标注信息读取出来。在读取具体的标注文件之前，我们先完成一件事情，就是将昆虫的类别名字（字符串）转化成数字表示的类别。因为神经网络里面计算时需要的输入类型是数值型的，所以需要将字符串表示的类别转化成具体的数字。</p><p>昆虫类别名称的列表是：[‘Boerner’, ‘Leconte’, ‘Linnaeus’, ‘acuminatus’, ‘armandi’, ‘coleoptera’, ‘linnaeus’]，这里我们约定此列表中：’Boerner’对应类别0，’Leconte’对应类别1，…，’linnaeus’对应类别6。使用下面的程序可以得到表示名称字符串和数字类别之间映射关系的字典。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">INSECT_NAMES = [<span class="string">&#x27;Boerner&#x27;</span>, <span class="string">&#x27;Leconte&#x27;</span>, <span class="string">&#x27;Linnaeus&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;acuminatus&#x27;</span>, <span class="string">&#x27;armandi&#x27;</span>, <span class="string">&#x27;coleoptera&#x27;</span>, <span class="string">&#x27;linnaeus&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_insect_names</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    return a dict, as following,</span></span><br><span class="line"><span class="string">        &#123;&#x27;Boerner&#x27;: 0,</span></span><br><span class="line"><span class="string">         &#x27;Leconte&#x27;: 1,</span></span><br><span class="line"><span class="string">         &#x27;Linnaeus&#x27;: 2, </span></span><br><span class="line"><span class="string">         &#x27;acuminatus&#x27;: 3,</span></span><br><span class="line"><span class="string">         &#x27;armandi&#x27;: 4,</span></span><br><span class="line"><span class="string">         &#x27;coleoptera&#x27;: 5,</span></span><br><span class="line"><span class="string">         &#x27;linnaeus&#x27;: 6</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    It can map the insect name into an integer label.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    insect_category2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(INSECT_NAMES):</span><br><span class="line">        insect_category2id[item] = i <span class="comment"># 构建键值对</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> insect_category2id</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cname2cid = get_insect_names()</span><br><span class="line">cname2cid</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;Boerner&#39;: 0, &#39;Leconte&#39;: 1, &#39;Linnaeus&#39;: 2, &#39;acuminatus&#39;: 3, &#39;armandi&#39;: 4, &#39;coleoptera&#39;: 5, &#39;linnaeus&#39;: 6&#125;</code></pre><p>调用get_insect_names函数返回一个dict，描述了昆虫名称和数字类别之间的映射关系。</p><h3 id="3）转化标注信息"><a href="#3）转化标注信息" class="headerlink" title="3）转化标注信息"></a><strong>3）转化标注信息</strong></h3><p>下面的程序从<code>annotations/xml</code>目录下面读取所有文件标注信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">filenames = os.listdir(os.path.join(<span class="string">&#x27;/home/aistudio/work/insects/train&#x27;</span>, <span class="string">&#x27;annotations&#x27;</span>, <span class="string">&#x27;xmls&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filenames[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;2220.xml&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_annotations</span>(<span class="params">cname2cid, datadir</span>):</span><br><span class="line">    filenames = os.listdir(os.path.join(datadir, <span class="string">&#x27;annotations&#x27;</span>, <span class="string">&#x27;xmls&#x27;</span>))</span><br><span class="line">    records = []</span><br><span class="line">    ct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> filenames:</span><br><span class="line">        fid = fname.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>] <span class="comment"># 序号</span></span><br><span class="line">        fpath = os.path.join(datadir, <span class="string">&#x27;annotations&#x27;</span>, <span class="string">&#x27;xmls&#x27;</span>, fname) <span class="comment"># 路径</span></span><br><span class="line">        img_file = os.path.join(datadir, <span class="string">&#x27;images&#x27;</span>, fid + <span class="string">&#x27;.jpeg&#x27;</span>) <span class="comment"># 序号--&gt;图片</span></span><br><span class="line">        tree = ET.parse(fpath) <span class="comment"># 读取xml文件</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tree.find(<span class="string">&#x27;id&#x27;</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            im_id = np.array([ct])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            im_id = np.array([<span class="built_in">int</span>(tree.find(<span class="string">&#x27;id&#x27;</span>).text)])</span><br><span class="line"></span><br><span class="line">        objs = tree.findall(<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">        im_w = <span class="built_in">float</span>(tree.find(<span class="string">&#x27;size&#x27;</span>).find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">        im_h = <span class="built_in">float</span>(tree.find(<span class="string">&#x27;size&#x27;</span>).find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line">        gt_bbox = np.zeros((<span class="built_in">len</span>(objs), <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        gt_class = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        is_crowd = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        difficult = np.zeros((<span class="built_in">len</span>(objs), ), dtype=np.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, obj <span class="keyword">in</span> <span class="built_in">enumerate</span>(objs):</span><br><span class="line">            cname = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">            gt_class[i] = cname2cid[cname]</span><br><span class="line">            _difficult = <span class="built_in">int</span>(obj.find(<span class="string">&#x27;difficult&#x27;</span>).text)</span><br><span class="line">            x1 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;xmin&#x27;</span>).text)</span><br><span class="line">            y1 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;ymin&#x27;</span>).text)</span><br><span class="line">            x2 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;xmax&#x27;</span>).text)</span><br><span class="line">            y2 = <span class="built_in">float</span>(obj.find(<span class="string">&#x27;bndbox&#x27;</span>).find(<span class="string">&#x27;ymax&#x27;</span>).text)</span><br><span class="line">            x1 = <span class="built_in">max</span>(<span class="number">0</span>, x1)</span><br><span class="line">            y1 = <span class="built_in">max</span>(<span class="number">0</span>, y1)</span><br><span class="line">            x2 = <span class="built_in">min</span>(im_w - <span class="number">1</span>, x2)</span><br><span class="line">            y2 = <span class="built_in">min</span>(im_h - <span class="number">1</span>, y2)</span><br><span class="line">            <span class="comment"># 这里将原二点表示转换为xywh格式，使用xywh格式来表示目标物体真实框</span></span><br><span class="line">            gt_bbox[i] = [(x1+x2)/<span class="number">2.0</span> , (y1+y2)/<span class="number">2.0</span>, x2-x1+<span class="number">1.</span>, y2-y1+<span class="number">1.</span>]</span><br><span class="line">            is_crowd[i] = <span class="number">0</span></span><br><span class="line">            difficult[i] = _difficult</span><br><span class="line"></span><br><span class="line">        voc_rec = &#123;</span><br><span class="line">            <span class="string">&#x27;im_file&#x27;</span>: img_file,</span><br><span class="line">            <span class="string">&#x27;im_id&#x27;</span>: im_id,</span><br><span class="line">            <span class="string">&#x27;h&#x27;</span>: im_h,</span><br><span class="line">            <span class="string">&#x27;w&#x27;</span>: im_w,</span><br><span class="line">            <span class="string">&#x27;is_crowd&#x27;</span>: is_crowd,</span><br><span class="line">            <span class="string">&#x27;gt_class&#x27;</span>: gt_class,</span><br><span class="line">            <span class="string">&#x27;gt_bbox&#x27;</span>: gt_bbox,</span><br><span class="line">            <span class="string">&#x27;gt_poly&#x27;</span>: [],</span><br><span class="line">            <span class="string">&#x27;difficult&#x27;</span>: difficult</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(objs) != <span class="number">0</span>:</span><br><span class="line">            records.append(voc_rec)</span><br><span class="line">        ct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> records</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TRAINDIR = <span class="string">&#x27;/home/aistudio/work/insects/train&#x27;</span></span><br><span class="line">TESTDIR = <span class="string">&#x27;/home/aistudio/work/insects/test&#x27;</span></span><br><span class="line">VALIDDIR = <span class="string">&#x27;/home/aistudio/work/insects/val&#x27;</span></span><br><span class="line">cname2cid = get_insect_names()</span><br><span class="line">records = get_annotations(cname2cid, TRAINDIR)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(records))</span><br><span class="line">records[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>1693&#123;&#39;im_file&#39;: &#39;/home/aistudio/work/insects/train/images/1194.jpeg&#39;, &#39;im_id&#39;: array([0]), &#39;h&#39;: 1234.0, &#39;w&#39;: 1234.0, &#39;is_crowd&#39;: array([0, 0, 0, 0, 0, 0, 0], dtype=int32), &#39;gt_class&#39;: array([1, 1, 0, 4, 6, 3, 5], dtype=int32), &#39;gt_bbox&#39;: array([[472. , 585.5, 121. ,  72. ],        [428. , 767.5, 115. ,  82. ],        [713. , 568. , 101. , 107. ],        [583. , 390. ,  45. ,  85. ],        [573. , 853. ,  83. ,  63. ],        [655. , 481. ,  55. ,  31. ],        [722.5, 718.5,  72. ,  30. ]], dtype=float32), &#39;gt_poly&#39;: [], &#39;difficult&#39;: array([0, 0, 0, 0, 0, 0, 0], dtype=int32)&#125;</code></pre><p><code>records</code>中可以读取到以下信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">voc_rec = &#123;</span><br><span class="line">    &#x27;im_file&#x27;: im_fname,         # 一张图像的完整路径</span><br><span class="line">    &#x27;im_id&#x27;: np.array([img_id]), # 一张图像的ID序号</span><br><span class="line">    &#x27;h&#x27;: im_h,                   # 图像高度</span><br><span class="line">    &#x27;w&#x27;: im_w,                   # 图像宽度</span><br><span class="line">    &#x27;is_crowd&#x27;: is_crowd,        # 是否是群落对象, 默认为0 (VOC中无此字段)</span><br><span class="line">    &#x27;gt_class&#x27;: gt_class,        # 标注框标签名称的ID序号</span><br><span class="line">    &#x27;gt_bbox&#x27;: gt_bbox,          # 标注框坐标(xmin, ymin, xmax, ymax)</span><br><span class="line">    &#x27;gt_score&#x27;: gt_score,        # 标注框置信度得分</span><br><span class="line">    &#x27;gt_poly&#x27;: gt_poly,          # 分割掩码，此字段只在coco_rec中出现，默认为None</span><br><span class="line">    &#x27;difficult&#x27;: difficult       # 是否是困难样本，此字段只在voc_rec中出现，默认为0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过上面的程序，将所有训练数据集的标注数据全部读取出来了，存放在records列表下面，其中每一个元素是一张图片的标注数据，包含了图片存放地址，图片id，图片高度和宽度，图片中所包含的目标物体的种类和位置。</p><h2 id="2-数据读取和预处理"><a href="#2-数据读取和预处理" class="headerlink" title="2 数据读取和预处理"></a><strong>2 数据读取和预处理</strong></h2><p>数据预处理是训练神经网络时非常重要的步骤。合适的预处理方法，可以帮助模型更好的收敛并防止过拟合。首先我们需要从磁盘读入数据，然后需要对这些数据进行预处理，为了保证网络运行的速度，通常还要对数据预处理进行加速。</p><h3 id="1）读取图片信息"><a href="#1）读取图片信息" class="headerlink" title="1）读取图片信息"></a><strong>1）读取图片信息</strong></h3><p>前面已经将图片的所有描述信息保存在records中了，其中每一个元素都包含了一张图片的描述，下面的程序展示了如何根据records里面的描述读取图片及标注。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据读取</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_bbox</span>(<span class="params">gt_bbox, gt_class</span>):</span><br><span class="line">    <span class="comment"># 对于一般的检测任务来说，一张图片上往往会有多个目标物体</span></span><br><span class="line">    <span class="comment"># 设置参数MAX_NUM = 50， 即一张图片最多取50个真实框；</span></span><br><span class="line">    <span class="comment"># 如果真实框的数目少于50个，则将不足部分的gt_bbox, gt_class和gt_score的各项数值全设置为0</span></span><br><span class="line">    MAX_NUM = <span class="number">50</span></span><br><span class="line">    gt_bbox2 = np.zeros((MAX_NUM, <span class="number">4</span>))</span><br><span class="line">    gt_class2 = np.zeros((MAX_NUM,))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gt_bbox)):</span><br><span class="line">        gt_bbox2[i, :] = gt_bbox[i, :]</span><br><span class="line">        gt_class2[i] = gt_class[i]</span><br><span class="line">        <span class="keyword">if</span> i &gt;= MAX_NUM:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> gt_bbox2, gt_class2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_data_from_file</span>(<span class="params">record</span>):</span><br><span class="line">    im_file = record[<span class="string">&#x27;im_file&#x27;</span>]</span><br><span class="line">    h = record[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">    w = record[<span class="string">&#x27;w&#x27;</span>]</span><br><span class="line">    is_crowd = record[<span class="string">&#x27;is_crowd&#x27;</span>]</span><br><span class="line">    gt_class = record[<span class="string">&#x27;gt_class&#x27;</span>]</span><br><span class="line">    gt_bbox = record[<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">    difficult = record[<span class="string">&#x27;difficult&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    img = cv2.imread(im_file)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 机器校验</span></span><br><span class="line">    <span class="keyword">assert</span> img.shape[<span class="number">0</span>] == <span class="built_in">int</span>(h), \</span><br><span class="line">             <span class="string">&quot;image height of &#123;&#125; inconsistent in record(&#123;&#125;) and img file(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">               im_file, h, img.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> img.shape[<span class="number">1</span>] == <span class="built_in">int</span>(w), \</span><br><span class="line">             <span class="string">&quot;image width of &#123;&#125; inconsistent in record(&#123;&#125;) and img file(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">               im_file, w, img.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    gt_boxes, gt_labels = get_bbox(gt_bbox, gt_class)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gt_bbox 用相对值</span></span><br><span class="line">    gt_boxes[:, <span class="number">0</span>] = gt_boxes[:, <span class="number">0</span>] / <span class="built_in">float</span>(w)</span><br><span class="line">    gt_boxes[:, <span class="number">1</span>] = gt_boxes[:, <span class="number">1</span>] / <span class="built_in">float</span>(h)</span><br><span class="line">    gt_boxes[:, <span class="number">2</span>] = gt_boxes[:, <span class="number">2</span>] / <span class="built_in">float</span>(w)</span><br><span class="line">    gt_boxes[:, <span class="number">3</span>] = gt_boxes[:, <span class="number">3</span>] / <span class="built_in">float</span>(h)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> img, gt_boxes, gt_labels, (h, w)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">record = records[<span class="number">0</span>]</span><br><span class="line">img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape, gt_boxes.shape, gt_labels, scales)</span><br></pre></td></tr></table></figure><pre><code>(1234, 1234, 3) (50, 4) [1. 1. 0. 4. 6. 3. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (1234.0, 1234.0)</code></pre><p><code>get_img_data_from_file()</code>函数可以返回图片数据的数据，它们是图像数据img，真实框坐标gt_boxes，真实框包含的物体类别gt_labels，图像尺寸scales。</p><h3 id="2）数据预处理"><a href="#2）数据预处理" class="headerlink" title="2）数据预处理"></a><strong>2）数据预处理</strong></h3><p>在计算机视觉中，通常会对图像做一些随机的变化，产生相似但又不完全相同的样本。主要作用是扩大训练数据集，抑制过拟合，提升模型的泛化能力，常用的方法主要有以下几种：</p><ul><li>随机改变亮暗、对比度和颜色</li><li>随机填充</li><li>随机缩放</li><li>随机翻转</li><li>随机打乱真实框排列顺序</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义可视化函数，用于对比原图和图像增强的效果</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize</span>(<span class="params">srcimg, img_enhance</span>):</span><br><span class="line">    plt.figure(num=<span class="number">2</span>, figsize=(<span class="number">6</span>,<span class="number">12</span>))</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Src Image&#x27;</span>, color=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 不显示坐标轴</span></span><br><span class="line">    plt.imshow(srcimg) <span class="comment"># 显示原图片</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对原图做随机改变亮暗、对比度和颜色等数据增强</span></span><br><span class="line">    srcimg_gtbox = records[<span class="number">0</span>][<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">    srcimg_label = records[<span class="number">0</span>][<span class="string">&#x27;gt_class&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Enhance Image&#x27;</span>, color=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 不显示坐标轴</span></span><br><span class="line">    plt.imshow(img_enhance/<span class="number">255</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageEnhance</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机改变亮暗、对比度和颜色等</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_distort</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="comment"># 随机改变亮度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_brightness</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Brightness(img).enhance(e)</span><br><span class="line">    <span class="comment"># 随机改变对比度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_contrast</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Contrast(img).enhance(e)</span><br><span class="line">    <span class="comment"># 随机改变颜色</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_color</span>(<span class="params">img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span></span>):</span><br><span class="line">        e = np.random.uniform(lower, upper)</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Color(img).enhance(e)</span><br><span class="line"></span><br><span class="line">    ops = [random_brightness, random_contrast, random_color]</span><br><span class="line">    np.random.shuffle(ops)</span><br><span class="line"></span><br><span class="line">    img = Image.fromarray(img)</span><br><span class="line">    img = ops[<span class="number">0</span>](img)</span><br><span class="line">    img = ops[<span class="number">1</span>](img)</span><br><span class="line">    img = ops[<span class="number">2</span>](img)</span><br><span class="line">    img = np.asarray(img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">image_path = records[<span class="number">0</span>][<span class="string">&#x27;im_file&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;read image from file &#123;&#125;&quot;</span>.<span class="built_in">format</span>(image_path))</span><br><span class="line">srcimg = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="comment"># 将PIL读取的图像转换成array类型</span></span><br><span class="line">srcimg = np.array(srcimg)</span><br><span class="line"></span><br><span class="line">img_enhance = random_distort(srcimg)</span><br><span class="line">visualize(srcimg, img_enhance)</span><br></pre></td></tr></table></figure><pre><code>read image from file /home/aistudio/work/insects/train/images/1194.jpeg</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/65.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机填充</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_expand</span>(<span class="params">img,</span></span><br><span class="line"><span class="params">                  gtboxes,</span></span><br><span class="line"><span class="params">                  max_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">                  fill=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                  keep_ratio=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                  thresh=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &gt; thresh:</span><br><span class="line">        <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_ratio &lt; <span class="number">1.0</span>:</span><br><span class="line">        <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line">    h, w, c = img.shape</span><br><span class="line">    ratio_x = random.uniform(<span class="number">1</span>, max_ratio)</span><br><span class="line">    <span class="keyword">if</span> keep_ratio:</span><br><span class="line">        ratio_y = ratio_x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ratio_y = random.uniform(<span class="number">1</span>, max_ratio)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过缩放与平移实现填充的目的</span></span><br><span class="line">    oh = <span class="built_in">int</span>(h * ratio_y)</span><br><span class="line">    ow = <span class="built_in">int</span>(w * ratio_x)</span><br><span class="line">    off_x = random.randint(<span class="number">0</span>, ow - w)</span><br><span class="line">    off_y = random.randint(<span class="number">0</span>, oh - h)</span><br><span class="line"></span><br><span class="line">    out_img = np.zeros((oh, ow, c))</span><br><span class="line">    <span class="keyword">if</span> fill <span class="keyword">and</span> <span class="built_in">len</span>(fill) == c:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(c):</span><br><span class="line">            out_img[:, :, i] = fill[i] * <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">    out_img[off_y:off_y + h, off_x:off_x + w, :] = img</span><br><span class="line">    gtboxes[:, <span class="number">0</span>] = ((gtboxes[:, <span class="number">0</span>] * w) + off_x) / <span class="built_in">float</span>(ow)</span><br><span class="line">    gtboxes[:, <span class="number">1</span>] = ((gtboxes[:, <span class="number">1</span>] * h) + off_y) / <span class="built_in">float</span>(oh)</span><br><span class="line">    gtboxes[:, <span class="number">2</span>] = gtboxes[:, <span class="number">2</span>] / ratio_x</span><br><span class="line">    gtboxes[:, <span class="number">3</span>] = gtboxes[:, <span class="number">3</span>] / ratio_y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_img.astype(<span class="string">&#x27;uint8&#x27;</span>), gtboxes <span class="comment"># 返回填充后的图片与边界框</span></span><br><span class="line"></span><br><span class="line">srcimg_gtbox = records[<span class="number">0</span>][<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">img_enhance, new_gtbox = random_expand(srcimg, srcimg_gtbox)</span><br><span class="line">visualize(srcimg, img_enhance)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/66.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_enhance.shape, new_gtbox</span><br></pre></td></tr></table></figure><pre><code>((3113, 3113, 3), array([[-101.2339   ,  126.889565 ,   26.098473 ,   15.52967  ],        [ -91.73983  ,  166.16049  ,   24.804335 ,   17.686567 ],        [-153.23549  ,  123.11352  ,   21.784676 ,   23.078815 ],        [-125.18484  ,   84.70569  ,    9.706043 ,   18.333637 ],        [-123.02711  ,  184.60918  ,   17.902258 ,   13.588462 ],        [-140.72058  ,  104.34115  ,   11.862942 ,    6.6863856],        [-155.28534  ,  155.58754  ,   15.52967  ,    6.470696 ]],       dtype=float32))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机翻转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_flip</span>(<span class="params">img, gtboxes, thresh=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &gt; thresh:</span><br><span class="line">        <span class="comment"># 调整步长</span></span><br><span class="line">        img = img[:, ::-<span class="number">1</span>, :]</span><br><span class="line">        gtboxes[:, <span class="number">0</span>] = <span class="number">1.0</span> - gtboxes[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> img, gtboxes</span><br><span class="line"></span><br><span class="line">img_enhance, box_enhance = random_flip(srcimg, srcimg_gtbox)</span><br><span class="line">visualize(srcimg, img_enhance)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/67.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机打乱真实框排列顺序</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffle_gtbox</span>(<span class="params">gtbox, gtlabel</span>):</span><br><span class="line">    gt = np.concatenate([gtbox, gtlabel[:, np.newaxis]], axis=<span class="number">1</span>)</span><br><span class="line">    idx = np.arange(gt.shape[<span class="number">0</span>])</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line">    gt = gt[idx, :]</span><br><span class="line">    <span class="keyword">return</span> gt[:, :<span class="number">4</span>], gt[:, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机缩放</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_interp</span>(<span class="params">img, size, interp=<span class="literal">None</span></span>):</span><br><span class="line">    interp_method = [cv2.INTER_NEAREST,   <span class="comment"># 最近邻插值</span></span><br><span class="line">                    cv2.INTER_LINEAR,     <span class="comment"># 线性插值</span></span><br><span class="line">                    cv2.INTER_AREA,       <span class="comment"># 区域插值</span></span><br><span class="line">                    cv2.INTER_CUBIC,      <span class="comment"># 三次样条插值</span></span><br><span class="line">                    cv2.INTER_LANCZOS4]   <span class="comment"># LANCZOS4插值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机选择缩放时的插值方法</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> interp <span class="keyword">or</span> interp <span class="keyword">not</span> <span class="keyword">in</span> interp_method:</span><br><span class="line">        interp = interp_method[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(interp_method) - <span class="number">1</span>)]</span><br><span class="line">    h, w, _ = img.shape</span><br><span class="line">    im_scale_x = size / <span class="built_in">float</span>(w)</span><br><span class="line">    im_scale_y = size / <span class="built_in">float</span>(h)</span><br><span class="line">    <span class="comment"># 调整图像大小</span></span><br><span class="line">    img = cv2.resize(img, <span class="literal">None</span>, <span class="literal">None</span>, fx=im_scale_x, fy=im_scale_y, interpolation=interp)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">img_enhance = random_interp(srcimg, <span class="number">640</span>)</span><br><span class="line">visualize(srcimg, img_enhance)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/68.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img.shape, img_enhance.shape</span><br></pre></td></tr></table></figure><pre><code>((1234, 1234, 3), (640, 640, 3))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像增广方法汇总</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_augment</span>(<span class="params">img, gtboxes, gtlabels, size, means=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 随机改变亮暗、对比度和颜色等</span></span><br><span class="line">    img = random_distort(img)</span><br><span class="line">    <span class="comment"># 随机填充</span></span><br><span class="line">    img, gtboxes = random_expand(img, gtboxes, fill=means)</span><br><span class="line">    <span class="comment"># 随机缩放</span></span><br><span class="line">    img = random_interp(img, size)</span><br><span class="line">    <span class="comment"># 随机翻转</span></span><br><span class="line">    img, gtboxes = random_flip(img, gtboxes)</span><br><span class="line">    <span class="comment"># 随机打乱真实框排列顺序</span></span><br><span class="line">    gtboxes, gtlabels = shuffle_gtbox(gtboxes, gtlabels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img.astype(<span class="string">&#x27;float32&#x27;</span>), gtboxes.astype(<span class="string">&#x27;float32&#x27;</span>), gtlabels.astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">srcimg = Image.<span class="built_in">open</span>(records[<span class="number">0</span>][<span class="string">&#x27;im_file&#x27;</span>])</span><br><span class="line">srcimg = np.array(srcimg)</span><br><span class="line"></span><br><span class="line">srcimg_gtbox = records[<span class="number">0</span>][<span class="string">&#x27;gt_bbox&#x27;</span>]</span><br><span class="line">srcimg_label = records[<span class="number">0</span>][<span class="string">&#x27;gt_class&#x27;</span>]</span><br><span class="line">img_enhance, img_box, img_label = image_augment(srcimg, srcimg_gtbox, srcimg_label, size=<span class="number">320</span>)</span><br><span class="line">visualize(srcimg, img_enhance)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/69.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)</span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">img, gt_boxes, gt_labels = image_augment(img, gt_boxes, gt_labels, size)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape, gt_boxes.shape, gt_labels.shape)</span><br></pre></td></tr></table></figure><pre><code>(512, 512, 3) (50, 4) (50,)</code></pre><p>这里得到的img数据数值需要调整，需要除以255，并且减去均值和方差，再将维度从[H, W, C]调整为[C, H, W]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">record = records[<span class="number">0</span>]</span><br><span class="line">image_path = record[<span class="string">&#x27;im_file&#x27;</span>]</span><br><span class="line">srcimg = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">srcimg = np.array(srcimg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)</span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">img, gt_boxes, gt_labels = image_augment(img, gt_boxes, gt_labels, size)</span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">img = img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">srcimg.shape, img.shape</span><br></pre></td></tr></table></figure><pre><code>((1234, 1234, 3), (3, 512, 512))</code></pre><p>将上面的过程整理成一个<code>get_img_data</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_data</span>(<span class="params">record, size=<span class="number">640</span></span>):</span><br><span class="line">    img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)</span><br><span class="line">    img, gt_boxes, gt_labels = image_augment(img, gt_boxes, gt_labels, size)</span><br><span class="line">    mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">    std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">    img = img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> img, gt_boxes, gt_labels, scales</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TRAINDIR = <span class="string">&#x27;/home/aistudio/work/insects/train&#x27;</span></span><br><span class="line">TESTDIR = <span class="string">&#x27;/home/aistudio/work/insects/test&#x27;</span></span><br><span class="line">VALIDDIR = <span class="string">&#x27;/home/aistudio/work/insects/val&#x27;</span></span><br><span class="line">cname2cid = get_insect_names()</span><br><span class="line">records = get_annotations(cname2cid, TRAINDIR)</span><br><span class="line"></span><br><span class="line">record = records[<span class="number">0</span>]</span><br><span class="line">img, gt_boxes, gt_labels, scales = get_img_data(record, size=<span class="number">480</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape, gt_boxes.shape, gt_labels.shape, scales)</span><br></pre></td></tr></table></figure><pre><code>(3, 480, 480) (50, 4) (50,) (1006.0, 1006.0)</code></pre><p><strong>使用飞桨高层API快速实现数据增强</strong></p><p>详细可查阅<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/Overview_cn.html#paddle-vision">paddle.vision.transforms</a>模块，transforms模块中提供了数十种数据增强方式，包括亮度增强(<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/adjust_brightness_cn.html#cn-api-vision-transforms-adjust-brightness">adjust_brightness</a>)，对比度增强(<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/adjust_contrast_cn.html#cn-api-vision-transforms-adjust-contrast">adjust_contrast</a>)，随机裁剪(<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/RandomCrop_cn.html#cn-api-vision-transforms-randomcrop">RandomCrop</a>)等等。更多的关于高层API的使用方法，请登录飞桨官网。</p><p><code>paddle.vision.transforms</code>模块中的数据增强使用方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对图像随机裁剪</span></span><br><span class="line"><span class="comment"># 从paddle.vision.transforms模块中import随机剪切的API RandomCrop</span></span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> RandomCrop</span><br><span class="line"></span><br><span class="line">transform = RandomCrop(<span class="number">640</span>)</span><br><span class="line"><span class="comment"># 将图像转换为PIL.Image格式</span></span><br><span class="line">srcimg = Image.fromarray(np.array(srcimg))</span><br><span class="line">img_res = transform(srcimg)</span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">visualize(srcimg, np.array(img_res))</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/70.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对图像随机亮度增强</span></span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> BrightnessTransform</span><br><span class="line"></span><br><span class="line">transform = BrightnessTransform(<span class="number">0.4</span>)</span><br><span class="line"><span class="comment"># 将图像转换为PIL.Image格式</span></span><br><span class="line">srcimg = Image.fromarray(np.array(srcimg))</span><br><span class="line">img_res = transform(srcimg)</span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">visualize(srcimg, np.array(img_res))</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/71.png" alt="png"></p><h3 id="3）批量数据读取与加速"><a href="#3）批量数据读取与加速" class="headerlink" title="3）批量数据读取与加速"></a><strong>3）批量数据读取与加速</strong></h3><p>上面的程序展示了如何读取一张图片的数据并加速，下面的代码实现了批量数据读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取一个批次内样本随机缩放的尺寸</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img_size</span>(<span class="params">mode</span>):</span><br><span class="line">    <span class="keyword">if</span> (mode == <span class="string">&#x27;train&#x27;</span>) <span class="keyword">or</span> (mode == <span class="string">&#x27;valid&#x27;</span>):</span><br><span class="line">        inds = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">        ii = np.random.choice(inds)</span><br><span class="line">        img_size = <span class="number">320</span> + ii * <span class="number">32</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_size = <span class="number">608</span></span><br><span class="line">    <span class="keyword">return</span> img_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将list形式的batch数据转化成多个array构成的tuple</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_array</span>(<span class="params">batch_data</span>):</span><br><span class="line">    img_array = np.array([item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    gt_box_array = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    gt_labels_array = np.array([item[<span class="number">2</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    img_scale = np.array([item[<span class="number">3</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> img_array, gt_box_array, gt_labels_array, img_scale</span><br></pre></td></tr></table></figure><p>由于数据预处理耗时较长，可能会成为网络训练速度的瓶颈，所以需要对预处理部分进行优化。</p><p>通过使用飞桨提供的 <a href="https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/io/DataLoader_en.html">paddle.io.DataLoader</a> API中的num_workers参数设置进程数量，实现多进程读取数据，具体实现代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据读取类，继承Paddle.io.Dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, datadir, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        self.datadir = datadir</span><br><span class="line">        cname2cid = get_insect_names()</span><br><span class="line">        self.records = get_annotations(cname2cid, datadir)</span><br><span class="line">        self.img_size = <span class="number">640</span>  <span class="comment"># get_img_size(mode)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        record = self.records[idx]</span><br><span class="line">        <span class="comment"># print(&quot;print: &quot;, record)</span></span><br><span class="line">        img, gt_bbox, gt_labels, im_shape = get_img_data(record, size=self.img_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, gt_bbox, gt_labels, np.array(im_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.records)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据读取类</span></span><br><span class="line">train_dataset = TrainDataset(TRAINDIR, mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddle.io.DataLoader创建数据读取器，并设置batchsize，进程数量num_workers等参数</span></span><br><span class="line">train_loader = paddle.io.DataLoader(</span><br><span class="line">                                    train_dataset, </span><br><span class="line">                                    batch_size=<span class="number">2</span>, </span><br><span class="line">                                    shuffle=<span class="literal">True</span>, </span><br><span class="line">                                    num_workers=<span class="number">2</span>, </span><br><span class="line">                                    drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = paddle.io.DataLoader(train_dataset, batch_size=<span class="number">3</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">1</span>, drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img, gt_boxes, gt_labels, im_shape = <span class="built_in">next</span>(d())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape, gt_boxes.shape, gt_labels.shape, im_shape.shape)</span><br></pre></td></tr></table></figure><pre><code>[3, 3, 640, 640] [3, 50, 4] [3, 50] [3, 2]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">im_shape</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[3, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=True,       [[1240., 1240.],        [1240., 1240.],        [1250., 1250.]])</code></pre><p>至此，我们完成了如何查看数据集中的数据、提取数据标注信息、从文件读取图像和标注数据、图像增广、批量读取和加速等过程，通过<code>paddle.io.Dataset</code>可以返回<code>img, gt_boxes, gt_labels, im_shape</code>等数据，接下来就可以将它们输入到神经网络，应用到具体算法上了。</p><p>在开始具体的算法讲解之前，先补充一下读取测试数据的代码。测试数据没有标注信息，也不需要做图像增广，代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 将 list形式的batch数据 转化成多个array构成的tuple</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_test_array</span>(<span class="params">batch_data</span>):</span><br><span class="line">    img_name_array = np.array([item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data])</span><br><span class="line">    img_data_array = np.array([item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    img_scale_array = np.array([item[<span class="number">2</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch_data], dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> img_name_array, img_data_array, img_scale_array</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据读取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_data_loader</span>(<span class="params">datadir, batch_size= <span class="number">10</span>, test_image_size=<span class="number">608</span>, mode=<span class="string">&#x27;test&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载测试用的图片，测试数据没有groundtruth标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image_names = os.listdir(datadir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        batch_data = []</span><br><span class="line">        img_size = test_image_size</span><br><span class="line">        <span class="keyword">for</span> image_name <span class="keyword">in</span> image_names:</span><br><span class="line">            file_path = os.path.join(datadir, image_name)</span><br><span class="line">            img = cv2.imread(file_path)</span><br><span class="line">            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">            H = img.shape[<span class="number">0</span>]</span><br><span class="line">            W = img.shape[<span class="number">1</span>]</span><br><span class="line">            img = cv2.resize(img, (img_size, img_size))</span><br><span class="line"></span><br><span class="line">            mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">            std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">            mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">            std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">            out_img = (img / <span class="number">255.0</span> - mean) / std</span><br><span class="line">            out_img = out_img.astype(<span class="string">&#x27;float32&#x27;</span>).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            img = out_img <span class="comment">#np.transpose(out_img, (2,0,1))</span></span><br><span class="line">            im_shape = [H, W]</span><br><span class="line"></span><br><span class="line">            batch_data.append((image_name.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>], img, im_shape))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_data) == batch_size:</span><br><span class="line">                <span class="keyword">yield</span> make_test_array(batch_data)</span><br><span class="line">                batch_data = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_data) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">yield</span> make_test_array(batch_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习6.1-目标检测基本概念</title>
      <link href="/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.1-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.1-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a><strong>目标检测</strong></h1><p>对计算机而言，能够“看到”的是图像被编码之后的数字，但它很难理解高层语义概念，比如图像或者视频帧中出现的目标是人还是物体，更无法定位目标出现在图像中哪个区域。目标检测的主要目的是让计算机可以自动识别图片或者视频帧中所有目标的类别，并在该目标周围绘制边界框，标示出每个目标的位置，如 <strong>图1</strong> 所示。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e25116d994724f83abe3bef7f033c1c89bf34e083075494bb7833947c557f4fc" width = "700"  div align=center" width = "800"></center><center><br>图1：图像分类和目标检测示意图</br></center><p><br></br></p><ul><li>图1（a）是图像分类任务，只需识别出这是一张斑马的图片。 </li><li>图1（b）是目标检测任务，不仅要识别出这是一张斑马的图片，还要标出图中斑马的位置。</li></ul><p>目标检测在多个领域中被广泛使用。 例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路。 机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。</p><h2 id="一、目标检测的发展历程"><a href="#一、目标检测的发展历程" class="headerlink" title="一、目标检测的发展历程"></a>一、目标检测的发展历程</h2><p>在之前的课程中我们学习了图像分类处理基本流程，先使用卷积神经网络提取图像特征，然后再用这些特征预测分类概率，根据训练样本标签建立起分类损失函数，开启端到端的训练，如 <strong>图2</strong> 所示。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/cf21664a4a1f4392945f86812d466879132426ab602846c38dab0eca0600e579" width = "800"></center><center><br>图2：图像分类流程示意图</br></center><p><br></br></p><p>但对于目标检测问题，按照 <strong>图2</strong> 的流程则行不通。因为在图像分类任务中，对整张图提取特征的过程中没能体现出不同目标之间的区别，最终也就没法分别标示出每个物体所在的位置。</p><p>为了解决这个问题，结合图片分类任务取得的成功经验，我们可以将目标检测任务进行拆分。假设我们现在有某种方式可以在输入图片上生成一系列可能包含物体的区域，这些区域称为候选区域，在一张图上可以生成很多个候选区域。然后对每个候选区域，可以把它单独当成一幅图像来看待，使用图像分类模型对它进行分类，看它属于哪个类别或者背景（即不包含任何物体的类别）。</p><p>上一节我们学过如何解决图像分类任务，使用卷积神经网络对一幅图像进行分类不再是一件困难的事情。那么，现在问题的关键就是如何产生候选区域？比如我们可以使用<strong>穷举法</strong>来产生候选区域，如图3所示。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/57755ac8e95a460f9262afc7c37a0db51f66027ff86c40e2967a2e22524c20a1" width = "800"></center><center><br>图3：候选区域</br></center><p><br></br></p><p>A为图像上的某个像素点，B为A右下方另外一个像素点，A、B两点可以确定一个矩形框，记作AB。</p><ul><li>如图3（a）所示：A在图片左上角位置，B遍历除A之外的所有位置，生成矩形框A1B1, …, A1Bn, …</li><li>如图3（b）所示：A在图片中间某个位置，B遍历A右下方所有位置，生成矩形框AkB1, …, AkBn, …</li></ul><p>当A遍历图像上所有像素点，B则遍历它右下方所有的像素点，最终生成的矩形框集合{AiBj}将会包含图像上所有可以选择的区域，这个候选区域也被称为<strong>感兴趣区域（Region of interest, ROI)</strong>。只要我们对每个候选区域的分类足够的准确，则一定能找到跟实际物体足够接近的区域来。整体流程如下图所示：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/7b27f6bc0d944e55ad9cf5187bf2ebc294a25d87bf1a4cd8890b2700274b6af7" width = "800"></center><center><br>图4：检测流程</br></center><p><br></br></p><p>穷举法也许能得到正确的预测结果，但其计算量也是非常巨大的，其所生成的总候选区域数目约为$\frac{W^2 H^2}{4}$，假设$H=W=100$，总数将会达到$2.5 \times 10^{7}$个，如此多的候选区域使得这种方法几乎没有什么实用性。但是通过这种方式，我们可以看出，假设分类任务完成的足够完美，从理论上来讲检测任务也是可以解决的，亟待解决的问题是如何设计出合适的方法来产生候选区域。好的目标检测任务必须拥有一个合理的产生候选区域的方法：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/3b49cb672fe34e1d829ab483e9a843e6a745d7591aed46799b5cc81f8561eed8" width = "800"></center><center><br>图5：检测算法核心步骤</br></center><p><br></br></p><hr><blockquote><p>检测算法亟需解决的问题主要是以下两个方面：</p><ul><li>如何产生候选区域，并对他们进行标注；</li><li>如何提取图像特征，并将提取到的特征图与候选区域的类别和位置进行关联。</li></ul></blockquote><hr><p>科学家们开始思考，是否可以应用传统图像算法先产生候选区域，然后再用卷积神经网络对这些区域进行分类？</p><ul><li>2013年，<a href="https://arxiv.org/abs/1311.2524">Ross Girshick</a> 等人于首次将CNN的方法应用在目标检测任务上，他们使用传统图像算法<a href="https://link.springer.com/article/10.1007/s11263-013-0620-5">Selective Search</a>产生候选区域，取得了极大的成功，这就是对目标检测领域影响深远的区域卷积神经网络(R-CNN)模型。</li><li>2015年，<a href="https://arxiv.org/abs/1504.08083">Ross Girshick</a> 对此方法进行了改进，提出了Fast R-CNN模型。通过将不同区域的物体共用卷积层的计算，大大缩减了计算量，提高了处理速度，而且还引入了调整目标物体位置的回归方法，进一步提高了位置预测的准确性。</li><li>2015年，<a href="https://arxiv.org/abs/1506.01497">Shaoqing Ren</a> 等人提出了Faster R-CNN模型，提出了RPN的方法来产生物体的候选区域，这一方法不再需要使用传统的图像处理算法来产生候选区域，进一步提升了处理速度。</li><li>2017年，<a href="https://arxiv.org/abs/1703.06870">Kaiming He</a> 等人提出了Mask R-CNN模型，只需要在Faster R-CNN模型上添加比较少的计算量，就可以同时实现目标检测和物体实例分割两个任务。</li></ul><p>以上都是基于R-CNN系列的著名模型，对目标检测方向的发展有着较大的影响力。此外，还有一些其他模型，比如<a href="https://arxiv.org/abs/1512.02325">SSD</a>、YOLO(<a href="https://arxiv.org/abs/1506.02640v5">1</a>, <a href="https://arxiv.org/abs/1612.08242">2</a>, <a href="https://arxiv.org/abs/1804.02767">3</a>)、<a href="https://arxiv.org/abs/1605.06409">R-FCN</a>等也都是目标检测领域流行的模型结构。</p><p>R-CNN的系列算法分成两个阶段，先在图像上产生候选区域，再对候选区域进行分类并预测目标物体位置，它们通常被叫做两阶段检测算法。SSD和YOLO算法则只使用一个网络同时产生候选区域并预测出物体的类别和位置，所以它们通常被叫做单阶段检测算法。由于篇幅所限，本章将重点介绍YOLOv3算法，并用其完成林业病虫害检测任务，主要涵盖如下内容：</p><ul><li>图像检测基础概念：介绍与目标检测相关的基本概念，包括边界框、锚框和交并比等。</li><li>林业病虫害数据集：介绍数据集结构及数据预处理方法。</li><li>YOLOv3目标检测模型：介绍算法原理，及如何应用林业病虫害数据集进行模型训练和测试。</li></ul><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h3><ul><li>[1] R-CNN: Ross Girshick,Jeff Donahue,Trevor Darrell and Jitendra Malik,Rich feature hierarchies for accurate obiect detection and semantic segmentation Tech report,arXiv:1311.2524v5</li><li>[2] Fast R-CNN: Ross Girshick,Fast R-CNN,arXiv:1504.08083v2</li><li>[3] Faster R-CNN: Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun,Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks,arXiv:1504.08083v2</li><li>[4] R-FCN: Jifeng Dai,Yi Li,Kaiming He and Jian Sun,R-FCN: Object Detection via Region-based Fully Convolutional Networks,arXiv:1605.06409v2</li><li>[5] SSD:Wei Liu,Dragomir Anguelov,Dumitru Erhan,Christian Szegedy,Scot Reed,Cheng-Yang Fu,and Alexander C.Berg,SSD: Single Shot MultiBox Detector,arXiv:1512.02325v5</li><li>[6] YOLO-V1: Joseph Redmon,Santosh Divvala,Ross Girshick and Ali Farhadi,You Only Look Once:Unified,Real-Time Object Detection,arXiv:1506.02640v5</li><li>[7] YOLO-V2: Joseph Redmon and Ali Farhadi,YOLO9000:Better,Faster,Stronger,arXiv:1612.08242v1</li><li>[8] YOLO-V3: Joseph Redmon and Ali Farhadi,YOLOv3:An Incremental lmprovement,arXiv:1804.02767v1</li></ul><h2 id="二、目标检测基础概念"><a href="#二、目标检测基础概念" class="headerlink" title="二、目标检测基础概念"></a>二、目标检测基础概念</h2><p>在介绍目标检测算法之前，先介绍一些跟检测相关的基本概念，包括边界框、锚框和交并比等。</p><h3 id="1）边界框"><a href="#1）边界框" class="headerlink" title="1）边界框"></a><strong>1）边界框</strong></h3><p>检测任务需要同时预测物体的类别和位置，因此需要引入一些跟位置相关的概念。通常使用<strong>边界框（bounding box，bbox）</strong> 来表示物体的位置，边界框是正好能包含物体的矩形框，如 <strong>图6</strong> 所示，图中3个人分别对应3个边界框。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f581e1bfd07a414596368c9c03a1b30ea115a2e30a014be68b2c26961f5c38fa" width = "400"  div align=center"></center><center><br>图6：边界框</br></center><p><br></br></p><p>通常有两种格式来表示边界框的位置：</p><ol><li>$xyxy$，即$(x_1, y_1, x_2, y_2)$，其中$(x_1, y_1)$是矩形框左上角的坐标，$(x_2, y_2)$是矩形框右下角的坐标。图4中3个红色矩形框用$xyxy$格式表示如下：</li></ol><ul><li>左：$(40.93, 141.1, 226.99, 515.73)$。</li><li>中：$(214.29, 325.03, 399.82, 631.37)$。</li><li>右：$(247.2, 131.62, 480.0, 639.32)$。</li></ul><ol><li>$xywh$，即$(x, y, w, h)$，其中$(x, y)$是矩形框中心点的坐标，$w$是矩形框的宽度，$h$是矩形框的高度。</li></ol><hr><blockquote><p><strong>注意：</strong></p><ol><li>在阅读代码时，请注意使用的是哪一种格式的表示方式。</li><li>图片坐标的原点在左上角，$x$轴向右为正方向，$y$轴向下为正方向。</li></ol></blockquote><hr><ul><li>在检测任务中，训练数据集的标签里会给出目标物体真实边界框所对应的$(x_1, y_1, x_2, y_2)$，这样的边界框也被称为<strong>真实框（ground truth box, gt_box）</strong>，如 <strong>图4</strong> 所示，图中画出了3个人像所对应的真实框。</li><li>模型会对目标物体可能出现的位置进行预测，由模型预测出的边界框则称为<strong>预测框（prediction box, pred_box）</strong>。要完成一项检测任务，我们通常希望模型能够根据输入的图片，输出一些预测的边界框，以及边界框中所包含的物体的类别或者说属于某个类别的概率，例如这种格式: $[L, P, x_1, y_1, x_2, y_2]$，其中$L$是类别标签，$P$是物体属于该类别的概率（得分）。一张输入图片可能会产生多个预测框。</li></ul><h3 id="2）锚框"><a href="#2）锚框" class="headerlink" title="2）锚框"></a><strong>2）锚框</strong></h3><p>锚框与物体边界框不同，是由人们假想预测出来的一种框。先设定好锚框的大小和形状，再以图像上某一个点为中心画出矩形框。在下图中，以像素点[300, 500]为中心可以使用下面的程序生成3【数量可以自己调整，这里我们用3个，因为3个框基本可以覆盖整张图片了】个框，如图中蓝色框所示，其中锚框A1跟人像区域非常接近。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画图展示如何绘制边界框和锚框</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画矩形框的程序    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_rectangle</span>(<span class="params">currentAxis, bbox, edgecolor = <span class="string">&#x27;k&#x27;</span>, facecolor = <span class="string">&#x27;y&#x27;</span>, fill=<span class="literal">False</span>, linestyle=<span class="string">&#x27;-&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># currentAxis，坐标轴，通过plt.gca()获取</span></span><br><span class="line">    <span class="comment"># bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]</span></span><br><span class="line">    <span class="comment"># edgecolor，边框线条颜色</span></span><br><span class="line">    <span class="comment"># facecolor，填充颜色</span></span><br><span class="line">    <span class="comment"># fill, 是否填充</span></span><br><span class="line">    <span class="comment"># linestype，边框线型</span></span><br><span class="line">    <span class="comment"># patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数</span></span><br><span class="line">    rect=patches.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>]+<span class="number">1</span>, bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]+<span class="number">1</span>, linewidth=<span class="number">1</span>,</span><br><span class="line">                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)</span><br><span class="line">    currentAxis.add_patch(rect)</span><br><span class="line">    <span class="comment">#print(currentAxis)</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;work/000000086956.jpg&#x27;</span></span><br><span class="line">im = imread(filename)</span><br><span class="line">plt.imshow(im)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用xyxy格式表示物体真实框</span></span><br><span class="line">bbox1 = [<span class="number">214.29</span>, <span class="number">325.03</span>, <span class="number">399.82</span>, <span class="number">631.37</span>]</span><br><span class="line">bbox2 = [<span class="number">40.93</span>, <span class="number">141.1</span>, <span class="number">226.99</span>, <span class="number">515.73</span>]</span><br><span class="line">bbox3 = [<span class="number">247.2</span>, <span class="number">131.62</span>, <span class="number">480.0</span>, <span class="number">639.32</span>]</span><br><span class="line"></span><br><span class="line">currentAxis=plt.gca()<span class="comment"># 获取图像的轴</span></span><br><span class="line"></span><br><span class="line">draw_rectangle(currentAxis, bbox1, edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">draw_rectangle(currentAxis, bbox2, edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">draw_rectangle(currentAxis, bbox3,edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/62.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义画矩形框的程序    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_rectangle</span>(<span class="params">currentAxis, bbox, edgecolor = <span class="string">&#x27;k&#x27;</span>, facecolor = <span class="string">&#x27;y&#x27;</span>, fill=<span class="literal">False</span>, linestyle=<span class="string">&#x27;-&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># currentAxis，坐标轴，通过plt.gca()获取</span></span><br><span class="line">    <span class="comment"># bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]</span></span><br><span class="line">    <span class="comment"># edgecolor，边框线条颜色</span></span><br><span class="line">    <span class="comment"># facecolor，填充颜色</span></span><br><span class="line">    <span class="comment"># fill, 是否填充</span></span><br><span class="line">    <span class="comment"># linestype，边框线型</span></span><br><span class="line">    <span class="comment"># patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数</span></span><br><span class="line">    rect=patches.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>]+<span class="number">1</span>, bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]+<span class="number">1</span>, linewidth=<span class="number">1</span>,</span><br><span class="line">                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)</span><br><span class="line">    currentAxis.add_patch(rect)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;work/000000086956.jpg&#x27;</span></span><br><span class="line">im = imread(filename)</span><br><span class="line">plt.imshow(im)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用xyxy格式表示物体真实框</span></span><br><span class="line">bbox1 = [<span class="number">214.29</span>, <span class="number">325.03</span>, <span class="number">399.82</span>, <span class="number">631.37</span>]</span><br><span class="line">bbox2 = [<span class="number">40.93</span>, <span class="number">141.1</span>, <span class="number">226.99</span>, <span class="number">515.73</span>]</span><br><span class="line">bbox3 = [<span class="number">247.2</span>, <span class="number">131.62</span>, <span class="number">480.0</span>, <span class="number">639.32</span>]</span><br><span class="line"></span><br><span class="line">currentAxis=plt.gca()</span><br><span class="line"></span><br><span class="line">draw_rectangle(currentAxis, bbox1, edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">draw_rectangle(currentAxis, bbox2, edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">draw_rectangle(currentAxis, bbox3,edgecolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制锚框</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_anchor_box</span>(<span class="params">center, length, scales, ratios, img_height, img_width</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    以center为中心，产生一系列锚框</span></span><br><span class="line"><span class="string">    其中length指定了一个基准的长度</span></span><br><span class="line"><span class="string">    scales是包含多种尺寸比例的list</span></span><br><span class="line"><span class="string">    ratios是包含多种长宽比的list</span></span><br><span class="line"><span class="string">    img_height和img_width是图片的尺寸，生成的锚框范围不能超出图片尺寸之外</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bboxes = []</span><br><span class="line">    <span class="keyword">for</span> scale <span class="keyword">in</span> scales:</span><br><span class="line">        <span class="keyword">for</span> ratio <span class="keyword">in</span> ratios:</span><br><span class="line">            h = length*scale*math.sqrt(ratio)</span><br><span class="line">            w = length*scale/math.sqrt(ratio) </span><br><span class="line">            x1 = <span class="built_in">max</span>(center[<span class="number">0</span>] - w/<span class="number">2.</span>, <span class="number">0.</span>)</span><br><span class="line">            y1 = <span class="built_in">max</span>(center[<span class="number">1</span>] - h/<span class="number">2.</span>, <span class="number">0.</span>)</span><br><span class="line">            x2 = <span class="built_in">min</span>(center[<span class="number">0</span>] + w/<span class="number">2.</span> - <span class="number">1.0</span>, img_width - <span class="number">1.0</span>) <span class="comment"># - 1.0 考虑到边框不超过边界</span></span><br><span class="line">            y2 = <span class="built_in">min</span>(center[<span class="number">1</span>] + h/<span class="number">2.</span> - <span class="number">1.0</span>, img_height - <span class="number">1.0</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;中心坐标：(&#123;&#125;,&#123;&#125;) 高宽：(&#123;&#125;,&#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(center[<span class="number">0</span>], center[<span class="number">1</span>], w, h))</span><br><span class="line">            <span class="built_in">print</span>([x1, y1, x2, y2])</span><br><span class="line">            bboxes.append([x1, y1, x2, y2])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> bbox <span class="keyword">in</span> bboxes:</span><br><span class="line">        draw_rectangle(currentAxis, bbox, edgecolor = <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img_height = im.shape[<span class="number">0</span>]</span><br><span class="line">img_width = im.shape[<span class="number">1</span>]        </span><br><span class="line">draw_anchor_box([<span class="number">300.</span>, <span class="number">500.</span>], <span class="number">100.</span>, [<span class="number">2.0</span>], [<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], img_height, img_width)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################# 以下为添加文字说明和箭头###############################</span></span><br><span class="line"></span><br><span class="line">plt.text(<span class="number">285</span>, <span class="number">285</span>, <span class="string">&#x27;G1&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.arrow(<span class="number">300</span>, <span class="number">288</span>, <span class="number">30</span>, <span class="number">40</span>, color=<span class="string">&#x27;red&#x27;</span>, width=<span class="number">0.001</span>, length_includes_head=<span class="literal">True</span>, \</span><br><span class="line">         head_width=<span class="number">5</span>, head_length=<span class="number">10</span>, shape=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(<span class="number">190</span>, <span class="number">320</span>, <span class="string">&#x27;A1&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.arrow(<span class="number">200</span>, <span class="number">320</span>, <span class="number">30</span>, <span class="number">40</span>, color=<span class="string">&#x27;blue&#x27;</span>, width=<span class="number">0.001</span>, length_includes_head=<span class="literal">True</span>, \</span><br><span class="line">         head_width=<span class="number">5</span>, head_length=<span class="number">10</span>, shape=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(<span class="number">160</span>, <span class="number">370</span>, <span class="string">&#x27;A2&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.arrow(<span class="number">170</span>, <span class="number">370</span>, <span class="number">30</span>, <span class="number">40</span>, color=<span class="string">&#x27;blue&#x27;</span>, width=<span class="number">0.001</span>, length_includes_head=<span class="literal">True</span>, \</span><br><span class="line">         head_width=<span class="number">5</span>, head_length=<span class="number">10</span>, shape=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(<span class="number">115</span>, <span class="number">420</span>, <span class="string">&#x27;A3&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.arrow(<span class="number">127</span>, <span class="number">420</span>, <span class="number">30</span>, <span class="number">40</span>, color=<span class="string">&#x27;blue&#x27;</span>, width=<span class="number">0.001</span>, length_includes_head=<span class="literal">True</span>, \</span><br><span class="line">         head_width=<span class="number">5</span>, head_length=<span class="number">10</span>, shape=<span class="string">&#x27;full&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>中心坐标：(300.0,500.0) 高宽：(282.84271247461896,141.4213562373095)[158.57864376269052, 429.28932188134524, 440.4213562373095, 569.7106781186548]中心坐标：(300.0,500.0) 高宽：(200.0,200.0)[200.0, 400.0, 399.0, 599.0]中心坐标：(300.0,500.0) 高宽：(141.42135623730948,282.842712474619)[229.28932188134524, 358.5786437626905, 369.71067811865476, 639.0]</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/63.png" alt="png"></p><p>在目标检测任务中，通常会以某种规则在图片上生成一系列锚框，将这些锚框当成可能的候选区域。模型对这些候选区域是否包含物体进行预测，如果包含目标物体，则还需要进一步预测出物体所属的类别。还有更为重要的一点是，由于锚框位置是固定的，它不大可能刚好跟物体边界框重合，所以需要在锚框的基础上进行<strong>微调</strong>以形成能准确描述物体位置的预测框，模型需要预测出微调的幅度。在训练过程中，模型通过学习不断的调整参数，最终能学会<em><code>如何判别出锚框所代表的候选区域是否包含物体，如果包含物体的话，物体属于哪个类别，以及物体边界框相对于锚框位置需要调整的幅度。</code></em></p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4a1b76daa0364112a4cef060bc33663a960528d03c0d434bb86d73bd460bdc66" width = "600"  div align=center"></center><center><br>图7：锚框</br></center><p><br></br></p><p>不同的模型往往有着不同的生成锚框的方式，在后面的内容中，会详细介绍YOLOv3算法里面产生锚框的规则，理解了它的设计方案，也很容易类推到其它模型上。</p><h3 id="3）交并比"><a href="#3）交并比" class="headerlink" title="3）交并比"></a><strong>3）交并比</strong></h3><p>上面我们画出了以点$(300, 500)$为中心，生成的三个锚框，我们可以看到锚框A1 与真实框 G1的重合度比较好。那么这里的“好”该如何如何量化呢？ 直观地说，我们可以衡量锚框和真实边界框之间的相似性。在检测任务中，使用<strong>交并比（Intersection of Union，IoU）</strong> 作为衡量指标。这一概念来源于数学中的集合，用来描述两个集合$A$和$B$之间的关系，它等于两个集合的交集里面所包含的元素个数，除以它们的并集里面所包含的元素个数，具体计算公式如下：</p><script type="math/tex; mode=display">IoU = \frac{A\cap B}{A \cup B}</script><p>我们将用这个概念来描述两个框之间的重合度。两个框可以看成是两个像素的集合，它们的交并比等于两个框重合部分的面积除以它们合并起来的面积。下图“交集”中青色区域是两个框的重合面积，图“并集”中蓝色区域是两个框的相并面积。用这两个面积相除即可得到它们之间的交并比，如 <strong>图5</strong> 所示。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/c2095c01997044f8a054d676ab585f3beed4400961ea40379771a1fd6d8bf2ea" width = "500"></center><center><br>图5：交并比</br></center><p><br></br></p><p>假设两个矩形框A和B的位置分别为：</p><script type="math/tex; mode=display">A:  [x_{a1}, y_{a1}, x_{a2}, y_{a2}]</script><script type="math/tex; mode=display">B:  [x_{b1}, y_{b1}, x_{b2}, y_{b2}]</script><p>假如位置关系如 <strong>图6</strong> 所示：<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8abb3525989044fa8f512f82dfcd0f2f244594ff79cc4f14a900a09d464b22b8" width = "300"></center><center><br>图6：计算交并比</br></center><p><br></br></p><p>如果二者有相交部分，则相交部分左上角坐标为：</p><script type="math/tex; mode=display">x_1 = max(x_{a1}, x_{b1}), \ \ \ \ \ y_1 = max(y_{a1}, y_{b1})</script><p>相交部分右下角坐标为：</p><script type="math/tex; mode=display">x_2 = min(x_{a2}, x_{b2}), \ \ \ \ \ y_2 = min(y_{a2}, y_{b2})</script><p>计算先交部分面积（+ 1.0 包括边界）：</p><script type="math/tex; mode=display">intersection = max(x_2 - x_1 + 1.0, 0) \cdot max(y_2 - y_1 + 1.0, 0)</script><p>矩形框A和B的面积分别是：</p><script type="math/tex; mode=display">S_A = (x_{a2} - x_{a1} + 1.0) \cdot (y_{a2} - y_{a1} + 1.0)</script><script type="math/tex; mode=display">S_B = (x_{b2} - x_{b1} + 1.0) \cdot (y_{b2} - y_{b1} + 1.0)</script><p>计算相并部分面积：</p><script type="math/tex; mode=display">union = S_A + S_B - intersection</script><p>计算交并比：</p><script type="math/tex; mode=display">IoU = \frac{intersection}{union}</script><hr><blockquote><p><strong>思考：</strong></p><p>两个矩形框之间的相对位置关系，除了上面的示意图之外，还有哪些可能？</p><ul><li>两个矩形框相聚的很远，没有相交部分，则IoU为0；</li><li>两个矩形框相切【左上角和右下角相切】，这里我们认为相交了一个像素点‘；</li><li>两个矩形框的相交部分很小；</li><li>两个矩形框的的相交部分很大。</li></ul></blockquote><hr><p>交并比计算程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：计算IoU，矩形框的坐标形式为xyxy，这个函数会被保存在box_utils.py文件中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou_xyxy</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    <span class="comment"># 获取box1左上角和右下角的坐标</span></span><br><span class="line">    x1min, y1min, x1max, y1max = box1[<span class="number">0</span>], box1[<span class="number">1</span>], box1[<span class="number">2</span>], box1[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 计算box1的面积</span></span><br><span class="line">    s1 = (y1max - y1min + <span class="number">1.</span>) * (x1max - x1min + <span class="number">1.</span>)</span><br><span class="line">    <span class="comment"># 获取box2左上角和右下角的坐标</span></span><br><span class="line">    x2min, y2min, x2max, y2max = box2[<span class="number">0</span>], box2[<span class="number">1</span>], box2[<span class="number">2</span>], box2[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 计算box2的面积</span></span><br><span class="line">    s2 = (y2max - y2min + <span class="number">1.</span>) * (x2max - x2min + <span class="number">1.</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算相交矩形框的坐标</span></span><br><span class="line">    xmin = np.maximum(x1min, x2min)</span><br><span class="line">    ymin = np.maximum(y1min, y2min)</span><br><span class="line">    xmax = np.minimum(x1max, x2max)</span><br><span class="line">    ymax = np.minimum(y1max, y2max)</span><br><span class="line">    <span class="comment"># 计算相交矩形行的高度、宽度、面积</span></span><br><span class="line">    inter_h = np.maximum(ymax - ymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">    inter_w = np.maximum(xmax - xmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">    intersection = inter_h * inter_w</span><br><span class="line">    <span class="comment"># 计算相并面积</span></span><br><span class="line">    union = s1 + s2 - intersection</span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = intersection / union</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bbox1 = [<span class="number">100.</span>, <span class="number">100.</span>, <span class="number">200.</span>, <span class="number">200.</span>]</span><br><span class="line">bbox2 = [<span class="number">120.</span>, <span class="number">120.</span>, <span class="number">220.</span>, <span class="number">220.</span>]</span><br><span class="line">iou = box_iou_xyxy(bbox1, bbox2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;IoU is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(iou))  </span><br></pre></td></tr></table></figure><pre><code>IoU is 0.47402644317607107</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法二：计算IoU，矩形框的坐标形式为xywh</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou_xywh</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    x1min, y1min = box1[<span class="number">0</span>] - box1[<span class="number">2</span>]/<span class="number">2.0</span>, box1[<span class="number">1</span>] - box1[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    x1max, y1max = box1[<span class="number">0</span>] + box1[<span class="number">2</span>]/<span class="number">2.0</span>, box1[<span class="number">1</span>] + box1[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    s1 = box1[<span class="number">2</span>] * box1[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    x2min, y2min = box2[<span class="number">0</span>] - box2[<span class="number">2</span>]/<span class="number">2.0</span>, box2[<span class="number">1</span>] - box2[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    x2max, y2max = box2[<span class="number">0</span>] + box2[<span class="number">2</span>]/<span class="number">2.0</span>, box2[<span class="number">1</span>] + box2[<span class="number">3</span>]/<span class="number">2.0</span></span><br><span class="line">    s2 = box2[<span class="number">2</span>] * box2[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    xmin = np.maximum(x1min, x2min)</span><br><span class="line">    ymin = np.maximum(y1min, y2min)</span><br><span class="line">    xmax = np.minimum(x1max, x2max)</span><br><span class="line">    ymax = np.minimum(y1max, y2max)</span><br><span class="line">    inter_h = np.maximum(ymax - ymin, <span class="number">0.</span>)</span><br><span class="line">    inter_w = np.maximum(xmax - xmin, <span class="number">0.</span>)</span><br><span class="line">    intersection = inter_h * inter_w</span><br><span class="line"></span><br><span class="line">    union = s1 + s2 - intersection</span><br><span class="line">    iou = intersection / union</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line"></span><br><span class="line">bbox1 = [<span class="number">100.</span>, <span class="number">100.</span>, <span class="number">200.</span>, <span class="number">200.</span>]</span><br><span class="line">bbox2 = [<span class="number">120.</span>, <span class="number">120.</span>, <span class="number">220.</span>, <span class="number">220.</span>]</span><br><span class="line">iou = box_iou_xywh(bbox1, bbox2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;IoU is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(iou))  </span><br></pre></td></tr></table></figure><pre><code>IoU is 0.6902485659655831</code></pre><p>为了直观的展示交并比的大小跟重合程度之间的关系，<strong>图7</strong> 示意了不同交并比下两个框之间的相对位置关系，从 IoU = 0.95 到 IoU = 0.<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/52faaa317c6c4de59088de70e7ba0d478969aacffa6e40b4998a3ca9203a9ee2" width = "500"></center><center><br>图7：不同交并比下两个框之间相对位置示意图</br></center><p><br></br></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习高级_计算机视觉之目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习5.4-图像识别模型关键组件之图像增广与迁移学习</title>
      <link href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.4-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF%E4%B8%8E%E5%BE%AE%E8%B0%83/"/>
      <url>/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.4-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF%E4%B8%8E%E5%BE%AE%E8%B0%83/</url>
      
        <content type="html"><![CDATA[<h1 id="一、图像增广"><a href="#一、图像增广" class="headerlink" title="一、图像增广"></a>一、图像增广</h1><p>:label:<code>sec_image_augmentation</code></p><p>在<code>Alexnet</code>中，我们提到过大型数据集是成功应用深度神经网络的先决条件。图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。<br>数据增广是深度学习中常用的技巧之一，主要用于增加训练数据集【增加在当前数据集上，并在项目中直接使用，不会占用本地内存】，让数据集尽可能的多样化，使得训练的模型具有更强的泛化能力。现有的各大深度学习框架都已经自带了数据增广，在实际应用中，并非所有的增广方式都适用当前的训练数据，你需要根据自己的数据集特征来确定应该使用哪几种数据增广方式。</p><h2 id="1-1-数据不足的问题"><a href="#1-1-数据不足的问题" class="headerlink" title="1.1 数据不足的问题"></a>1.1 数据不足的问题</h2><p>我们常常会遇到数据不足的情况。我们来思考一个问题：目前现在流行的最先进的神经网络都是成千上万的图片数据，足够大的数据集是效果好的保证。<br>但是在自己的小数据集上能够使神经网络表现好吗？答案是不确定的。</p><p>众所周知，进行模型训练时，数据越多,得到的结果越准确。那么怎么让数据变得多呢？这时，使用合理的数据增强，便解决了数据不足的问题。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/b75a81055f1b423192d8a8dd073d3504c91eb6f2eb8f4390a566cc07a829d9e4" width="500" hegiht="" ></center><center><br>图1：扩充数据</br></center><p><br></br></p><p>就如上图的猫咪，不论是左转多少度，右转多少度，虽然它都是一只猫咪，但是<strong>每一个图片却不是同一个数据</strong>，这就是对数据的增强处理。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖；我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度，提高模型的泛化能力。<br>可以说，图像增广技术对于AlexNet的成功是必不可少的。在本节中，我们将讨论这项广泛应用于计算机视觉的技术。</p><p>总结来说，数据增强是指增加一个已有的数据集，使得其拥有更多的多样性，这通常都是在线生成的。我们读入一张原始图片，在之后对它随机做增强，再将处理完的图片放入模型中进行训练。<br>当然，测试的时候就不需要进行数据增强了，因此这相当于在训练数据中加入了正则项。除了图片上的数据增强，这一技术在语音类数据和文本类数据上同样适用。</p><p>我们可以手动对图片进行处理，例如下面的操作：更改图像尺寸和更改模式。</p><blockquote><p>一般图像都是RGB三个通道，但是有一种图片格式<code>jpg</code>有四个通道，RGBA。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;更改原图像&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入一张图片</span></span><br><span class="line"><span class="comment"># 查看数据形状，其形状是[H, W, 通道数]</span></span><br><span class="line">img1 = Image.<span class="built_in">open</span>(<span class="string">&#x27;./work/cat.jpg&#x27;</span>)</span><br><span class="line">img1_shape = np.array(img1)</span><br><span class="line"><span class="built_in">print</span>(img1_shape.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改尺寸</span></span><br><span class="line">img2 = img1.resize((<span class="number">500</span>,<span class="number">400</span>))</span><br><span class="line">img2_shape = np.array(img2) </span><br><span class="line"><span class="built_in">print</span>(img2_shape.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改模式</span></span><br><span class="line"><span class="comment"># RGBA 即红色、绿色、蓝色、透明度(英语:Red, Green, Blue、Alpha)。</span></span><br><span class="line"><span class="built_in">print</span>(img1)</span><br><span class="line">img1 = img1.convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(img1)</span><br><span class="line">img1.save(<span class="string">&quot;cat_RGB.jpg&quot;</span>)<span class="comment"># 保存图片</span></span><br></pre></td></tr></table></figure><p>虽然以上方法完美的实现了我们的需求，但是这种方式太低效了。下面我们会介绍几种常见的图像增广方法，我们可以快速地使用飞桨API来实现这些操作。</p><h2 id="1-2-常用的图像增广方法"><a href="#1-2-常用的图像增广方法" class="headerlink" title="1.2 常用的图像增广方法"></a>1.2 常用的图像增广方法</h2><p>在对常用图像增广方法的探索时，我们将使用下面这个尺寸为$400\times 500$的图像作为示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;./work/animal.jpg&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;lesser panda&#x27;</span>)</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/53.png" alt="png"></p><p>大多数图像增广方法都具有一定的随机性。为了便于观察图像增广的效果，我们下面定义辅助函数<code>apply</code>。此函数在输入图像<code>img</code>上多次运行图像增广方法<code>aug</code>并显示所有结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params"></span></span><br><span class="line"><span class="params">    img, </span></span><br><span class="line"><span class="params">    aug,</span></span><br><span class="line"><span class="params">    num_rows=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">    num_cols=<span class="number">5</span>, </span></span><br><span class="line"><span class="params">    scale=<span class="number">1.5</span>,</span></span><br><span class="line"><span class="params">    titles=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    img, 图片：传入的图片</span></span><br><span class="line"><span class="string">    aug, 规则：对图片进行变换的方式</span></span><br><span class="line"><span class="string">    num_rows=2, 行数</span></span><br><span class="line"><span class="string">    num_cols=5, 列数</span></span><br><span class="line"><span class="string">    scale=1.5, 缩放系数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Y = [aug(img) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_rows * num_cols)]</span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, Y)):</span><br><span class="line">        <span class="keyword">if</span> paddle.is_tensor(img):</span><br><span class="line">        <span class="comment"># Tensor Image</span></span><br><span class="line">            ax.imshow(img.numpy())<span class="comment"># 将张量类型转换为数值类型</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># PIL Image</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><h3 id="1）翻转"><a href="#1）翻转" class="headerlink" title="1）翻转"></a><strong>1）翻转</strong></h3><p>左右翻转图像通常不会改变对象的类别。这是最早且最广泛使用的图像增广方法之一。接下来，我们使用<code>transforms</code>模块来创建<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/RandomHorizontalFlip_cn.html#randomhorizontalflip">RandomHorizontalFlip</a>实例，这样就各有50%的几率使图像向左或向右翻转。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RandomHorizontalFlip 基于概率来执行图片的水平翻转。</span></span><br><span class="line"><span class="comment"># prob (float) - 图片执行水平翻转的概率，默认0.5</span></span><br><span class="line">apply(img, vision.transforms.RandomHorizontalFlip())</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/54.png" alt="png"></p><p>上下翻转图像不如左右图像翻转那样常用，原因在于很有可能造成误解，例如建筑物翻转之后就完全不同。接下来，我们创建一个<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/RandomVerticalFlip_cn.html#randomverticalflip">RandomVerticalFlip</a>实例，使图像各有50%的几率向上或向下翻转。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RandomVerticalFlip 基于概率来执行图片的垂直翻转。</span></span><br><span class="line"><span class="comment"># prob (float) - 执行图片垂直翻转的概率，默认0.5</span></span><br><span class="line">apply(img, vision.transforms.RandomVerticalFlip())</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/55.png" alt="png"></p><h3 id="2）裁剪切割"><a href="#2）裁剪切割" class="headerlink" title="2）裁剪切割"></a><strong>2）裁剪切割</strong></h3><p>在我们使用的示例图像中，主角位于图像的中间，但并非所有图像都是这样。在池化层一节中，我们解释了池化层可以降低卷积层对目标位置的敏感性。另外，我们可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置，这也可以降低模型对目标位置的敏感性。</p><p>在下面的代码中，我们<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/RandomResizedCrop_cn.html#randomresizedcrop">随机裁剪</a>一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5到2之间随机取值。然后，区域的宽度和高度都被缩放到200像素。在本节中，$a$和$b$之间的随机数指的是在区间$[a, b]$中通过均匀采样获得的连续值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RandomResizedCrop 将输入图像按照随机大小和长宽比进行裁剪。</span></span><br><span class="line">shape_aug = vision.transforms.RandomResizedCrop(</span><br><span class="line">    (<span class="number">200</span>, <span class="number">200</span>), <span class="comment"># 宽度和高度都被缩放到200像素</span></span><br><span class="line">    scale=(<span class="number">0.1</span>, <span class="number">1</span>), <span class="comment"># 面积为原始面积10%到100%的区域, 默认值：0.08至1.0</span></span><br><span class="line">    ratio=(<span class="number">0.5</span>, <span class="number">2</span>) <span class="comment"># 宽高比从0.5到2之间随机取值, 默认值：3./4至4./3</span></span><br><span class="line">)</span><br><span class="line">apply(img, shape_aug)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/56.png" alt="png"></p><h3 id="3）改变颜色"><a href="#3）改变颜色" class="headerlink" title="3）改变颜色"></a><strong>3）改变颜色</strong></h3><p>另一种增广方法是<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/transforms/ColorJitter_cn.html#colorjitter">改变颜色</a>。我们可以改变图像颜色的四个方面：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。在下面的示例中，我们随机更改图像的四个数值，随机值为原始图像的50%（$1-0.5$）到150%（$1+0.5$）之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ColorJitter 随机调整图像的亮度、对比度、饱和度和色调。</span></span><br><span class="line">apply(img, </span><br><span class="line">    vision.transforms.ColorJitter(</span><br><span class="line">        brightness=<span class="number">0.5</span>,</span><br><span class="line">        contrast=<span class="number">0</span>,</span><br><span class="line">        saturation=<span class="number">0</span>,</span><br><span class="line">        hue=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/57.png" alt="png"></p><p>我们还可以创建一个<code>RandomColorJitter</code>实例，并设置同时随机更改图像的亮度（<code>brightness</code>）、对比度（<code>contrast</code>）、饱和度（<code>saturation</code>）和色调（<code>hue</code>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">color_aug = vision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, </span><br><span class="line">    contrast=<span class="number">0.5</span>, </span><br><span class="line">    saturation=<span class="number">0.5</span>, </span><br><span class="line">    hue=<span class="number">0.5</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">apply(img, color_aug)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/58.png" alt="png"></p><h3 id="4）结合多种图像增广方法"><a href="#4）结合多种图像增广方法" class="headerlink" title="4）结合多种图像增广方法"></a><strong>4）结合多种图像增广方法</strong></h3><p>在实践中，我们将结合多种图像增广方法。比如，我们可以通过使用一个<code>Compose</code>实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">augs = vision.transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">    vision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    color_aug,</span><br><span class="line">    shape_aug,</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">apply(img, augs)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/59.png" alt="png"></p><h2 id="1-3-应用图像增广"><a href="#1-3-应用图像增广" class="headerlink" title="1.3 应用图像增广"></a>1.3 应用图像增广</h2><p>让我们使用图像增广来训练模型。这里，我们使用<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10数据集</a>CIFAR-10数据集中对象的颜色和大小差异更明显。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_images = vision.datasets.Cifar10(mode=<span class="string">&#x27;train&#x27;</span>, data_file=<span class="string">&#x27;data/data149232/cifar-10-python.tar.gz&#x27;</span>, download=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line">ppl.show_images([all_images[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>)], <span class="number">4</span>, <span class="number">8</span>, scale=<span class="number">0.8</span>);</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/60.png" alt="png"></p><p>为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，且在预测过程中不使用随机操作的图像增广，在这里，我们只使用最简单的随机左右翻转。<br>此外，我们使用<code>ToTensor</code>实例将一批图像转换为深度学习框架所要求的格式，即形状为（批量大小，通道数，高度，宽度）的32位浮点数，取值范围为0到1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vision.transforms.Compose 将用于数据集预处理的接口以列表的方式进行组合</span></span><br><span class="line"><span class="comment"># 应用简单的左右翻转</span></span><br><span class="line"><span class="comment"># ToTensor 生成数据格式为（批量大小，通道数量，高度，宽度）</span></span><br><span class="line"></span><br><span class="line">train_augs = vision.transforms.Compose([</span><br><span class="line">    vision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    vision.transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">test_augs = vision.transforms.Compose([</span><br><span class="line">    vision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_cifar10</span>(<span class="params">is_train, augs, batch_size</span>):</span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        dataset = vision.datasets.Cifar10(</span><br><span class="line">            mode=<span class="string">&#x27;train&#x27;</span>, data_file=<span class="string">&#x27;data/data149232/cifar-10-python.tar.gz&#x27;</span>, transform=augs, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataset = vision.datasets.Cifar10(</span><br><span class="line">            mode=<span class="string">&#x27;test&#x27;</span>, data_file=<span class="string">&#x27;data/data149232/cifar-10-python.tar.gz&#x27;</span>, transform=augs, download=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    dataloader = paddle.io.DataLoader(dataset, </span><br><span class="line">                                    batch_size=batch_size,</span><br><span class="line">                                    shuffle=is_train,</span><br><span class="line">                                    num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_iter = load_cifar10(<span class="literal">True</span>, train_augs, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>[10, 3, 32, 32] [10]</code></pre><p>这样我们就构造好了数据迭代器，我们将使用下面章节讲的内容来将图像增广技术应用到自己的图像识别模型中。</p><h1 id="二、微调"><a href="#二、微调" class="headerlink" title="二、微调"></a>二、微调</h1><p>:label:<code>sec_fine_tuning</code></p><p>在前面的一些章节中，我们介绍了如何在只有6万张图像的MNIST训练数据集上训练模型。我们还描述了学术界当下使用最广泛的大规模图像数据集ImageNet，它有超过1000万的图像和1000类的物体。然而，我们平常接触到的数据集的规模通常在这两者之间。</p><p>假如我们想识别图片中不同类型的椅子，然后向用户推荐购买链接。一种可能的方法是首先识别100把普通椅子，为每把椅子拍摄1000张不同角度的图像，然后在收集的图像数据集上训练一个分类模型，尽管这个椅子数据集可能大于Fashion-MNIST数据集，但实例数量仍然不到ImageNet中的十分之一。适合ImageNet的复杂模型可能会在这个椅子数据集上过拟合。此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求。</p><p>为了解决上述问题，一个显而易见的解决方案是收集更多的数据，但是，收集和标记数据可能需要大量的时间和金钱。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究资金。尽管目前的数据收集成本已大幅降低，但这一成本仍不能忽视。</p><p>另一种解决方案是应用<strong>迁移学习（transfer learning）</strong> ，将从源数据集学到的知识迁移到目标数据集。例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合，这些类似的特征也可能有效地识别椅子。</p><h2 id="2-1-步骤"><a href="#2-1-步骤" class="headerlink" title="2.1 步骤"></a>2.1 步骤</h2><p>在本节中，我们将介绍迁移学习中的常见技巧：<strong>微调（fine-tuning）</strong>。微调包括以下四个步骤：</p><ol><li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型。</li><li>创建一个新的神经网络模型，即目标模型。复制源模型上的所有模型设计及其参数（输出层除外）。</li><li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li><li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。</li></ol><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e68ea652432448a0b7ecb994cdc3cef3ac2a74eb45f64a43a2c89bfc850f68a9" width="800" hegiht="" ></center><center><br>图2：微调模型</br></center><p><br></br></p><p><strong>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。</strong></p><h2 id="2-2-热狗识别"><a href="#2-2-热狗识别" class="headerlink" title="2.2 热狗识别"></a>2.2 热狗识别</h2><p>通过热狗识别的例子了解Fine-Tuning的用法，我们将在一个小型数据集上微调ResNet模型，该ResNet模型已在ImageNet数据集上进行了预训练。<br>这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> ppl</span><br></pre></td></tr></table></figure><h3 id="1）获取数据集"><a href="#1）获取数据集" class="headerlink" title="1）获取数据集"></a><strong>1）获取数据集</strong></h3><p>我们使用的热狗数据集来源于网络。该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像。含着两个类别的1000张图片用于训练，其余的则用于测试。<br>解压下载的数据集，我们获得了两个文件夹<code>hotdog/train</code>和<code>hotdog/test</code>。<br>这两个文件夹都有<code>hotdog</code>（有热狗）和<code>not-hotdog</code>（无热狗）两个子文件夹，<br>子文件夹内都包含相应类的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压文件</span></span><br><span class="line"><span class="comment"># -o 不必先询问用户，unzip执行后覆盖原有文件</span></span><br><span class="line">!unzip -o -q data/data149507/hotdog.<span class="built_in">zip</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir = <span class="string">&#x27;hotdog&#x27;</span><span class="comment"># 记录图片的路径</span></span><br></pre></td></tr></table></figure><p>我们创建两个实例来分别读取训练和测试数据集中的所有图像文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ImageFolder 一种通用的数据加载方式</span></span><br><span class="line">train_imgs = vision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>))</span><br><span class="line">test_imgs = vision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>))</span><br></pre></td></tr></table></figure><p>下面显示了前8个正类样本图片和最后8张负类样本图片。正如你所看到的，图像的大小和纵横比各有不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hotdogs = [train_imgs[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">not_hotdogs = [train_imgs[-i - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">ppl.show_images(hotdogs + not_hotdogs, <span class="number">2</span>, <span class="number">8</span>, scale=<span class="number">1.4</span>); <span class="comment"># 分号用于一条语句的结束标识</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/61.png" alt="png"></p><h3 id="2）数据增广"><a href="#2）数据增广" class="headerlink" title="2）数据增广"></a><strong>2）数据增广</strong></h3><ul><li>在训练期间：我们首先将输入图像按照随机大小和长宽比进行裁剪，然后将该区域缩放为$224 \times 224$输入图像。进行水平翻转。</li><li>在测试过程中：由于图像的高宽比都不同，因此我们将图像的高度和宽度都缩放到256像素，然后裁剪中央$224 \times 224$区域作为输入。</li></ul><p>此外，对于RGB（红、绿和蓝）颜色通道，我们分别标准化每个通道：具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">normalize = vision.transforms.Normalize(</span><br><span class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line">train_augs = vision.transforms.Compose([</span><br><span class="line">    vision.transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    vision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    vision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br><span class="line"></span><br><span class="line">test_augs = vision.transforms.Compose([</span><br><span class="line">    vision.transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    vision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    vision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br></pre></td></tr></table></figure><h3 id="3）定义和初始化模型"><a href="#3）定义和初始化模型" class="headerlink" title="3）定义和初始化模型"></a><strong>3）定义和初始化模型</strong></h3><p>我们使用在ImageNet数据集上预训练的ResNet-18作为源模型。在这里，我们指定<code>pretrained=True</code>以自动下载预训练的模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pretrained (bool，可选) - 是否加载在imagenet数据集上的预训练权重。默认值：False。</span></span><br><span class="line">pretrained_net = vision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>预训练的源模型实例包含许多特征层和一个输出层<code>fc</code>，下面给出了源模型的成员变量<code>fc</code>。此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调，在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的1000个类输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(pretrained_net)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;#################################################&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;输出层&#x27;</span>, pretrained_net.fc)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;#################################################&quot;</span>)</span><br></pre></td></tr></table></figure><p>之后，我们构建一个新的神经网络作为目标模型。它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。</p><p>在下面的代码中，目标模型<code>finetune_net</code>中成员变量<code>features</code>的参数被初始化为源模型相应层的模型参数。由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数。</p><p>成员变量<code>output</code>的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。假设<code>Trainer</code>实例中的学习率为$\eta$，我们将成员变量<code>output</code>中参数的学习率设置为$10\eta$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = vision.models.resnet18(pretrained=<span class="literal">True</span>) <span class="comment"># 模型参数初始化为源模型相应层的模型参数</span></span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.weight.shape[<span class="number">0</span>], <span class="number">2</span>) <span class="comment"># 调整输出层（512,2）</span></span><br><span class="line">finetune_net.fc.weight.set_value = nn.initializer.XavierUniform() <span class="comment"># 只对输出层设置初始化参数策略</span></span><br></pre></td></tr></table></figure><h3 id="4）训练微调模型"><a href="#4）训练微调模型" class="headerlink" title="4）训练微调模型"></a><strong>4）训练微调模型</strong></h3><p>首先，我们定义了一个训练函数<code>train_fine_tuning</code>，该函数使用微调，因此可以多次调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch</span>(<span class="params">net, X, y, loss, trainer</span>): <span class="comment"># 每个批量的训练过程【内层循环】</span></span><br><span class="line">    net.train()<span class="comment">#开始循环</span></span><br><span class="line">    trainer.clear_grad()<span class="comment"># 进行梯度</span></span><br><span class="line">    pred = net(X)<span class="comment"># 计算预测值</span></span><br><span class="line">    l = loss(pred, y)<span class="comment"># 计算损失</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()<span class="comment"># 反向传播</span></span><br><span class="line">    trainer.step()<span class="comment"># 更新梯度</span></span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()<span class="comment"># 临时存储损失值</span></span><br><span class="line">    train_acc_sum = ppl.accuracy(pred, y)<span class="comment"># 临时存储精度值</span></span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs</span>):<span class="comment">#外层循环，训练过程</span></span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = ppl.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            l, acc = train_batch(net, features, labels, loss, trainer)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="built_in">print</span>(metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>], metric[<span class="number">2</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;#####################################&quot;</span>)</span><br><span class="line">        test_acc = ppl.evaluate_accuracy(net, test_iter)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>,  &#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行微调</span></span><br><span class="line"><span class="comment"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>, param_group=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    train_iter = paddle.io.DataLoader(</span><br><span class="line">        vision.datasets.DatasetFolder(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = paddle.io.DataLoader(</span><br><span class="line">        vision.datasets.DatasetFolder(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>) <span class="comment"># reduction 指定应用于输出结果的计算方式</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 学习率策略</span></span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        params_1x = [</span><br><span class="line">            param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        trainer = paddle.optimizer.SGD(</span><br><span class="line">            parameters=[ <span class="comment"># 参数对应的学习率</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;, <span class="comment"># params_1x使用正常的learning_rate</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(),<span class="string">&#x27;learning_rate&#x27;</span>: learning_rate * <span class="number">10</span>&#125; <span class="comment"># fc.parameters()使用十倍的learning_rate</span></span><br><span class="line">                ],</span><br><span class="line">            learning_rate = learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trainer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=learning_rate,weight_decay=<span class="number">0.001</span>)</span><br><span class="line">        </span><br><span class="line">    train(net, train_iter, test_iter, loss, trainer, num_epochs)</span><br></pre></td></tr></table></figure><p>我们使用较小的学习率，通过微调预训练获得的模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.222, train_acc 0.916,test_acc 0.900</span></span><br><span class="line"><span class="string">总体看效果还是不错的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>为了进行比较，我们定义了一个相同的模型，但是将其所有模型参数初始化为随机值。由于整个模型需要从头开始训练，因此我们需要使用更大的学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scratch_net = vision.models.resnet18() <span class="comment"># 不加载在imagenet数据集上的预训练权重</span></span><br><span class="line">scratch_net.fc = nn.Linear(scratch_net.fc.weight.shape[<span class="number">0</span>], <span class="number">2</span>)</span><br><span class="line">train_fine_tuning(scratch_net, <span class="number">5e-4</span>, param_group=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss 0.398, train_acc 0.861, test_acc 0.840</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>意料之中，微调模型往往表现更好，因为它的初始参数值更有效。</p><h1 id="三、小结"><a href="#三、小结" class="headerlink" title="三、小结"></a>三、小结</h1><ul><li>图像增广基于现有的训练数据生成随机图像，来提高模型的泛化能力。</li><li>为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，而在预测过程中不使用带随机操作的图像增广。</li><li>深度学习框架提供了许多不同的图像增广方法，这些方法可以被同时应用。</li><li>迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调是迁移学习的常见技巧。</li><li>除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。</li><li>通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。</li><li>在一个目标数据集上进行训练任务，但是使用了更强的正则化（使用了更小的学习率 使用更少的数据迭代），原因在于源模型已经很好了，保留优势。</li><li>微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来提升精度。</li><li>微调通常速度更快，精度更高。</li><li>微调普遍应用于计算机视觉领域。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_卷积基本概念及经典模型复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习5.3-图像识别模型关键组件之数据处理</title>
      <link href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.3-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
      <url>/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.3-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="一、数据处理"><a href="#一、数据处理" class="headerlink" title="一、数据处理"></a>一、数据处理</h1><p>在工业实践中，我们面临的任务和数据环境千差万别，通常需要自己编写适合当前任务的数据处理程序，一般涉及如下五个环节：</p><ul><li>读入数据</li><li>划分数据集</li><li>生成批次数据</li><li>训练样本集乱序</li><li>校验数据有效性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据处理部分之前的代码，加入部分数据处理的库</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h2 id="1-1读入数据并划分数据集"><a href="#1-1读入数据并划分数据集" class="headerlink" title="1.1读入数据并划分数据集"></a><strong>1.1读入数据并划分数据集</strong></h2><p>在实际应用中，保存到本地的数据存储格式多种多样，如MNIST数据集以json格式存储在本地，其数据存储结构如 <strong>图1</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/7075f5ca75c54e4e8553c10b696913a1a178dad37c5c460a899cd75635cd7961" width="500" hegiht="" ></center><center><br>图1：MNIST数据集的存储结构</br></center><p><br></br></p><p><strong>data</strong>包含三个元素的列表：train_set、val_set、 test_set，包括50 000条训练样本、10 000条验证样本、10 000条测试样本。每个样本包含手写数字图片和对应的标签【没有给标签，则可以自己写代码给打上标签】。</p><ul><li><strong>train_set（训练集）</strong>：用于确定模型参数。</li><li><strong>val_set（验证集）</strong>：用于调节模型超参数（如多个网络结构、正则化权重的最优选择）。</li><li><strong>test_set（测试集）</strong>：用于估计应用效果（没有在模型中应用过的数据，更贴近模型在真实场景应用的效果）。</li></ul><p><strong>train_set</strong>包含两个元素的列表：train_images、train_labels。</p><ul><li><strong>train_images</strong>：[50 000, 784]的二维列表，包含50 000张图片。每张图片用一个长度为784的向量表示，内容是28*28尺寸的像素灰度值（黑白图片）。</li><li><strong>train_labels</strong>：[50 000, ]的列表，表示这些图片对应的分类标签，即0~9之间的一个数字。</li></ul><p>在本地<code>./work/</code>目录下读取文件名称为<code>mnist.json.gz</code>的MNIST数据，并拆分成训练集、验证集和测试集，实现方法如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明数据集文件位置</span></span><br><span class="line">datafile = <span class="string">&#x27;work/mnist.json.gz&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loading mnist dataset from &#123;&#125; ......&#x27;</span>.<span class="built_in">format</span>(datafile))</span><br><span class="line"><span class="comment"># 加载json数据文件</span></span><br><span class="line">data = json.load(gzip.<span class="built_in">open</span>(datafile))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mnist dataset load done&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取到的数据区分训练集，验证集，测试集</span></span><br><span class="line">train_set, val_set, eval_set = data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察训练集数据</span></span><br><span class="line">imgs, labels = train_set[<span class="number">0</span>], train_set[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集数量: &quot;</span>, <span class="built_in">len</span>(imgs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察验证集数量</span></span><br><span class="line">imgs, labels = val_set[<span class="number">0</span>], val_set[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;验证数据集数量: &quot;</span>, <span class="built_in">len</span>(imgs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察测试集数量</span></span><br><span class="line">imgs, labels = val= eval_set[<span class="number">0</span>], eval_set[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集数量: &quot;</span>, <span class="built_in">len</span>(imgs))</span><br></pre></td></tr></table></figure><pre><code>loading mnist dataset from work/mnist.json.gz ......mnist dataset load done训练数据集数量:  50000验证数据集数量:  10000测试数据集数量:  10000</code></pre><h3 id="扩展阅读：为什么针对固定数据集的模型总在不断精进呢？"><a href="#扩展阅读：为什么针对固定数据集的模型总在不断精进呢？" class="headerlink" title="扩展阅读：为什么针对固定数据集的模型总在不断精进呢？"></a><strong>扩展阅读：为什么针对固定数据集的模型总在不断精进呢？</strong></h3><p>通常某组织发布一个新任务的训练集和测试集数据后，全世界的科学家都针对该数据集进行创新研究，随后大量针对该数据集的论文会陆续发表。论文1的A模型声称在测试集的准确率70%，论文2的B模型声称在测试集的准确率提高到72%，论文N的X模型声称在测试集的准确率提高到90% …</p><p>然而这些论文中的模型在测试集上准确率提升真实有效么？我们不妨大胆猜测一下。</p><p>假设所有论文共产生1000个模型，这些模型使用的是测试数据集来评判模型效果，并最终选出效果最优的模型。这相当于把原始的测试集当作了验证集，使得测试集失去了真实评判模型效果的能力，正如机器学习领域非常流行的一句话：“拷问数据足够久，它终究会招供”。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/61ff2532a33346cb9641db0e5b97b9f30f2ad9d17bad444193c07191cdacd189" width="600" hegiht="" ></center><center><br>图2：拷问数据足够久，它总会招供</br></center><p><br></br></p><p>那么当我们需要将各种论文中模型复用于工业项目时，应该如何选择呢？当几个模型的准确率在测试集上差距不大时，尽量选择网络结构相对简单的模型。往往越精巧设计的模型和方法，越不容易在不同的数据集之间迁移。</p><hr><h2 id="1-2-训练样本乱序、生成批次数据"><a href="#1-2-训练样本乱序、生成批次数据" class="headerlink" title="1.2 训练样本乱序、生成批次数据"></a><strong>1.2 训练样本乱序、生成批次数据</strong></h2><ul><li><strong>训练样本乱序：</strong> 先将样本按顺序进行编号，建立ID集合index_list。然后将index_list乱序，最后按乱序后的顺序读取数据。</li></ul><p>通过大量实验发现，模型对最后出现的数据印象更加深刻。训练数据导入后，越接近模型训练结束，最后几个批次数据对模型参数的影响越大。为了避免模型记忆影响训练效果，需要进行样本乱序操作。</p><ul><li><strong>生成批次数据：</strong> 先设置合理的batch_size，再将数据转变成符合模型输入要求的np.array格式返回。同时，在返回数据时将Python生成器设置为<code>yield</code>模式，以减少内存占用。</li></ul><p>在执行如上两个操作之前，需要先将数据处理代码封装成load_data函数，方便后续调用。load_data有三种模型：<code>train</code>、<code>valid</code>、<code>eval</code>，分为对应返回的数据是训练集、验证集、测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">imgs, labels = train_set[<span class="number">0</span>], train_set[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集数量: &quot;</span>, <span class="built_in">len</span>(imgs))</span><br><span class="line"><span class="comment"># 获得数据集长度</span></span><br><span class="line">imgs_length = <span class="built_in">len</span>(imgs)</span><br><span class="line"><span class="comment"># 定义数据集每个数据的序号，根据序号读取数据</span></span><br><span class="line">index_list = <span class="built_in">list</span>(<span class="built_in">range</span>(imgs_length))</span><br><span class="line"><span class="comment"># 读入数据时用到的批次大小</span></span><br><span class="line">BATCHSIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打乱训练数据的索引序号</span></span><br><span class="line">random.shuffle(index_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据生成器，返回批次数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_generator</span>():</span><br><span class="line">    imgs_list = []</span><br><span class="line">    labels_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index_list:</span><br><span class="line">        <span class="comment"># 将数据处理成希望的类型</span></span><br><span class="line">        img = np.array(imgs[i]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        label = np.array(labels[i]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        imgs_list.append(img) </span><br><span class="line">        labels_list.append(label)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(imgs_list) == BATCHSIZE:</span><br><span class="line">            <span class="comment"># 获得一个batchsize的数据，并返回</span></span><br><span class="line">            <span class="keyword">yield</span> np.array(imgs_list), np.array(labels_list)</span><br><span class="line">            <span class="comment"># 清空数据读取列表</span></span><br><span class="line">            imgs_list = []</span><br><span class="line">            labels_list = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果剩余数据的数目小于BATCHSIZE，</span></span><br><span class="line">    <span class="comment"># 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(imgs_list) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">yield</span> np.array(imgs_list), np.array(labels_list)</span><br><span class="line">    <span class="keyword">return</span> data_generator</span><br></pre></td></tr></table></figure><pre><code>训练数据集数量:  50000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明数据读取函数，从训练集中读取数据</span></span><br><span class="line">train_loader = data_generator</span><br><span class="line"><span class="comment"># 以迭代的形式读取数据</span></span><br><span class="line"><span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">    image_data, label_data = data</span><br><span class="line">    <span class="keyword">if</span> batch_id == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 打印数据shape和类型</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;打印第一个batch数据的维度:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图像维度: &#123;&#125;, 标签维度: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(image_data.shape, label_data.shape))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>打印第一个batch数据的维度:图像维度: (100, 784), 标签维度: (100,)# 100张图片，784是28×28后的展平结果</code></pre><h2 id="1-3-校验数据有效性"><a href="#1-3-校验数据有效性" class="headerlink" title="1.3 校验数据有效性"></a><strong>1.3 校验数据有效性</strong></h2><p>在实际应用中，原始数据可能存在标注不准确、数据杂乱或格式不统一等情况。因此在完成数据处理流程后，还需要进行数据校验，一般有两种方式：</p><ul><li>机器校验：加入一些校验和清理数据的操作。</li><li>人工校验：先打印数据输出结果，观察是否是设置的格式。再从训练的结果验证数据处理和读取的有效性。</li></ul><h3 id="机器校验"><a href="#机器校验" class="headerlink" title="机器校验"></a><strong>机器校验</strong></h3><p>如下代码所示，如果数据集中的图片数量和标签数量不等，说明数据逻辑存在问题，可使用assert语句校验图像数量和标签数据是否一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">imgs_length = <span class="built_in">len</span>(imgs)</span><br><span class="line"><span class="comment"># 条件不满足，报错AssertionError</span></span><br><span class="line"><span class="comment"># 条件满足，不发生变化没有返回值</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(imgs) == <span class="built_in">len</span>(labels), \</span><br><span class="line">    <span class="string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>\</span><br><span class="line">    .<span class="built_in">format</span>(<span class="built_in">len</span>(imgs), <span class="built_in">len</span>(label))</span><br></pre></td></tr></table></figure><h3 id="人工校验"><a href="#人工校验" class="headerlink" title="人工校验"></a><strong>人工校验</strong></h3><p>人工校验是指打印数据输出结果，观察是否是预期的格式。实现数据处理和加载函数后，我们可以调用它读取一次数据，观察数据的shape和类型是否与函数中设置的一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明数据读取函数，从训练集中读取数据</span></span><br><span class="line">train_loader = data_generator</span><br><span class="line"><span class="comment"># 以迭代的形式读取数据</span></span><br><span class="line"><span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">    image_data, label_data = data</span><br><span class="line">    <span class="keyword">if</span> batch_id == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 打印数据shape和类型</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;打印第一个batch数据的维度，以及数据的类型:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图像维度: &#123;&#125;, 标签维度: &#123;&#125;, 图像数据类型: &#123;&#125;, 标签数据类型: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(image_data.shape, label_data.shape, <span class="built_in">type</span>(image_data), <span class="built_in">type</span>(label_data)))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>打印第一个batch数据的维度，以及数据的类型:图像维度: (100, 784), 标签维度: (100,), 图像数据类型: &lt;class &#39;numpy.ndarray&#39;&gt;, 标签数据类型: &lt;class &#39;numpy.ndarray&#39;&gt;</code></pre><h2 id="1-4-封装数据读取与处理函数"><a href="#1-4-封装数据读取与处理函数" class="headerlink" title="1.4 封装数据读取与处理函数"></a><strong>1.4 封装数据读取与处理函数</strong></h2><p>上文，我们从读取数据、划分数据集、到打乱训练数据、构建数据读取器以及数据数据校验，完成了一整套一般性的数据处理流程，下面将这些步骤放在一个函数中实现，方便在神经网络训练时直接调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">    datafile = <span class="string">&#x27;./work/mnist.json.gz&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loading mnist dataset from &#123;&#125; ......&#x27;</span>.<span class="built_in">format</span>(datafile))</span><br><span class="line">    <span class="comment"># 加载json数据文件</span></span><br><span class="line">    data = json.load(gzip.<span class="built_in">open</span>(datafile))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;mnist dataset load done&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 读取到的数据区分训练集，验证集，测试集</span></span><br><span class="line">    train_set, val_set, eval_set = data</span><br><span class="line">    <span class="keyword">if</span> mode==<span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        <span class="comment"># 获得训练数据集</span></span><br><span class="line">        imgs, labels = train_set[<span class="number">0</span>], train_set[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> mode==<span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">        <span class="comment"># 获得验证数据集</span></span><br><span class="line">        imgs, labels = val_set[<span class="number">0</span>], val_set[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> mode==<span class="string">&#x27;eval&#x27;</span>:</span><br><span class="line">        <span class="comment"># 获得测试数据集</span></span><br><span class="line">        imgs, labels = eval_set[<span class="number">0</span>], eval_set[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;mode can only be one of [&#x27;train&#x27;, &#x27;valid&#x27;, &#x27;eval&#x27;]&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练数据集数量: &quot;</span>, <span class="built_in">len</span>(imgs))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 校验数据</span></span><br><span class="line">    imgs_length = <span class="built_in">len</span>(imgs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(imgs) == <span class="built_in">len</span>(labels), \</span><br><span class="line">          <span class="string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(imgs), <span class="built_in">len</span>(labels))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获得数据集长度</span></span><br><span class="line">    imgs_length = <span class="built_in">len</span>(imgs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义数据集每个数据的序号，根据序号读取数据</span></span><br><span class="line">    index_list = <span class="built_in">list</span>(<span class="built_in">range</span>(imgs_length))</span><br><span class="line">    <span class="comment"># 读入数据时用到的批次大小</span></span><br><span class="line">    BATCHSIZE = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义数据生成器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data_generator</span>():</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            <span class="comment"># 训练模式下打乱数据</span></span><br><span class="line">            random.shuffle(index_list)</span><br><span class="line">        imgs_list = []</span><br><span class="line">        labels_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> index_list:</span><br><span class="line">            <span class="comment"># 将数据处理成希望的类型</span></span><br><span class="line">            img = np.array(imgs[i]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            label = np.array(labels[i]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            imgs_list.append(img) </span><br><span class="line">            labels_list.append(label)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(imgs_list) == BATCHSIZE:</span><br><span class="line">                <span class="comment"># 获得一个batchsize的数据，并返回</span></span><br><span class="line">                <span class="keyword">yield</span> np.array(imgs_list), np.array(labels_list)</span><br><span class="line">                <span class="comment"># 清空数据读取列表</span></span><br><span class="line">                imgs_list = []</span><br><span class="line">                labels_list = []</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 如果剩余数据的数目小于BATCHSIZE，</span></span><br><span class="line">        <span class="comment"># 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(imgs_list) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">yield</span> np.array(imgs_list), np.array(labels_list)</span><br><span class="line">    <span class="keyword">return</span> data_generator</span><br></pre></td></tr></table></figure><p>下面定义一层神经网络，利用定义好的数据处理函数，完成神经网络的训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据处理部分之后的代码，数据读取的部分调用Load_data函数</span></span><br><span class="line"><span class="comment"># 定义网络结构，同上一节所使用的网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MNIST</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MNIST, self).__init__()</span><br><span class="line">        <span class="comment"># 定义一层全连接层，输出维度是1</span></span><br><span class="line">        self.fc = paddle.nn.Linear(in_features=<span class="number">784</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练配置，并启动训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model</span>):</span><br><span class="line">    model = MNIST()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment">#调用加载数据的函数</span></span><br><span class="line">    train_loader = load_data(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.001</span>, parameters=model.parameters())</span><br><span class="line">    EPOCH_NUM = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            <span class="comment">#准备数据，变得更加简洁</span></span><br><span class="line">            images, labels = data</span><br><span class="line">            images = paddle.to_tensor(images)</span><br><span class="line">            labels = paddle.to_tensor(labels) </span><br><span class="line"></span><br><span class="line">            <span class="comment">#前向计算的过程</span></span><br><span class="line">            predits = model(images)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#计算损失，取一个批次样本损失的平均值</span></span><br><span class="line">            loss = F.square_error_cost(predits, labels)</span><br><span class="line">            avg_loss = paddle.mean(loss)      </span><br><span class="line">            </span><br><span class="line">            <span class="comment">#每训练了200批次的数据，打印下当前Loss的情况</span></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch_id, batch_id, avg_loss.numpy()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#后向传播，更新参数的过程</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.clear_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    paddle.save(model.state_dict(), <span class="string">&#x27;./mnist.pdparams&#x27;</span>)</span><br><span class="line"><span class="comment"># 创建模型           </span></span><br><span class="line">model = MNIST()</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">train(model)</span><br></pre></td></tr></table></figure><h1 id="二、异步数据读取"><a href="#二、异步数据读取" class="headerlink" title="二、异步数据读取"></a>二、异步数据读取</h1><p>上面提到的数据读取采用的是同步数据读取方式。对于样本量较大、数据读取较慢的场景，建议采用异步数据读取方式。异步读取数据时，数据读取和模型训练并行执行，从而加快了数据读取速度，牺牲一小部分内存换取数据读取效率的提升，二者关系如 <strong>图3</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a5fd990c5355426183a71b95aa28a59f979014f6905144ddb415c5a4fe647441" width="500" ></center><center><br>图3：同步数据读取和异步数据读取示意图</br></center><p><br></br></p><ul><li><strong>同步数据读取</strong>：数据读取与模型训练串行。当模型需要数据时，才运行数据读取函数获得当前批次的数据。在读取数据期间，模型一直等待数据读取结束才进行训练，数据读取速度相对较慢。</li><li><strong>异步数据读取</strong>：数据读取和模型训练并行。读取到的数据不断的放入缓存区，无需等待模型训练就可以启动下一轮数据读取。当模型训练完一个批次后，不用等待数据读取过程，直接从缓存区获得下一批次数据进行训练，从而加快了数据读取速度。</li><li><strong>异步队列</strong>：数据读取和模型训练交互的仓库，二者均可以从仓库中读取数据，它的存在使得两者的工作节奏可以解耦。</li></ul><p>使用飞桨实现异步数据读取非常简单，只需要两个步骤：</p><ol><li>构建一个继承paddle.io.Dataset类的数据读取器。</li><li>通过paddle.io.DataLoader创建异步数据读取的迭代器。</li></ol><p>首先，我们创建定义一个paddle.io.Dataset类，使用随机函数生成一个数据读取器，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 构建一个类，继承paddle.io.Dataset，创建数据读取器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_samples</span>):</span><br><span class="line">        <span class="comment"># 样本数量</span></span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 随机产生数据和label</span></span><br><span class="line">        image = np.random.random([<span class="number">784</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        label = np.random.randint(<span class="number">0</span>, <span class="number">9</span>, (<span class="number">1</span>, )).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回样本总数量</span></span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据读取器：num_samples个</span></span><br><span class="line">dataset = RandomDataset(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    <span class="built_in">print</span>(dataset[i])</span><br></pre></td></tr></table></figure><p>在定义完 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html#dataset">paddle.io.Dataset</a> 后，使用 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html#dataloader">paddle.io.DataLoader</a> API即可实现异步数据读取，数据会由Python线程预先读取，并异步送入一个队列中。</p><blockquote><p><em>class</em> paddle.io.DataLoader(dataset, batch_size=100, shuffle=True, num_workers=2)</p></blockquote><p>DataLoader支持单进程和多进程的数据加载方式。当 num_workers=0时，使用单进程方式异步加载数据；当 num_workers=n(n&gt;0)时，主进程将会开启n个子进程异步加载数据。<br>DataLoader返回一个迭代器，迭代的返回dataset中的数据内容；使用paddle.io.DataLoader API以batch的方式进行迭代数据，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loader = paddle.io.DataLoader(dataset, </span><br><span class="line">                            batch_size=<span class="number">3</span>, </span><br><span class="line">                            shuffle=<span class="literal">True</span>, </span><br><span class="line">                            drop_last=<span class="literal">True</span>, <span class="comment">#是否的丢弃样本，比如一共10个数据3个3个的读，最后一个丢弃。</span></span><br><span class="line">                            num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader()):</span><br><span class="line">    images, labels = data[<span class="number">0</span>], data[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch_id: &#123;&#125;, 训练数据shape: &#123;&#125;, 标签数据shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, images.shape, labels.shape))</span><br></pre></td></tr></table></figure><pre><code>batch_id: 0, 训练数据shape: [3, 784], 标签数据shape: [3, 1]batch_id: 1, 训练数据shape: [3, 784], 标签数据shape: [3, 1]batch_id: 2, 训练数据shape: [3, 784], 标签数据shape: [3, 1]</code></pre><p>通过上面的学习，我们指导了如何自定义、使用paddle.io.Dataset和paddle.io.DataLoader，下面我们以MNIST数据为例，生成对应的Dataset和DataLoader。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个类MnistDataset，继承paddle.io.Dataset 这个类</span></span><br><span class="line"><span class="comment"># MnistDataset的作用和上面load_data()函数的作用相同，均是构建一个迭代器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MnistDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mode</span>):</span><br><span class="line">        datafile = <span class="string">&#x27;./work/mnist.json.gz&#x27;</span></span><br><span class="line">        data = json.load(gzip.<span class="built_in">open</span>(datafile))</span><br><span class="line">        <span class="comment"># 读取到的数据区分训练集，验证集，测试集</span></span><br><span class="line">        train_set, val_set, eval_set = data</span><br><span class="line">        <span class="keyword">if</span> mode==<span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            <span class="comment"># 获得训练数据集</span></span><br><span class="line">            imgs, labels = train_set[<span class="number">0</span>], train_set[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> mode==<span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">            <span class="comment"># 获得验证数据集</span></span><br><span class="line">            imgs, labels = val_set[<span class="number">0</span>], val_set[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> mode==<span class="string">&#x27;eval&#x27;</span>:</span><br><span class="line">            <span class="comment"># 获得测试数据集</span></span><br><span class="line">            imgs, labels = eval_set[<span class="number">0</span>], eval_set[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;mode can only be one of [&#x27;train&#x27;, &#x27;valid&#x27;, &#x27;eval&#x27;]&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 校验数据</span></span><br><span class="line">        imgs_length = <span class="built_in">len</span>(imgs)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(imgs) == <span class="built_in">len</span>(labels), \</span><br><span class="line">            <span class="string">&quot;length of train_imgs(&#123;&#125;) should be the same as train_labels(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(imgs), <span class="built_in">len</span>(labels))</span><br><span class="line">        </span><br><span class="line">        self.imgs = imgs</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img = np.array(self.imgs[idx]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        label = np.array(self.labels[idx]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.imgs)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明数据加载函数，使用MnistDataset数据集</span></span><br><span class="line">train_dataset = MnistDataset(mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用paddle.io.DataLoader 定义DataLoader对象用于加载Python生成器产生的数据，</span></span><br><span class="line"><span class="comment"># DataLoader 返回的是一个批次数据迭代器，并且是异步的；</span></span><br><span class="line">data_loader = paddle.io.DataLoader(train_dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 迭代的读取数据并打印数据的形状</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader()):</span><br><span class="line">    images, labels = data</span><br><span class="line">    <span class="built_in">print</span>(i, images.shape, labels.shape)</span><br><span class="line">    <span class="keyword">if</span> i&gt;=<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>0 [100, 784] [100]1 [100, 784] [100]2 [100, 784] [100]</code></pre><p>异步数据读取并训练的完整案例代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model</span>):</span><br><span class="line">    model = MNIST()</span><br><span class="line">    model.train()</span><br><span class="line">    opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.001</span>, parameters=model.parameters())</span><br><span class="line">    EPOCH_NUM = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader()):</span><br><span class="line">            images, labels = data</span><br><span class="line">            images = paddle.to_tensor(images)</span><br><span class="line">            labels = paddle.to_tensor(labels).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#前向计算的过程  </span></span><br><span class="line">            predicts = model(images)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#计算损失，取一个批次样本损失的平均值</span></span><br><span class="line">            loss = F.square_error_cost(predicts, labels)</span><br><span class="line">            avg_loss = paddle.mean(loss)       </span><br><span class="line">            </span><br><span class="line">            <span class="comment">#每训练了200批次的数据，打印下当前Loss的情况</span></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch_id, batch_id, avg_loss.numpy()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#后向传播，更新参数的过程</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.clear_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存模型参数</span></span><br><span class="line">    paddle.save(model.state_dict(), <span class="string">&#x27;mnist&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建模型</span></span><br><span class="line">model = MNIST()</span><br><span class="line"><span class="comment">#启动训练过程</span></span><br><span class="line">train(model)</span><br></pre></td></tr></table></figure><p>从异步数据读取的训练结果来看，损失函数下降与同步数据读取训练结果一致。注意，异步读取数据只在数据量规模巨大时会带来显著的性能提升，对于多数场景采用同步数据读取的方式已经足够。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_卷积基本概念及经典模型复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习5.2-图像分类中经典模型的组网方式</title>
      <link href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F/"/>
      <url>/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="图像分类中经典模型的组网方式"><a href="#图像分类中经典模型的组网方式" class="headerlink" title="图像分类中经典模型的组网方式"></a>图像分类中经典模型的组网方式</h1><p>图像分类是根据图像的语义信息对不同类别图像进行区分，是计算机视觉的核心，是物体检测、图像分割、物体跟踪、行为分析、人脸识别等其他高层次视觉任务的基础。</p><p>图像分类在许多领域都有着广泛的应用，如：安防领域的人脸识别和智能视频分析等，交通领域的交通场景识别，互联网领域基于内容的图像检索和相册自动归类，医学领域的图像识别等。</p><p>上一节主要介绍了卷积神经网络常用的一些基本模块，本节将基于眼疾分类数据集 <a href="https://ai.baidu.com/broad/introduction">iChallenge-PM</a>，对图像分类领域的经典卷积神经网络进行剖析，介绍如何应用这些基础模块构建卷积神经网络，解决图像分类问题。按照被提出的时间顺序，涵盖如下卷积神经网络：</p><ul><li>LeNet：Yan LeCun 等人于 1998 年第一次将卷积神经网络应用到图像分类任务上[1]，在手写数字识别任务上取得了巨大成功。</li><li>AlexNet：Alex Krizhevsky 等人在 2012 年提出了AlexNet[2], 并应用在大尺寸图片数据集 ImageNet 上，获得了 2012 年 ImageNet 比赛冠军 （ImageNet Large Scale Visual Recognition Challenge，ILSVRC）。</li><li>VGG：Simonyan 和 Zisserman 于 2014 年提出了 VGG 网络结构[3]，是当前最流行的卷积神经网络之一，由于其结构简单、应用性极强而深受广大研究者欢迎。</li><li>GoogLeNet：Christian Szegedy 等人在 2014 提出了 GoogLeNet[4]，并取得了 2014 年 ImageNet 比赛冠军。</li><li>ResNet：Kaiming He 等人在 2015 年提出了 ResNet[5]，通过引入残差模块加深网络层数，在 ImagNet 数据集上的错误率降低到3.6%，超越了人眼识别水平。ResNet 的设计思想深刻地影响了后来的深度神经网络的设计。</li></ul><blockquote><p>各个组网中主要的模型搭建的部分，训练部分的代码都是差不多的！</p></blockquote><h1 id="一、LeNet"><a href="#一、LeNet" class="headerlink" title="一、LeNet"></a>一、LeNet</h1><p>LeNet 是最早的卷积神经网络之一，它是卷积神经网络的 HelloWorld。1998年，Yann LeCun 第一次将 LeNet 卷积神经网络应用到图像分类上，在手写数字识别任务中取得了巨大成功。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/12e79a7c294e49dea0a0778ffe47161b851054d9e1ea4a5e859ffdb7c55e8366" width = "600"></center><center><br>图1：手写数字识别</br></center><p><br></br></p><p>LeNet 通过连续使用卷积和池化层的组合提取图像特征，其架构如 <strong>图2</strong> 所示，这里展示的是用于MNIST手写体数字识别任务中的 LeNet-5 模型：<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/82e4124e2e6a4231bcde17e086bc86ba732d3e81dcd7415f86fb4ef050aa7772" width = "800"></center><center><br>图2：LeNet模型网络结构示意图</br></center><p><br></br></p><ul><li><p>第一模块：包含 5×5 的 6 通道卷积和 2×2 的池化。卷积提取图像中包含的特征模式（激活函数使用 Sigmoid），图像尺寸从 28 减小到 24。经过池化层可以降低输出特征图对空间位置的敏感性，图像尺寸减到 12【高宽减半】。</p></li><li><p>第二模块：和第一模块尺寸相同，通道数由 6 增加为 16。卷积操作使图像尺寸减小到 8，经过池化后变成 4。</p></li><li><p>第三模块：包含 4×4 的 120 通道卷积。卷积之后的图像尺寸减小到 1，但是通道数增加为 120。将经过第 3 次卷积提取到的特征图输入到全连接层。第一个全连接层的输出神经元的个数是 64，第二个全连接层的输出神经元个数是分类标签的类别数，对于手写数字识别的类别数是 10。然后使用 Softmax 函数即可计算出每个类别的预测概率。</p></li></ul><hr><p><strong>【提示】：</strong></p><p>卷积层的输出特征图如何当作全连接层的输入使用呢？</p><p>卷积层的输出数据格式是$[N, C, H, W]$，在输入全连接层的时候，会自动将数据拉平，</p><p>也就是对每个样本，自动将其转化为长度为 $K$ 的向量，</p><p>其中 $K = C \times H \times W$，一个 mini-batch 的数据维度变成了 $N\times K$ 的二维向量。</p><hr><h2 id="1-1-LeNet在手写数字识别上的应用"><a href="#1-1-LeNet在手写数字识别上的应用" class="headerlink" title="1.1 LeNet在手写数字识别上的应用"></a><strong>1.1 LeNet在手写数字识别上的应用</strong></h2><p>MNIST 手写数字数据库拥有 60,000 个样本的训练集和 10,000 个样本的测试集。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a35d80f579d44552b22c19f7d36128d944db99f95a28414099e695d96ef3fd63" width = "500"></center><center><br>图3：手写数字识别数据集</br></center><p><br></br></p><p>LeNet网络的实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear</span><br><span class="line"></span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LeNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 下面定义层，如图2所示，一共七个层</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 尺寸的逻辑：输入层将数据拉平[B,C,H,W] -&gt; [B,C*H*W] [1,120,1,1]</span></span><br><span class="line">        <span class="comment"># 输入size是[28,28]，经过三次卷积和两次池化之后，C*H*W等于120</span></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">120</span>, out_features=<span class="number">64</span>)</span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">64</span>, out_features=num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 网络的前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># 尺寸的逻辑：输入层将数据拉平[B,C,H,W] -&gt; [B,C*H*W]</span></span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>]) <span class="comment"># -1：我不知道，机器算，size一致</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>]) <span class="comment"># paddle.reshape(x, shape)</span></span><br><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>飞桨会根据实际图像数据的尺寸和卷积核参数自动推断中间层数据的 W 和 H 等，只需要用户表达通道数即可。下面的程序使用随机数作为输入，查看经过 LeNet-5 的每一层作用之后，输出数据的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据形状是 [N, 1, H, W]</span></span><br><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为输入数据</span></span><br><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line"><span class="comment"># print(&#x27;输入的X是：&#x27;, x)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 通过调用LeNet从基类继承的sublayers()函数，查看LeNet中所包含的子层</span></span><br><span class="line">model.sublayers()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="comment"># item是LeNet类中的一个子层</span></span><br><span class="line">    <span class="comment"># 查看经过子层之后的输出数据形状</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="comment"># 查看卷积和全连接层的数据和参数的形状，</span></span><br><span class="line">        <span class="comment"># 其中item.parameters()[0]是权重参数w，item.parameters()[1]是偏置参数b</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="number">0</span>].shape, item.parameters()[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 池化层没有参数</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure><p>卷积 Conv2D 的 padding 参数默认为 0，stride 参数默认为 1。</p><ul><li>当输入形状为 [Bx1x28x28] 时，B 是 batch_size，经过第一层卷积（kernel_size=5, out_channels=6）和 maxpool 之后，得到形状为 [Bx6x12x12] 的特征图；</li><li>经过第二层卷积(kernel_size=5, out_channels=16)和 maxpool 之后，得到形状为 [Bx16x4x4] 的特征图；</li><li>经过第三层卷积(out_channels=120, kernel_size=4)之后，得到形状为 [Bx120x1x1] 的特征图；</li><li>在 FC 层计算之前，将输入特征从卷积得到的四维特征 reshape 到格式为 [B, 120x1x1] 的特征，这也是 LeNet 中第一层全连接层输入 shape 为 120 的原因。</li></ul><h3 id="【训练】"><a href="#【训练】" class="headerlink" title="【训练】"></a><strong>【训练】</strong></h3><p>下面我们将训练过程与验证过程打包好，最终我们将模型的参数保存下来，方便后续的调用。</p><p>具体的流程设计请参考之前的项目：<a href="https://aistudio.baidu.com/aistudio/projectdetail/4957701"><strong>使用极简方法实现手写数字识别任务</strong></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># LeNet 识别手写数字</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">from</span> paddle.vision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, opt, train_loader, valid_loader</span>):</span><br><span class="line">    <span class="comment"># 开启0号GPU训练</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start training ... &#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            img = data[<span class="number">0</span>]</span><br><span class="line">            label = data[<span class="number">1</span>] </span><br><span class="line">            <span class="comment"># 计算模型输出</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            <span class="comment"># 计算损失函数</span></span><br><span class="line">            loss_func = paddle.nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">            loss = loss_func(logits, label)</span><br><span class="line">            avg_loss = paddle.mean(loss)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, batch_id, <span class="built_in">float</span>(avg_loss.numpy())))</span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.clear_grad()</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        accuracies = []</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_loader()):</span><br><span class="line">            img = data[<span class="number">0</span>]</span><br><span class="line">            label = data[<span class="number">1</span>] </span><br><span class="line">            <span class="comment"># 计算模型输出</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            pred = F.softmax(logits)</span><br><span class="line">            <span class="comment"># 计算损失函数</span></span><br><span class="line">            loss_func = paddle.nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">            loss = loss_func(logits, label)</span><br><span class="line">            acc = paddle.metric.accuracy(pred, label)</span><br><span class="line">            accuracies.append(acc.numpy())</span><br><span class="line">            losses.append(loss.numpy())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[validation] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(accuracies), np.mean(losses)))</span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型参数</span></span><br><span class="line">    paddle.save(model.state_dict(), <span class="string">&#x27;mnist.pdparams&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 设置迭代轮数</span></span><br><span class="line">EPOCH_NUM = <span class="number">5</span></span><br><span class="line"><span class="comment"># 设置优化器为 Momentum，学习率为 0.001</span></span><br><span class="line">opt = paddle.optimizer.Momentum(</span><br><span class="line">                                    learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                    momentum=<span class="number">0.9</span>, </span><br><span class="line">                                    parameters=model.parameters())</span><br><span class="line"><span class="comment"># 定义数据读取器</span></span><br><span class="line">train_loader = paddle.io.DataLoader(</span><br><span class="line">                                    MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=ToTensor()), </span><br><span class="line">                                    batch_size=<span class="number">10</span>, </span><br><span class="line">                                    shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = paddle.io.DataLoader(</span><br><span class="line">                                    MNIST(mode=<span class="string">&#x27;test&#x27;</span>, </span><br><span class="line">                                    transform=ToTensor()), </span><br><span class="line">                                    batch_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">train(model, opt, train_loader, valid_loader)</span><br></pre></td></tr></table></figure><h3 id="【预测】"><a href="#【预测】" class="headerlink" title="【预测】"></a><strong>【预测】</strong></h3><p>下面我们读入一张手写数字，测试模型的实际预测效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入图像读取第三方库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取一张本地的样例图片，转变成模型输入的格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">img_path</span>):</span><br><span class="line">    <span class="comment"># 从img_path中读取图像，并转为灰度图，进行下采样</span></span><br><span class="line">    im = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">    im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = <span class="number">1</span> - im / <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;work/1.png&#x27;</span>).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">im</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/49.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义预测过程</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line">params_file_path = <span class="string">&#x27;mnist.pdparams&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;work/1.png&#x27;</span></span><br><span class="line"><span class="comment"># 加载模型参数</span></span><br><span class="line">param_dict = paddle.load(params_file_path)</span><br><span class="line">model.load_dict(param_dict)</span><br><span class="line"><span class="comment"># 灌入数据</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">tensor_img = load_image(img_path).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">result = model(paddle.to_tensor(tensor_img)).argmax(axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result&#x27;</span>, result)</span><br></pre></td></tr></table></figure><pre><code>result Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=False,       [1])</code></pre><p>通过运行结果可以看出，LeNet 在手写数字识别 MNIST 验证数据集上的准确率在 95% 以上。那么对于其它数据集效果如何呢？我们通过眼疾识别数据集 iChallenge-PM 验证一下。</p><h2 id="1-2-LeNet在眼疾识别数据集iChallenge-PM上的应用"><a href="#1-2-LeNet在眼疾识别数据集iChallenge-PM上的应用" class="headerlink" title="1.2 LeNet在眼疾识别数据集iChallenge-PM上的应用"></a><strong>1.2 LeNet在眼疾识别数据集iChallenge-PM上的应用</strong></h2><p><a href="https://ai.baidu.com/broad/introduction">iChallenge-PM</a> 是百度大脑和中山大学中山眼科中心联合举办的 iChallenge 比赛中，提供的关于病理性近视（Pathologic Myopia，PM）的医疗类数据集，包含 1200 个受试者的眼底视网膜图片，训练、验证和测试数据集各 400 张。下面我们详细介绍 LeNet 在 iChallenge-PM 上的训练过程。</p><hr><p><strong>说明：</strong></p><p>如今近视已经成为困扰人们健康的一项全球性负担，在近视人群中，有超过 35% 的人患有重度近视。近视会拉长眼睛的光轴，也可能引起视网膜或者络网膜的病变。随着近视度数的不断加深，高度近视有可能引发病理性病变，这将会导致以下几种症状：视网膜或者络网膜发生退化、视盘区域萎缩、漆裂样纹损害、Fuchs 斑等。因此，及早发现近视患者眼睛的病变并采取治疗，显得非常重要。</p><p>数据可以从AI Studio<a href="https://aistudio.baidu.com/aistudio/datasetdetail/19065">下载</a></p><hr><h3 id="1）数据集准备"><a href="#1）数据集准备" class="headerlink" title="1）数据集准备"></a><strong>1）数据集准备</strong></h3><p><code>/home/aistudio/data/data23828</code> 目录包括如下三个文件，解压缩后存放在 <code>/home/aistudio/work/palm</code> 目录下。</p><ul><li><strong>training.zip</strong>：包含训练中的图片和标签</li><li><strong>validation.zip</strong>：包含验证集的图片</li><li><strong>valid_gt.zip</strong>：包含验证集的标签</li></ul><p>!unzip相关指令可以查看以下链接：<a href="https://www.runoob.com/linux/linux-comm-unzip.html">!unzip指令</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果已经解压过，不需要运行此段代码（注释），否则由于文件已经存在，解压时会报错</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Extracting files... ...&#x27;</span>)</span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/training.<span class="built_in">zip</span></span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/validation.<span class="built_in">zip</span></span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/valid_gt.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;File extraction succeeded.&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%cd /home/aistudio/work/palm/PALM-Training400/</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Extracting the training datasets... ...&#x27;</span>)</span><br><span class="line">!unzip -q PALM-Training400.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;File extraction succeeded.&#x27;</span>)</span><br></pre></td></tr></table></figure><hr><blockquote><p><strong>【注意】</strong>：</p><p><code>valid_gt.zip</code>（验证集的标签）文件解压缩之后，需要将<code>/home/aistudio/work/palm/PALM-Validation-GT/</code>目录下的<code>PM_Label_and_Fovea_Location.xlsx</code>标签文件转存成<code>.csv</code>格式。</p></blockquote><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 转成文件labels.csv</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xlsx_to_csv_pd</span>():</span><br><span class="line">    <span class="comment"># 指定第0列为索引列</span></span><br><span class="line">    data_xls = pd.read_excel(<span class="string">&#x27;/home/aistudio/work/palm/PALM-Validation-GT/PM_Label_and_Fovea_Location.xlsx&#x27;</span>, index_col=<span class="number">0</span>)</span><br><span class="line">    data_xls.to_csv(<span class="string">&#x27;/home/aistudio/labels.csv&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    xlsx_to_csv_pd()</span><br></pre></td></tr></table></figure><h3 id="2）查看数据集图片"><a href="#2）查看数据集图片" class="headerlink" title="2）查看数据集图片"></a><strong>2）查看数据集图片</strong></h3><p>iChallenge-PM 中既有病理性近视患者的眼底图片，也有非病理性近视患者的图片，命名规则如下：</p><ul><li><p>病理性近视（PM）：文件名以P开头</p></li><li><p>非病理性近视（non-PM）：</p><ul><li><p>高度近视（high myopia）：文件名以H开头</p></li><li><p>正常眼睛（normal）：文件名以N开头</p></li></ul></li></ul><p>我们将病理性患者的图片作为正样本，标签为 1； 非病理性患者的图片作为负样本，标签为 0。下面从数据集中选取两张图片，并将图片显示出来。之后通过 LeNet 提取特征，构建分类器，对正负样本进行分类，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line"><span class="comment"># 文件名以N开头的是正常眼底图片，以P开头的是病变眼底图片</span></span><br><span class="line">file1 = <span class="string">&#x27;N0012.jpg&#x27;</span></span><br><span class="line">file2 = <span class="string">&#x27;P0095.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">img1 = Image.<span class="built_in">open</span>(os.path.join(DATADIR, file1))</span><br><span class="line">img1 = np.array(img1)</span><br><span class="line">img2 = Image.<span class="built_in">open</span>(os.path.join(DATADIR, file2))</span><br><span class="line">img2 = np.array(img2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出读取的图片，绘制子图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;Normal&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.imshow(img1)</span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;PM&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.imshow(img2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/50.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看图片形状</span></span><br><span class="line">img1.shape, img2.shape</span><br></pre></td></tr></table></figure><pre><code>((2056, 2124, 3), (2056, 2124, 3))</code></pre><h3 id="3）定义数据读取器"><a href="#3）定义数据读取器" class="headerlink" title="3）定义数据读取器"></a><strong>3）定义数据读取器</strong></h3><p>使用 OpenCV 从磁盘读入图片，将每张图缩放到$224\times224$大小，并且将像素值调整到$[-1, 1]$之间，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对读入的图像数据进行预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_img</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="comment"># 将图片尺寸缩放道 224x224</span></span><br><span class="line">    img = cv2.resize(img, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># 读入的图像数据格式是[H, W, C]，使用转置操作将其变成[C, H, W]</span></span><br><span class="line">    img = np.transpose(img, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    img = img.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将数据范围调整到[-1.0, 1.0]之间</span></span><br><span class="line">    img = img / <span class="number">255.</span></span><br><span class="line">    img = img * <span class="number">2.0</span> - <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p><br></p><p><strong>①定义训练集数据读取器</strong></p><p>训练集读取时通过文件名来确定样本标签，在读取的过程中随机打乱数据的顺序，最终返回以数据迭代器的形式返回。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; -------定义训练集数据读取器--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_loader</span>(<span class="params">datadir, batch_size=<span class="number">10</span>, mode = <span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># 将datadir目录下的文件列出来，每条文件都要读入</span></span><br><span class="line">    filenames = os.listdir(datadir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            <span class="comment"># 训练时随机打乱数据顺序</span></span><br><span class="line">            random.shuffle(filenames)</span><br><span class="line">        batch_imgs = []</span><br><span class="line">        batch_labels = []</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> filenames:</span><br><span class="line">            filepath = os.path.join(datadir, name)</span><br><span class="line">            img = cv2.imread(filepath)</span><br><span class="line">            img = transform_img(img)</span><br><span class="line">            <span class="keyword">if</span> name[<span class="number">0</span>] == <span class="string">&#x27;H&#x27;</span> <span class="keyword">or</span> name[<span class="number">0</span>] == <span class="string">&#x27;N&#x27;</span>:</span><br><span class="line">                <span class="comment"># H开头的文件名表示高度近似，N开头的文件名表示正常视力</span></span><br><span class="line">                <span class="comment"># 高度近视和正常视力的样本，都不是病理性的，属于负样本，标签为0</span></span><br><span class="line">                label = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> name[<span class="number">0</span>] == <span class="string">&#x27;P&#x27;</span>:</span><br><span class="line">                <span class="comment"># P开头的是病理性近视，属于正样本，标签为1</span></span><br><span class="line">                label = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span>(<span class="string">&#x27;Not excepted file name&#x27;</span>)</span><br><span class="line">            <span class="comment"># 每读取一个样本的数据，就将其放入数据列表中</span></span><br><span class="line">            batch_imgs.append(img)</span><br><span class="line">            batch_labels.append(label)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) == batch_size:</span><br><span class="line">                <span class="comment"># 当数据列表的长度等于batch_size的时候，</span></span><br><span class="line">                <span class="comment"># 把这些数据当作一个mini-batch，并作为数据生成器（yield）的一个输出</span></span><br><span class="line">                imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">                labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line">                batch_imgs = []</span><br><span class="line">                batch_labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch</span></span><br><span class="line">            imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><p><br></p><p><strong>②定义验证集数据读取器</strong></p><p>训练集读取时通过文件名来确定样本标签，验证集则通过<code>labels.csv</code>来读取每个图片对应的标签。</p><p>查看解压后的验证集标签数据可以发现，<code>labels.csv</code>文件所包含的内容格式如下，每一行代表一个样本：</p><ul><li>其中第一列是图片id，第二列是文件名，第三列是图片标签，</li><li>第四列和第五列是<strong>正样本区域（Fovea）</strong> 的坐标，与分类任务无关。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; -------定义验证集数据读取器--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">valid_data_loader</span>(<span class="params">datadir, csvfile, batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;valid&#x27;</span></span>):</span><br><span class="line">    filelists = <span class="built_in">open</span>(csvfile).readlines()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        batch_imgs = []</span><br><span class="line">        batch_labels = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> filelists[<span class="number">1</span>:]:</span><br><span class="line">            line = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            name = line[<span class="number">1</span>]</span><br><span class="line">            label = <span class="built_in">int</span>(line[<span class="number">2</span>])</span><br><span class="line">            <span class="comment"># 根据图片文件名加载图片，并对图像数据作预处理</span></span><br><span class="line">            filepath = os.path.join(datadir, name)</span><br><span class="line">            img = cv2.imread(filepath)</span><br><span class="line">            img = transform_img(img)</span><br><span class="line">            <span class="comment"># 每读取一个样本的数据，就将其放入数据列表中</span></span><br><span class="line">            batch_imgs.append(img)</span><br><span class="line">            batch_labels.append(label)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) == batch_size:</span><br><span class="line">                <span class="comment"># 当数据列表的长度等于batch_size的时候，</span></span><br><span class="line">                <span class="comment"># 把这些数据当作一个mini-batch，并作为数据生成器的一个输出</span></span><br><span class="line">                imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">                labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line">                batch_imgs = []</span><br><span class="line">                batch_labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch</span></span><br><span class="line">            imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看数据形状</span></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line">train_loader = data_loader(DATADIR, </span><br><span class="line">                           batch_size=<span class="number">10</span>, </span><br><span class="line">                           mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">data_reader = train_loader()</span><br><span class="line">data = <span class="built_in">next</span>(data_reader)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">0</span>].shape, data[<span class="number">1</span>].shape)</span><br><span class="line"></span><br><span class="line">eval_loader = data_loader(DATADIR, </span><br><span class="line">                           batch_size=<span class="number">10</span>, </span><br><span class="line">                           mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line">data_reader = eval_loader()</span><br><span class="line">data = <span class="built_in">next</span>(data_reader)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">0</span>].shape, data[<span class="number">1</span>].shape)</span><br></pre></td></tr></table></figure><pre><code>(10, 3, 224, 224) (10, 1)(10, 3, 224, 224) (10, 1)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(data_reader)</span><br></pre></td></tr></table></figure><h3 id="4）启动训练"><a href="#4）启动训练" class="headerlink" title="4）启动训练"></a><strong>4）启动训练</strong></h3><p>首先我们定义好训练和验证的基本流程，同时我们需要将模型训练 5 个迭代周期后的模型参数保存下来，方便我们后续进行评估，此处是为了模拟真实场景下的评估方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># LeNet 识别眼疾图片</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line">DATADIR2 = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Validation400&#x27;</span></span><br><span class="line">CSVFILE = <span class="string">&#x27;/home/aistudio/labels.csv&#x27;</span></span><br><span class="line"><span class="comment"># 设置迭代轮数</span></span><br><span class="line">EPOCH_NUM = <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_pm</span>(<span class="params">model, optimizer</span>):</span><br><span class="line">    <span class="comment"># 开启0号GPU训练</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start training ... &#x27;</span>)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    <span class="comment"># 定义数据读取器，训练数据读取器和验证数据读取器</span></span><br><span class="line">    train_loader = data_loader(DATADIR, batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    valid_loader = valid_data_loader(DATADIR2, CSVFILE)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            x_data, y_data = data</span><br><span class="line">            img = paddle.to_tensor(x_data)</span><br><span class="line">            label = paddle.to_tensor(y_data)</span><br><span class="line">            <span class="comment"># 运行模型前向计算，得到预测值</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            loss = F.binary_cross_entropy_with_logits(logits, label)</span><br><span class="line">            avg_loss = paddle.mean(loss)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                iters.append(<span class="built_in">iter</span>)</span><br><span class="line">                train_losses.append(avg_loss.numpy())</span><br><span class="line">                <span class="built_in">iter</span> = <span class="built_in">iter</span> + <span class="number">2</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, batch_id, <span class="built_in">float</span>(avg_loss.numpy())))</span><br><span class="line">            <span class="comment"># 反向传播，更新权重，清除梯度</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.clear_grad()</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        accuracies = []</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_loader()):</span><br><span class="line">            x_data, y_data = data</span><br><span class="line">            img = paddle.to_tensor(x_data)</span><br><span class="line">            label = paddle.to_tensor(y_data)</span><br><span class="line">            <span class="comment"># 运行模型前向计算，得到预测值</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            <span class="comment"># 二分类，sigmoid计算后的结果以0.5为阈值分两个类别</span></span><br><span class="line">            <span class="comment"># 计算sigmoid后的预测概率，进行loss计算</span></span><br><span class="line">            pred = F.sigmoid(logits)</span><br><span class="line">            loss = F.binary_cross_entropy_with_logits(logits, label)</span><br><span class="line">            <span class="comment"># 计算预测概率小于0.5的类别</span></span><br><span class="line">            pred2 = pred * (-<span class="number">1.0</span>) + <span class="number">1.0</span></span><br><span class="line">            <span class="comment"># 得到两个类别的预测概率，并沿第一个维度级联</span></span><br><span class="line">            pred = paddle.concat([pred2, pred], axis=<span class="number">1</span>)</span><br><span class="line">            acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            accuracies.append(acc.numpy())</span><br><span class="line">            losses.append(loss.numpy())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[validation] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(accuracies), np.mean(losses)))</span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">        paddle.save(model.state_dict(), <span class="string">&#x27;palm.pdparams&#x27;</span>)</span><br><span class="line">        paddle.save(optimizer.state_dict(), <span class="string">&#x27;palm.pdopt&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> iters, losses</span><br></pre></td></tr></table></figure><p>接下来我们定义评估的过程，首先需要将之前保存的模型参数重新载入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义评估过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluation</span>(<span class="params">model, params_file_path</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启0号GPU预估</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start evaluation .......&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载模型参数</span></span><br><span class="line">    model_state_dict = paddle.load(params_file_path)</span><br><span class="line">    model.load_dict(model_state_dict)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    eval_loader = data_loader(DATADIR, </span><br><span class="line">                        batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    acc_set = []</span><br><span class="line">    avg_loss_set = []</span><br><span class="line">    <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_loader()):</span><br><span class="line">        x_data, y_data = data</span><br><span class="line">        img = paddle.to_tensor(x_data)</span><br><span class="line">        label = paddle.to_tensor(y_data)</span><br><span class="line">        y_data = y_data.astype(np.int64)</span><br><span class="line">        label_64 = paddle.to_tensor(y_data)</span><br><span class="line">        <span class="comment"># 计算预测和精度</span></span><br><span class="line">        prediction, acc = model(img, label_64)</span><br><span class="line">        <span class="comment"># 计算损失函数值</span></span><br><span class="line">        loss = F.binary_cross_entropy_with_logits(prediction, label)</span><br><span class="line">        avg_loss = paddle.mean(loss)</span><br><span class="line">        acc_set.append(<span class="built_in">float</span>(acc.numpy()))</span><br><span class="line">        avg_loss_set.append(<span class="built_in">float</span>(avg_loss.numpy()))</span><br><span class="line">    <span class="comment"># 求平均精度</span></span><br><span class="line">    acc_val_mean = np.array(acc_set).mean()</span><br><span class="line">    avg_loss_val_mean = np.array(avg_loss_set).mean()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss=&#123;:.4f&#125;, acc=&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(avg_loss_val_mean, acc_val_mean))</span><br></pre></td></tr></table></figure><p>对比本章最初定义的 LeNet，发现两个 LeNet 的第一层全连接层的输入特征维度不同，一个是 120，一个是 300000。</p><p>这个不同是由输入数据的形状不同引起的，手写数字识别的图像输入形状比较小，第三层卷积之前的特征维度是 [B, 120x1x1]，但是 PALM 数据的输入数据形状较大，形状为 [B, 120x50x50]，120x50x50 等于 300000，所以不同的输入大小，会影响卷积后全连接层的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, Dropout</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LeNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 创建卷积和池化层块，每个卷积层使用Sigmoid激活函数，后面跟着一个2x2的池化</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 创建第3个卷积层</span></span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 创建全连接层，第一个全连接层的输出神经元个数为64</span></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">300000</span>, out_features=<span class="number">64</span>)</span><br><span class="line">        <span class="comment"># 第二个全连接层输出神经元个数为分类标签的类别数</span></span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">64</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 网络的前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, label=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> self.num_classes == <span class="number">1</span>:</span><br><span class="line">                pred = F.sigmoid(x)</span><br><span class="line">                pred = paddle.concat([<span class="number">1.0</span> - pred, pred], axis=<span class="number">1</span>)</span><br><span class="line">                acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc = paddle.metric.accuracy(x, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            <span class="keyword">return</span> x, acc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">model = LeNet(num_classes=<span class="number">1</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="number">0</span>].shape, item.parameters()[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure><pre><code>conv2d_12 [1, 6, 220, 220] [6, 3, 5, 5] [6]max_pool2d_8 [1, 6, 110, 110]conv2d_13 [1, 16, 106, 106] [16, 6, 5, 5] [16]max_pool2d_9 [1, 16, 53, 53]conv2d_14 [1, 120, 50, 50] [120, 16, 4, 4] [120]linear_8 [1, 64] [300000, 64] [64]linear_9 [1, 1] [64, 1] [1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters())</span><br><span class="line">iters, train_losses = train_pm(model, optimizer=opt)</span><br><span class="line">evaluation(model, params_file_path=<span class="string">&quot;palm.pdparams&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_losses</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;LeNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters, train_losses, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/51.png" alt="png"></p><p>通过运行结果可以看出，在眼疾筛查数据集iChallenge-PM上，LeNet的loss很难下降，模型没有收敛。这是因为MNIST数据集的图片尺寸比较小（$28\times28$），但是眼疾筛查数据集图片尺寸比较大（原始图片尺寸约为$2000 \times 2000$，经过缩放之后变成$224 \times 224$），LeNet模型很难进行有效分类。这说明在图片尺寸比较大时，LeNet在图像分类任务上存在局限性。</p><h1 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h1><p>通过上面的实际训练可以看到，虽然LeNet在手写数字识别数据集上取得了很好的结果，但在更大的数据集上表现却并不好。自从1998年LeNet问世以来，接下来十几年的时间里，神经网络并没有在计算机视觉领域取得很好的结果，反而一度被其它算法所超越。原因主要有两方面，一是神经网络的计算比较复杂，对当时计算机的算力来说，训练神经网络是件非常耗时的事情；另一方面，当时还没有专门针对神经网络做算法和训练技巧的优化，神经网络的收敛是件非常困难的事情。</p><p>随着技术的进步和发展，计算机的算力越来越强大，尤其是在GPU并行计算能力的推动下，复杂神经网络的计算也变得更加容易实施。另一方面，互联网上涌现出越来越多的数据，极大的丰富了数据库。同时也有越来越多的研究人员开始专门针对神经网络做算法和模型的优化，Alex Krizhevsky等人提出的AlexNet以很大优势获得了2012年ImageNet比赛的冠军。这一成果极大的激发了产业界对神经网络的兴趣，开创了使用深度神经网络解决图像问题的途径，随后也在这一领域涌现出越来越多的优秀成果。</p><p>AlexNet的具体结构如 <strong>图4</strong> 所示：</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/630059b01a9a4e8c8eded2e7584412daa27bc7c034a8441fabadd713dac29d77" width = "1000"></center><center><br>图4：AlexNet模型网络结构示意图</br></center><p><br></br></p><h2 id="AlexNet的重要构成"><a href="#AlexNet的重要构成" class="headerlink" title="AlexNet的重要构成"></a><strong>AlexNet的重要构成</strong></h2><p>AlexNet与LeNet相比，具有更深的网络结构，包含5层卷积和3层全连接，同时使用了如下三种方法改进模型的训练过程：</p><ul><li><p>数据增广：深度学习中常用的一种处理方式，通过对训练随机加一些变化，比如平移、缩放、裁剪、旋转、翻转或者增减亮度等，产生一系列跟原始图片相似但又不完全相同的样本，从而扩大训练数据集。通过这种方式，可以随机改变训练样本，避免模型过度依赖于某些属性，能从一定程度上抑制过拟合。</p></li><li><p>使用Dropout抑制过拟合。</p></li><li><p>使用ReLU激活函数减少梯度消失现象。</p></li></ul><hr><p><strong>说明：</strong></p><p>之后会详细介绍数据增广的具体实现方式。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/91c51b2d7dac42338bf91df0120ad202a2c4f937abe840889f4dce024a3fde1d" width = "500"></center><center><br>图5：图像增广技术</br></center><p><br></br></p><hr><h3 id="1）激活函数的改变"><a href="#1）激活函数的改变" class="headerlink" title="1）激活函数的改变"></a><strong>1）激活函数的改变</strong></h3><p>AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/cc4b7ab6ebe844aa9a833c4e5bfdd94108bf7f60cee14145807be3f19621e61c" width = "700"></center><center><br>图6：激活函数的改变</br></center><p><br></br></p><p>一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 当sigmoid激活函数的输出非常接近于正无穷或负无穷时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p><h3 id="2）容量控制和预处理"><a href="#2）容量控制和预处理" class="headerlink" title="2）容量控制和预处理"></a><strong>2）容量控制和预处理</strong></h3><p>AlexNet通过Dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减。<br>Dropout的意思是每次训练的时候随机损失掉一些神经元，这些神经元被Dropped-out了，换句话讲，这些神经元在正向传播时对下游的启动影响被忽略，反向传播时也不会更新权重。Dropout的效果是，网络对某个神经元的权重变化更不敏感，增加泛化能力，减少过拟合。</p><p>AlexNet在眼疾筛查数据集iChallenge-PM上具体实现的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, Dropout</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 AlexNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        <span class="comment"># AlexNet与LeNet一样也会同时使用卷积和池化层提取图像特征</span></span><br><span class="line">        <span class="comment"># 与LeNet不同的是激活函数换成了‘relu’</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = Conv2D(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = Conv2D(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.max_pool5 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">12544</span>, out_features=<span class="number">4096</span>)</span><br><span class="line">        self.drop_ratio1 = <span class="number">0.5</span></span><br><span class="line">        self.drop1 = Dropout(self.drop_ratio1)</span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>)</span><br><span class="line">        self.drop_ratio2 = <span class="number">0.5</span></span><br><span class="line">        self.drop2 = Dropout(self.drop_ratio2)</span><br><span class="line">        self.fc3 = Linear(in_features=<span class="number">4096</span>, out_features=num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool5(x)</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="comment"># 在全连接之后使用dropout抑制过拟合</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>我们构造一个高度和宽度都为224，3通道的数据来观察每一层输出的形状。 它与AlexNet架构相匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = AlexNet()</span><br><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure><pre><code>conv2d_18 [1, 96, 56, 56]max_pool2d_12 [1, 96, 28, 28]conv2d_19 [1, 256, 28, 28]max_pool2d_13 [1, 256, 14, 14]conv2d_20 [1, 384, 14, 14]conv2d_21 [1, 384, 14, 14]conv2d_22 [1, 256, 14, 14]max_pool2d_14 [1, 256, 7, 7]linear_12 [1, 4096]dropout_0 [1, 4096]linear_13 [1, 4096]dropout_1 [1, 4096]linear_14 [1, 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = AlexNet()</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">opt = paddle.optimizer.Adam(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                            parameters=model.parameters())</span><br><span class="line"></span><br><span class="line">iters_A, train_losses_A = train_pm(model, optimizer=opt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;AlexNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_A, train_losses_A, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/52.png" alt="png"></p><p>通过运行结果可以发现，在眼疾筛查数据集iChallenge-PM上使用AlexNet，loss能有效下降，经过5个epoch的训练，在验证集上的准确率可以达到94%左右。</p><h1 id="三、VGG"><a href="#三、VGG" class="headerlink" title="三、VGG"></a>三、VGG</h1><p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。这也就是为什么有人会问：AlexNet为什么要这么设计？想要把网络设计的更好，我应该怎么去优化我的网络？在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。</p><p>VGG是当前最流行的CNN模型之一，2014年由Simonyan和Zisserman提出，其命名来源于论文作者所在的实验室视觉几何组(Visual Geometry Group)。</p><p>AlexNet模型通过构造多层网络，取得了较好的效果，但是并没有给出深度神经网络设计的方向。VGG通过使用<strong>一系列大小为3x3的小尺寸卷积核和池化层</strong>构造深度卷积神经网络，并取得了较好的效果。VGG模型因为结构简单、应用性极强而广受研究者欢迎，尤其是它的网络结构设计方法，为构建深度神经网络提供了方向。</p><p><strong>图7</strong> 是VGG-16的网络结构示意图，有13层卷积和3层全连接层。VGG网络的设计严格使用$3\times 3$的卷积层和池化层来提取特征，并在网络的最后面使用三层全连接层，将最后一层全连接层的输出作为分类的预测。在VGG中每层卷积将使用ReLU作为激活函数，在全连接层之后添加dropout来抑制过拟合。</p><p>VGG的创新之处在于使用两层$3\times 3$卷积层，而不是使用$5 \times 5$的卷积层，为什么这么做呢？</p><p>原因是使用小的卷积核能够有效地减少参数的个数，使得训练和测试变得更加有效。由于卷积核比较小，可以堆叠更多的卷积层，加深网络的深度，这对于图像分类任务来说是有利的。VGG模型的成功证明了增加网络的深度，可以更好的学习图像中的特征模式。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/3b6e1725e5934d2293e03b9c0a83e1d48660137f3c4449ba89bf9766d4380f3a" width = "1000"></center><center><br>图7：VGG模型网络结构示意图</br></center><p><br></br></p><p>VGG在眼疾识别数据集iChallenge-PM上的具体实现如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># VGG模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="comment"># from paddle.nn import Conv2D, MaxPool2D, BatchNorm, Linear</span></span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, BatchNorm2D, Linear</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义vgg网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        in_channels = [<span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">512</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义第一个block，包含两个卷积</span></span><br><span class="line">        self.conv1_1 = Conv2D(in_channels=in_channels[<span class="number">0</span>], out_channels=in_channels[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv1_2 = Conv2D(in_channels=in_channels[<span class="number">1</span>], out_channels=in_channels[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第二个block，包含两个卷积</span></span><br><span class="line">        self.conv2_1 = Conv2D(in_channels=in_channels[<span class="number">1</span>], out_channels=in_channels[<span class="number">2</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2_2 = Conv2D(in_channels=in_channels[<span class="number">2</span>], out_channels=in_channels[<span class="number">2</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第三个block，包含三个卷积</span></span><br><span class="line">        self.conv3_1 = Conv2D(in_channels=in_channels[<span class="number">2</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3_2 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3_3 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第四个block，包含三个卷积</span></span><br><span class="line">        self.conv4_1 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4_2 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4_3 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第五个block，包含三个卷积</span></span><br><span class="line">        self.conv5_1 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5_2 = Conv2D(in_channels=in_channels[<span class="number">5</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5_3 = Conv2D(in_channels=in_channels[<span class="number">5</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用Sequential 将全连接层和relu组成一个线性结构（fc + relu）</span></span><br><span class="line">        <span class="comment"># 当输入为224x224时，经过五个卷积块和池化层后，特征维度变为[512x7x7]</span></span><br><span class="line">        self.fc1 = paddle.nn.Sequential(paddle.nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), paddle.nn.ReLU())</span><br><span class="line">        self.drop1_ratio = <span class="number">0.5</span></span><br><span class="line">        self.dropout1 = paddle.nn.Dropout(self.drop1_ratio, mode=<span class="string">&#x27;upscale_in_train&#x27;</span>)</span><br><span class="line">        <span class="comment"># 使用Sequential 将全连接层和relu组成一个线性结构（fc + relu）</span></span><br><span class="line">        self.fc2 = paddle.nn.Sequential(paddle.nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), paddle.nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.drop2_ratio = <span class="number">0.5</span></span><br><span class="line">        self.dropout2 = paddle.nn.Dropout(self.drop2_ratio, mode=<span class="string">&#x27;upscale_in_train&#x27;</span>)</span><br><span class="line">        self.fc3 = paddle.nn.Linear(<span class="number">4096</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = paddle.nn.ReLU()</span><br><span class="line">        self.pool = MaxPool2D(stride=<span class="number">2</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu(self.conv1_1(x))</span><br><span class="line">        x = self.relu(self.conv1_2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv2_1(x))</span><br><span class="line">        x = self.relu(self.conv2_2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv3_1(x))</span><br><span class="line">        x = self.relu(self.conv3_2(x))</span><br><span class="line">        x = self.relu(self.conv3_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv4_1(x))</span><br><span class="line">        x = self.relu(self.conv4_2(x))</span><br><span class="line">        x = self.relu(self.conv4_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv5_1(x))</span><br><span class="line">        x = self.relu(self.conv5_2(x))</span><br><span class="line">        x = self.relu(self.conv5_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = paddle.flatten(x, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        x = self.dropout1(self.relu(self.fc1(x)))</span><br><span class="line">        x = self.dropout2(self.relu(self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = VGG()</span><br><span class="line">model.sublayers()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_V, train_losses_V = train_pm(model, optimizer=opt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;VGGNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_V, train_losses_V, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过运行结果可以发现，在眼疾筛查数据集iChallenge-PM上使用VGG，loss能有效的下降，经过5个epoch的训练，在验证集上的准确率可以达到94%左右。</p><h1 id="四、GoogLeNet"><a href="#四、GoogLeNet" class="headerlink" title="四、GoogLeNet"></a>四、GoogLeNet</h1><p>GoogLeNet是2014年ImageNet比赛的冠军，它的主要特点是网络不仅有深度，还在横向上具有“宽度”。由于图像信息在空间尺寸上的巨大差异，如何选择合适的卷积核来提取特征就显得比较困难了。空间分布范围更广的图像信息适合用较大的卷积核来提取其特征；而空间分布范围较小的图像信息则适合用较小的卷积核来提取其特征。为了解决这个问题，GoogLeNet提出了一种被称为Inception模块的方案。如 <strong>图8</strong> 所示：</p><hr><p><strong>说明：</strong></p><ul><li>Google的研究人员为了向LeNet致敬，特地将模型命名为GoogLeNet。</li><li>Inception一词来源于电影《盗梦空间》（Inception）。</li></ul><hr><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/ebc171e0281549a9b6aace1113f92fb72df08b947059446ca62a07b9af22e4b4" width = "1000"></center><center><br>图8：Inception模块结构示意图</br></center><p><br></br></p><p>图8(a)是Inception模块的设计思想，使用3个不同大小的卷积核对输入图片进行卷积操作，并附加最大池化，将这4个操作的输出沿着通道这一维度进行拼接，构成的输出特征图将会包含经过不同大小的卷积核提取出来的特征，从而达到捕捉不同尺度信息的效果。</p><p>Inception模块采用多通路(multi-path)的设计形式，每个支路使用不同大小的卷积核，最终输出特征图的通道数是每个支路输出通道数的总和，这将会导致输出通道数变得很大，尤其是使用多个Inception模块串联操作的时候，模型参数量会变得非常大。</p><p>为了减小参数量，Inception模块使用了图(b)中的设计方式，在每个3x3和5x5的卷积层之前，增加1x1的卷积层来控制输出通道数；在最大池化层后面增加1x1卷积层减小输出通道数。基于这一设计思想，形成了上图(b)中所示的结构。下面这段程序是Inception块的具体实现方式，可以对照图(b)和代码一起阅读。</p><hr><p><strong>提示：</strong></p><p>可能有读者会问，经过3x3的最大池化之后图像尺寸不会减小吗，为什么还能跟另外3个卷积输出的特征图进行拼接？这是因为池化操作可以指定窗口大小$k_h = k_w = 3$，stride=1和padding=1，输出特征图尺寸可以保持不变。</p><hr><p>Inception模块的具体实现如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GoogLeNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Inception块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c0, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inception模块的实现代码，</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        c1,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        c2,图(b)中第二条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c2[0]是1x1卷积的输出通道数，c2[1]是3x3</span></span><br><span class="line"><span class="string">        c3,图(b)中第三条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c3[0]是1x1卷积的输出通道数，c3[1]是3x3</span></span><br><span class="line"><span class="string">        c4,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 依次创建Inception块每条支路上使用到的操作</span></span><br><span class="line">        self.p1_1 = Conv2D(in_channels=c0,out_channels=c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_1 = Conv2D(in_channels=c0,out_channels=c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = Conv2D(in_channels=c2[<span class="number">0</span>],out_channels=c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p3_1 = Conv2D(in_channels=c0,out_channels=c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = Conv2D(in_channels=c3[<span class="number">0</span>],out_channels=c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.p4_1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = Conv2D(in_channels=c0,out_channels=c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 支路1只包含一个1x1卷积</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 支路2包含 1x1卷积 + 3x3卷积</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        <span class="comment"># 支路3包含 1x1卷积 + 5x5卷积</span></span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        <span class="comment"># 支路4包含 最大池化和1x1卷积</span></span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 将每个支路的输出特征图拼接在一起作为最终的输出结果</span></span><br><span class="line">        <span class="keyword">return</span> paddle.concat([p1, p2, p3, p4], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>GoogLeNet的架构如 <strong>图9</strong> 所示，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3 ×3最大池化层来减小输出高宽。</p><ul><li>第一模块使用一个64通道的7 × 7卷积层。</li><li>第二模块使用2个卷积层:首先是64通道的1 × 1卷积层，然后是将通道增大3倍的3 × 3卷积层。</li><li>第三模块串联2个完整的Inception块。</li><li>第四模块串联了5个Inception块。</li><li>第五模块串联了2 个Inception块。</li><li>第五模块的后面紧跟输出层，使用全局平均池化层来将每个通道的高和宽变成1，最后接上一个输出个数为标签类别数的全连接层。</li></ul><hr><blockquote><p>说明：<br>在原作者的论文中添加了图中所示的softmax1和softmax2两个辅助分类器，如下图所示，训练时将三个分类器的损失函数进行加权求和，以缓解梯度消失现象。这里的程序作了简化，没有加入辅助分类器。</p></blockquote><hr><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9d0794b330934bc9be72cba9f056d62eb77d3ba6c2ac450fae64cf86d86f2e04" width = "800"></center><center><br>图9：GoogLeNet模型网络结构示意图</br></center><p><br></br></p><p>GoogLeNet的具体实现如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GoogLeNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Inception块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c0, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inception模块的实现代码，</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        c1,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        c2,图(b)中第二条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c2[0]是1x1卷积的输出通道数，c2[1]是3x3</span></span><br><span class="line"><span class="string">        c3,图(b)中第三条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c3[0]是1x1卷积的输出通道数，c3[1]是3x3</span></span><br><span class="line"><span class="string">        c4,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 依次创建Inception块每条支路上使用到的操作</span></span><br><span class="line">        self.p1_1 = Conv2D(in_channels=c0,out_channels=c1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p2_1 = Conv2D(in_channels=c0,out_channels=c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = Conv2D(in_channels=c2[<span class="number">0</span>],out_channels=c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p3_1 = Conv2D(in_channels=c0,out_channels=c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = Conv2D(in_channels=c3[<span class="number">0</span>],out_channels=c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p4_1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = Conv2D(in_channels=c0,out_channels=c4, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 新加一层batchnorm稳定收敛【批量归一化层】</span></span><br><span class="line">        self.batchnorm = paddle.nn.BatchNorm2D(c1+c2[<span class="number">1</span>]+c3[<span class="number">1</span>]+c4)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 支路1只包含一个1x1卷积</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 支路2包含 1x1卷积 + 3x3卷积</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        <span class="comment"># 支路3包含 1x1卷积 + 5x5卷积</span></span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        <span class="comment"># 支路4包含 最大池化和1x1卷积</span></span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 将每个支路的输出特征图拼接在一起作为最终的输出结果</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.batchnorm(paddle.concat([p1, p2, p3, p4], axis=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        <span class="comment"># GoogLeNet包含五个模块，每个模块后面紧跟一个池化层</span></span><br><span class="line">        <span class="comment"># 第一个模块包含1个卷积层</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第二个模块包含2个卷积层</span></span><br><span class="line">        self.conv2_1 = Conv2D(in_channels=<span class="number">64</span>,out_channels=<span class="number">64</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2_2 = Conv2D(in_channels=<span class="number">64</span>,out_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool2 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第三个模块包含2个Inception块</span></span><br><span class="line">        self.block3_1 = Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>)</span><br><span class="line">        self.block3_2 = Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool3 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第四个模块包含5个Inception块</span></span><br><span class="line">        self.block4_1 = Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_2 = Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_3 = Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_4 = Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_5 = Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool4 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第五个模块包含2个Inception块</span></span><br><span class="line">        self.block5_1 = Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        self.block5_2 = Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 全局池化，用的是global_pooling，不需要设置pool_stride</span></span><br><span class="line">        self.pool5 = AdaptiveAvgPool2D(output_size=<span class="number">1</span>)</span><br><span class="line">        self.fc = Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))</span><br><span class="line">        x = self.pool3(self.block3_2(self.block3_1(x)))</span><br><span class="line">        x = self.block4_3(self.block4_2(self.block4_1(x)))</span><br><span class="line">        x = self.pool4(self.block4_5(self.block4_4(x)))</span><br><span class="line">        x = self.pool5(self.block5_2(self.block5_1(x)))</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = GoogLeNet()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(model.parameters()))</span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters(), </span><br><span class="line">                                weight_decay=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_G, train_losses_G = train_pm(model, opt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;GoogLeNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_G, train_losses_G, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过运行结果可以发现，使用GoogLeNet在眼疾筛查数据集iChallenge-PM上，loss能有效的下降。</p><h1 id="五、ResNet"><a href="#五、ResNet" class="headerlink" title="五、ResNet"></a>五、ResNet</h1><p>ResNet是2015年ImageNet比赛的冠军，将识别错误率降低到了3.6%，这个结果甚至超出了正常人眼识别的精度。</p><p>通过前面几个经典模型学习，我们可以发现随着深度学习的不断发展，模型的层数越来越多，网络结构也越来越复杂。那么是否加深网络结构，就一定会得到更好的效果呢？从理论上来说，假设新增加的层都是恒等映射，只要原有的层学出跟原模型一样的参数，那么深模型结构就能达到原模型结构的效果。换句话说，原模型的解只是新模型的解的子空间，在新模型解的空间里应该能找到比原模型解对应的子空间更好的结果。但是实践表明，增加网络的层数之后，训练误差往往不降反升。</p><p>Kaiming He等人提出了残差网络ResNet来解决上述问题，其基本思想如 <strong>图10</strong>所示。</p><ul><li>图10(a)：表示增加网络的时候，将$x$映射成$y=F(x)$输出。</li><li>图10(b)：对图10(a)作了改进，输出$y=F(x) + x$。这时不是直接学习输出特征$y$的表示，而是学习$y-x$。<ul><li>如果想学习出原模型的表示，只需将$F(x)$的参数全部设置为0，则$y=x$是恒等映射。</li><li>$F(x) = y - x$也叫做残差项，如果$x\rightarrow y$的映射接近恒等映射，图10(b)中通过学习残差项也比图10(a)学习完整映射形式更加容易。</li></ul></li></ul><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e10f22f054704daabf4261ab46719629a36749631db74eb0a368499de3e5d3d6" width = "500"></center><center><br>图10：残差块设计思想</br></center><p><br></br></p><p>图10(b)的结构是残差网络的基础，这种结构也叫做<strong>残差块（Residual block）</strong>。输入$x$通过跨层连接，能更快的向前传播数据，或者向后传播梯度。通俗的比喻，在火热的电视节目《王牌对王牌》上有一个“传声筒”的游戏，排在队首的嘉宾把看到的影视片段表演给后面一个嘉宾看，经过四五个嘉宾后，最后一个嘉宾如果能表演出更多原剧的内容，就能取得高分。我们常常会发现刚开始的嘉宾往往表演出最多的信息（类似于Loss），而随着表演的传递，有效的表演信息越来越少（类似于梯度弥散）。如果每个嘉宾都能看到原始的影视片段，那么相信传声筒的效果会好很多。类似的，由于ResNet每层都存在直连的旁路，相当于每一层都和最终的损失有“直接对话”的机会，自然可以更好的解决梯度弥散的问题。</p><p>残差块的具体设计方案如 <strong>图11</strong> 所示，这种设计方案也常称作瓶颈结构（BottleNeck）。1*1的卷积核可以非常方便的调整中间层的通道数，在进入3*3的卷积层之前减少通道数（256-&gt;64），经过该卷积层后再恢复通道数(64-&gt;256)，可以显著减少网络的参数量。这个结构（256-&gt;64-&gt;256）像一个中间细，两头粗的瓶颈，所以被称为“BottleNeck”。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/322b26358d43401ba81546dd134a310cfb11ecafb3314aab88b5885ff642870b" width = "500"></center><center><br>图11：残差块结构示意图</br></center><p><br></br></p><p>下图表示出了ResNet-50的结构，一共包含49层卷积和1层全连接，所以被称为ResNet-50。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8f42b3b5b7b34e45847a9c61580f1f8239a80ca6fa67448e8baeeb0209a2d556" width = "1000"></center><center><br>图12：ResNet-50模型网络结构示意图</br></center><p><br></br></p><p>ResNet-50的具体实现如下代码所示：</p><hr><p><strong>说明</strong> ：</p><p>带泄露修正线性单元（Leaky ReLU）函数是经典（以及广泛使用的）的ReLu激活函数的变体，该函数输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢），解决了Relu函数进入负区间后，导致神经元不学习的问题。</p><p><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d75c6e01a5db456a990bc6060647b8a3a9ff83e3797a45469aca303485de1bcb" width = "500"></center><center><br>图13：Leaky ReLU</br></center><p><br></br></p><p><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/functional/leaky_relu_cn.html#leaky-relu">paddle.nn.functional.leaky_relu(x, negative_slope=0.01, name=None)</a></p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------定义卷积批归一化块-------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNLayer</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_channels,</span></span><br><span class="line"><span class="params">                 num_filters,</span></span><br><span class="line"><span class="params">                 filter_size,</span></span><br><span class="line"><span class="params">                 stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 act=<span class="literal">None</span></span>):</span><br><span class="line">       </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        num_channels, 卷积层的输入通道数</span></span><br><span class="line"><span class="string">        num_filters, 卷积层的输出通道数</span></span><br><span class="line"><span class="string">        filter_size, 卷积核大小</span></span><br><span class="line"><span class="string">        stride, 卷积层的步幅</span></span><br><span class="line"><span class="string">        groups, 卷积核组数，默认groups=1不使用分组卷积</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建卷积层</span></span><br><span class="line">        self._conv = nn.Conv2D(</span><br><span class="line">            in_channels=num_channels,</span><br><span class="line">            out_channels=num_filters,</span><br><span class="line">            kernel_size=filter_size,</span><br><span class="line">            stride=stride,</span><br><span class="line">            padding=(filter_size - <span class="number">1</span>) // <span class="number">2</span>,</span><br><span class="line">            groups=groups,</span><br><span class="line">            bias_attr=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建BatchNorm层</span></span><br><span class="line">        <span class="comment"># ResNet中使用了BatchNorm层，在卷积层的后面加上BatchNorm以提升数值稳定性</span></span><br><span class="line">        self._batch_norm = paddle.nn.BatchNorm2D(num_filters)</span><br><span class="line">        </span><br><span class="line">        self.act = act</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self._conv(inputs)</span><br><span class="line">        y = self._batch_norm(y)</span><br><span class="line">        <span class="keyword">if</span> self.act == <span class="string">&#x27;leaky&#x27;</span>:</span><br><span class="line">            y = F.leaky_relu(x=y, negative_slope=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.act == <span class="string">&#x27;relu&#x27;</span>:</span><br><span class="line">            y = F.relu(x=y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;--------定义残差块---------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 每个残差块会对输入图片做三次卷积，然后跟输入图片进行短接</span></span><br><span class="line"><span class="comment"># 如果残差块中第三次卷积输出特征图的形状与输入不一致，则对输入图片做1x1卷积，将其输出形状调整成一致</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BottleneckBlock</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_channels,</span></span><br><span class="line"><span class="params">                 num_filters,</span></span><br><span class="line"><span class="params">                 stride,</span></span><br><span class="line"><span class="params">                 shortcut=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BottleneckBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 创建第一个卷积层 1x1</span></span><br><span class="line">        self.conv0 = ConvBNLayer(</span><br><span class="line">            num_channels=num_channels,</span><br><span class="line">            num_filters=num_filters,</span><br><span class="line">            filter_size=<span class="number">1</span>,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 创建第二个卷积层 3x3</span></span><br><span class="line">        self.conv1 = ConvBNLayer(</span><br><span class="line">            num_channels=num_filters,</span><br><span class="line">            num_filters=num_filters,</span><br><span class="line">            filter_size=<span class="number">3</span>,</span><br><span class="line">            stride=stride,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 创建第三个卷积 1x1，但输出通道数乘以4</span></span><br><span class="line">        self.conv2 = ConvBNLayer(</span><br><span class="line">            num_channels=num_filters,</span><br><span class="line">            num_filters=num_filters * <span class="number">4</span>,</span><br><span class="line">            filter_size=<span class="number">1</span>,</span><br><span class="line">            act=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果conv2的输出跟此残差块的输入数据形状一致，则shortcut=True</span></span><br><span class="line">        <span class="comment"># 否则shortcut = False，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> shortcut:</span><br><span class="line">            self.short = ConvBNLayer(</span><br><span class="line">                num_channels=num_channels,</span><br><span class="line">                num_filters=num_filters * <span class="number">4</span>,</span><br><span class="line">                filter_size=<span class="number">1</span>,</span><br><span class="line">                stride=stride)</span><br><span class="line"></span><br><span class="line">        self.shortcut = shortcut</span><br><span class="line"></span><br><span class="line">        self._num_channels_out = num_filters * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self.conv0(inputs)</span><br><span class="line">        conv1 = self.conv1(y)</span><br><span class="line">        conv2 = self.conv2(conv1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果shortcut=True，直接将inputs跟conv2的输出相加</span></span><br><span class="line">        <span class="comment"># 否则需要对inputs进行一次卷积，将形状调整成跟conv2输出一致</span></span><br><span class="line">        <span class="keyword">if</span> self.shortcut:</span><br><span class="line">            short = inputs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            short = self.short(inputs)</span><br><span class="line">        y = paddle.add(x=short, y=conv2)</span><br><span class="line">        y = F.relu(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;--------定义ResNet模型--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers=<span class="number">50</span>, class_dim=<span class="number">1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        layers, 网络层数，可以是50, 101或者152</span></span><br><span class="line"><span class="string">        class_dim，分类标签的类别数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 支持的resnet层数</span></span><br><span class="line">        supported_layers = [<span class="number">18</span>, <span class="number">34</span>, <span class="number">50</span>, <span class="number">101</span>, <span class="number">152</span>]</span><br><span class="line">        <span class="keyword">assert</span> layers <span class="keyword">in</span> supported_layers, \</span><br><span class="line">            <span class="string">&quot;supported layers are &#123;&#125; but input layer is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(supported_layers, layers)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> layers == <span class="number">18</span>:</span><br><span class="line">            <span class="comment">#ResNet18包含多个模块，其中第2到第5个模块分别包含2、2、2、2个残差块</span></span><br><span class="line">            depth = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">34</span> <span class="keyword">or</span> layers == <span class="number">50</span>:</span><br><span class="line">            <span class="comment">#ResNet34/50包含多个模块，其中第2到第5个模块分别包含3、4、6、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">101</span>:</span><br><span class="line">            <span class="comment">#ResNet101包含多个模块，其中第2到第5个模块分别包含3、4、23、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">152</span>:</span><br><span class="line">            <span class="comment">#ResNet152包含多个模块，其中第2到第5个模块分别包含3、8、36、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 残差块中使用到的卷积的输出通道数</span></span><br><span class="line">        num_filters = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ResNet的第一个模块，包含1个7x7卷积，后面跟着1个最大池化层</span></span><br><span class="line">        self.conv = ConvBNLayer(</span><br><span class="line">            num_channels=<span class="number">3</span>,</span><br><span class="line">            num_filters=<span class="number">64</span>,</span><br><span class="line">            filter_size=<span class="number">7</span>,</span><br><span class="line">            stride=<span class="number">2</span>,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.pool2d_max = nn.MaxPool2D(</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">2</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ResNet的第二到第五个模块c2、c3、c4、c5</span></span><br><span class="line">        self.bottleneck_block_list = []</span><br><span class="line">        num_channels = <span class="number">64</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(depth)):</span><br><span class="line">            shortcut = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth[block]):</span><br><span class="line">                <span class="comment"># c3、c4、c5将会在第一个残差块使用stride=2；其余所有残差块stride=1</span></span><br><span class="line">                <span class="comment"># add_sublayer方法：返回一个由所有子层组成的列表</span></span><br><span class="line">                <span class="comment"># 字符串格式化: %d把后面对应的值用整数的形式显示</span></span><br><span class="line">                bottleneck_block = self.add_sublayer(</span><br><span class="line">                    <span class="string">&#x27;bottleneck_block_%d_%d&#x27;</span> % (block, i),</span><br><span class="line">                    BottleneckBlock(</span><br><span class="line">                        num_channels=num_channels,</span><br><span class="line">                        num_filters=num_filters[block],</span><br><span class="line">                        stride=<span class="number">2</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> block != <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span>, </span><br><span class="line">                        shortcut=shortcut))</span><br><span class="line">                num_channels = bottleneck_block._num_channels_out</span><br><span class="line">                self.bottleneck_block_list.append(bottleneck_block)</span><br><span class="line">                shortcut = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在c5的输出特征图上使用全局池化</span></span><br><span class="line">        self.pool2d_avg = paddle.nn.AdaptiveAvgPool2D(output_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stdv用来作为全连接层随机初始化参数的方差</span></span><br><span class="line">        <span class="keyword">import</span> math</span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(<span class="number">2048</span> * <span class="number">1.0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建全连接层，输出大小为类别数目，经过残差网络的卷积和全局池化后，</span></span><br><span class="line">        <span class="comment"># 卷积特征的维度是[B,2048,1,1]，故最后一层全连接的输入维度是2048</span></span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">2048</span>, out_features=class_dim,</span><br><span class="line">                      weight_attr=paddle.ParamAttr(</span><br><span class="line">                          initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self.conv(inputs)</span><br><span class="line">        y = self.pool2d_max(y)</span><br><span class="line">        <span class="keyword">for</span> bottleneck_block <span class="keyword">in</span> self.bottleneck_block_list:</span><br><span class="line">            y = bottleneck_block(y)</span><br><span class="line">        y = self.pool2d_avg(y)</span><br><span class="line">        y = paddle.reshape(y, [y.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        y = self.out(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;bottleneck_block_%d_%d&#x27;</span> % (<span class="number">1.2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;bottleneck_block_1_1&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = ResNet(layers=<span class="number">18</span>)</span><br><span class="line">model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters(), </span><br><span class="line">                                weight_decay=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_R, train_losses_R = train_pm(model, opt)</span><br></pre></td></tr></table></figure><p>通过运行结果可以发现，使用ResNet在眼疾筛查数据集iChallenge-PM上，loss能有效的下降。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h1><ul><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li><li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一，先使用卷积层学习图片空间信息，然后使用全连接层来转换到类别空间。</li><li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li><li>今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。</li><li>尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。</li><li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li><li>块的使用使得网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li><li>我们发现深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效。</li><li>不同的超参数可以得到不同复杂度的变种。</li><li>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状（超参数）的卷积层和最大池化层来并行抽取信息，并使用$1×1$卷积层减少每像素级别上的通道维数从而降低模型复杂度，从而降低计算复杂度。</li><li>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li><li>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</li><li>GoogLeNet使用了9个Inception块，这是第一个达到上百层的网络，且在后续有了一系列的改进。</li><li>利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h1><p>[1] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learn- ing applied to document recognition. Proc. of the IEEE, 86(11):2278–2324, 1998 </p><p>[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. </p><p>[3] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014b. </p><p>[4]Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolu- tions. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015. </p><p>[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016a. </p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_卷积基本概念及经典模型复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习5.1-从全连接层到卷积</title>
      <link href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF/"/>
      <url>/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="从全连接层到卷积"><a href="#从全连接层到卷积" class="headerlink" title="从全连接层到卷积"></a><strong>从全连接层到卷积</strong></h1><p>:label:<code>sec_why-conv</code></p><p>学习本节，希望你能够掌握以下知识点：</p><ul><li>能够分析全连接层的局限，分析利弊；</li><li>理解卷积神经网络理论提出的依据，即空间不变性；</li></ul><hr><h2 id="一、全连接层的局限"><a href="#一、全连接层的局限" class="headerlink" title="一、全连接层的局限"></a>一、全连接层的局限</h2><p>我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。</p><p>对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。也就是说，MLP是一种稠密特征提取方式，而大多数时候，我们学习到的很可能是一种稀疏的特征。比如下面的图片识别：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9868a70ef4af44329e7220f869d239ba1df8397ad1cc41beb279754572d82cfc" width = "500"></center><center><br>图1：实际场景中的图片数据</br></center><hr><blockquote><p><strong>扩展</strong>：稠密特征与稀疏特征</p><p>假如每个维度上都有值，本来是在 n 个维度上来表达这个样本，而现在只有个别维度上有效地表达了这个样本，这个特征可以说是稀疏特征。</p><p>稠密特征一般是相对稀疏特征来说的，类别特征经过独热编码之后比较稀疏，比如类别<code>[&#39;小猫&#39;,&#39;小狗&#39;,&#39;小熊&#39;,&#39;小猴&#39;]</code>被独热编码后的数据结构为<code>[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]</code>。但是像桌子的长度这种稠密特征数据的表达方式就是<code>[3.4,2.6,8.9,6.7]</code>。</p></blockquote><hr><p>我们都知道，图片在计算机内部以像素值的方式被存储，也就是说两张图在计算机看来，其实是这样子的。其中1代表白色，-1代表黑色。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/492f53fee2aa4c07864943a1952c463bb0f3ccd95ff043878d97b78f8caa1cb8" width = "500"></center><center><br>图2：计算机眼中的图片数据</br></center><p>如果把像素拆开，按照每像素逐个比较肯定是不科学的，因为单个像素是无法反映整体的，这样做会导致输入数据的<code>空间信息</code>被丢失。</p><p>除此以外，使用全连接网络参数过多时，网络训练缓慢。例如，在猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有$10^6 \times 10^3 = 10^9$个参数。想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。</p><p>虽然我们会认为要求百万像素的分辨率可能不是必要的。然而，即使分辨率减小为十万像素，使用 1000 个隐藏单元的隐藏层也可能不足以学习到良好的图像特征，在真实的系统中我们仍然需要数十亿个参数。</p><p>为了解决上述问题，我们引入<strong>卷积神经网络进行特征提取</strong>，既能提取到<strong>相邻像素点之间</strong>的特征模式，又能保证参数的个数不随图片尺寸变化。</p><p>下图是一个典型的卷积神经网络结构，多层卷积和池化层组合作用在输入图片上，在网络的最后通常会加入一系列全连接层，ReLU 激活函数一般加在卷积或者全连接层的输出上，网络中通常还会加入 Dropout 来防止过拟合。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/6d1440daa10944c899a7c98e1bed3931a09bae52730d4c20a65b322193d284e1" width = "1000"></center><center><br>图3：卷积神经网络经典结构</br></center><hr><h2 id="二、空间不变性"><a href="#二、空间不变性" class="headerlink" title="二、空间不变性"></a>二、空间不变性</h2><p>想象一下，假设你想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。理想情况下，我们的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。但是，如果一只猪出现在图片顶部，我们还是应该认出它。</p><p>我们可以从儿童游戏”沃尔多在哪里”中得到灵感：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/207441057fea4c52a46c4362b5158dbae7ca2ffec7214b1f876f34929f5d00df" width = "700"></center><center><br>图4：Where's Waldo?</br></center><p>在这个游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。尽管沃尔多的装扮很有特点，但是在眼花缭乱的场景中找到他也如大海捞针。</p><p>然而沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像，该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。</p><p>卷积神经网络正是将<strong>空间不变性（spatial invariance）</strong> 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构：</p><ol><li><p><strong>平移不变性（translation invariance）</strong>：识别器的标准是统一的，不管检测对象出现在图像中的哪个位置，识别器应该对相同的图像区域具有相似的反应。</p></li><li><p><strong>局部性（locality）</strong>：识别器应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li></ol><p>让我们看看这些原则是如何转化为数学表示的。首先，多层感知机的输入是二维图像$\mathbf{X}$（宽度、高度），其隐藏表示$\mathbf{H}$在数学上是一个矩阵，在代码中表示为二维张量。<br>其中$\mathbf{X}$和$\mathbf{H}$具有相同的形状。使用$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置（$i$,$j$）处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$\mathsf{W}$。也就是说，我们从接收输入的长度变化变成了接收输入的高宽两个维度的变化，输入和输出各增加了一个维度，因此我们的权重变为<code>四维</code>。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/2c7a0cf210544349a96eb6c3c0e42cf7bfd86b3674ae4d5e90614e09f3a20b12" width = "600"></center><center><br>图5：全连接与卷积对比</br></center><p>假设$\mathbf{U}$包含偏置参数，我们可以将全连接层形式化地表示为</p><script type="math/tex; mode=display">\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=   [\mathbf{U}]_{i, j} +\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}</script><p>其中，从$\mathsf{W}$到$\mathsf{V}$的转换只是形式上的转换，因为在这两个四阶张量的元素之间存在一一对应的关系。<br>我们只需重新索引下标$(k, l)$，原来的$(k, l)$代表的是像素在图片中的绝对位置，现在我们引入了索引$a$和$b$通过在正偏移和负偏移之间移动覆盖了整个图像，使$k = i+a$、$l = j+b$，由此可得$[\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}$。<br>也就是说，我们使用$a$和$b$来表示相对于某一任意点$(i, j)$的相对位置。对于隐藏表示中任意给定位置（$i$,$j$）处的像素值$[\mathbf{H}]_{i, j}$，可以通过在$x$中以$(i, j)$为中心对像素进行加权求和得到，加权使用的权重为$[\mathsf{V}]_{i, j, a, b}$。</p><blockquote><p>只考虑当前观察器扫到的区域，不考虑图片的其他他区域，所以用相对位置更为方便。</p></blockquote><h3 id="1）平移不变性"><a href="#1）平移不变性" class="headerlink" title="1）平移不变性"></a>1）平移不变性</h3><p>现在引用上述的第一个原则：平移不变性。这意味着检测对象在输入$\mathbf{X}$中的平移，应该仅导致隐藏表示$\mathbf{H}$中的平移。也就是说，$\mathsf{V}$和$\mathbf{U}$实际上不依赖于$(i, j)$的值，即$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$。无论识别出来的目标特征在什么位置，我们的权重都是不变的。</p><p>我们希望，图像的一部分的统计特性与其他部分是一样的，因为我们可以将这一部分学习的特征用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。这就是<strong>卷积核（convolution kernel）</strong> 的概念，我们可以使用同一张卷积核遍历整张图片。并且$\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\mathbf{H}$定义为：</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}.</script><p>这就是<strong>卷积（convolution）</strong>。我们是在使用系数$[\mathbf{V}]_{a, b}$对位置$(i, j)$附近的像素$(i+a, j+b)$进行加权得到$[\mathbf{H}]_{i, j}$。注意，$[\mathbf{V}]_{a, b}$的系数比$[\mathsf{V}]_{i, j, a, b}$少很多，因为前者不再依赖于图像中的位置。</p><p>在神经网络中，卷积被定义为不同位置的特征检测器，也就意味着，无论目标出现在图像中的哪个位置，它都会检测到同样的这些特征，输出同样的响应。比如人脸被移动到了图像左下角，卷积核直到移动到左下角的位置才会检测到它的特征。</p><h3 id="2）局部性"><a href="#2）局部性" class="headerlink" title="2）局部性"></a>2）局部性</h3><p>现在引用上述的第二个原则：局部性。如上所述，为了收集用来训练参数$[\mathbf{H}]_{i, j}$的相关信息，我们不应偏离到距$(i, j)$很远的地方。这意味着在$|a|&gt; \Delta$或$|b| &gt; \Delta$的范围之外，我们可以设置$[\mathbf{V}]_{a, b} = 0$。以 $(i, y)$ 为中心位置，我们允许向前向后移动 ${\Delta}$ 个位置，因此，我们可以将$[\mathbf{H}]_{i, j}$重写为</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.</script><p>通过这种方式，我们卷积层的权指的数量会成倍的减少，但仍然考虑到了整个图片的权指的信息。既考虑到整个的属性，又照顾到了跟它局部相关的点。对全连接层使用平移不变性和局部性会得到卷积层，所以卷积是一个特殊的全连接层。</p><h3 id="3）多通道"><a href="#3）多通道" class="headerlink" title="3）多通道"></a><strong>3）多通道</strong></h3><p>然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量，比如包含$1024 \times 1024 \times 3$个像素。前两个轴与像素的空间位置有关，而第三个轴可以看作是每个像素的多维表示。因此，我们将$\mathsf{X}$索引为$[\mathsf{X}]_{i, j, k}$。由此卷积相应地调整为$[\mathsf{V}]_{a,b,c}$，而不是$[\mathbf{V}]_{a,b}$。</p><p>此外，由于输入图像是三维的，我们的隐藏表示$\mathsf{H}$也最好采用三维张量。换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。</p><p>因此，我们可以把隐藏表示想象为一系列具有二维张量的通道（channel）。这些通道有时也被称为<strong>特征映射（feature maps）</strong>，因为每个通道都向后续层提供一组空间化的学习特征。在实际应用中，CNN可能使用更多的甚至几十个特征映射，以手写数字识别为例，学习到的特征如下，这20张图片对应了20个特征映射，每一个特征映射由 <em>5X5</em> 的图像表示：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9474ec1bab0b4081b18fd6e34c77063d640d7618f993459fb68d6270bb94553f" width = "600"></center><center><br>图6：特征映射</br></center><p>为了支持输入$\mathsf{X}$和隐藏表示$\mathsf{H}$中的多个通道，我们可以在$\mathsf{V}$中添加第四个坐标，即$[\mathsf{V}]_{a, b, c, d}$。综上所述，</p><script type="math/tex; mode=display">[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},</script><p>其中隐藏表示$\mathsf{H}$中的索引$d$表示输出通道，而随后的输出将继续以三维张量$\mathsf{H}$作为输入进入下一个卷积层。所以这个最终的公式可以定义具有多个通道的卷积层，而其中$\mathsf{V}$是该卷积层的权重。</p><h2 id="三、卷积（Convolution）"><a href="#三、卷积（Convolution）" class="headerlink" title="三、卷积（Convolution）"></a>三、卷积（Convolution）</h2><p>这一小节将介绍卷积算法的原理和实现方案，并通过具体的案例展示如何使用卷积对图片进行操作，学习本节需要掌握如下内容：</p><ul><li><p>卷积计算</p></li><li><p>填充（padding）</p></li><li><p>步幅（stride）</p></li><li><p>感受野（Receptive Field）</p></li><li><p>多输入通道、多输出通道</p></li><li><p>飞桨卷积API介绍</p></li><li><p>卷积算子应用举例</p></li></ul><hr><h3 id="1）卷积计算"><a href="#1）卷积计算" class="headerlink" title="1）卷积计算"></a>1）卷积计算</h3><blockquote><p>将卷积核与图片关联起来。</p></blockquote><p>卷积是数学分析中的一种积分变换的方法，在图像处理中采用的是卷积的离散形式。这里需要说明的是，在卷积神经网络中，卷积层的实现方式实际上是数学中定义的<strong>互相关（cross-correlation）</strong> 运算，与数学分析中的卷积定义有所不同，这里跟其他框架和卷积神经网络的教程保持一致，都使用互相关运算作为卷积的定义，互相关是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。</p><p>具体的计算过程如 <strong>图7</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d5019afe174745efbf7a3d3c604b9c85eeddc947f7184446a9147d128863864d" width = "700"></center><center><br>图7：卷积计算过程</br></center><hr><blockquote><p><strong>说明：</strong></p><p><strong>卷积核（kernel）</strong> 也被叫做<strong>滤波器（filter）</strong>，假设卷积核的高和宽分别为$k_h$和$k_w$，则将称为$k_h\times k_w$卷积，比如$3\times5$卷积，就是指卷积核的高为3, 宽为5。</p></blockquote><hr><ul><li>如图7（a）所示：左边的图大小是$3\times3$，表示输入数据是一个维度为$3\times3$的二维数组；中间的图大小是$2\times2$，表示一个维度为$2\times2$的二维数组，我们将这个二维数组称为卷积核。先将卷积核的左上角与输入数据的左上角（即：输入数据的(0, 0)位置）对齐，把卷积核的每个元素跟其位置对应的输入数据中的元素相乘，再把所有乘积相加，得到卷积输出的第一个结果：</li></ul><script type="math/tex; mode=display">0\times1 + 1\times2 + 2\times4 + 3\times5 = 25  \ \ \ \ \ \ \ (a)</script><ul><li>如图7（b）所示：将卷积核向右滑动，让卷积核左上角与输入数据中的(0,1)位置对齐，同样将卷积核的每个元素跟其位置对应的输入数据中的元素相乘，再把这4个乘积相加，得到卷积输出的第二个结果：</li></ul><script type="math/tex; mode=display">0\times2 + 1\times3 + 2\times5 + 3\times6 = 31  \ \ \ \ \ \ \ (b)</script><ul><li>如图7（c）所示：将卷积核向下滑动，让卷积核左上角与输入数据中的(1, 0)位置对齐，可以计算得到卷积输出的第三个结果：</li></ul><script type="math/tex; mode=display">0\times4 + 1\times5 + 2\times7 + 3\times8 = 43   \ \ \ \ \ \ \ (c)</script><ul><li>如图7（d）所示：将卷积核向右滑动，让卷积核左上角与输入数据中的(1, 1)位置对齐，可以计算得到卷积输出的第四个结果：</li></ul><script type="math/tex; mode=display">0\times5 + 1\times6 + 2\times8 + 3\times9 = 49   \ \ \ \ \ \ \ (d)</script><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4219539eac53411281d1318343d84c38e1d21662c69742b8bd89df7c906cbe45" width = "700"></center><p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，而卷积核只与图像中每个大小完全适合的位置进行互相关运算，这就导致必然会有一些值不参与运算。</p><p>所以，输出大小等于输入大小减去卷积核大小$k_h \times k_w$再加上舍弃掉的值，即：</p><script type="math/tex; mode=display">H_{out} = H - k_h + 1</script><script type="math/tex; mode=display">W_{out} = W - k_w + 1</script><p>我们可以使用一个式子来表示输出值$Y$，其中，$W$和$b$都是我们需要学习的参数：</p><script type="math/tex; mode=display">Y = X·W + b</script><p>因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过<strong>在图像边界周围填充零</strong>来保证有足够的空间移动卷积核，从而保持输出大小不变。</p><p><strong>实现卷积运算</strong></p><p>接下来，我们在<code>corr2d</code>函数中实现如上过程，该函数接受输入张量<code>X</code>和卷积核张量<code>K</code>，并返回输出张量<code>Y</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">X = paddle.to_tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]])</span><br><span class="line">K = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    <span class="comment"># 根据上面公式计算输出的h和w</span></span><br><span class="line">    Y = paddle.zeros(shape=(X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i+h, j:j+w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(corr2d(X, K))</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[25., 31.],        [43., 49.]])</code></pre><h3 id="2）填充"><a href="#2）填充" class="headerlink" title="2）填充"></a><strong>2）填充</strong></h3><p>在上面的例子中，输入图片尺寸为$3\times3$，输出图片尺寸为$2\times2$，经过一次卷积之后，图片尺寸变小。卷积输出特征图的尺寸计算方法如下（卷积核的高和宽分别为$k_h$和$k_w$）：</p><script type="math/tex; mode=display">H_{out} = H - k_h + 1</script><script type="math/tex; mode=display">W_{out} = W - k_w + 1</script><p>如果输入尺寸为4，卷积核大小为3时，输出尺寸为$4-3+1=2$。当卷积核尺寸大于1时，输出特征图的尺寸会小于输入图片尺寸。如果经过多次卷积，输出图片尺寸会不断减小。为了避免卷积之后图片尺寸变小，通常会在图片的外围进行<strong>填充(padding)</strong>：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/01d311ec2c65435f85059953a84ec7ea8ef2fd236452450e912346a7da201c5f" width = "700"></center><center><br>图8：图形填充 </br></center><ul><li><p>如图8（a）所示：填充的大小为1，填充值为0。填充之后，输入图片尺寸从$4\times4$变成了$6\times6$，使用$3\times3$的卷积核，输出图片尺寸为$4\times4$。</p></li><li><p>如图8（b）所示：填充的大小为2，填充值为0。填充之后，输入图片尺寸从$4\times4$变成了$8\times8$，使用$3\times3$的卷积核，输出图片尺寸为$6\times6$。</p></li></ul><p>如果在图片高度方向，在第一行之前填充$p_{h1}$行，在最后一行之后填充$p_{h2}$行；在图片的宽度方向，在第1列之前填充$p_{w1}$列，在最后1列之后填充$p_{w2}$列；则填充之后的图片尺寸为$(H + p_{h1} + p_{h2})\times(W + p_{w1} + p_{w2})$。经过大小为$k_h\times k_w$的卷积核操作之后，输出图片的尺寸为：</p><script type="math/tex; mode=display">H_{out} = H + p_{h1} + p_{h2} - k_h + 1</script><script type="math/tex; mode=display">W_{out} = W + p_{w1} + p_{w2} - k_w + 1</script><p>在卷积计算过程中，通常会在高度或者宽度的两侧采取等量填充，即$p_{h1} = p_{h2} = p_h,\ \ p_{w1} = p_{w2} = p_w$，上面计算公式也就变为：</p><script type="math/tex; mode=display">H_{out} = H + 2p_h - k_h + 1</script><script type="math/tex; mode=display">W_{out} = W + 2p_w - k_w + 1</script><p>卷积核大小通常使用<code>1，3，5，7</code>这样的奇数，如果使用的填充大小为$p_h=(k_h-1)/2 ，p_w=(k_w-1)/2$，则卷积之后图像尺寸不变。例如当<code>卷积核大小为3时，padding大小为1</code>，卷积之后图像尺寸不变；同理，如果<code>卷积核大小为5，padding大小为2</code>，也能保持图像尺寸不变。</p><p>比如，在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1，X为（批量大小，通道数，高度，宽度）</span></span><br><span class="line">    X = X.reshape([<span class="number">1</span>, <span class="number">1</span>] + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:]) <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;填充&#x27;&#x27;&#x27;</span></span><br><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment"># 这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">X = paddle.rand(shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 4, 4][4, 4]</code></pre><p>当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>)) <span class="comment">#ph = kh-1</span></span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 4, 4][4, 4]</code></pre><h3 id="3）步幅"><a href="#3）步幅" class="headerlink" title="3）步幅"></a><strong>3）步幅</strong></h3><p><strong>图8</strong> 中卷积核每次滑动一个像素点，这是<strong>步幅（stride）</strong> 为1的特殊情况。<strong>图9</strong> 是步幅为2的卷积过程，卷积核在图片上移动时，每次移动大小为2个像素点。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/afdae9af02fc45eabdd9663ee6474e4da86675fa1f444c78aea0e21539b32cf0" width = "800"></center><center><br>图9：步幅为2的卷积过程 </br></center><p>当宽和高方向的步幅分别为$s_h$和$s_w$时，输出特征图尺寸的计算公式是：</p><script type="math/tex; mode=display">H_{out} = \frac{H + 2p_h - k_h}{s_h} + 1</script><script type="math/tex; mode=display">W_{out} = \frac{W + 2p_w - k_w}{s_w} + 1</script><p>假设输入图片尺寸是$H\times W = 100 \times 100$，卷积核大小$k_h \times k_w = 3 \times 3$，填充$p_h = p_w = 1$，步幅为$s_h = s_w = 2$，则输出特征图的尺寸为：</p><script type="math/tex; mode=display">H_{out} = \frac{100 + 2 - 3}{2} + 1 = 50</script><script type="math/tex; mode=display">W_{out} = \frac{100 + 2 - 3}{2} + 1 = 50</script><p>下面，我将高度和宽度的步幅设置为2，从而将输入的高度和宽度减半。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;步幅&#x27;&#x27;&#x27;</span></span><br><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 2, 2][2, 2]</code></pre><p>接下来，看一个稍微复杂的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.rand(shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># [(4+0-3)/3]+1[(4+2-5)/4]+1</span></span><br><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 1][1, 1]</code></pre><h3 id="4）感受野"><a href="#4）感受野" class="headerlink" title="4）感受野"></a><strong>4）感受野</strong></h3><p>输出特征图上每个点的数值，是由输入图片上大小为$k_h\times k_w$的区域的元素与卷积核每个元素相乘再相加得到的，所以输入图像上$k_h\times k_w$区域内每个元素数值的改变，都会影响输出点的像素值。我们将这个区域叫做输出特征图上对应点的<strong>感受野（Receptive Field）</strong>。感受野内每个元素数值的变动，都会影响输出点的数值变化。比如$3\times3$卷积对应的感受野大小就是$3\times3$，如 <strong>图10</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/1021536721524f4d8f4c1aefa89693c4b0fd388f21a347b583d413b3ac41241b" width = "800"></center><center><br>图10：感受野为3×3的卷积 </br></center><p>而当通过两层$3\times3$的卷积之后，感受野的大小将会增加到$5\times5$，如 <strong>图11</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/ac14916db81e40a48a25ab894d7a95e33fa0eece71d44a55af7bffab462fb7a7" width = "800"></center><center><br>图11：感受野为5×5的卷积 </br></center><p>因此，当增加卷积网络深度的同时，感受野将会增大，输出特征图中的一个像素点将会包含更多的图像语义信息。</p><h3 id="5）多输入通道、多输出通道"><a href="#5）多输入通道、多输出通道" class="headerlink" title="5）多输入通道、多输出通道"></a>5）多输入通道、多输出通道</h3><p>前面介绍的卷积计算过程比较简单，实际应用时，处理的问题要复杂的多。例如：对于彩色图片有RGB三个通道，需要处理多输入通道的场景。输出特征图往往也会具有多个通道，而且在神经网络的计算中常常是把一个批次的样本放在一起计算，所以卷积算子需要具有批量处理多输入和多输出通道数据的功能，下面将分别介绍这几种场景的操作方式。</p><p><strong>多输入通道场景</strong></p><p>上面的例子中，卷积层的数据是一个2维数组，但实际上一张图片往往含有RGB三个通道，要计算卷积的输出结果，卷积核的形式也会发生变化。假设输入图片的通道数为$C_{in}$，输入数据的形状是$C_{in}\times{H_{in}}\times{W_{in}}$，计算过程如 <strong>图12</strong> 所示。</p><ol><li><p>对每个通道分别设计一个2维数组作为卷积核，卷积核数组的形状是$C_{in}\times{k_h}\times{k_w}$。</p></li><li><p>对任一通道$C_{in} \in [0, C_{in})$，分别用大小为$k_h\times{k_w}$的卷积核在大小为$H_{in}\times{W_{in}}$的二维数组上做卷积。</p></li><li><p>将这$C_{in}$个通道的计算结果相加，得到的是一个形状为$H_{out}\times{W_{out}}$的二维数组。</p></li></ol><center><img src="https://ai-studio-static-online.cdn.bcebos.com/92186667b8424a7ca781b22de6766fa62e31512cf2e24e33a4b796541177c9dd" width = "800"></center><center><br>图12：多输入通道计算过程 </br></center><p>为了加深理解，我们实现一下多输入通道互相关运算。简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。</p><hr><blockquote><p>说明：</p><p><code>zip()</code>函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</p></blockquote><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K)) </span><br><span class="line"></span><br><span class="line">X = paddle.to_tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line"></span><br><span class="line">K = paddle.to_tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[56. , 72. ],        [104., 120.]])</code></pre><p><strong>多输出通道场景</strong></p><p>一般来说，卷积操作的输出特征图也会具有多个通道$C_{out}$，这时我们需要设计$C_{out}$个维度为$C_{in}\times{k_h}\times{k_w}$的卷积核，卷积核数组的维度是$C_{out}\times C_{in}\times{k_h}\times{k_w}$，如 <strong>图13</strong> 所示。</p><ol><li>对任一输出通道$c_{out} \in [0, C_{out})$，分别使用上面描述的形状为$C_{in}\times{k_h}\times{k_w}$的卷积核对输入图片做卷积。</li><li>将这$C_{out}$个形状为$H_{out}\times{W_{out}}$的二维数组拼接在一起，形成维度为$C_{out}\times{H_{out}}\times{W_{out}}$的三维数组。</li></ol><hr><blockquote><p><strong>说明：</strong></p><p>通常将卷积核的输出通道数叫做卷积核的个数。</p><p>一组输入一个输出</p></blockquote><hr><center><img src="https://ai-studio-static-online.cdn.bcebos.com/cf1fbddc141349e4b7aaeade9a201b78a16d249e069c4f8aaeb77e0ea1a95c31" width = "800"></center><center><br>图13：多输出通道计算过程 </br></center><p>如下所示，我们实现一个计算多个通道的输出的互相关函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># paddle.stack 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="comment"># 输入 x 为 N 个 Shape 为 [A, B]的 Tensor, 如果 axis = 0 , 则输出 Tensor 的 Shape 为 [N, A, B]</span></span><br><span class="line"></span><br><span class="line">K = paddle.to_tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line">K = paddle.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;(C_out,C_in,k_h,k_w)：&#x27;</span>, K.shape)</span><br><span class="line"></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br></pre></td></tr></table></figure><pre><code>(C_out,C_in,k_h,k_w)： [3, 2, 2, 2]Tensor(shape=[3, 2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[56. , 72. ],         [104., 120.]],        [[76. , 100.],         [148., 172.]],        [[96. , 128.],         [192., 224.]]])</code></pre><p><strong>批量操作</strong></p><p>在卷积神经网络的计算中，通常将多个样本放在一起形成一个mini-batch进行批量操作，即输入数据的维度是$N\times{C_{in}}\times{H_{in}}\times{W_{in}}$。由于会对每张图片使用同样的卷积核进行卷积操作，卷积核的维度与上面多输出通道的情况一样，仍然是$C_{out}\times C_{in}\times{k_h}\times{k_w}$，输出特征图的维度是$N\times{C_{out}}\times{H_{out}}\times{W_{out}}$，如 <strong>图14</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/60760d68001c40d6a6c500b17f57d8deae7b5921631b4b6b896b057b904d24b1" width = "800"></center><center><br>图14：批量操作 </br></center><h3 id="6）1-times-1-卷积层"><a href="#6）1-times-1-卷积层" class="headerlink" title="6）1 $\times$ 1 卷积层"></a><strong>6）1 $\times$ 1 卷积层</strong></h3><p>因为使用了最小窗口，$1\times 1$卷积失去了卷积层的特有能力，即在高度和宽度维度上，识别相邻元素间相互作用的能力。其实$1\times 1$卷积的作用在于：它不识别空间模式，只是融合通道。下图展示了使用$1\times 1$卷积核与$3$个输入通道和$2$个输出通道的互相关计算：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/1b206e97ea5f4e3aabbd335848830262280ee32c91cd4b5e8bad5b8b0f9ed8d4" width = "450"></center><center><br>图15：1×1卷积 </br></center><blockquote><p>通道融合：将三个通道的数×卷积并相加在了一起。</p></blockquote><p>这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。我们可以将$1\times 1$卷积层看作是在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。因为这仍然是一个卷积层，所以跨像素的权重是一致的。我们可以简单的认为，这样的目的更多的是为了实现通道的线性变换，重新排列组合为更好的特征基底。</p><p>下面，我们使用全连接层和互相关函数分别实现$1 \times 1$卷积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;1 x 1 的卷积层&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    Y = paddle.matmul(K, X) <span class="comment"># matmul 矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line">X = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)) <span class="comment">#（输入，h*w）</span></span><br><span class="line">K = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># （输出，输入，1*1）</span></span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(paddle.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span> <span class="comment"># 由于浮点数精度的区别，几乎认为Y1=Y2，也就是全连接与1*1互相关运算的结果相同。</span></span><br><span class="line">paddle.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,       [0.00000015])</code></pre><h2 id="四、飞桨卷积API介绍"><a href="#四、飞桨卷积API介绍" class="headerlink" title="四、飞桨卷积API介绍"></a>四、飞桨卷积API介绍</h2><p>飞桨卷积算子对应的API是 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/Conv2D_cn.html">paddle.nn.Conv2D</a>，用户可以直接调用API进行计算，也可以在此基础上修改。Conv2D名称中的“2D”表明卷积核是二维的，多用于处理图像数据。类似的，也有Conv3D可以用于处理视频数据（图像的序列）。</p><blockquote><p><em>class</em> paddle.nn.Conv2D(<em>in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode=’zeros’, weight_attr=None, bias_attr=None, data_format=’NCHW’</em>)</p></blockquote><p>常用的参数如下：</p><ul><li><code>in_channels(int)</code> - 输入图像的通道数。</li><li><code>out_channels(int)</code> - 卷积核的个数，和输出特征图通道数相同，相当于上文中的$C_{out}$。</li><li><code>kernel_size(int|list|tuple)</code> - 卷积核大小，可以是整数，比如3，表示卷积核的高和宽均为3 ；或者是两个整数的list，例如[3,2]，表示卷积核的高为3，宽为2。</li><li><code>stride(int|list|tuple，可选)</code> - 步长大小，可以是整数，默认值为1，表示垂直和水平滑动步幅均为1；或者是两个整数的list，例如[3,2]，表示垂直滑动步幅为3，水平滑动步幅为2。</li><li><code>padding(int|list|tuple|str，可选)</code> - 填充大小，可以是整数，比如1，表示竖直和水平边界填充大小均为1；或者是两个整数的list，例如[2,1]，表示竖直边界填充大小为2，水平边界填充大小为1。</li></ul><hr><blockquote><p><strong>强调</strong>：</p><ul><li>输入数据维度：$[N, C_{in}, H_{in}, W_{in}]$     【图片数,输入通道数,图片的高,图片的宽】</li><li>输出数据维度：$[N, out_channels, H_{out}, W_{out}]$     【批量维：生成的特征组数，输出通道数(可以调整)】</li><li>权重参数$w$的维度：$[out_channels, C_{in}, filter_size_h, filter_size_w]$</li><li>偏置参数$b$的维度是：$[out_channels]$</li></ul><p>注意，<strong>即使输入只有一张灰度图片$[H_{in}, W_{in}]$，也需要处理成四个维度的输入向量$[1, 1, H_{in}, W_{in}]$</strong>。</p></blockquote><hr><h3 id="卷积算子应用举例"><a href="#卷积算子应用举例" class="headerlink" title="卷积算子应用举例"></a><strong>卷积算子应用举例</strong></h3><p>下面介绍卷积算子在图片中应用的三个案例，并观察其计算结果。</p><h5 id="案例1——简单的黑白边界检测"><a href="#案例1——简单的黑白边界检测" class="headerlink" title="案例1——简单的黑白边界检测"></a>案例1——简单的黑白边界检测</h5><p>下面是使用Conv2D算子完成一个图像边界检测的任务。图像左边为光亮部分，右边为黑暗部分，需要检测出光亮跟黑暗的分界处。</p><p>设置宽度方向的卷积核为$[1, 0, -1]$，此卷积核会将宽度方向间隔为1的两个像素点的数值相减。当卷积核在图片上滑动时，如果它所覆盖的像素点位于亮度相同的区域，则左右间隔为1的两个像素点数值的差为0。只有当卷积核覆盖的像素点有的处于光亮区域，有的处在黑暗区域时，左右间隔为1的两个点像素值的差才不为0。将此卷积核作用到图片上，输出特征图上只有对应黑白分界线的地方像素值才不为0。具体代码如下所示，结果输出在下方的图案中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> paddle.nn.initializer <span class="keyword">import</span> Assign</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入图片，图片左边的像素点取值为1，右边的像素点取值为0</span></span><br><span class="line">img = np.ones([<span class="number">50</span>, <span class="number">50</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">img[:, <span class="number">30</span>:] = <span class="number">0.</span></span><br><span class="line">plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片形状调整为[N, C, H, W]的形式</span></span><br><span class="line">x = img.reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">50</span>, <span class="number">50</span>])</span><br><span class="line">x = paddle.to_tensor(x)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/45.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建初始化权重参数w</span></span><br><span class="line">w = np.array([<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 将权重参数调整成维度为[cout, cin, kh, kw]的四维张量</span></span><br><span class="line">w = w.reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建卷积算子，设置输出通道数，卷积核大小，和初始化权重参数，通过参数属性weight_attr指定参数初始化方式（默认Xavier）</span></span><br><span class="line">conv = Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       weight_attr=paddle.ParamAttr(initializer=Assign(value=w)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用卷积算子作用在输入图片上</span></span><br><span class="line">y = conv(x)</span><br><span class="line"><span class="comment"># 将输出tensor转化为numpy.ndarray</span></span><br><span class="line">out = y.numpy()</span><br></pre></td></tr></table></figure><p>卷积算子Conv2D输出数据形状为<code>[N, C, H, W]</code>形式，此处输出数据形状为<code>[1, 1, H, W]</code>，是 4 维数组，但是画图函数<code>plt.imshow</code>画灰度图时，只接受 2 维数组。通过 <code>numpy.squeeze</code> 函数将大小为1的维度消除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制子图</span></span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;input image&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;output featuremap&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(out.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/46.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看卷积层的权重参数名字和数值</span></span><br><span class="line"><span class="built_in">print</span>(conv.weight)</span><br><span class="line"><span class="comment"># 参看卷积层的偏置参数名字和数值</span></span><br><span class="line"><span class="built_in">print</span>(conv.bias)</span><br></pre></td></tr></table></figure><pre><code>Parameter containing:Tensor(shape=[1, 1, 1, 3], dtype=float32, place=Place(cpu), stop_gradient=False,       [[[[ 1.,  0., -1.]]]])Parameter containing:Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [0.])</code></pre><h5 id="案例2——图像中物体边缘检测"><a href="#案例2——图像中物体边缘检测" class="headerlink" title="案例2——图像中物体边缘检测"></a>案例2——图像中物体边缘检测</h5><p>上面展示的是一个人为构造出来的简单图片，使用卷积网络检测图片明暗分界处的示例。</p><p>对于真实的图片，也可以使用合适的卷积核（$3X3$卷积核的中间值是 $8$ ，周围一圈的值是 $-1$ ）对其进行操作，用来检测物体的外形轮廓，观察输出特征图跟原图之间的对应关系，如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> paddle.nn.initializer <span class="keyword">import</span> Assign</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;work/1.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 将读入的图片转化为float32类型的numpy.ndarray</span></span><br><span class="line">x = np.array(img).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)<span class="comment"># (672,960,3)</span></span><br><span class="line"><span class="comment"># 图片读入成ndarry时，形状是[H, W, 3]，将通道这一维度调整到最前面</span></span><br><span class="line">x = np.transpose(x, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(x.shape)<span class="comment"># (3,672,960)</span></span><br><span class="line"><span class="comment"># 将数据形状调整为[N, C, H, W]格式</span></span><br><span class="line">x = x.reshape(<span class="number">1</span>, <span class="number">3</span>, img.height, img.width)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape) [<span class="number">1</span>, <span class="number">3</span>, <span class="number">672</span>, <span class="number">960</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置卷积核参数</span></span><br><span class="line"><span class="comment"># 下面除以8的原因是向对卷积核的数值进行算放，表示成分数</span></span><br><span class="line">w = np.array([[-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>], [-<span class="number">1</span>,<span class="number">8</span>,-<span class="number">1</span>], [-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)/<span class="number">8</span></span><br><span class="line">w = w.reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 由于输入通道数是3，将卷积核的形状从[1,1,3,3]通过repeat方式调整为[1,3,3,3]</span></span><br><span class="line">w = np.repeat(w, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 创建卷积算子，输出通道数为1，卷积核大小为3x3，并使用上面的设置好的数值作为卷积核权重的初始化参数</span></span><br><span class="line">conv = Conv2D(in_channels=<span class="number">3</span>, out_channels=<span class="number">1</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], </span><br><span class="line">            weight_attr=paddle.ParamAttr(initializer=Assign(value=w)))</span><br><span class="line"></span><br><span class="line">y = conv(x)</span><br><span class="line">out = y.numpy()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制子图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">10</span>))</span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;input image&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"></span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;output feature map&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.imshow(out.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/47.png" alt="png"></p><h5 id="案例3——图像均值模糊"><a href="#案例3——图像均值模糊" class="headerlink" title="案例3——图像均值模糊"></a>案例3——图像均值模糊</h5><p>另外一种比较常见的卷积核（$5X5$的卷积核中每个值均为$1$）是用当前像素跟它邻域内的像素取平均，这样可以能够有效的抑制噪声，平滑图像（高斯滤波），如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> paddle.nn.initializer <span class="keyword">import</span> Assign</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入图片并转成numpy.ndarray</span></span><br><span class="line"><span class="comment"># 换成灰度图</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;work/2.jpg&#x27;</span>).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">img = np.array(img)</span><br><span class="line">x = img.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x = x.reshape(<span class="number">1</span>, <span class="number">1</span>, img.shape[<span class="number">0</span>], img.shape[<span class="number">1</span>])</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 720, 1280], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[191., 195., 195., ..., 185., 184., 184.],          [189., 193., 194., ..., 185., 186., 186.],          [191., 195., 195., ..., 185., 186., 186.],          ...,          [184., 183., 181., ..., 210., 209., 216.],          [178., 179., 179., ..., 208., 208., 213.],          [171., 172., 174., ..., 211., 210., 210.]]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建初始化参数</span></span><br><span class="line">w = np.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>], dtype = <span class="string">&#x27;float32&#x27;</span>)/<span class="number">25</span></span><br><span class="line">conv = Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=[<span class="number">5</span>, <span class="number">5</span>], </span><br><span class="line">        weight_attr=paddle.ParamAttr(initializer=Assign(value=w)))</span><br><span class="line"></span><br><span class="line">y = conv(x)</span><br><span class="line">out = y.numpy()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;input image&#x27;</span>)</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;output feature map&#x27;</span>)</span><br><span class="line">out = out.squeeze()<span class="comment"># 剔除掉前面的两个维度，得到二维的数据</span></span><br><span class="line">plt.imshow(out, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/48.png" alt="png"></p><h2 id="五、池化（Pooling）"><a href="#五、池化（Pooling）" class="headerlink" title="五、池化（Pooling）"></a>五、池化（Pooling）</h2><p>​    池化是使用某一位置的相邻输出的总体统计特征代替网络在该位置的输出，其好处是当输入数据做出少量平移时，经过池化函数后的大多数输出还能保持不变。比如：当识别一张图像是否是人脸时，我们需要知道人脸左边有一只眼睛，右边也有一只眼睛，而不需要知道眼睛的精确位置，这时候通过池化某一片区域的像素点来得到总体统计特征会显得很有用。</p><p>​    使用池化层，使得图像压缩时去掉一些无关紧要的信息，而留下的信息则是具有<strong>尺度不变性</strong>的特征，是最能表达图像的特征。由于池化之后特征图会变得更小，如果后面连接的是全连接层，能有效的减小神经元的个数，节省存储空间并提高计算效率。</p><p>​    总结来说，池化层的主要目的是：<code>降低卷积层对位置的敏感性</code>。</p><h3 id="5-1-平均池化与最大池化"><a href="#5-1-平均池化与最大池化" class="headerlink" title="5.1 平均池化与最大池化"></a>5.1 平均池化与最大池化</h3><p>如 <strong>图15</strong> 所示，将一个$2\times 2$的区域<strong>池化（Pooling）</strong> 成一个像素点。</p><p>不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为最大池化（maximum pooling）和平均池化（average pooling）。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/5479daa3734d424bb710615d3c4f7e017ba2558808a8421ca7c914f3fced0a48" width = "600"></center><center><br>图15：池化 </br></center><ul><li>如图15（a）：平均池化。这里使用大小为$2\times2$的池化窗口，每次移动的步幅为2，对池化窗口覆盖区域内的像素取平均值，得到相应的输出特征图的像素值。</li><li>如图15（b）：最大池化。对池化窗口覆盖区域内的像素取最大值，得到输出特征图的像素值。当池化窗口在图片上滑动时，会得到整张输出特征图。池化窗口的大小称为池化大小，用$k_h \times k_w$表示。在卷积神经网络中用的比较多的是窗口大小为$2 \times 2$，步幅为2的池化。</li></ul><p>与卷积核类似，池化窗口在图片上滑动时，每次移动的步长称为步幅，当宽和高方向的移动大小不一样时，分别用$s_w$和$s_h$表示。也可以对需要进行池化的图片进行填充，填充方式与卷积类似，假设在第一行之前填充$p_{h1}$行，在最后一行后面填充$p_{h2}$行。在第一列之前填充$p_{w1}$列，在最后一列之后填充$p_{w2}$列，则池化层的输出特征图大小为：</p><script type="math/tex; mode=display">H_{out} = \frac{H + p_{h1} + p_{h2} - k_h}{s_h} + 1</script><script type="math/tex; mode=display">W_{out} = \frac{W + p_{w1} + p_{w2} - k_w}{s_w} + 1</script><p>在卷积神经网络中，通常使用$2\times2$大小的池化窗口，步幅也使用2，填充为0，则输出特征图的尺寸为：</p><script type="math/tex; mode=display">H_{out} = \frac{H}{2}</script><script type="math/tex; mode=display">W_{out} = \frac{W}{2}</script><p>通过这种方式的池化，输出特征图的高和宽都减半，但通道数不会改变。</p><p>在下面的代码中的<code>pool2d</code>函数，我们实现池化层的前向传播。这类似于之前中的<code>corr2d</code>函数。然而，这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">X = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(X, X.shape)<span class="comment"># [3,3]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># X为需要池化的输入数据</span></span><br><span class="line">    <span class="comment"># pool_size为池化的大小</span></span><br><span class="line">    <span class="comment"># mode表示是以最大池化还是均值池化</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    <span class="comment"># 计算Y的形状</span></span><br><span class="line">    Y = paddle.zeros(shape=(X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 下面计算数值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 下面移动的步长都为1</span></span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>))</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,       [[0., 1., 2.],        [3., 4., 5.],        [6., 7., 8.]]) [3, 3]Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[4., 5.],        [7., 8.]])Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[2., 3.],        [5., 6.]])</code></pre><h3 id="5-2-填充和步幅"><a href="#5-2-填充和步幅" class="headerlink" title="5.2 填充和步幅"></a>5.2 填充和步幅</h3><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。</p><p>下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。我们首先构造了一个输入张量$X$，它有四个维度，其中样本数和通道数都是1。</p><p><code>paddle.nn.MaxPool2D(kernel_size, stride=None, padding=0)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.arange(<span class="number">16</span>, dtype=paddle.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[0. , 1. , 2. , 3. ],          [4. , 5. , 6. , 7. ],          [8. , 9. , 10., 11.],          [12., 13., 14., 15.]]]])</code></pre><p>默认情况下，深度学习框架中的步幅与池化窗口的大小相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 这里是2×2的池化窗口，步长为2，最大池化</span></span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[5. , 7. ],          [13., 15.]]]])</code></pre><p>当然，我们可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2D((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 这里2×3的池化窗口，高度2步长宽度3步长，高度方向不填充，宽度方向上左右各填充一列</span></span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 1, 2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[5. , 7. ],          [13., 15.]]]])</code></pre><h3 id="5-3-多个通道"><a href="#5-3-多个通道" class="headerlink" title="5.3 多个通道"></a>5.3 多个通道</h3><p>在处理多通道输入数据时，池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。这意味着<strong>池化层的输出通道数与输入通道数相同</strong>。</p><p>下面，我们将在通道维度上连结张量$X$和$X + 1$，以构建具有 2 个通道的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.concat((X, X + <span class="number">1</span>), <span class="number">1</span>)X</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 2, 4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[0. , 1. , 2. , 3. ],          [4. , 5. , 6. , 7. ],          [8. , 9. , 10., 11.],          [12., 13., 14., 15.]],         [[1. , 2. , 3. , 4. ],          [5. , 6. , 7. , 8. ],          [9. , 10., 11., 12.],          [13., 14., 15. 16.]]]])</code></pre><p>如下所示，池化后输出通道的数量仍然是2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2D(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 2, 2, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[[[5. , 7. ],          [13., 15.]],         [[6. , 8. ],          [14., 16.]]]])</code></pre><h2 id="六、批归一化（Batch-Normalization）"><a href="#六、批归一化（Batch-Normalization）" class="headerlink" title="六、批归一化（Batch Normalization）"></a>六、批归一化（Batch Normalization）</h2><p><a href="https://arxiv.org/abs/1502.03167">批归一化方法</a>（Batch Normalization，BatchNorm）是由Ioffe和Szegedy于2015年提出的，已被广泛应用在深度学习中，其目的是对神经网络中间层的输出进行标准化处理，使得中间层的输出更加稳定。</p><p>通常我们会对神经网络的数据进行标准化处理，处理后的样本数据集满足均值为0，方差为1的统计分布，这是因为当输入数据的分布比较固定时，有利于算法的稳定和收敛。对于深度神经网络来说，由于参数是不断更新的，即使输入数据已经做过标准化处理，但是对于比较靠后的那些层，其接收到的输入仍然是剧烈变化的，通常会导致数值不稳定，模型很难收敛。BatchNorm能够使神经网络中间层的输出变得更加稳定，并有如下三个优点：</p><ul><li><p>使学习快速进行（能够使用较大的学习率）</p></li><li><p>降低模型对初始值的敏感性</p></li><li><p>从一定程度上抑制过拟合</p></li></ul><p>BatchNorm主要思路是在训练时以mini-batch为单位，对神经元的数值进行归一化，使数据的分布满足均值为0，方差为1。具体计算过程如下：</p><p><strong>1. 计算mini-batch内样本的均值</strong></p><script type="math/tex; mode=display">\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^mx^{(i)}</script><p>其中$x^{(i)}$表示mini-batch中的第$i$个样本。</p><p>例如输入mini-batch包含3个样本，每个样本有2个特征，分别是：</p><script type="math/tex; mode=display">x^{(1)} = (1,2), \ \ x^{(2)} = (3,6), \ \ x^{(3)} = (5,10)</script><p>对每个特征分别计算mini-batch内样本的均值：</p><script type="math/tex; mode=display">\mu_{B0} = \frac{1+3+5}{3} = 3, \ \ \ \mu_{B1} = \frac{2+6+10}{3} = 6</script><p>则样本均值是:</p><script type="math/tex; mode=display">\mu_{B} = (\mu_{B0}, \mu_{B1}) = (3, 6)</script><p><strong>2. 计算mini-batch内样本的方差</strong></p><script type="math/tex; mode=display">\sigma_B^2 \leftarrow \frac{1}{m}\sum_{i=1}^m(x^{(i)} - \mu_B)^2</script><p>上面的计算公式先计算一个批次内样本的均值$\mu_B$和方差$\sigma_B^2$，然后再对输入数据做归一化，将其调整成均值为0，方差为1的分布。</p><p>对于上述给定的输入数据$x^{(1)}, x^{(2)}, x^{(3)}$，可以计算出每个特征对应的方差：</p><script type="math/tex; mode=display">\sigma_{B0}^2 = \frac{1}{3} \cdot ((1-3)^2 + (3-3)^2 + (5-3)^2) = \frac{8}{3}</script><script type="math/tex; mode=display">\sigma_{B1}^2 = \frac{1}{3} \cdot ((2-6)^2 + (6-6)^2 + (10-6)^2) = \frac{32}{3}</script><p>则样本方差是：</p><script type="math/tex; mode=display">\sigma_{B}^2 = (\sigma_{B0}^2, \sigma_{B1}^2) = (\frac{8}{3}, \frac{32}{3})</script><p><strong>3. 计算标准化之后的输出</strong></p><script type="math/tex; mode=display">\hat{x}^{(i)} \leftarrow \frac{x^{(i)} - \mu_B}{\sqrt{(\sigma_B^2 + \epsilon)}}</script><p>其中$\epsilon$是一个微小值（例如$1e-7$），其主要作用是为了防止分母为0。</p><p>对于上述给定的输入数据$x^{(1)}, x^{(2)}, x^{(3)}$，可以计算出标准化之后的输出：</p><script type="math/tex; mode=display">\hat{x}^{(1)} = (\frac{1 - 3}{\sqrt{\frac{8}{3}}}, \ \ \frac{2 - 6}{\sqrt{\frac{32}{3}}}) = (-\sqrt{\frac{3}{2}}, \ \ -\sqrt{\frac{3}{2}})</script><script type="math/tex; mode=display">\hat{x}^{(2)} = (\frac{3 - 3}{\sqrt{\frac{8}{3}}}, \ \ \frac{6 - 6}{\sqrt{\frac{32}{3}}}) = (0, \ \ 0) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \</script><script type="math/tex; mode=display">\hat{x}^{(3)} = (\frac{5 - 3}{\sqrt{\frac{8}{3}}}, \ \ \frac{10 - 6}{\sqrt{\frac{32}{3}}}) = (\sqrt{\frac{3}{2}}, \ \ \sqrt{\frac{3}{2}}) \ \ \ \</script><hr><p>如果强行限制输出层的分布是标准化的，可能会导致某些特征模式的丢失，所以在标准化之后，BatchNorm会紧接着对数据做缩放和平移。</p><script type="math/tex; mode=display">y_i \leftarrow \gamma \hat{x_i} + \beta</script><p>其中$\gamma$和$\beta$是可学习的参数，可以赋初始值$\gamma = 1, \beta = 0$，在训练过程中不断学习调整。</p><p>上面列出的是BatchNorm方法的计算逻辑，下面针对两种类型的输入数据格式分别进行举例。飞桨支持输入数据的维度大小为2、3、4、5四种情况，这里给出的是维度大小为$2$【表格】和$4$【图像】的示例。</p><ul><li><strong>示例一：</strong> 当输入数据形状是$[N, K]$【N为批量，K为实际的X的取值】时，一般对应全连接层的输出，示例代码如下所示。 </li></ul><p>这种情况下会分别对K的每一个分量计算N个样本的均值和方差，数据和参数对应如下：</p><ul><li>输入 x, [N, K]</li><li>输出 y, [N, K]</li><li>均值 $\mu_B$，[K, ]</li><li>方差 $\sigma_B^2$, [K, ]</li><li>缩放参数$\gamma$, [K, ]</li><li>平移参数$\beta$, [K, ]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据形状是 [N, K]时的示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> BatchNorm1D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用BatchNorm1D计算归一化的输出</span></span><br><span class="line"><span class="comment"># 输入数据维度[N, K]，num_features等于K</span></span><br><span class="line">bn = BatchNorm1D(num_features=<span class="number">3</span>)    <span class="comment"># 传入的是特征的个数</span></span><br><span class="line">x = paddle.to_tensor(data)</span><br><span class="line">y = bn(x)<span class="comment"># 将x放到批量归一化的函数中</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output of BatchNorm1D Layer: \n &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(y.numpy()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Numpy计算均值、方差和归一化的输出</span></span><br><span class="line"><span class="comment"># 这里对第0个特征进行验证</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>])<span class="comment">#[2,5,8] [3,6,9]是第2、3个特征</span></span><br><span class="line">a_mean = a.mean()</span><br><span class="line">a_std = a.std()</span><br><span class="line">b = (a - a_mean) / a_std</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;std &#123;&#125;, mean &#123;&#125;, \n output &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(a_mean, a_std, b))</span><br><span class="line"><span class="comment"># 可以看到如下结果是一样的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">output of BatchNorm1D Layer: </span></span><br><span class="line"><span class="string"> [[-1.2247438 -1.2247438 -1.2247438]</span></span><br><span class="line"><span class="string"> [ 0.         0.         0.       ]</span></span><br><span class="line"><span class="string"> [ 1.2247438  1.2247438  1.2247438]]</span></span><br><span class="line"><span class="string">std 4.0, mean 2.449489742783178, </span></span><br><span class="line"><span class="string"> output [-1.22474487  0.          1.22474487]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建议读者对第1和第2个特征进行验证，观察numpy计算结果与paddle计算结果是否一致</span></span><br></pre></td></tr></table></figure><ul><li><strong>示例二：</strong> 当输入数据形状是$[N, C, H, W]$时， 一般对应卷积层的输出，示例代码如下所示。</li></ul><p>这种情况下会沿着C这一维度进行展开，分别对每一个通道计算N个样本中总共$N\times H \times W$个像素点的均值和方差，数据和参数对应如下：</p><ul><li>输入 x, [N, C, H, W]</li><li>输出 y, [N, C, H, W]</li><li>均值 $\mu_B$，[C, ]</li><li>方差 $\sigma_B^2$, [C, ]</li><li>缩放参数$\gamma$, [C, ]</li><li>平移参数$\beta$, [C, ]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据形状是[N, C, H, W]时的batchnorm示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> BatchNorm2D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，这样可以保证每次运行结果一致</span></span><br><span class="line">np.random.seed(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">data = np.random.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用BatchNorm2D计算归一化的输出</span></span><br><span class="line"><span class="comment"># 输入数据维度[N, C, H, W]，num_features等于C</span></span><br><span class="line">bn = BatchNorm2D(num_features=<span class="number">3</span>)<span class="comment"># 传入的是通道的维数</span></span><br><span class="line">x = paddle.to_tensor(data)</span><br><span class="line">y = bn(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input of BatchNorm2D Layer: \n &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(x.numpy()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output of BatchNorm2D Layer: \n &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(y.numpy()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取出data中第0通道的数据，</span></span><br><span class="line"><span class="comment"># 使用numpy计算均值、方差及归一化的输出</span></span><br><span class="line">a = data[:, <span class="number">0</span>, :, :]</span><br><span class="line">a_mean = a.mean()</span><br><span class="line">a_std = a.std()</span><br><span class="line">b = (a - a_mean) / a_std</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;channel 0 of input data: \n &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;std &#123;&#125;, mean &#123;&#125;, \n output: \n &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(a_mean, a_std, b))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提示：这里通过numpy计算出来的输出</span></span><br><span class="line"><span class="comment"># 与BatchNorm2D算子的结果略有差别，</span></span><br><span class="line"><span class="comment"># 因为在BatchNorm2D算子为了保证数值的稳定性，</span></span><br><span class="line"><span class="comment"># 在分母里面加上了一个比较小的浮点数 epsilon=1e-05</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">input of BatchNorm2D Layer: </span></span><br><span class="line"><span class="string"> [[[[0.54340494 0.2783694  0.4245176 ]</span></span><br><span class="line"><span class="string">   [0.84477615 0.00471886 0.12156912]</span></span><br><span class="line"><span class="string">   [0.67074907 0.82585275 0.13670659]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[0.5750933  0.89132196 0.20920213]</span></span><br><span class="line"><span class="string">   [0.18532822 0.10837689 0.21969749]</span></span><br><span class="line"><span class="string">   [0.9786238  0.8116832  0.17194101]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[0.81622475 0.27407375 0.4317042 ]</span></span><br><span class="line"><span class="string">   [0.9400298  0.81764936 0.33611196]</span></span><br><span class="line"><span class="string">   [0.17541045 0.37283206 0.00568851]]]</span></span><br><span class="line"><span class="string">     [[0.98092085 0.05994199 0.89054596]</span></span><br><span class="line"><span class="string">   [0.5769015  0.7424797  0.63018394]</span></span><br><span class="line"><span class="string">   [0.5818422  0.02043913 0.21002658]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[0.5446849  0.76911515 0.25069523]</span></span><br><span class="line"><span class="string">   [0.2858957  0.8523951  0.9750065 ]</span></span><br><span class="line"><span class="string">   [0.8848533  0.35950786 0.59885895]]]]</span></span><br><span class="line"><span class="string">output of BatchNorm2D Layer: </span></span><br><span class="line"><span class="string"> [[[[ 0.4126078  -0.46198368  0.02029109]</span></span><br><span class="line"><span class="string">   [ 1.4071034  -1.3650038  -0.97940934]</span></span><br><span class="line"><span class="string">   [ 0.832831    1.344658   -0.9294571 ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 0.2520175   1.2038351  -0.84927964]</span></span><br><span class="line"><span class="string">   [-0.9211378  -1.1527538  -0.8176896 ]</span></span><br><span class="line"><span class="string">   [ 1.4666051   0.96413004 -0.961432  ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 0.9541142  -0.9075856  -0.36629617]</span></span><br><span class="line"><span class="string">   [ 1.37925     0.9590063  -0.6945517 ]</span></span><br><span class="line"><span class="string">   [-1.2463869  -0.5684581  -1.8291974 ]]]</span></span><br><span class="line"><span class="string">     [[ 1.473519   -1.2985382   1.2014993 ]</span></span><br><span class="line"><span class="string">   [ 0.25745988  0.7558342   0.41783488]</span></span><br><span class="line"><span class="string">   [ 0.27233088 -1.4174379  -0.8467981 ]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  [[ 0.02166975  0.79234385 -0.98786545]</span></span><br><span class="line"><span class="string">   [-0.86699003  1.0783203   1.4993572 ]</span></span><br><span class="line"><span class="string">   [ 1.1897788  -0.6142123   0.20769882]]]]</span></span><br><span class="line"><span class="string">   </span></span><br><span class="line"><span class="string">channel 0 of input data: </span></span><br><span class="line"><span class="string"> [[[0.54340494 0.2783694  0.4245176 ]</span></span><br><span class="line"><span class="string">  [0.84477615 0.00471886 0.12156912]</span></span><br><span class="line"><span class="string">  [0.67074907 0.82585275 0.13670659]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[0.25242636 0.7956625  0.01525497]</span></span><br><span class="line"><span class="string">  [0.5988434  0.6038045  0.10514768]</span></span><br><span class="line"><span class="string">  [0.38194343 0.03647606 0.89041156]]]</span></span><br><span class="line"><span class="string">std 0.4183686077594757, mean 0.3030227720737457, </span></span><br><span class="line"><span class="string"> output: </span></span><br><span class="line"><span class="string"> [[[ 0.41263014 -0.46200886  0.02029219]</span></span><br><span class="line"><span class="string">  [ 1.4071798  -1.3650781  -0.9794626 ]</span></span><br><span class="line"><span class="string">  [ 0.8328762   1.3447311  -0.92950773]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[-0.54762304  1.2451009  -1.3303081 ]</span></span><br><span class="line"><span class="string">  [ 0.5955816   0.61195374 -1.0336547 ]</span></span><br><span class="line"><span class="string">  [-0.12020606 -1.2602768   1.5577804 ]]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>预测时使用BatchNorm</strong></p><p>上面介绍了在训练过程中使用BatchNorm对一批样本进行归一化的方法，但如果使用同样的方法对需要预测的一批样本进行归一化，则预测结果会出现不确定性。</p><p>例如样本A、样本B作为一批样本计算均值和方差，与样本A、样本C和样本D作为一批样本计算均值和方差，得到的结果一般来说是不同的。那么样本A的预测结果就会变得不确定，这对预测过程来说是不合理的。解决方法是在训练过程中将大量样本的均值和方差保存下来，预测时直接使用保存好的值而不再重新计算。实际上，在BatchNorm的具体实现中，训练时会计算均值和方差的移动平均值。在飞桨中，默认是采用如下方式计算：</p><script type="math/tex; mode=display">saved\_\mu_B \leftarrow \ saved\_\mu_B \times 0.9 + \mu_B \times (1 - 0.9)</script><script type="math/tex; mode=display">saved\_\sigma_B^2 \leftarrow \ saved\_\sigma_B^2 \times 0.9 + \sigma_B^2 \times (1 - 0.9)</script><p>在训练过程的最开始将$saved_\mu_B$和$saved_\sigma_B^2$设置为0，每次输入一批新的样本，计算出$\mu_B$和$\sigma_B^2$，然后通过上面的公式更新$saved_\mu_B$和$saved_\sigma_B^2$，在训练的过程中不断的更新它们的值，并作为BatchNorm层的参数保存下来。预测的时候将会加载参数$saved_\mu_B$和$saved_\sigma_B^2$，用他们来代替$\mu_B$和$\sigma_B^2$，增加预测结果的可行性。</p><h2 id="七、小结"><a href="#七、小结" class="headerlink" title="七、小结"></a>七、小结</h2><ul><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li><li>填充可以在输入周围额为增加行和列，由此增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽，填充通常用来控制输出形状的减少。</li><li>步幅是每次滑动核窗口时行和列的步长，可以成倍减少输出的形状。</li><li>填充和步幅是卷积层的超参数。</li><li>输出通道数是卷积层的超参数。</li><li>每个输入通道有一个独立的二维卷积核，所有通道结果相加得到一个输出通道结果。每个输出通道有独立的三维卷积核。</li><li>当以每像素为基础应用时，$1\times 1$卷积层相当于全连接层。</li><li>$1\times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是减轻卷积层对位置的过度敏感。</li><li>我们可以指定池化层的填充和步幅。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>池化层的输出通道数与输入通道数相同。</li><li>池化层同样有窗口大小、填充和步幅作为超参数。池化层没有需要学习的参数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_卷积基本概念及经典模型复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习4.2-使用极简方法实现手写数字识别任务</title>
      <link href="/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.2-%E4%BD%BF%E7%94%A8%E6%9E%81%E7%AE%80%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1/"/>
      <url>/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.2-%E4%BD%BF%E7%94%A8%E6%9E%81%E7%AE%80%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="一、使用极简方法实现手写数字识别任务✍"><a href="#一、使用极简方法实现手写数字识别任务✍" class="headerlink" title="一、使用极简方法实现手写数字识别任务✍"></a>一、使用极简方法实现手写数字识别任务✍</h1><p>数字识别是计算机从纸质文档、照片或其他来源接收、理解并识别可读的数字的能力，目前比较受关注的是手写数字识别。手写数字识别是一个典型的图像分类问题，已经被广泛应用于汇款单号识别、手写邮政编码识别等领域，大大缩短了业务处理时间，提升了工作效率和质量。</p><p>在处理如 <strong>图1</strong> 所示的手写邮政编码的简单图像分类任务时，可以使用基于MNIST数据集的手写数字识别模型。MNIST是深度学习领域标准、易用的成熟数据集，包含 50000 条训练样本和 10000 条测试样本。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/04ab1f9d699e40659a4b69ee069a5136a15cb04bb9d848c2be536da68a8abe5e" width="800"></center><center><br>图1：手写数字识别任务示意图</br></center><ul><li>任务输入：一系列手写数字图片，其中每张图片都是 28x28 的像素矩阵。</li><li>任务输出：经过了大小归一化和居中处理，输出对应的 0~9 的数字标签。</li></ul><h2 id="MNIST-数据集"><a href="#MNIST-数据集" class="headerlink" title="MNIST 数据集"></a><strong><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> 数据集</strong></h2><p>MNIST 数据集是从 NIST（美国国家标准与技术研究院）的 Special Database 3（SD-3）和 Special Database 1（SD-1）构建而来。Yann LeCun等人从 SD-1 和 SD-3 中各取一半数据作为 MNIST 训练集和测试集。</p><hr><blockquote><p><strong>扩展</strong>：</p><p>MNIST 数据库由 NIST 的特殊数据库 3 和特殊数据库 1 构建而成，其中包含手写数字的二进制图像。NIST 最初指定 SD-3 作为他们的训练集，SD-1 作为他们的测试集。但是，SD-3 比 SD-1 更清晰，更容易识别。这是因为，在人口普查局工作人员中收集了 SD-3，而在高中生中收集了 SD-1。从学习实验中得出合理的结论，要求实验结果独立于完整样本集中训练集和测试的选择。因此，有必要混合NIST的数据集建立一个新的数据库。</p></blockquote><hr><p>MNIST数据集的发布，吸引了大量科学家训练模型。1998 年，LeCun 分别用单层线性分类器、多层感知器（Multilayer Perceptron, MLP）和多层卷积神经网络 LeNet 进行实验，使得测试集的误差不断下降（从12% 下降到0.7% ）。在研究过程中，LeCun 提出了卷积神经网络（Convolutional Neural Network，CNN），大幅度地提高了手写字符的识别能力，也因此成为了深度学习领域的奠基人之一。</p><p>如今在深度学习领域，卷积神经网络占据了至关重要的地位，从最早 LeCun 提出的简单 LeNet，到如今 ImageNet 大赛上的优胜模型 VGGNet、GoogLeNet、ResNet 等，人们在图像分类领域，利用卷积神经网络得到了一系列惊人的结果。</p><h2 id="构建流程对比"><a href="#构建流程对比" class="headerlink" title="构建流程对比"></a><strong>构建流程对比</strong></h2><p>使用飞桨完成手写数字识别模型任务的代码结构如 <strong>图2</strong> 所示，与使用飞桨完成房价预测模型任务的流程一致，下面我们将详细介绍每个步骤的具体实现方法和优化思路。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/38b467ff3b6e4705b9b4d34c6b13431073e449640ef847f396923475b11c913b" width="800" hegiht="" ></center><center><br>图2：使用飞桨框架构建神经网络过程</br></center><p><br></br></p><p>在探讨手写数字识别任务的实现方案之前，我们先看一下程序代码。不难发现，与上一章学习的房价预测任务的代码比较，二者是极为相似的，如 <strong>图3</strong> 所示。<br><br></br></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/30041d662b894ef387e5f97328c6f9c149f67a6463904413ae78251158014fa2" width="1200" hegiht="" ></center><center><br>图3：房价预测和手写数字识别的实现代码“神似”</br></center><p><br></br></p><ul><li><p>从代码结构上看，模型均为<code>数据处理、定义网络结构和训练过程</code>三个部分。</p></li><li><p>从代码细节来看，两个模型也很相似。</p></li></ul><p>这就是使用飞桨框架搭建深度学习模型的优势，只要完成一个模型的案例学习，其它任务即可触类旁通。在工业实践中，程序员用飞桨框架搭建模型，无需每次都另起炉灶，多数情况是先在飞桨模型库中寻找与目标任务类似的模型，再在该模型的基础上修改少量代码即可完成新的任务。</p><h1 id="二、通过极简方案快速构建手写数字识别模型✍"><a href="#二、通过极简方案快速构建手写数字识别模型✍" class="headerlink" title="二、通过极简方案快速构建手写数字识别模型✍"></a>二、通过极简方案快速构建手写数字识别模型✍</h1><p>在数据处理前，首先要加载飞桨平台与“手写数字识别”模型相关的类库，实现方法如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载飞桨和相关类库</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1.数据处理"></a><strong>1.数据处理</strong></h2><p>飞桨提供了多个封装好的数据集API，涵盖计算机视觉、自然语言处理、推荐系统等多个领域，帮助读者快速完成深度学习任务。如在手写数字识别任务中，通过<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/datasets/MNIST_cn.html">paddle.vision.datasets.MNIST</a>可以直接获取处理好的MNIST训练集、测试集。</p><p>通过<code>paddle.vision.datasets.MNIST</code> API设置数据读取器，代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置数据读取器，API自动读取MNIST数据训练集</span></span><br><span class="line">train_dataset = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>)</span><br></pre></td></tr></table></figure><p> 通过如下代码读取任意一个数据内容，观察打印结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_data0 = np.array(train_dataset[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">train_label_0 = np.array(train_dataset[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(<span class="string">&quot;Image&quot;</span>) <span class="comment"># 图像窗口名称</span></span><br><span class="line">plt.figure(figsize=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># 显示第一batch的第一个图像</span></span><br><span class="line">plt.imshow(train_data0, cmap=<span class="string">&#x27;gray&#x27;</span>)<span class="comment">#gray灰色</span></span><br><span class="line">plt.title(<span class="string">&#x27;image&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像数据形状和对应数据为:&quot;</span>, train_data0.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像标签形状和对应数据为:&quot;</span>, train_label_0.shape, train_label_0)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;打印第一个batch的第一个图像，对应标签数字为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_label_0))</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/42.png" alt="png"></p><pre><code>图像数据形状和对应数据为: (28, 28)图像标签形状和对应数据为: (1,) [5]打印第一个batch的第一个图像，对应标签数字为[5]</code></pre><blockquote><p><strong>扩展</strong>：</p><p><code>matplotlib.pyplot.imshow(X, cmap=None)</code>：cmap 即颜色图谱（colormap)，默认绘制为 RGB(A) 颜色空间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">autumn 红-橙-黄</span><br><span class="line">bone 黑-白</span><br><span class="line">cool 青-洋红</span><br><span class="line">copper 黑-铜</span><br><span class="line">flag 红-白-蓝-黑</span><br><span class="line">gray 黑-白</span><br><span class="line">hot 黑-红-黄-白</span><br><span class="line">hsv 红-黄-绿-青-蓝-洋红-红</span><br><span class="line">inferno 黑-红-黄</span><br><span class="line">jet 蓝-青-黄-红</span><br><span class="line">magma 黑-红-白</span><br><span class="line">pink 黑-粉-白</span><br><span class="line">plasma 绿-红-黄</span><br><span class="line">prism 红-黄-绿-蓝-紫-...-绿模式</span><br><span class="line">spring 洋红-黄</span><br><span class="line">summer 绿-黄</span><br><span class="line">viridis 蓝-绿-黄</span><br><span class="line">winter 蓝-绿</span><br></pre></td></tr></table></figure></blockquote><p>接下来我们将读取的数据进行数据预处理，执行归一化，将数据范围为 [0, 255] 的图像归一化到 [0, 1] 。最后将图像调整为<code>[batch_size, H*W]</code>的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像归一化函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">norm_img</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="comment"># 验证传入数据格式是否正确，img的shape为[batch_size, 28, 28]</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(img.shape) == <span class="number">3</span></span><br><span class="line">    batch_size, img_h, img_w = img.shape[<span class="number">0</span>], img.shape[<span class="number">1</span>], img.shape[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化图像数据</span></span><br><span class="line">    img = img / <span class="number">255</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将img图像reshape为[batch_size, 784]</span></span><br><span class="line">    img = paddle.reshape(img, [batch_size, img_h*img_w])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机生成一张图像序列: (批量, 高, 宽)</span></span><br><span class="line">image = paddle.rand([<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line"><span class="built_in">print</span>(image.shape)<span class="comment"># 打印图片形状</span></span><br><span class="line">norm_img(image)<span class="comment"># 调用函数，对图片进行处理</span></span><br></pre></td></tr></table></figure><h2 id="2-模型设计"><a href="#2-模型设计" class="headerlink" title="2.模型设计"></a><strong>2.模型设计</strong></h2><p>在房价预测深度学习任务中，我们使用了单层且没有非线性变换的模型，取得了理想的预测效果。在手写数字识别中，我们依然使用这个模型预测输入的图形数字值。其中，模型的输入为 784 维（28×28）数据，输出为1维数据，如 <strong>图4</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/9c146e7d9c4a4119a8cd09f7c8b5ee61f2ac1820a221429a80430291728b9c4a" width="400" hegiht="" ></center><center><br>图4：手写数字识别网络模型</br></center><p><br></br></p><p>输入像素的位置排布信息对理解图像内容非常重要，因此网络的输入设计为 28×28 的尺寸，而不是 1×784 ，以便于模型能够正确处理像素之间的空间信息。</p><hr><p><strong>说明：</strong></p><p>事实上，采用只有一层的简单网络（对输入求加权和）时并没有处理位置关系信息，因此可以猜测出此模型的预测效果可能有限。在后续优化环节介绍的卷积神经网络则更好的考虑了这种位置关系信息，模型的预测效果也会有显著提升。</p><hr><p>下面以类的方式组建手写数字识别的网络，实现方法如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义mnist数据识别网络结构，同房价预测网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MNIST</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MNIST, self).__init__()<span class="comment"># 父类函数模型的初始化，参数可写可不写</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义一层全连接层，输出维度是1</span></span><br><span class="line">        self.fc = paddle.nn.Linear(in_features=<span class="number">784</span>, out_features=<span class="number">1</span>)<span class="comment"># 输入784 输出1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 定义网络结构的前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = self.fc(inputs)<span class="comment">#这里没有任何非线性的转换，直接进行加权和的计算输出</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h2 id="3-训练配置"><a href="#3-训练配置" class="headerlink" title="3.训练配置"></a><strong>3.训练配置</strong></h2><p>训练配置需要先生成模型实例（设为“训练”状态），再设置优化算法和学习率（使用随机梯度下降SGD，学习率设置为0.001），实现方法如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明网络结构</span></span><br><span class="line">model = MNIST()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># 启动训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 加载训练集 batch_size 设为 16</span></span><br><span class="line">    train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>), </span><br><span class="line">                                        batch_size=<span class="number">16</span>, </span><br><span class="line">                                        shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001，更新参数为模型所有的参数</span></span><br><span class="line">    opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.001</span>, parameters=model.parameters())</span><br></pre></td></tr></table></figure><h2 id="4-训练过程"><a href="#4-训练过程" class="headerlink" title="4.训练过程"></a><strong>4.训练过程</strong></h2><p>训练过程采用二层循环嵌套方式，训练完成后需要保存模型参数，以便后续使用。</p><ul><li><code>内层循环</code>：负责整个数据集的一次遍历，遍历数据集采用分批次（batch）方式。</li><li><code>外层循环</code>：定义遍历数据集的次数，本次训练中外层循环 10 次，通过参数 EPOCH_NUM 设置。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="comment"># 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型</span></span><br><span class="line">paddle.vision.set_image_backend(<span class="string">&#x27;cv2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明网络结构</span></span><br><span class="line">model = MNIST()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># 启动训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 加载训练集 batch_size 设为 16</span></span><br><span class="line">    train_loader = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>), </span><br><span class="line">                                        batch_size=<span class="number">16</span>, </span><br><span class="line">                                        shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001</span></span><br><span class="line">    opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.001</span>, parameters=model.parameters())</span><br><span class="line">    </span><br><span class="line">    EPOCH_NUM = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            <span class="comment"># 将图像数据转换为 float32 类型，为的是神经网络可以识别</span></span><br><span class="line">            images = norm_img(data[<span class="number">0</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            labels = data[<span class="number">1</span>].astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向计算的过程</span></span><br><span class="line">            predicts = model(images)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = F.square_error_cost(predicts, labels)</span><br><span class="line">            avg_loss = paddle.mean(loss)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#每训练了1000批次的数据，打印下当前Loss的情况一共是 50000/16 次，每1000次打印一下</span></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch_id: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, batch_id, avg_loss.numpy()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#后向传播，更新参数的过程</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.clear_grad()</span><br><span class="line">            </span><br><span class="line">train(model)<span class="comment">#调用函数 进行训练</span></span><br><span class="line"></span><br><span class="line">paddle.save(model.state_dict(), <span class="string">&#x27;data/mnist.pdparams&#x27;</span>)<span class="comment"># 保存参数</span></span><br></pre></td></tr></table></figure><pre><code>epoch_id: 0, batch_id: 0, loss is: [25.360065]epoch_id: 0, batch_id: 1000, loss is: [5.9149103]epoch_id: 0, batch_id: 2000, loss is: [5.9967823]epoch_id: 0, batch_id: 3000, loss is: [2.5753934]。。。epoch_id: 9, batch_id: 0, loss is: [2.2884061]epoch_id: 9, batch_id: 1000, loss is: [3.1237333]epoch_id: 9, batch_id: 2000, loss is: [3.7671528]epoch_id: 9, batch_id: 3000, loss is: [2.7026415]</code></pre><p>另外，从训练过程中损失所发生的变化可以发现，虽然损失整体上在降低，但到训练的最后一轮，损失函数值依然较高。可以猜测手写数字识别完全复用房价预测的代码，训练效果并不好。接下来我们通过模型测试，获取模型训练的真实效果。</p><h2 id="5-模型测试"><a href="#5-模型测试" class="headerlink" title="5.模型测试"></a><strong>5.模型测试</strong></h2><p>模型测试的主要目的是验证训练好的模型是否能正确识别出数字，包括如下四步：</p><ul><li>声明实例</li><li>加载模型：加载训练过程中保存的模型参数，</li><li>灌入数据：将测试样本传入模型，模型的状态设置为校验状态（eval），显式告诉框架我们接下来只会使用前向计算的流程，不会计算梯度和梯度反向传播。</li><li>获取预测结果，取整后作为预测标签输出。</li></ul><p>在模型测试之前，需要先从<code>&#39;./work/test.png&#39;</code>文件中读取样例图片，并进行归一化处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入图像读取第三方库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./work/test.jpg&#x27;</span></span><br><span class="line"><span class="comment"># 读取原始图像并显示</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">plt.imshow(im)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/43.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将原始图像转为灰度图</span></span><br><span class="line">im = im.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始图像shape: &#x27;</span>, np.array(im).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Image.ANTIALIAS方式采样原始图片 --&gt; 此处由模型输入维度决定，实为对测试图像的预处理</span></span><br><span class="line">im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">plt.imshow(im)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;采样后图片shape: &quot;</span>, np.array(im).shape)</span><br></pre></td></tr></table></figure><pre><code>原始图像shape:  (28, 28)采样后图片shape:  (28, 28)</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/44.png" alt="png"></p><p>接下来我们将上面对测试图像的处理方式封装为函数，方便调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取一张本地的样例图片，转变成模型输入的格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">img_path</span>):</span><br><span class="line">    <span class="comment"># 从img_path中读取图像，并转为灰度图</span></span><br><span class="line">    im = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">    im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">    im = np.array(im).reshape(<span class="number">1</span>, -<span class="number">1</span>).astype(np.float32) <span class="comment"># 将原图像调整为(1, h*w)的形状</span></span><br><span class="line">    <span class="comment"># 图像归一化，保持和数据集的数据范围一致，因为一个是白低黑字，一个是黑底白字</span></span><br><span class="line">    im = <span class="number">1</span> - im / <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><p>开启预测过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义预测过程</span></span><br><span class="line">model = MNIST()</span><br><span class="line">params_file_path = <span class="string">&#x27;data/mnist.pdparams&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;./work/test.jpg&#x27;</span></span><br><span class="line"><span class="comment"># 加载模型参数</span></span><br><span class="line">param_dict = paddle.load(params_file_path)</span><br><span class="line">model.load_dict(param_dict)</span><br><span class="line"><span class="comment"># 灌入数据</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">tensor_img = load_image(img_path)</span><br><span class="line">result = model(paddle.to_tensor(tensor_img))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result&#x27;</span>,result)</span><br><span class="line"><span class="comment">#  预测输出取整，即为预测的数字，打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;本次预测的数字是&quot;</span>, result.numpy().astype(<span class="string">&#x27;int32&#x27;</span>))</span><br></pre></td></tr></table></figure><pre><code>result Tensor(shape=[1, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[1.14087784]])本次预测的数字是 [[1]]</code></pre><p>这里只是验证了一个样本的情况，如果我们尝试更多的样本，可发现许多数字图片识别结果是错误的。因此完全复用房价预测的实验并不适用于手写数字识别任务！</p><p>接下来我们会对手写数字识别实验模型进行逐一改进，直到获得令人满意的结果。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_基础模型实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习4.1-使用飞桨实现房价预测任务</title>
      <link href="/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.1-%E4%BD%BF%E7%94%A8%E9%A3%9E%E6%A1%A8%E5%AE%9E%E7%8E%B0%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1/"/>
      <url>/2022/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04.1-%E4%BD%BF%E7%94%A8%E9%A3%9E%E6%A1%A8%E5%AE%9E%E7%8E%B0%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="🏡使用飞桨重写波士顿房价预测任务"><a href="#🏡使用飞桨重写波士顿房价预测任务" class="headerlink" title="🏡使用飞桨重写波士顿房价预测任务"></a><strong>🏡使用飞桨重写波士顿房价预测任务</strong></h1><p>学习本节，希望你能够掌握以下知识点：</p><ul><li>了解辨别基础模型的方法；</li><li>熟悉进行预测任务与搭建基础模型的流程；</li><li>掌握层与块的基本概念，掌握自定义层的方法。</li></ul><hr><p>波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的 “Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型。</p><p>如<strong>图1</strong>所示，对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/abce0cb2a92f4e679c6855cfa520491597171533a0b0447e8d51d904446e213e" width="550" hegiht="" ></center><center><br>图1：波士顿房价影响因素示意图</br></center><p>深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 <strong>图2</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/17a932875f5f4f28a62bf060f96678618094841fbfb54f098eac798bf0e44ca6" width="700" hegiht="" ></center><center><br>图2：使用飞桨框架构建神经网络过程</br></center><p>正是由于深度学习的建模和训练的过程存在通用性，在构建不同的模型时，只有<strong>模型三要素</strong>【模型、损失、优化器】不同，其它步骤基本一致，深度学习框架才有用武之地。在数据处理之前，需要先加载飞桨框架的相关类库。</p><hr><p><strong>扩展：</strong> 端到端的学习</p><p><strong>端到端学习 (End-to-End Learning)</strong>，也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标。通俗来讲，其实就是不做其他额外处理，从原始数据输入到任务结果输出，整个训练和预测过程都是在模型里完成的。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载飞桨、NumPy和相关类库</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><p>代码中参数含义如下：</p><ul><li><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/Overview_cn.html">paddle</a>：飞桨的主库，paddle 根目录下保留了常用API的别名，当前包括：paddle.tensor、paddle.framework、paddle.device目录下的所有API；</li><li><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/Linear_cn.html#linear">Linear</a>：神经网络的全连接层函数，包含所有输入权重相加的基本神经元结构。在房价预测任务中，使用只有一层的神经网络（全连接层）实现线性回归模型。</li><li><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/Overview_cn.html#paddle-nn">paddle.nn</a>：组网相关的API，包括 Linear、卷积 Conv2D、循环神经网络LSTM、损失函数CrossEntropyLoss、激活函数ReLU等；</li><li>paddle.nn.functional：与paddle.nn一样，包含组网相关的API，如：Linear、激活函数ReLU等，二者包含的同名模块功能相同，运行性能也基本一致。 差别在于paddle.nn目录下的模块均是类，每个类自带模块参数；<strong>paddle.nn.functional目录下的模块均是函数，需要手动传入函数计算所需要的参数。</strong> 在实际使用时，卷积、全连接层等本身具有可学习的参数，建议使用paddle.nn；而激活函数、池化等操作没有可学习参数，可以考虑使用paddle.nn.functional。</li></ul><hr><p><strong>说明：</strong></p><p>飞桨支持两种深度学习建模编写方式，更方便调试的动态图模式和性能更好并便于部署的静态图模式。</p><ul><li>动态图模式（命令式编程范式，类比Python）：解析式的执行方式。用户无需预先定义完整的网络结构，每写一行网络代码，即可同时获得计算结果；</li><li>静态图模式（声明式编程范式，类比C++）：先编译后执行的方式。用户需预先定义完整的网络结构，再对网络结构进行编译优化后，才能执行获得计算结果。</li></ul><p>飞桨框架2.0及之后的版本，默认使用动态图模式进行编码，同时提供了完备的动转静支持，开发者仅需添加一个装饰器（ <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/jit/to_static_cn.html#to-static">to_static</a> ），飞桨会自动将动态图的程序转换为静态图的program，并使用该program训练并可保存静态模型以实现推理部署。</p><hr><h2 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1 数据处理"></a><strong>1 数据处理</strong></h2><p>数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装<code>load data</code>函数。数据预处理后，才能被模型调用。</p><h3 id="1-1-读入数据"><a href="#1-1-读入数据" class="headerlink" title="1.1 读入数据"></a><strong>1.1 读入数据</strong></h3><p>通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下<code>work/housing.data</code>文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入需要用到的package</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># 读入训练数据</span></span><br><span class="line">datafile = <span class="string">&#x27;work/housing.data&#x27;</span></span><br><span class="line">data = np.fromfile(datafile, sep=<span class="string">&#x27; &#x27;</span>, dtype=np.float32)</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[6.320e-03 1.800e+01 2.310e+00 ... 3.969e+02 7.880e+00 1.190e+01] (7084,)</span><br><span class="line">读入的数据是不标准的，是全堆在一列，</span><br><span class="line">这里可以看到读到了7084行，所以需要对数据进行形状变换</span><br></pre></td></tr></table></figure><h3 id="1-2-数据形状变换"><a href="#1-2-数据形状变换" class="headerlink" title="1.2 数据形状变换"></a><strong>1.2 数据形状变换</strong></h3><p>由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$X$（影响房价的特征）和一个$Y$（该类型房屋的均价）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... </span></span><br><span class="line"><span class="comment"># 这里对原始数据做reshape，变成N x 14的形式</span></span><br><span class="line">feature_names = [ <span class="string">&#x27;CRIM&#x27;</span>, <span class="string">&#x27;ZN&#x27;</span>, <span class="string">&#x27;INDUS&#x27;</span>, <span class="string">&#x27;CHAS&#x27;</span>, <span class="string">&#x27;NOX&#x27;</span>, <span class="string">&#x27;RM&#x27;</span>, <span class="string">&#x27;AGE&#x27;</span>,<span class="string">&#x27;DIS&#x27;</span>, </span><br><span class="line">                 <span class="string">&#x27;RAD&#x27;</span>, <span class="string">&#x27;TAX&#x27;</span>, <span class="string">&#x27;PTRATIO&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;LSTAT&#x27;</span>, <span class="string">&#x27;MEDV&#x27;</span> ]</span><br><span class="line">feature_num = <span class="built_in">len</span>(feature_names)</span><br><span class="line">data = data.reshape([data.shape[<span class="number">0</span>] // feature_num, feature_num])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">x = data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br></pre></td></tr></table></figure><pre><code>(14,)[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01](506, 14)</code></pre><h3 id="1-3-数据集划分"><a href="#1-3-数据集划分" class="headerlink" title="1.3 数据集划分"></a><strong>1.3 数据集划分</strong></h3><p>将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，而不能直接应用于模型训练呢？这与学生时代的授课和考试关系比较类似，如 <strong>图3</strong> 所示。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a1c845a50e28474d9aa72028edfea33f1a3deca1d54d40ec94ba366d3a18c408" width="600" hegiht="" ></center><center><br>图3：训练集和测试集拆分的意义</br></center><p>上学时总有一些自作聪明的同学，平时不认真学习，考试前临阵抱佛脚，将习题死记硬背下来，但是成绩往往并不好。因为学校期望学生掌握的是知识，而不仅仅是习题本身。另出新的考题，才能鼓励学生努力去掌握习题背后的原理。同样我们期望模型学习的是任务的本质规律，而不是训练数据本身，模型训练未使用的数据，才能更真实的评估模型的效果。</p><p>在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">0.8</span></span><br><span class="line">offset = <span class="built_in">int</span>(data.shape[<span class="number">0</span>] * ratio)</span><br><span class="line">training_data = data[:offset]</span><br><span class="line">test_data = data[offset:]</span><br><span class="line">training_data.shape</span><br><span class="line">text_data.shape</span><br></pre></td></tr></table></figure><pre><code>(404, 14)(102, 14)</code></pre><h3 id="1-4-数据归一化处理"><a href="#1-4-数据归一化处理" class="headerlink" title="1.4 数据归一化处理"></a><strong>1.4 数据归一化处理</strong></h3><p>对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：<strong>一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度</strong>（因为每个特征值本身的范围相同）。归一化公式如下：</p><script type="math/tex; mode=display">x' = \frac{x-x_{min}}{x_{max}-x_{min}}</script><center><img src="https://ai-studio-static-online.cdn.bcebos.com/2eb06823355c4aa9bf25a95c4e259ae54f02b15e00fb4a119d5ed13a3adc0b67" width="650" hegiht="" ></center><center><br>图4：数据归一化的优势</br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算train数据集的最大值，最小值，平均值</span></span><br><span class="line">maximums, minimums, avgs = training_data.<span class="built_in">max</span>(axis=<span class="number">0</span>), \</span><br><span class="line">                            training_data.<span class="built_in">min</span>(axis=<span class="number">0</span>),\</span><br><span class="line">                            training_data.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / training_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行归一化处理</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_num):</span><br><span class="line">    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])</span><br></pre></td></tr></table></figure><h3 id="1-5-封装成load-data函数"><a href="#1-5-封装成load-data函数" class="headerlink" title="1.5 封装成load data函数"></a><strong>1.5 封装成load data函数</strong></h3><p>将上述几个数据处理操作封装成<code>load data</code>函数，以便下一步模型的调用，实现方法如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    <span class="comment"># 从文件导入数据</span></span><br><span class="line">    datafile = <span class="string">&#x27;./work/housing.data&#x27;</span></span><br><span class="line">    data = np.fromfile(datafile, sep=<span class="string">&#x27; &#x27;</span>, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数</span></span><br><span class="line">    feature_names = [ <span class="string">&#x27;CRIM&#x27;</span>, <span class="string">&#x27;ZN&#x27;</span>, <span class="string">&#x27;INDUS&#x27;</span>, <span class="string">&#x27;CHAS&#x27;</span>, <span class="string">&#x27;NOX&#x27;</span>, <span class="string">&#x27;RM&#x27;</span>, <span class="string">&#x27;AGE&#x27;</span>, \</span><br><span class="line">                      <span class="string">&#x27;DIS&#x27;</span>, <span class="string">&#x27;RAD&#x27;</span>, <span class="string">&#x27;TAX&#x27;</span>, <span class="string">&#x27;PTRATIO&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;LSTAT&#x27;</span>, <span class="string">&#x27;MEDV&#x27;</span> ]</span><br><span class="line">    feature_num = <span class="built_in">len</span>(feature_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将原始数据进行Reshape，变成[N, 14]这样的形状</span></span><br><span class="line">    data = data.reshape([data.shape[<span class="number">0</span>] // feature_num, feature_num])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将原数据集拆分成训练集和测试集</span></span><br><span class="line">    <span class="comment"># 这里使用80%的数据做训练，20%的数据做测试</span></span><br><span class="line">    <span class="comment"># 测试集和训练集必须是没有交集的</span></span><br><span class="line">    ratio = <span class="number">0.8</span></span><br><span class="line">    offset = <span class="built_in">int</span>(data.shape[<span class="number">0</span>] * ratio)</span><br><span class="line">    training_data = data[:offset]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算train数据集的最大值，最小值</span></span><br><span class="line">    maximums, minimums = training_data.<span class="built_in">max</span>(axis=<span class="number">0</span>), training_data.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 记录数据的归一化参数，在预测时对数据做归一化[方便后续的反归一化的使用]</span></span><br><span class="line">    <span class="keyword">global</span> max_values</span><br><span class="line">    <span class="keyword">global</span> min_values</span><br><span class="line">   </span><br><span class="line">    max_values = maximums</span><br><span class="line">    min_values = minimums</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对数据进行归一化处理</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_num):</span><br><span class="line">        data[:, i] = (data[:, i] - min_values[i]) / (maximums[i] - minimums[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集和测试集的划分比例</span></span><br><span class="line">    training_data = data[:offset]</span><br><span class="line">    test_data = data[offset:]</span><br><span class="line">    <span class="keyword">return</span> training_data, test_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">training_data, test_data = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line"><span class="built_in">print</span>(training_data.shape, test_data.shape)</span><br><span class="line"><span class="built_in">print</span>(training_data[<span class="number">1</span>,:])<span class="comment">#第一条样本</span></span><br></pre></td></tr></table></figure><pre><code>(404, 14) (102, 14)[2.35922547e-04 0.00000000e+00 2.62405723e-01 0.00000000e+00 1.72839552e-01 5.47997713e-01 7.82698274e-01 3.48961979e-01 4.34782617e-02 1.14822544e-01 5.53191364e-01 1.00000000e+00 2.04470202e-01 3.68888885e-01]</code></pre><h2 id="2-模型设计"><a href="#2-模型设计" class="headerlink" title="2 模型设计"></a><strong>2 模型设计</strong></h2><h3 id="【层与块】"><a href="#【层与块】" class="headerlink" title="【层与块】"></a><strong>【层与块】</strong></h3><p>前几篇讨论的模型复杂度有限，最复杂的是含一个隐藏层的多层感知机模型，然而实际情况中，这种模型过于简单，表达效果不够好，因此需要研究较为复杂的模型。然而较为复杂的模型也是由简单模型组成，因此有必要研究简单模型如何组成复杂模型。事实证明可以通过 “堆叠” 简单模型实现，为了描述如何堆叠，引入<strong>块（block）</strong> 的概念。块可以描述单层、多层组成的组件或者整个模型。使用块的一个好处是可以将一些块组合成更大的组件，这一过程通常是递归的，因此可以通过定义代码来生成任意复杂度的块，可以通过简洁的代码实现复杂的神经网络。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/ef8b24068e894be8ab3fede36a6ad7ab4b275bbd21ba409b893f3ad8b58ffb20" width="700" hegiht="" ></center><center><br>图5：“堆叠”的块</br></center><p>事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。对于多层感知机而言，整个模型及其组成层都是这种架构。 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。</p><p>在构造自定义块之前，我们先回顾一下多层感知机的代码。下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F <span class="comment"># nn下的实用功能组件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义具有一个隐藏层的多层感知机，如上图5的第一个图</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2是批量大小，20是X的维度值，所以这里是传进来了两条20维的样本</span></span><br><span class="line"><span class="comment"># 这里必须是20，因为上面定义网络时接受的数据就是20维</span></span><br><span class="line">X = paddle.rand((<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">net(X)<span class="comment"># 把x放进网络中查看输出结果</span></span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 20], dtype=float32, place=Place(cpu), stop_gradient=True,       [[0.10209441, 0.16913104, 0.44511813, 0.28342801, 0.65742350, 0.44651571,         0.36694321, 0.03608739, 0.25556597, 0.75385630, 0.59597689, 0.00202843,         0.81059551, 0.68003124, 0.18284233, 0.79699272, 0.73747963, 0.67146826,         0.55906087, 0.61247164],        [0.69550031, 0.46067774, 0.51894343, 0.93576533, 0.81870192, 0.48760650,         0.13647318, 0.78110564, 0.49254259, 0.61881417, 0.73745596, 0.97040987,         0.49153054, 0.17309034, 0.07548618, 0.78019077, 0.56080866, 0.51286954,         0.66860092, 0.46016651]])Tensor(shape=[2, 10], dtype=float32, place=Place(cpu), stop_gradient=False,       [[ 0.02718867,  0.33097127, -0.22596562, -0.10938051,  0.30752018,          0.09025095, -0.10254925,  0.09890905, -0.44127771,  0.13845670],        [-0.18452074,  0.47807249, -0.28389767, -0.22750369,  0.37274802,          0.17845176, -0.08270003, -0.07816530, -0.30161276,  0.00834796]])</code></pre><p>在这个例子中，我们通过实例化<code>nn.Sequential</code>来构建我们的模型， <code>nn.Sequential</code>定义了一种特殊的<code>Module</code>（块）， 它维护了一个由<code>Module</code>组成的有序列表。</p><h3 id="【自定义块】"><a href="#【自定义块】" class="headerlink" title="【自定义块】"></a><strong>【自定义块】</strong></h3><p>模型定义的实质是定义线性回归的网络结构，飞桨建议通过创建Python类的方式完成模型网络的定义，该类需要继承paddle.nn.Layer父类，并且在类中定义<code>init</code>函数和<code>forward</code>函数。<code>forward</code>函数是框架指定实现前向计算逻辑的函数，程序在调用模型实例时会自动执行，<code>forward</code>函数中使用的网络层需要在<code>init</code>函数中声明。</p><ul><li><strong>定义<code>init</code>函数</strong>：在类的初始化函数中声明每一层网络的实现函数。</li><li><strong>定义<code>forward</code>函数</strong>：构建神经网络结构，实现前向计算过程，并返回预测结果。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 自定义块Sequential</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Layer):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()<span class="comment"># 调用MLP的父类Module的构造函数来执行必要的函数初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数。</span></span><br><span class="line">        self.hidden = nn.Linear(in_features=<span class="number">20</span>, out_features=<span class="number">256</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 输出层</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))<span class="comment">#函数调用方式：调用ReLu激活函数</span></span><br><span class="line"></span><br><span class="line">X = paddle.rand((<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">net = MLP()<span class="comment">#将MLP封装起来，封装成一个net网络</span></span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 10], dtype=float32, place=Place(cpu), stop_gradient=False,       [[-0.12201045, -0.15452111,  0.06742557, -0.00087534,  0.01926781,         -0.25647265, -0.11357559,  0.21616164,  0.00257475, -0.01310621],        [ 0.02261407, -0.16433953,  0.07217564, -0.11244217,  0.14721063,         -0.20428163, -0.14929581,  0.06752300,  0.04719778,  0.1380767    7]])</code></pre><p><strong>扩展：</strong> Module 类</p><p>Module 是一个基类，每次我们要搭建自己的神经网络的时候都要继承这个类，继承这个类会使得我们搭建网络的过程变得异常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *<span class="built_in">input</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_module</span>(<span class="params">self,name,module</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cuda</span>(<span class="params">self,device=<span class="literal">None</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cpu</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *<span class="built_in">input</span>, **kwargs</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parameters</span>(<span class="params">self, recurse=<span class="literal">True</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">named_parameters</span>(<span class="params">self, prefix=<span class="string">&#x27;&#x27;</span>, recurse=<span class="literal">True</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">children</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">named_children</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">modules</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">named_modules</span>(<span class="params">self,memo=<span class="literal">None</span>,prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,mode=<span class="literal">True</span></span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__dir__</span>(<span class="params">self</span>):</span><br></pre></td></tr></table></figure><p>我们可以使用这种方法来任意搭建神经网络。下面是一个<code>输入维度为20、输出维度为5，两个隐藏层、使用ReLU激活函数</code>的神经网络模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">myMLP</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()<span class="comment">#先调用一下父类，进行函数的初始化</span></span><br><span class="line">        self.hidden1 = nn.Linear(<span class="number">20</span>, <span class="number">32</span>)<span class="comment">#定义一个20输入32输出的隐藏层</span></span><br><span class="line">        self.hidden2 = nn.Linear(<span class="number">32</span>, <span class="number">64</span>)<span class="comment">#定义一个32输入64输出的隐藏层</span></span><br><span class="line">        self.act = nn.ReLU()<span class="comment">#层搭建方式：定义一个人ReLu激活函数</span></span><br><span class="line">        self.output = nn.Linear(<span class="number">64</span>, <span class="number">5</span>)<span class="comment">#定义一个64输入5输出的输出层</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):<span class="comment">#前向传播函数</span></span><br><span class="line">        y = self.hidden1(X)</span><br><span class="line">        y = self.act(y)</span><br><span class="line">        y = self.hidden2(y)</span><br><span class="line">        y = self.act(y)</span><br><span class="line">        y = self.output(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">X = paddle.rand((<span class="number">2</span>, <span class="number">20</span>))<span class="comment">#传入的X在这里必须是20的样本</span></span><br><span class="line"></span><br><span class="line">net = myMLP()<span class="comment">#定义好的MLP类封装起来</span></span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 5], dtype=float32, place=Place(cpu), stop_gradient=False,       [[ 0.33403778,  0.10073330, -0.62100554, -0.13195495,  0.38762105],        [ 0.50147879,  0.08661372, -0.59700704, -0.05069904,  0.40827096]])</code></pre><h3 id="【搭建房价预测模型】"><a href="#【搭建房价预测模型】" class="headerlink" title="【搭建房价预测模型】"></a><strong>【搭建房价预测模型】</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Regressor</span>(nn.Layer):<span class="comment">#这里定义了一个简单的单层网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.fc = Linear(<span class="number">13</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="3-训练配置"><a href="#3-训练配置" class="headerlink" title="3 训练配置"></a><strong>3 训练配置</strong></h2><p>训练配置过程如下图所示：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/96075d4df5ae4e01ac1491ebf176fa557bd122b646ba49238f65c9b38a98cab4" width="700" hegiht="" ></center><center><br>图6：训练配置流程示意图</br></center><ul><li>本教程默认使用AI Studio训练模型，因此无需指定机器资源；</li><li>声明定义好的回归模型实例为Regressor，并将模型的状态设置为<code>train</code>；</li><li>使用<code>load_data</code>函数加载训练数据和测试数据；</li><li>设置优化算法和学习率，优化算法采用随机梯度下降SGD，学习率设置为0.01。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明定义好的线性回归模型</span></span><br><span class="line">model = Regressor()</span><br><span class="line"><span class="comment"># 开启模型训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">training_data, test_data = load_data()</span><br><span class="line"><span class="comment"># 定义优化算法，使用随机梯度下降SGD</span></span><br><span class="line"><span class="comment"># 学习率设置为0.01</span></span><br><span class="line">opt = paddle.optimizer.SGD(learning_rate=<span class="number">0.01</span>, parameters=model.parameters())</span><br></pre></td></tr></table></figure><hr><p><strong>说明：</strong></p><p>模型实例有两种状态：训练状态<code>.train()</code>和预测状态<code>.eval()</code>。训练时要执行正向计算和反向传播梯度两个过程，而预测时只需要执行正向计算。</p><hr><h2 id="4-训练过程"><a href="#4-训练过程" class="headerlink" title="4 训练过程"></a><strong>4 训练过程</strong></h2><p>训练过程采用二层循环嵌套方式：</p><ul><li><p><strong>内层循环：</strong> 负责整个数据集的一次遍历，采用分批次方式（batch）。假设数据集样本数量为1000，一个批次有10个样本，则遍历一次数据集的批次数量是1000/10=100，即内层循环需要执行100次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> iter_id, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(mini_batches):</span><br></pre></td></tr></table></figure></li><li><p><strong>外层循环：</strong> 定义遍历数据集的次数，通过参数EPOCH_NUM设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br></pre></td></tr></table></figure></li></ul><hr><p><strong>说明</strong>:</p><p>batch的取值会影响模型训练效果，batch过大，会增大内存消耗和计算时间，且训练效果并不会明显提升（每次参数只向梯度反方向移动一小步，因此方向没必要特别精确）；batch过小，每个batch的样本数据没有统计意义，计算的梯度方向可能偏差较大。由于房价预测模型的训练数据集较小，因此将batch设置为10。</p><hr><p>每次内层循环都需要执行如 <strong>图7</strong> 所示的步骤：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8154cf612a024a3f9144b4e31f59568ef9ad59c155b344919221d63bb9ccfcc8" width="700" hegiht="" ></center><center><br>图7：内循环计算过程</br></center><ul><li>数据准备：将一个批次的数据先转换成 nparray 格式，再转换成<code>Tensor</code>格式；</li><li>前向计算：将一个批次的样本数据灌入网络中，计算输出结果；</li><li>计算损失函数：以前向计算结果和真实房价作为输入，通过损失函数 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/2.4rc/api/paddle/nn/functional/square_error_cost_cn.html#square-error-cost">square_error_cost</a> API计算出损失函数值（Loss）；</li><li>反向传播：执行梯度反向传播<code>backward</code>函数，即从后到前逐层计算每一层的梯度，并根据设置的优化算法更新参数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">EPOCH_NUM = <span class="number">10</span>   <span class="comment"># 设置外层循环次数</span></span><br><span class="line">BATCH_SIZE = <span class="number">10</span>  <span class="comment"># 设置batch大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义外层循环</span></span><br><span class="line"><span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">    <span class="comment"># 在每轮迭代开始之前，将训练数据的顺序随机的打乱</span></span><br><span class="line">    np.random.shuffle(training_data)</span><br><span class="line">    <span class="comment"># 将训练数据进行拆分，得到小批量的数据，每个batch包含10条数据</span></span><br><span class="line">    mini_batches = [training_data[k:k+BATCH_SIZE] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(training_data), BATCH_SIZE)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义内层循环</span></span><br><span class="line">    <span class="keyword">for</span> iter_id, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(mini_batches):</span><br><span class="line">        x = np.array(mini_batch[:, :-<span class="number">1</span>]) <span class="comment"># 获得当前批次训练数据</span></span><br><span class="line">        y = np.array(mini_batch[:, -<span class="number">1</span>:]) <span class="comment"># 获得当前批次训练标签（真实房价）</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将numpy数据转为飞桨动态图tensor的格式,方便数据在整个paddle框架中通用</span></span><br><span class="line">        house_features = paddle.to_tensor(x)</span><br><span class="line">        prices = paddle.to_tensor(y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向计算，得到预测的y</span></span><br><span class="line">        predicts = model(house_features)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = F.square_error_cost(predicts, label=prices)</span><br><span class="line">        avg_loss = paddle.mean(loss)<span class="comment">#平均损失</span></span><br><span class="line">        <span class="keyword">if</span> iter_id%<span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, iter: &#123;&#125;, loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch_id, iter_id, avg_loss.numpy()))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播损失，计算每层参数的梯度值</span></span><br><span class="line">        avg_loss.backward()</span><br><span class="line">        <span class="comment"># 优化器更新参数，根据设置好的学习率迭代一步</span></span><br><span class="line">        opt.step()</span><br><span class="line">        <span class="comment"># 清空梯度变量，以备下一轮计算</span></span><br><span class="line">        opt.clear_grad()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 下面是训练结果，再执行一遍程序，会继续向下训练，而不是从头开始</span></span><br></pre></td></tr></table></figure><pre><code>epoch: 0, iter: 0, loss is: [0.02828562]epoch: 1, iter: 0, loss is: [0.01003132]epoch: 2, iter: 0, loss is: [0.02788146]epoch: 3, iter: 0, loss is: [0.01141991]epoch: 4, iter: 0, loss is: [0.04065816]epoch: 5, iter: 0, loss is: [0.0243281]epoch: 6, iter: 0, loss is: [0.03753167]epoch: 7, iter: 0, loss is: [0.02115335]epoch: 8, iter: 0, loss is: [0.0295791]epoch: 9, iter: 0, loss is: [0.01928121]</code></pre><h2 id="5-保存并测试模型"><a href="#5-保存并测试模型" class="headerlink" title="5 保存并测试模型"></a><strong>5 保存并测试模型</strong></h2><h3 id="5-1-保存模型"><a href="#5-1-保存模型" class="headerlink" title="5.1 保存模型"></a><strong>5.1 保存模型</strong></h3><p>使用 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/save_cn.html#save">paddle.save API</a> 将模型当前的参数数据 <code>model.state_dict()</code> 保存到文件中，用于模型预测或校验的程序调用。</p><blockquote><p>data目录每次启动项目会清空，永久保存需要放到work目录下</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型的整个参数，文件名为 xxx.pdparams</span></span><br><span class="line">paddle.save(model.state_dict(), <span class="string">&#x27;data/LR_model.pdparams&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型保存成功，模型参数保存在data/LR_model.pdparams中&quot;</span>)</span><br></pre></td></tr></table></figure><hr><p><strong>说明：</strong></p><p>理论而言，直接使用模型实例即可完成预测，但是在实际应用中，训练模型和使用模型往往是不同的场景。模型训练通常使用大量的线下服务器（不对外向企业的客户/用户提供在线服务）；模型预测则通常使用线上提供预测服务的服务器实现或者将已经完成的预测模型嵌入手机或其他终端设备中使用。因此“先保存模型，再加载模型”更贴合真实场景的使用方法。</p><hr><h3 id="5-2-测试模型"><a href="#5-2-测试模型" class="headerlink" title="5.2 测试模型"></a><strong>5.2 测试模型</strong></h3><p>下面选择一条数据样本，测试下模型的预测效果。测试过程和在应用场景中使用模型的过程一致，主要可分成如下三个步骤：</p><ol><li>配置模型预测的机器资源。本案例默认使用本机，因此无需写代码指定。</li><li>将训练好的模型参数加载到模型实例中。由两个语句完成，第一句是从文件中读取模型参数；第二句是将参数内容加载到模型。加载完毕后，需要将模型的状态调整为<code>eval()</code>（校验）。上文中提到，训练状态的模型需要同时支持前向计算和反向传导梯度，模型的实现较为臃肿，而校验和预测状态的模型只需要支持前向计算，模型的实现更加简单，性能更好。</li><li>将待预测的样本特征输入到模型中，打印输出的预测结果。</li></ol><p>通过<code>load_one_example</code>函数实现从数据集中抽一条样本作为测试样本，具体实现代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_one_example</span>():</span><br><span class="line">    <span class="comment"># 从上边已加载的测试集中，随机选择一条作为测试数据</span></span><br><span class="line">    idx = np.random.randint(<span class="number">0</span>, test_data.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;抽取的是第&#x27;</span>, idx, <span class="string">&#x27;条样本&#x27;</span>)</span><br><span class="line">    one_data, label = test_data[idx, :-<span class="number">1</span>], test_data[idx, -<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;特征是：&#x27;</span>, one_data, one_data.shape)<span class="comment"># 即x</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;标签是：&#x27;</span>, label, label.shape)<span class="comment"># 即y</span></span><br><span class="line">    <span class="comment"># 修改该条数据shape为[1,13]</span></span><br><span class="line">    one_data =  one_data.reshape([<span class="number">1</span>,-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> one_data, label</span><br></pre></td></tr></table></figure><p>将训练好的模型参数加载到模型实例中。同时抽取一条样本：<code>特征+标签</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数为保存模型参数的文件地址</span></span><br><span class="line">model_dict = paddle.load(<span class="string">&#x27;data/LR_model.pdparams&#x27;</span>)</span><br><span class="line">model.load_dict(model_dict)<span class="comment"># 将参数铺到模型当中</span></span><br><span class="line">model.<span class="built_in">eval</span>()<span class="comment"># 开启评估模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数为数据集的文件地址</span></span><br><span class="line">one_data, label = load_one_example()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转为动态图的variable格式 </span></span><br><span class="line">one_data = paddle.to_tensor(one_data)</span><br><span class="line">predict = model(one_data)<span class="comment"># 放入数据得到预测结果</span></span><br></pre></td></tr></table></figure><pre><code>抽取的是第 76 条样本特征是： [0.06538943 0.         0.7002779  0.         0.30246916 0.51369995 0.6364572  0.2086588  1.         1.         0.8085107  1. 0.2486203 ] (13,)标签是： 0.4 ()</code></pre><p>因为我们在正式开始训练之前对数据进行了归一化处理，实际取出的特征与标签值并不符合实际数据，因此在这里我们需要对预测结果进行反归一化处理。</p><script type="math/tex; mode=display">x' = \frac{x-x_{min}}{x_{max}-x_{min}}</script><p>反归一化公式如下：</p><script type="math/tex; mode=display">x = x'·({x_{max}-x_{min}}) + x_{min}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对结果做反归一化处理</span></span><br><span class="line">predict = predict * (max_values[-<span class="number">1</span>] - min_values[-<span class="number">1</span>]) + min_values[-<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 对label数据做反归一化处理</span></span><br><span class="line">label = label * (max_values[-<span class="number">1</span>] - min_values[-<span class="number">1</span>]) + min_values[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Inference result is &#123;&#125;, the corresponding label is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(predict.numpy(), label))</span><br></pre></td></tr></table></figure><pre><code>Inference result is [[23.333654]], the corresponding label is 23.0</code></pre><p>通过比较“模型预测值”和“真实房价”可见，模型的预测效果与真实房价接近。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_基础模型实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习3.2-模型选择与调优策略（下）</title>
      <link href="/2022/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
      <url>/2022/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="调优策略"><a href="#调优策略" class="headerlink" title="调优策略"></a>调优策略</h1><p>😎学习本节，希望你能够掌握以下知识点：</p><ol><li>针对系统中出现的问题选择不同的优化策略以提高系统的能力。</li></ol><hr><blockquote><h4 id="ppl-py文件"><a href="#ppl-py文件" class="headerlink" title="ppl.py文件"></a>ppl.py文件</h4><p>将在此使用的相同的函数放到一起，作为一个库，在使用其中的函数时，直接<code>import ppl</code>导入即可，或者<code>from ppl import 函数名</code>；</p><p>我将ppl.py文件放在了<code>C:\Users\21431\Desktop\计算机学习\人工智能\人工智能山大培训\深度学习</code>；</p><p>在本地使用时，复制放在同一目录下即可；</p><p>AI Studio中放到项目中即可。</p></blockquote><hr><h2 id="一、解决欠拟合与过拟合"><a href="#一、解决欠拟合与过拟合" class="headerlink" title="一、解决欠拟合与过拟合"></a>一、解决欠拟合与过拟合</h2><p>欠拟合是指模型不能再训练集上获得足够低的误差，即模型在训练集上的误差比人类水平达到的误差要高，此时模型还有提升的空间，可以通过增加模型深度和训练次数【迭代周期】或选择一些优化算法继续提高模型的表达能力。而过拟合是指学习时选择的模型包含的参数过多【模型的复杂度较高】，以至于这一模型对已知数据预测的很好，但对未知数据预测的很差。过拟合通常被称为模型的泛化能力不好，可以通过增加数据集、加入一些正则化方法或者改变超参数来调整。</p><p><strong>【模型复杂性】</strong></p><p>模型的训练程度可以用模型复杂度来衡量。当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近，很容易出现欠拟合；当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。</p><p>模型复杂性由什么构成是一个复杂的问题。你可以把它想象成模型复杂就是机器更智能。一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型也比较复杂。我们将重点介绍几个倾向于影响模型复杂度的因素：</p><ol><li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li></ol><p>例如，因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。</p><p><strong>【数据集大小】</strong></p><p>另一个重要因素是数据集的大小。<strong>数据的复杂度受到样本的个数、每个样本的元素个数、时间空间结构和多样性的影响</strong>。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小。</p><p>此外，一般来说，更多的数据不会有什么坏处。对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。能够拟合更复杂的模型可能是有益的。如果没有足够的数据，简单的模型可能更有用。</p><p>对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。从一定程度上来说，<strong>深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。</strong></p><p><strong>【实现多项式回归】</strong></p><p>我们现在可以通过多项式拟合来探索这些概念。通过对输入数据的控制分别展示欠拟合、正常拟合和过拟合的情况。</p><p>给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2} {2!} + 5.6 \frac{x^3} {3!} + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.1^2).</script><p>噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值，因此我们将特征从$x^i$调整为$\frac{x^i} {i!}$，这样可以避免很大的$i$带来的特别大的指数值。我们将为训练集和测试集各生成100个样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line">max_degree = <span class="number">20</span> <span class="comment"># 生成20维的数据样本，我们只需要前4个维度，后边的是噪声</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">true_w = np.zeros(shape=(max_degree,))</span><br><span class="line">true_w[<span class="number">0</span>: <span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))</span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">math.gamma(N)其表示N在N-1到0范围内的整数阶乘。</span></span><br><span class="line"><span class="string">公式为：gamma(N)=(N-1)*(N-2)*...*2*1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    paddle.to_tensor(x, dtype=paddle.float32) </span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(poly_features[:<span class="number">1</span>, :], <span class="string">&#x27;\n&#x27;</span>, labels[:<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1, 20], dtype=float32, place=Place(cpu), stop_gradient=True,       [[1.        , 1.28171623, 0.82139820, 0.35093310, 0.11244916, 0.02882558,         0.00615770, 0.00112749, 0.00018064, 0.00002573, 0.00000330, 0.00000038,         0.00000004, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000,         0.00000000, 0.00000000]])  Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,       [5.66237640])</code></pre><p>首先让我们实现一个函数来评估模型在给定数据集上的损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>) <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>现在读取数据并定义训练函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_array, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构造数据迭代器</span></span><br><span class="line">    dataset = TensorDataset(data_array)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter():</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, paddle.optimizer.Optimizer):</span><br><span class="line">            updater.clear_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            W, b = updater(W, b, X.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;updated W, b:&quot;</span>, W, b)</span><br><span class="line">            metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels, num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)<span class="comment">#均方损失</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias_attr=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    train_iter = load_array((train_features, train_labels.reshape((-<span class="number">1</span>, <span class="number">1</span>))), batch_size)</span><br><span class="line">    test_iter = load_array((test_features, test_labels.reshape((-<span class="number">1</span>, <span class="number">1</span>))), batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = paddle.optimizer.SGD(<span class="number">0.001</span>, net.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight)</span><br></pre></td></tr></table></figure><p><strong>1）三阶多项式函数拟合(正常)</strong></p><blockquote><p>如图所示：</p><p>train_loss就是训练误差；</p><p>test_loss就是泛化误差。</p></blockquote><p>现在我们正常将样本投入到训练中，学习到的模型参数也接近真实值$w = [5, 1.2, -3.4, 5.6]$。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d24d18e751094cc5a09e9fec7159ae410eb9d1df692b4f34b6e1796bcca3b7bf" width="350" hegiht="" ></center><center>图4：正常拟合 </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><pre><code>weight: Parameter containing:Tensor(shape=[4, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[ 4.98475456],        [ 1.18825924],        [-3.37832952],        [ 5.63073015]])</code></pre><p><strong>2）线性函数拟合(欠拟合)</strong></p><p>当我们没有提供足量的数据，只提供前两列（数据不全），<br>当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p><p>$w = [5, 1.2, -3.4, 5.6]$</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d8488a4e261c4493be2f8f916790a68c53f322094f9d4c8b93b2f44a2e3045d7" width="350" hegiht="" ></center><center>图5：欠拟合 </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>], </span><br><span class="line">        labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><pre><code>weight: Parameter containing:Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[3.09112906],        [4.18816757]])</code></pre><p><strong>3）高阶多项式函数拟合(过拟合)</strong></p><p>接下来把所有的数据都给模型，这个过于复杂的模型会轻易受到训练数据中噪声的影响。<br>虽然训练损失可以有效地降低，但测试损失仍然很高。<br>结果表明，复杂模型对数据造成了过拟合。</p><p>$w = [5, 1.2, -3.4, 5.6]$</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d0e8eec69cba4648bb56dcedf4f7c28f835fb7d2231944fd87bcf13591cf7855" width="350" hegiht="" ></center><center>图6：过拟合 </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure><pre><code>weight: Parameter containing:Tensor(shape=[20, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[ 4.96567392],        [ 1.28106725],        [-3.26861453],        [ 5.17804003],        [-0.45723167],        [ 1.17682230],        [ 0.58788991],        [ 0.11369380],        [-0.00772932],        [ 0.34586436],        [ 0.51956356],        [ 0.03097064],        [-0.40698799],        [ 0.23049854],        [-0.52523530],        [-0.27161127],        [-0.42720860],        [-0.45747021],        [-0.48819304],        [ 0.42260706]])</code></pre><h2 id="二、数值稳定性和模型初始化"><a href="#二、数值稳定性和模型初始化" class="headerlink" title="二、数值稳定性和模型初始化"></a>二、数值稳定性和模型初始化</h2><p>​        到目前为止，我们实现的每个模型都是根据某个预先指定的分布来初始化模型的参数。初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起，我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。在本节中，我们将更详细地探讨这些主题，并讨论一些有用的启发式方法。</p><p>​        深度学习的训练本质是优化损失，优化的方式是计算梯度，然后通过优化算法更新参数。开始之前，我们先来回忆一下梯度公式的数学意义，它描述了函数在某点函数值增加最快的方向，它的模就等于函数在该点方向导数的最大值。用直观的解释就是，假设你现在位于一座山上，则这一点的梯度是在该点坡度（或者说斜度）最陡的方向，梯度的大小告诉我们坡度到底有多陡。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/7b41140018cd482eb6b1438dc713be416375e9d920a44f63895f6f9ca26485b2" width="500" hegiht="" ></center><center>图1：梯度更新的原理 </center><p>根据梯度链式传递法则可以发现，<strong>激活函数的求导值和权重值会以连乘的形式参与到该层权重的梯度计算中</strong>，而<strong>预测值与真实值的偏差以及神经元的输入值只是以常数的形式参与计算</strong>。</p><p>梯度是矩阵与梯度向量的乘积，因此，我们容易受到数值下溢问题的影响。</p><p>不稳定梯度带来的风险不止在于数值表示，不稳定梯度也威胁到我们优化算法的稳定性。我们可能面临一些问题，要么是<strong>梯度爆炸（gradient exploding）</strong> 问题：参数更新过大，破坏了模型的稳定收敛；要么是<strong>梯度消失（gradient vanishing）</strong> 问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><p>总结一下 ，梯度消失与梯度爆炸是反向传播训练法则的先天性不足，本质是梯度反向传播中的连乘效应。梯度消失与梯度爆炸的产生原因在于神经网络的更新中，梯度的传递是采用连乘函数的形式。众所周知，指数级别的增长是爆炸式的，因此可能引起深层网络的浅层节点更新使用一个过小或者过大的梯度值，这种深度学习现象叫做梯度消失与梯度爆炸。</p><h3 id="2-1-梯度消失"><a href="#2-1-梯度消失" class="headerlink" title="2.1 梯度消失"></a>2.1 梯度消失</h3><p>曾经sigmoid函数很流行，因为它类似于阈值函数。由于早期的人工神经网络受到生物神经网络的启发，神经元要么完全激活要么完全不激活（就像生物神经元）的想法很有吸引力。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8e6c84ff23204602ae7e8a3bbe35c43fee9e0a818fd6452192594885139a4fb5" width="400" hegiht="" ></center><center>图2：softmax激活函数曲线 </center><p>然而，它却是导致梯度消失问题的一个常见的原因，当sigmoid函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。<br>因此，更稳定的ReLU系列函数已经成为从业者的默认选择，ReLU函数的导数在正数部分是恒等于1的，因此在深层网络中使用更优的激活函数（例如ReLU等）可以减轻梯度消失的问题。</p><h3 id="2-2-梯度爆炸"><a href="#2-2-梯度爆炸" class="headerlink" title="2.2 梯度爆炸"></a>2.2 梯度爆炸</h3><p>相反，梯度爆炸可能同样令人烦恼。为了更好地说明这一点，我们生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘。可以发现出现了梯度爆炸的情况，当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">M = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=[<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一个矩阵 \n&#x27;</span>,M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    M = paddle.mm(M,paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=[<span class="number">4</span>, <span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;乘以100个矩阵后\n&#x27;</span>, M)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">一个矩阵 </span></span><br><span class="line"><span class="string"> Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,</span></span><br><span class="line"><span class="string">       [[-1.19920492, -1.51656413, -0.66636622,  0.62597984],</span></span><br><span class="line"><span class="string">        [-0.00658856,  0.70853168,  0.45729700,  0.10293817],</span></span><br><span class="line"><span class="string">        [ 0.27025989, -0.19260350, -0.12201850, -1.42817235],</span></span><br><span class="line"><span class="string">        [-0.04502322,  0.19740644, -0.95305598,  0.35635617]])</span></span><br><span class="line"><span class="string">乘以100个矩阵后</span></span><br><span class="line"><span class="string"> Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,</span></span><br><span class="line"><span class="string">       [[ 303895516951572182466560. ,  232979873649558818914304. ,</span></span><br><span class="line"><span class="string">         -2250137589105981078372352., -5345116457122281304358912.],</span></span><br><span class="line"><span class="string">        [-136164064919334751830016. , -104411381703363541336064. ,</span></span><br><span class="line"><span class="string">          1008128975147034581401600.,  2394808197116681298903040.],</span></span><br><span class="line"><span class="string">        [ 113319943119065864732672. ,  86876427403012165599232.  ,</span></span><br><span class="line"><span class="string">         -839054477648321230929920. , -1993143315278582639493120.],</span></span><br><span class="line"><span class="string">        [ 13815522565998537342976.  ,  10588780127280403841024.  ,</span></span><br><span class="line"><span class="string">         -102303525940968309129216. , -243013515316971584880640. ]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>​        我们知道，梯度需要和学习率相结合，当出现过小的梯度（或为0），无论学习率怎么调整，我们都无法实现训练的进展，底部层梯度为0的情况会尤为严重，这种情况下，无论输入的值是多少，底部层都无法对数值进行处理，那么再深的网络结构都毫无意义，实际效果和单层神经网络的效果都是相同的。</p><p>​        同样地，梯度爆炸也会带来许多问题，这会使模型对学习率变得更加敏感，稍大的学习率就会导致参数动荡，无法收敛，始终处于一个大的参数值的状态；学习率小，虽然避免了上面的情况，但往往会导致训练无进展。因此我们需要在训练过程中不断地去调整学习率。</p><h3 id="2-3-合理的权重初始化的方法"><a href="#2-3-合理的权重初始化的方法" class="headerlink" title="2.3 合理的权重初始化的方法"></a>2.3 合理的权重初始化的方法</h3><p>​        针对着上面的情况，我们可以采用很多方法来实现参数的稳定，目标是使梯度值在一个合理的范围内，最终推动训练的稳定。<strong><em>本节主要介绍合理的权重初始化的方法</em></strong>。对于模型而言参数初始化是初始阶段比较重要的阶段，糟糕的初始化会导致模型不收敛或者根本训练没反应。</p><p><strong>1）打破对称性</strong></p><p>​        当所有初始值都相同时，例如将每个权重初始化为0，然后在进行反向传播时，所有权重将获得相同的梯度，因此进行相同的更新。<br>直观地说，这意味着所有节点都将学习相同的东西，而我们不希望那样，因为我们希望网络学习不同种类的特征。这就是所谓的打破对称性。</p><p>​        假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。在这种情况下，第一个隐藏单元与第二个隐藏单元没有什么特别的区别假设我们将隐藏层的所有参数初始化为$\mathbf{W}^{(1)} = c$，$c$为常量，会发生什么？在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数$\mathbf{W}^{(1)}$对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代之后，$\mathbf{W}^{(1)}$的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。这就是所谓的对称性。</p><p>​        因此，我们决不能把所有参数初始化为0，同样也不能初始化为任何相同的值，我们必须“打破对称性”！我们希望不同的节点（神经元）学习到不同的参数，但是如果参数相同以及输出值都一样，不同的节点根本学习不到不同的特征，这样就失去了网络学习特征的意义了。</p><p>​        解决（或至少减轻）上述问题的一种方法是进行参数初始化，这是通过随机初始化来实现的，因为这样梯度会不同，每个节点将变得与其他节点更加不同，从而实现多样化的特征提取。</p><p><strong>2）默认随机初始化</strong></p><p>​        在前面的部分中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。通常来说，$b$不用随机初始化，因为$w$随机之后，已经打破对称。</p><p><strong>3）Xavier初始化</strong></p><p>​        要描述“差异性”，首先就能想到概率统计中的方差这个基本统计量，为了使网络中的信息更好的流动，每一层输出的方差应该尽量相等。这就是现在标准且实用的Xavier初始化的基础。<br>下面我们使用公式来解读。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/7163751298e8498ab850641790f1307f8636695442ad47429b7893c946390d5b" width="150" hegiht="" ></center><p>其中$n$是上一层神经元的数量。因此，根据概率统计里的两个随机变量乘积的方差展开：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f691ddd1dc114039ac7b48b644adbbb1fea2e2fd72a5465fba5087dd5b367cfa" width="650" hegiht="" ></center><p> $E(X)$表示变量$X$的期望。可以得到，如果$E(xi)=E(wi)=0$，那么就有:</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4895a6a26c3b47908a5cc7ea9d0eeb89a1f8470b3bf44889bffc0ab24333f01a" width="290" hegiht="" ></center><p> 如果随机变量$xi$和$wi$再满足独立同分布的话:</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/2fb25022dc284683b1818e56c52e2f07e9acccc8406b410c8b3abefbea183b29" width="450" hegiht="" ></center><p>​        根据文章《激活函数》，整个大型前馈神经网络无非就是一个超级大映射，将原始样本稳定的映射成它的类别,也就是将样本空间映射到类别空间。</p><p>​        试想，如果样本空间与类别空间的分布差异很大【这里样本空间就是x，类别空间就是z】，比如说类别空间特别稠密，样本空间特别稀疏辽阔，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。</p><p>​        同样，如果类别空间特别稀疏，样本空间特别稠密，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。</p><p>​        因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，也就是要让它们的<strong>方差尽可能相等</strong>。</p><p>​        因此为了得到$Var(z)=Var(x)$，只能让$n×Var(w)=1$，也就是$Var(w)=1/n$。<br>同样的道理，正向传播时是从前往后计算的，因此$Var(w)=\frac{1} {n_{in} }$，反向传播时是从后往前计算的，因此$Var(w)=\frac{1} {n_{out} }$，然而$n_{in}$和$n_{out}$往往不相等，因此在这里我们选择他们的均值，即令：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d4936358413f4939af9a804e516d7b36fe65e32d33334343830721086c686d08" width="180" hegiht="" ></center><p>​        其中$n_{in}$与$n_{out}$分别是输入层与输出层的神经元个数。通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2} {n_\mathrm{in} + n_\mathrm{out} }$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\frac{a^2} {3}$。将$\frac{a^2} {3}$代入到$\sigma^2$的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6} {n_\mathrm{in} + n_\mathrm{out} }}, \sqrt{\frac{6} {n_\mathrm{in} + n_\mathrm{out} }}\right).</script><p>​        得到的这个结论就是Xavier初始化方法。这不是一个一劳永逸的方法，但除非你的网络设计的明显不满足Xavier的假设，否则使用Xavier往往不会出错。</p><p>（补充：若 X 服从 [ a , b ] 上的均匀分布，则数学期望 EX =（a + b）/ 2 ；方差 DX =（b - a）² / 12 ）</p><h3 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4 小结"></a>2.4 小结</h3><ul><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li></ul><h2 id="三、降低偏差—优化算法"><a href="#三、降低偏差—优化算法" class="headerlink" title="三、降低偏差—优化算法"></a>三、降低偏差—优化算法</h2><p>优化问题是指在满足一定条件下，在众多方案或参数值中寻找最优方案或参数值，以使得某个或多个功能指标达到最优，或使系统的某些性能指标达到最大值或最小值。</p><p>深度网络模型优化算法主要依据最小化或最大化目标函数，更新对模型的训练和表达能力造成影响的参数，使这些参数达到或尽可能接近目标函数的最优值，从而提高模型的学习能力，获得预期的网络模型。当模型出现欠拟合的状况时，可以通过调整优化算法来改善模型的训练，降低模型的预测偏差，提升模型的表达能力。</p><blockquote><p>神经网络中的优化是一个最小值的优化，优化的目标对象是损失。</p></blockquote><h3 id="【设置学习率】"><a href="#【设置学习率】" class="headerlink" title="【设置学习率】"></a><strong>【设置学习率】</strong></h3><p>在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：</p><ul><li>学习率不是越小越好。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如左图所示。</li><li>学习率不是越大越好。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如右图所示。</li></ul><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4266917491ee489e8ca0296565f1fe5edc2cb68d4a4546f3b8979e1200691bb5" width="550" hegiht="" ></center><center>图1：不同学习率的影响 </center><p>在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察$Loss$下降的情况判断合理的学习率。<br>学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和$Loss$的变化。经过研究员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam。</p><h3 id="3-1-梯度下降"><a href="#3-1-梯度下降" class="headerlink" title="3.1 梯度下降"></a>3.1 梯度下降</h3><h4 id="3-1-1-梯度下降"><a href="#3-1-1-梯度下降" class="headerlink" title="3.1.1 梯度下降"></a>3.1.1 梯度下降</h4><p>🎨<code>sec_gd</code></p><p>为什么梯度下降算法可以优化目标函数？一维中的梯度下降给我们很好的启发。</p><p>梯度下降法是一个一阶最优化算法，通常也称为最陡下降法。要使用梯度下降法找到一个函数的局部极小值，必须朝向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d44fbf84938642e1a0acf335cc7f4271444efbdbad5d4ff58222e19482fef8df" width="450" hegiht="" ></center><center>图2：梯度下降法 </center><p><strong>优化算法主要是针对凸函数</strong>。下面再来回顾以下有关理论：</p><ol><li>集合中任意两点的连线都在集合中，为凸集合。在二维中，我们可以将凸集视为一个形状，无论用什么线连接集中的两个点，都不会在集外。</li><li>二阶导数需大于0才是凸函数。二阶导数是一阶导数的导数。从原理上看，它表示一阶导数的变化率；从图形上看，它反映的是函数图像的凹凸性。</li><li>目标函数是凸函数，变量所属集合是凸集合的优化问题为凸优化问题，对于凸优化问题来说，局部最优解就是全局最优解。</li></ol><p>现在我们来看一些凸优化理论。我们知道，梯度下降法被应用于寻找代价函数的全局最小值。但是我们怎么知道存在一个全局最小值呢？当最小化函数时，凸函数可确保如果存在最小值，则它将是全局最小值。二次函数（例如线性最小二乘问题）是强凸的。这意味着该函数具有唯一的最小值，而该最小值是全局最小值。因此，当我们应用梯度下降算法时，我们可以确信它将收敛于正确的最小值。如果我们试图最小化的函数是非凸的，则梯度下降可能会收敛于局部最小值而不是全局最小值。这就是为什么使用非凸函数要困难得多，这很重要，因为许多机器学习模型（最著名的是神经网络）是非凸的。</p><h5 id="1）一维梯度下降"><a href="#1）一维梯度下降" class="headerlink" title="1）一维梯度下降"></a><strong>1）一维梯度下降</strong></h5><p>下面我们来展示如何实现梯度下降。</p><p>为了简单起见，我们选用目标函数$f(x)=x^2$。尽管我们知道$x=0$时$f(x)$能取得最小值，但我们仍然使用这个简单的函数来观察$x$的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x</span>): <span class="comment"># 目标函数的梯度（导数）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x</span><br></pre></td></tr></table></figure><p><strong>梯度下降三步走</strong>：</p><ul><li>选定初始$X_0$</li><li>对$t=1,2,3…T$</li><li>求梯度：$X_t = X_{t-1} - \eta f’(X_{t-1})$</li></ul><p>接下来，我们使用$x=10$作为初始值，并假设$\eta=0.2$。<br>使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gd</span>(<span class="params">eta, f_grad</span>):</span><br><span class="line">    x = <span class="number">10.0</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        x -= eta * f_grad(x)</span><br><span class="line">        results.append(<span class="built_in">float</span>(x))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch 10, x: %f&#x27;</span>%x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = gd(<span class="number">0.2</span>, f_grad)<span class="comment"># 此时学习率为0.02，这是经过多次得出来的适中的值。</span></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 0.060466</span></span><br><span class="line"><span class="string">[10.0, 6.0, 3.5999999999999996, 2.1599999999999997, 1.2959999999999998, 0.7775999999999998, 0.46655999999999986, 0.2799359999999999, 0.16796159999999993, 0.10077695999999996, 0.06046617599999997]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>对进行$x$优化的过程可以绘制如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace</span>(<span class="params">results, f</span>):</span><br><span class="line">    n = <span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="built_in">min</span>(results)), <span class="built_in">abs</span>(<span class="built_in">max</span>(results)))</span><br><span class="line">    <span class="comment">#print(n)</span></span><br><span class="line">    f_line = paddle.arange(-n, n, <span class="number">0.01</span>, dtype=paddle.float32)</span><br><span class="line">    tensor_list = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> f_line:</span><br><span class="line">        tensor_list.append(f(x))</span><br><span class="line">        <span class="comment">#print(tensor_list)</span></span><br><span class="line">    final_tensor=paddle.stack(tensor_list, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(final_tensor)</span></span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plot([f_line, results], </span><br><span class="line">            [final_tensor, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> results]], </span><br><span class="line">            <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, fmts=[<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>])</span><br><span class="line"></span><br><span class="line">show_trace(results, f)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/10.svg" alt="svg"></p><blockquote><h5 id="如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，梯度-学习率的值也就越来越小，更新的越来越慢。"><a href="#如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，梯度-学习率的值也就越来越小，更新的越来越慢。" class="headerlink" title="如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，梯度*学习率的值也就越来越小，更新的越来越慢。"></a>如上图所示，发现梯度下降越来越慢，原因是斜率越来越小，<code>梯度*学习率</code>的值也就越来越小，更新的越来越慢。</h5><h5 id="学习率适中，10轮后，得到的x值为0-06，已经非常接近最优解了"><a href="#学习率适中，10轮后，得到的x值为0-06，已经非常接近最优解了" class="headerlink" title="学习率适中，10轮后，得到的x值为0.06，已经非常接近最优解了"></a>学习率适中，10轮后，得到的x值为0.06，已经非常接近最优解了</h5></blockquote><p>我们可以尝试调整学习率的大小来查看它对于模型优化的影响。例如，考虑同一优化问题中$\eta = 0.05$的进度。如下所示，尽管经过了10个步骤，我们仍然离最优解很远。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">0.05</span>, f_grad), f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 3.486784</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/11.svg" alt="svg"></p><blockquote><h5 id="如上图所示-学习率太小，更新的幅度也就变小，经过轮过后，x值为3-48，与最优解相差较远。"><a href="#如上图所示-学习率太小，更新的幅度也就变小，经过轮过后，x值为3-48，与最优解相差较远。" class="headerlink" title="如上图所示,学习率太小，更新的幅度也就变小，经过轮过后，x值为3.48，与最优解相差较远。"></a>如上图所示,学习率太小，更新的幅度也就变小，经过轮过后，x值为3.48，与最优解相差较远。</h5></blockquote><p>相反，学习率也不可过大，过大可能在优化过程中错过梯度为0的最优点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">show_trace(gd(<span class="number">1.1</span>, f_grad), f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: 61.917364</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/12.svg" alt="svg"></p><blockquote><h5 id="如上图所示，学习率过高，在最优解附近振荡，达不到最优解。"><a href="#如上图所示，学习率过高，在最优解附近振荡，达不到最优解。" class="headerlink" title="如上图所示，学习率过高，在最优解附近振荡，达不到最优解。"></a>如上图所示，学习率过高，在最优解附近振荡，达不到最优解。</h5></blockquote><h5 id="2）局部最小值"><a href="#2）局部最小值" class="headerlink" title="2）局部最小值"></a><strong>2）局部最小值</strong></h5><p>为了演示非凸函数的梯度下降，考虑函数$f(x) = x \cdot \cos(cx)$，其中$c$为某常数。这个函数有无穷多个局部最小值。根据我们选择的学习率，我们最终可能只会得到许多解的一个。下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 局部最小值</span></span><br><span class="line">c = paddle.to_tensor(<span class="number">0.15</span> * np.pi) <span class="comment"># 0.15π</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * paddle.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.cos(c * x) - c * x * paddle.sin(c * x)</span><br></pre></td></tr></table></figure><p>对进行$x$优化的过程可以绘制如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace1</span>(<span class="params">results, f</span>):</span><br><span class="line">    n = <span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="built_in">min</span>(results)), <span class="built_in">abs</span>(<span class="built_in">max</span>(results)))</span><br><span class="line">    <span class="comment">#print(n)</span></span><br><span class="line">    f_line = paddle.arange(-n, n, <span class="number">0.01</span>, dtype=paddle.float32)</span><br><span class="line">    fx_in_fline = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> f_line:</span><br><span class="line">        fx_in_fline.append(f(x))</span><br><span class="line">        <span class="comment">#print(fx_in_fline)</span></span><br><span class="line">    fx_in_fline = paddle.stack(fx_in_fline, axis=<span class="number">0</span>)</span><br><span class="line">    fx_in_results = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> results:</span><br><span class="line">        fx_in_results.append(f(x))</span><br><span class="line">    fx_in_results = paddle.stack(fx_in_results, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plot([f_line, results], </span><br><span class="line">            [fx_in_fline, fx_in_results], </span><br><span class="line">            <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, fmts=[<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>])</span><br><span class="line"></span><br><span class="line">show_trace1(gd(<span class="number">2</span>, f_grad), f)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 10, x: -1.528165</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/13.svg" alt="svg"></p><blockquote><h5 id="如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。"><a href="#如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。" class="headerlink" title="如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。"></a>如上图所示，学习率过高，导致第一次的梯度更新，直接跳过了最优解，然后继续更新，最终找到的是局部最小值。</h5></blockquote><h5 id="3）多元梯度下降"><a href="#3）多元梯度下降" class="headerlink" title="3）多元梯度下降"></a><strong>3）多元梯度下降</strong></h5><p>我们构造一个目标函数$f(\mathbf{x})=x_1^2+2x_2^2$，并有二维向量$\mathbf{x} = [x_1, x_2]^\top$（列向量）作为输入，标量作为输出。梯度由$\nabla f(\mathbf{x}) = [2x_1, 4x_2]^\top$给出。我们将从初始位置$[-5, -2]$通过梯度下降观察$\mathbf{x}$的轨迹。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;多元梯度下降&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_2d</span>(<span class="params">trainer, steps=<span class="number">20</span>, f_grad=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用定制的训练机优化2D目标函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># s1 和 s2 是稍后将使用的内部状态变量</span></span><br><span class="line">    <span class="comment"># 状态变量：一个不可训练的变量，不会被优化器更新，但在评估或预测阶段可能是必要的。</span></span><br><span class="line">    <span class="comment"># 比如 BatchNorm 中的均值和方差。</span></span><br><span class="line">    <span class="comment"># 这里迭代周期我们定义为20，可以修改</span></span><br><span class="line">    x1, x2, s1, s2 = -<span class="number">5</span>, -<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">        <span class="keyword">if</span> f_grad: <span class="comment"># 如存在梯度，则需要更新梯度</span></span><br><span class="line">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)</span><br><span class="line">        results.append((x1, x2)) <span class="comment"># 获取x1和x2的坐标</span></span><br><span class="line">        <span class="comment">#print(f&#x27;epoch &#123;i+1&#125;, x1:&#123;float(x1): f&#125;, x2: &#123;float(x2):f&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_trace_2d</span>(<span class="params">f, results</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示优化过程中2D变量的轨迹&quot;&quot;&quot;</span></span><br><span class="line">    ppl.set_figsize()</span><br><span class="line">    ppl.plt.plot(*<span class="built_in">zip</span>(*results), <span class="string">&#x27;-o&#x27;</span>, color=<span class="string">&#x27;#ff7f0e&#x27;</span>)</span><br><span class="line">    x1, x2 = paddle.meshgrid( <span class="comment"># 输入变量为 k 个一维张量, 输出 k 个 k 维张量</span></span><br><span class="line">        <span class="comment"># paddle.arange(start=0, end=None, step=1)</span></span><br><span class="line">        paddle.arange(-<span class="number">5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, dtype=paddle.float32),</span><br><span class="line">        paddle.arange(-<span class="number">3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, dtype=paddle.float32)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># plt.contour是python中用于画等高线的函数</span></span><br><span class="line">    ppl.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">&#x27;#1f77b4&#x27;</span>)</span><br><span class="line">    ppl.plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">    ppl.plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br></pre></td></tr></table></figure><p>接下来，我们观察学习率$\eta = 0.05$时优化变量$\mathbf{x}$的轨迹。可以看到，经过20步之后，$\mathbf{x}$的值接近其位于$[0, 0]$的最小值。虽然进展相当顺利，但相当缓慢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d_grad</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数的梯度</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x1, <span class="number">4</span> * x2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gd_2d</span>(<span class="params">x1, x2, s1, s2, f_grad</span>):</span><br><span class="line">    g1, g2 = f_grad(x1, x2)</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * g1, x2 - eta * g2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.05</span><span class="comment"># 学习率</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/14.svg" alt="svg"></p><h4 id="3-1-2-随机梯度下降"><a href="#3-1-2-随机梯度下降" class="headerlink" title="3.1.2 随机梯度下降"></a>3.1.2 随机梯度下降</h4><p>🎨<code>sec_sgd</code></p><p>在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定$n$个样本的训练数据集，我们假设$f_i(\mathbf{x})$是关于索引$i$的训练样本的损失函数，其中$\mathbf{x}$是参数向量。然后我们得到目标函数</p><script type="math/tex; mode=display">f(\mathbf{x}) = \frac{1} {n} \sum_{i = 1}^n f_i(\mathbf{x}).</script><p>$\mathbf{x}$的目标函数的梯度计算为</p><script type="math/tex; mode=display">\nabla f(\mathbf{x}) = \frac{1} {n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).</script><p>如果使用梯度下降法，则每个自变量迭代的计算代价为$\mathcal{O}(n)$，它随$n$线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引$i$，其中$i\in\{1,\ldots, n\}$，并计算梯度$\nabla f_i(\mathbf{x})$以更新$\mathbf{x}$：</p><script type="math/tex; mode=display">\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),</script><p>其中$\eta$是学习率。我们可以看到，每次迭代的计算开销降到了常数级，而且随机梯度是对梯度的无偏估计，即使用随机梯度是对梯度的一个良好估计。</p><ul><li>梯度下降法是初始化模型参数，然后计算所有样本在本模型参数下的输出和梯度，再求梯度的均值，去更新网络参数。在下一次迭代，仍然计算全部样本。</li><li><p>随机梯度下降法是初始化模型参数，然后随机选出一个样本，计算该样本在当前参数下的输出和梯度，去更新一次模型参数。在下一次迭代中，再随机选择一个样本，计算输出和梯度再更新模型参数。</p><p>由于随机梯度下降对每个样本都进行更新，使得参数的变化过于频繁，参数之间的方差偏高，每次更新可能并不会按照正确的方向进行，因此可以带来优化波动。</p></li></ul><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8e183f5ad03545fdb5725146f5daaecbd9e092f889a9422d9ddaeb84f4a2bdc4" width="450" hegiht="" ></center><center>图3：SGD搅动 </center><p>现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_grad</span>(<span class="params">x1, x2</span>): <span class="comment"># 目标函数的梯度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x1, <span class="number">4</span> * x2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">x1, x2, s1, s2, f_grad</span>):</span><br><span class="line">    g1, g2 = f_grad(x1, x2)</span><br><span class="line">    <span class="comment"># paddle.normal(mean=0.0, std=1.0, shape=None)</span></span><br><span class="line">    g1 += paddle.normal(<span class="number">0.0</span>, <span class="number">1</span>, (<span class="number">1</span>, ))</span><br><span class="line">    g2 += paddle.normal(<span class="number">0.0</span>, <span class="number">1</span>, (<span class="number">1</span>, ))</span><br><span class="line">    eta_t = eta * lr()</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta_t * g1, x2 - eta_t * g2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constant_lr</span>(): <span class="comment"># 常量lr：学习率调度的任何功能都处于休眠状态（使用常量的学习率）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">lr = constant_lr <span class="comment"># eta*常量lr</span></span><br><span class="line">ppl.show_trace_2d(f, ppl.train_2d(sgd, steps=<span class="number">500</span>, f_grad=f_grad))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 500, x1:-0.085716, x2: -0.015954</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/15.svg" alt="svg"></p><p>正如我们所看到的，随机梯度下降中变量的轨迹比我们在之前梯度下降中观察到的轨迹嘈杂得多。这是由于梯度的随机性质。也就是说，即使我们接近最小值，我们仍然受到通过$\eta \nabla f_i(\mathbf{x})$的瞬间梯度所注入的不确定性的影响。即使经过50次迭代，质量仍然不那么好。更糟糕的是，经过额外的步骤，它不会得到改善。</p><h4 id="3-1-3-小批量随机梯度下降"><a href="#3-1-3-小批量随机梯度下降" class="headerlink" title="3.1.3 小批量随机梯度下降"></a>3.1.3 小批量随机梯度下降</h4><p>🎨<code>sec_minibatch_sgd</code></p><p>到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况：</p><ul><li>在<code>sec_gd</code>中使用整个训练数据集来计算梯度并更新参数，因此它有时也被称为<strong>批量梯度下降（batch gradient descent）</strong>。</li><li>在<code>sec_sgd</code>在每次迭代中只随机采样一个样本来计算梯度以取得进展。</li></ul><p>二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。而由于CPU和GPU无法充分利用向量化，硬件资源难得到完全利用，随机梯度下降并不特别“计算高效”。这暗示了两者之间可能有折中方案，这便涉及到<strong>小批量随机梯度下降（minibatch gradient descent）</strong>。批量为1就是随机梯度下降，为n就是梯度下降，为之间的值就是小批量随机梯度下降。</p><p><strong>1）向量化和缓存</strong></p><p>使用小批量的决策的核心是计算效率。减轻这些限制的方法是使用足够快的CPU缓存层次结构来为处理器提供数据。这是深度学习中批量处理背后的推动力。举一个简单的例子：矩阵-矩阵乘法。<br>比如$\mathbf{A} = \mathbf{B}\mathbf{C}$，我们有很多方法来计算$\mathbf{A}$。例如，我们可以尝试以下方法：</p><ul><li>我们可以计算$\mathbf{A}_{ij} = \mathbf{B}_{i,:} \mathbf{C}_{:,j}^\top$，也就是说，我们可以通过点积进行逐元素计算。</li><li>我们可以计算$\mathbf{A}_{:,j} = \mathbf{B} \mathbf{C}_{:,j}^\top$，也就是说，我们可以一次计算一列。同样，我们可以一次计算一行。</li><li>我们可以简单地计算$\mathbf{A} = \mathbf{B} \mathbf{C}$。</li><li>我们可以将$\mathbf{B}$和$\mathbf{C}$分成较小的区块矩阵，然后一次计算$\mathbf{A}$的一个区块。</li></ul><p>如果我们使用第一个选择，每次我们计算一个元素$\mathbf{A}_{ij}$时，都需要将一行和一列向量复制到CPU中。更糟糕的是，由于矩阵元素是按顺序对齐的，因此当从内存中读取它们时，我们需要访问两个向量中许多不相交的位置；第二种选择相对更有利：我们能够在遍历$\mathbf{B}$的同时，将列向量$\mathbf{C}_{:,j}$保留在CPU缓存中，它将内存带宽需求减半，相应地提高了访问速度；第三种选择表面上是最可取的，然而大多数矩阵可能不能完全放入缓存中；第四种选择提供了一个实践上很有用的方案：我们可以将矩阵的区块移到缓存中然后在本地将它们相乘。</p><p>让我们来看看这些操作在实践中的效率如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pickletools <span class="keyword">import</span> optimize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line">timer = ppl.Timer()</span><br><span class="line"><span class="comment"># 定义三个256*256的矩阵</span></span><br><span class="line">A = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">B = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">C = paddle.zeros((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐元素计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        A[i, j] = paddle.dot(B[i, :], C[:, j]) <span class="comment"># 计算向量的内积</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：8.538275241851807</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐列计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">    A[:, j] = paddle.mv(B, C[:, j]) <span class="comment"># 计算矩阵和向量的乘积</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：0.06382012367248535</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次性计算A=BC</span></span><br><span class="line">timer.start()</span><br><span class="line">A = paddle.mm(B, C) <span class="comment"># 矩阵相乘</span></span><br><span class="line">timer.stop()</span><br><span class="line"><span class="comment"># 运行结果：0.0013420581817626953</span></span><br></pre></td></tr></table></figure><p><strong>2）小批量</strong></p><p>🎨<code>sec_minibatches</code></p><p>之前我们会理所当然地读取数据的小批量，而不是观测单个数据来更新参数，现在简要解释一下原因。处理单个观测值需要我们执行许多单一矩阵-矢量（甚至矢量-矢量）乘法，这耗费相当大，而且对应深度学习框架也要巨大的开销。这既适用于计算梯度以更新参数时，也适用于用神经网络预测。也就是说，每当我们执行$\mathbf{w} \leftarrow \mathbf{w} - \eta_t \mathbf{g}_t$时，消耗巨大。其中：</p><script type="math/tex; mode=display">\mathbf{g}_t = \partial_{\mathbf{w} } f(\mathbf{x}_{t}, \mathbf{w})</script><p>我们可以通过将其应用于一个小批量观测值来提高此操作的计算效率。也就是说，我们将梯度$\mathbf{g}_t$替换为一个小批量而不是单个观测值：</p><script type="math/tex; mode=display">\mathbf{g}_t = \partial_{\mathbf{w} } \frac{1} {|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} f(\mathbf{x}_{i}, \mathbf{w})</script><p>每个迭代周期的耗时在梯度下降和随机梯度下降之间。让我们看看这对$\mathbf{g}_t$的统计属性有什么影响：由于$\mathbf{x}_t$和小批量$\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的预期保持不变，保留了随机梯度下降的优势（无偏估计）。另一方面，方差（数据离散程度）显著降低。</p><h4 id="3-1-4-实现梯度下降法"><a href="#3-1-4-实现梯度下降法" class="headerlink" title="3.1.4 实现梯度下降法"></a>3.1.4 实现梯度下降法</h4><p>让我们来看看如何从数据中有效地生成小批量。下面我们使用NASA开发的测试机翼的数据集<a href="https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise">不同飞行器产生的噪声</a>来比较这些优化算法。为方便起见，我们只使用前$1,500$样本并进行数据预处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data = np.genfromtxt(</span><br><span class="line">        <span class="string">&#x27;work/airfoil_self_noise.dat&#x27;</span>, <span class="comment"># 数据集在飞将的work文件夹下</span></span><br><span class="line">        dtype=np.float32,</span><br><span class="line">        delimiter=<span class="string">&#x27;\t&#x27;</span> <span class="comment"># 水平制表：tab</span></span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br><span class="line">data = paddle.to_tensor((data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_ch11</span>(<span class="params">batch_size=<span class="number">10</span>, n=<span class="number">1500</span></span>):<span class="comment"># 读取此数据集的前1500个数据，并按每10个数据打包</span></span><br><span class="line">    data = np.genfromtxt(</span><br><span class="line">        <span class="string">&#x27;work/airfoil_self_noise.dat&#x27;</span>,</span><br><span class="line">        dtype=np.float32,</span><br><span class="line">        delimiter=<span class="string">&#x27;\t&#x27;</span></span><br><span class="line">        )</span><br><span class="line">    data = paddle.to_tensor((data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>))</span><br><span class="line">    data_iter = ppl.load_array(</span><br><span class="line">        (data[:n, :-<span class="number">1</span>],</span><br><span class="line">        data[:n, -<span class="number">1</span>]),</span><br><span class="line">        batch_size,</span><br><span class="line">        is_train=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> data_iter, data.shape[<span class="number">1</span>]-<span class="number">1</span> </span><br></pre></td></tr></table></figure><p><strong>1）从零开始实现</strong></p><p>我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。具体来说，我们添加了一个状态输入<code>states</code>并将超参数放在字典<code>hyperparams</code>中。此外，我们将在训练函数里对各个小批量样本的损失求平均，因此优化算法中的梯度不需要除以批量大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="comment"># params为待更新的参数</span></span><br><span class="line">    <span class="comment"># states为状态变量</span></span><br><span class="line">    <span class="comment"># hyperparams为超参数</span></span><br><span class="line">    <span class="comment"># 在神经网络中我们能控制的就是这三个参数</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p.stop_gradient = <span class="literal">True</span><span class="comment"># 梯度停止更新</span></span><br><span class="line">        p.subtract_(hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad) <span class="comment"># subtract_算完之后即可进行赋值操作</span></span><br><span class="line">        p.grad.zero_()</span><br><span class="line">        p.stop_gradient = <span class="literal">False</span><span class="comment"># 梯度开始更新</span></span><br></pre></td></tr></table></figure><p>下面实现一个通用的训练函数【因为我们会学6大优化算法】，以方便本章后面介绍的其他优化算法使用。它初始化了一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其他算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch11</span>(<span class="params">trainer_fn, states, hyperparams, data_iter,</span></span><br><span class="line"><span class="params">               feature_dim, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    <span class="comment"># trainer_fn：线性回归模型</span></span><br><span class="line">    <span class="comment"># data_iter：数据集</span></span><br><span class="line">    <span class="comment"># feature_dim：特征数</span></span><br><span class="line">    <span class="comment"># num_epochs：迭代周期</span></span><br><span class="line">    w = paddle.normal(mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>, shape=(feature_dim, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: ppl.linreg(X, w, b), ppl.squared_loss</span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer_fn([w, b], states, hyperparams)</span><br><span class="line">            n += X.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter),</span><br><span class="line">                             (ppl.evaluate_loss(net, data_iter, loss),))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;timer.avg():<span class="number">.3</span>f&#125;</span> sec/epoch&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> timer.cumsum(), animator.Y[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_sgd</span>(<span class="params">lr, batch_size, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    data_iter, feature_dim = get_data_ch11(batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_ch11(</span><br><span class="line">        sgd, <span class="literal">None</span>, &#123;<span class="string">&#x27;lr&#x27;</span>: lr&#125;, data_iter, feature_dim, num_epochs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">gd_res = train_sgd(<span class="number">1</span>, <span class="number">1500</span>, <span class="number">10</span>)<span class="comment"># 批量梯度下降</span></span><br><span class="line">sgd_res = train_sgd(<span class="number">0.005</span>, <span class="number">1</span>)<span class="comment"># 随机梯度下降</span></span><br><span class="line">mini1_res = train_sgd(<span class="number">0.4</span>, <span class="number">100</span>)<span class="comment"># 小批量随机梯度下降【以100为批量数batch】</span></span><br><span class="line">mini2_res = train_sgd(<span class="number">0.05</span>, <span class="number">10</span>)<span class="comment"># 小批量随机梯度下降【以10为批量数batch】</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">上述四个模型是超参数不同的同类模型，它们要执行的任务都是同一个任务,都是去分析前面1500个关于测试机翼的数据，不同点是拿进来的批量不一样、学习率不一样，由此产生了不同的模型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ppl.set_figsize([<span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">ppl.plot(</span><br><span class="line">    *<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(gd_res, sgd_res, mini1_res, mini2_res))),</span><br><span class="line">    <span class="string">&#x27;time (sec)&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1e-2</span>, <span class="number">10</span>],</span><br><span class="line">    legend=[<span class="string">&#x27;gd&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>, <span class="string">&#x27;batch_size=100&#x27;</span>, <span class="string">&#x27;batch_size=10&#x27;</span>])</span><br><span class="line">ppl.plt.gca().set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/16.svg" alt="svg"></p><center>批量梯度下降</center><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/17.svg" alt="svg"></p><center>随机梯度下降</center><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/18.svg" alt="svg"></p><center>批量梯度下降</center><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/19.svg" alt="svg"></p><center>四个优化算法汇总</center><p>现在我们可以比较前四个实验的时间与损失。可以看出，尽管在处理的样本数方面，随机梯度下降的收敛速度快于梯度下降，但与梯度下降相比，它需要更多的时间来达到同样的损失，因为逐个样本来计算梯度并不那么有效。小批量随机梯度下降能够平衡收敛速度和计算效率。大小为10的小批量比随机梯度下降更有效；大小为100的小批量在运行时间上甚至优于梯度下降。</p><p><strong>2）简洁实现</strong></p><p>下面用深度学习框架自带算法实现一个通用的训练函数，我们将在本章中其它小结使用它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise_ch11</span>(<span class="params">trainer_fn, hyperparams, data_iter, num_epochs=<span class="number">4</span></span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            new_weight = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=m.weight.shape)</span><br><span class="line">            m.weight.set_value(new_weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line"></span><br><span class="line">    optimize = trainer_fn(parameters=net.parameters(), **hyperparams)</span><br><span class="line"></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimize.clear_grad()</span><br><span class="line">            out = net(X)</span><br><span class="line">            y = y.reshape(out.shape)</span><br><span class="line">            l = loss(out, y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            optimize.step()</span><br><span class="line">            n += X.shape[<span class="number">0</span>] <span class="comment"># 累加的批量（0--n，每次加b个）</span></span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(</span><br><span class="line">                    n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter),</span><br><span class="line">                    (ppl.evaluate_loss(net, data_iter, loss) / <span class="number">2</span>, ))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span> sec/epoch&#x27;</span>)</span><br></pre></td></tr></table></figure><p>下面使用这个训练函数，复现之前的实验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_iter, _ = get_data_ch11(<span class="number">10</span>)</span><br><span class="line">trainer = paddle.optimizer.SGD</span><br><span class="line">train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/20.svg" alt="svg"></p><h4 id="3-1-5-小结"><a href="#3-1-5-小结" class="headerlink" title="3.1.5 小结"></a>3.1.5 小结</h4><ul><li>学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。</li><li>对于凸问题，我们可以证明，对于广泛的学习率选择，随机梯度下降将收敛到最优解。</li><li>如果学习率太小或太大，就会出现问题。实际上，通常只有经过多次实验后才能找到合适的学习率。</li><li>当训练数据集中有更多样本时，计算梯度下降的每次迭代的代价更高，因此在这些情况下，首选随机梯度下降。</li><li>随机梯度下降的最佳性保证在非凸情况下一般不可用，因为需要检查的局部最小值的数量可能是指数级的。</li><li>在迭代过程中，由于存在噪音，下降会朝向最小值，但是不会精确收敛，只会在附近摆动，更重要的原因在于：学习率是一个固定值。</li><li>由于减少了深度学习框架的额外开销，使用更好的内存方位以及CPU和GPU上的缓存，向量化使代码更加高效。</li><li>随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。</li><li>在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。</li><li>在训练期间降低学习率有助于训练。</li><li>一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。</li><li>批量合适很重要：过大那么计算代价大，过小虽然收敛快但是计算慢。</li></ul><h3 id="3-2-动量法"><a href="#3-2-动量法" class="headerlink" title="3.2 动量法"></a>3.2 动量法</h3><p>🎨<code>sec_momentum</code></p><p>模型通过训练数据来拟合模型参数，使得模型能够根据输入来正确输出。那么如果输出距离我们正确答案有差异，我们将衡量差异的函数定义为目标函数（损失函数等）。<br>我们希望目标函数为0，这样就表明我们模型完全正确输出我们的期望。但是因为特征维度的丰富，目标函数的解析解，不可求或者不好求，所以运用计算机强大的运算能力来求数值解，即：只要误差在可接受范围内，我们就认为这个解释我们的目标解。</p><p>我们求解数值解的方法，我们采用了<strong>迭代逼近</strong>的方法。一次次迭代x的值，使其逐渐逼近误差函数的最小值。<br>但是这样速度太慢，为此我们考虑沿着反梯度方向可是实现最速下降，故引入了梯度下降法。</p><h4 id="3-2-1-质疑"><a href="#3-2-1-质疑" class="headerlink" title="3.2.1 质疑"></a>3.2.1 质疑</h4><p>有人觉得仅仅考虑沿梯度下降还不够。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量，在当前位置下降，有人提出质疑：仅仅考虑当前位置不行的，<br>要考虑整体方向，运动也有个惯性，历史也有大势所趋，肯定不能只看当前位置。所以在自变量更新的时候考虑引入速度变量，有速度就有惯性。惯性越大，越势不可挡。</p><p><strong>梯度下降的问题</strong></p><p>为了更好地了解动量法的几何属性，我们复习一下梯度下降。我们构造一个目标函数$f(\mathbf{x})=x_1^2+2x_2^2$，并有二维向量$\mathbf{x} = [x_1, x_2]^\top$作为输入，标量作为输出。在此基础上我们稍加改动。</p><script type="math/tex; mode=display">f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.</script><p>与之前一样，$f$在$(0, 0)$有最小值，该函数在$x_1$的方向上非常平坦。让我们看看在这个新函数上执行梯度下降时会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1**<span class="number">2</span> + <span class="number">2</span> * x2**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gd_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(gd_2d))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-4.257978, x2: -0.061181</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/21.svg" alt="svg"></p><p>可以看到，同一位置上，目标函数在竖直方向比在水平方向的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。<br>因此，我们陷入两难：如果选择较小的学习率，我们会确保解不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。</p><p>下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化：$x_1$方向上的收敛有所改善，但整体来看解的质量更差了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(gd_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.387814, x2: -1673.365109</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/22.svg" alt="svg"></p><p>在梯度下降中，我们需要一个较小的学习率从而避免自变量在梯度较大的方向上越过目标函数最优解，然而，这会造成自变量在梯度较小的方向上朝最优解移动变慢；我们试着将学习率调得稍大一点，此时自变量在梯度较大的方向又会不断越过最优解并逐渐发散。</p><h4 id="3-2-2-动量法地提出"><a href="#3-2-2-动量法地提出" class="headerlink" title="3.2.2 动量法地提出"></a>3.2.2 动量法地提出</h4><p><strong>动量法（momentum）</strong> 使我们能够解决上面描述的梯度下降问题。动量法又被称作基于动量的梯度下降法(SGD with momentum)，是一种使梯度向量向相关方向加速变化、最终实现加速收敛的方法。动量法引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。</p><p>在中学物理中，刻画惯性的物理量是动量，沿山谷滚下的铁球会收到沿坡道向下的力和与左右山壁碰撞的弹力，向下的力（重力）稳定不变，产生的动量不断累积，速度越来越快；左右的弹力总是在不停切换，动量累积的结果是相互抵消，减弱了球的来回震荡。因此，与随机梯度下降相比，动量方法的收敛速度更快，收敛曲线也更稳定，见下图。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/da1bdcde82e445b8be8070b97eeef064ba5c62d5a2ff45c6bd203482e2df3be1" width="500" hegiht="" ></center><center>图4：SGD与带动量SGD </center><p>每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即<strong>每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加</strong> 得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）喜欢稍重篮球的比例较高，一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。</p><p><strong>1）动量法公式</strong></p><p>动量法引入了速度$\mathbf{v}_t$，它包含了参数在参数空间移动的方向和速率。使用$\mathbf{v}_t$而不是梯度$\mathbf{g}_t$可以生成以下更新等式：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} + \eta_t \mathbf{g}_{t}, \\\mathbf{x}_t &\leftarrow \mathbf{x}_{t-1} -  \mathbf{v}_t.\end{aligned}</script><p>这里的$\mathbf{v}_t$就是动量，它是一段时间内平均预测的值，$\beta$为动量系数。对于$\beta = 0$，我们恢复常规的梯度下降。<br>在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。为了方便起见，我们在时间$t=0$初始化$\mathbf{v}_0 = 0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">momentum_2d</span>(<span class="params">x1, x2, v1, v2</span>):</span><br><span class="line">    <span class="comment">#print(v1,v2)</span></span><br><span class="line">    v1 = beta * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = beta * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, beta = <span class="number">0.6</span>, <span class="number">0.5</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(momentum_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1: 0.007188, x2: 0.002553</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/23.svg" alt="svg"></p><p>正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。让我们看看当降低动量参数时会发生什么。将其减半至$\beta = 0.25$会导致一条几乎没有收敛的轨迹。尽管如此，它比没有动量时解将会发散要好得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta, beta = <span class="number">0.6</span>, <span class="number">0.25</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(momentum_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.126340, x2: -0.186632</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/24.svg" alt="svg"></p><p><strong>2）指数加权平均</strong></p><p>我们可以用指数加权移动平均理解动量法。在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。</p><p><strong>滑动平均(exponential moving average)</strong>，或者叫做<strong>指数加权平均(exponentially weighted moving average)</strong>，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。指数移动加权平均法，是指各数值的加权系数随时间呈指数式递减，越靠近当前时刻的数值加权系数就越大，较传统的平均法来说，一是不需要保存过去所有的数值；二是计算量显著减小。</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} +({1-\beta}) \mathbf{θ}_{t}\end{aligned}</script><p>上式中 $θt$为时刻$t$ 的实际值；系数 $β$ 表示加权下降的速率，其值越小下降的越快；$vt$ 为 $t$时刻EWMA（指数加权移动平均） 的值。在实践中，我们通常将$\mathbf{v}_t$ 看做是对最近 $\frac{1} {1-\beta}$个时间步的$\mathbf{θ}_{t}$值的加权平均。我们使用$\beta = 0.9$来验证这个公式。</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_{100} = 0.9 \mathbf{v}_{99} + 0.1 \mathbf{θ}_{100}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_{99} = 0.9 \mathbf{v}_{98} + 0.1 \mathbf{θ}_{99}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_{98} = 0.9 \mathbf{v}_{97} + 0.1 \mathbf{θ}_{98}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}...\end{aligned}</script><p>我们将式子一步一步的带入得到最终的推导：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_{n} = 0.1 \mathbf{θ}_{n} + (0.9)^{1}0.1 \mathbf{θ}_{n-1} + (0.9)^{2}0.1 \mathbf{θ}_{n-2} ......+ (0.9)^{n-1}0.1 \mathbf{θ}_{1} \end{aligned}</script><p>所以我们计算出来的$\mathbf{v}_{n}$实际上是包含了以前所有的结果。对于$\mathbf{v}_{20}$各参数的数值如下图：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a21079e5e7f84d53b822c3786c1e750f1702d4a2f89542af8f6162a7e3ca08a4" width="600" hegiht="" ></center><center>图5：$\mathbf{v}_{20}$参数取值 </center><p>可以看出，对于$\mathbf{v}_{20}$来说，$\mathbf{θ}_{20}$的权重几乎是$\mathbf{θ}_{1}$的9倍，这个时候即使我们忽略$\mathbf{θ}_{1}$其实对结果也没有太大的影响，所以我们进一步规定，如果权重低于$\mathbf{θ}_{n}$的$\frac{1} {e}$，那么我们就认为该数值在平均中起很小的作用，在数学上一般会以$\frac{1} {e}$作为一个临界值，小于该值的加权系数的值不做考虑。这样上图我们可以将$\mathbf{v}_{20}$看作是红线以上的10个数据的指数加权平均，当$\beta = 0.9$时，指数加权平均考虑了估计点附近的10个数据。</p><p>现在我们可以对动量法的速度变量做变形：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} +({1-\beta}) (\frac{\eta_t} { {1-\beta} }\mathbf{g}_{t})\end{aligned}</script><p>指数加权平均是在将当前值用其前面时间的值表示，所以动量法中当前的动量就可以用前面的动量表示，再利用当前动量影响自变量。所以说，在动量法中，我们根据 $\beta$ 的值，确定当前的动量是根据前多少个时间步的动量做的指数加权移动平均，利用这些数据逐渐缩小参数收敛速率。</p><h4 id="3-2-3-实现动量法"><a href="#3-2-3-实现动量法" class="headerlink" title="3.2.3 实现动量法"></a>3.2.3 实现动量法</h4><p>相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度，它与梯度以及优化问题的变量具有相同的形状。类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，而步长则是时间，前一次步伐好比前一时刻的速度。标准梯度下降算法在每次行动时，都忽略前一时刻的速度，而重新根据当前时刻的加速度和时间来行走，因此当加速度趋于零时就很难继续移动。<br>而动量方法则考虑前一时刻速度和当前加速度的共同作用。</p><p><strong>1）从零开始实现</strong></p><p>在下面的实现中，我们称这些变量为<code>states</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>): <span class="comment"># 获取优化的特征个数，对应速度(状态变量)的个数</span></span><br><span class="line">    v_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;SGD动量&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>): <span class="comment"># 参数 状态变量 超参数</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + hyperparams[<span class="string">&#x27;learning_rate&#x27;</span>] * p.grad</span><br><span class="line">            p[:] -=  v</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_momentum</span>(<span class="params">lr, momentum, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    ppl.train_ch11(</span><br><span class="line">        sgd_momentum, init_momentum_states(feature_dim),</span><br><span class="line">        &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr, <span class="string">&#x27;momentum&#x27;</span>: momentum&#125;, data_iter,</span><br><span class="line">        feature_dim, num_epochs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>) <span class="comment"># 分别对应数据迭代器和特征个数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/25.svg" alt="svg"></p><p>在实际过程中，选择合适的动量参数对模型的收敛十分关键。当我们将动量超参数<code>momentum</code>增加到0.9时，它相当于有效样本数量增加到$\frac{1} {1 - 0.9} = 10$。可以看到，损失一直在动荡，甚至出现了剧增，模型不收敛，且效果不稳定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/26.svg" alt="svg"></p><p>降低学习率进一步解决了任何非平滑优化问题的困难，将其设置为$0.005$会产生良好的收敛性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_momentum(<span class="number">0.005</span>, <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/27.svg" alt="svg"></p><p><strong>2）简洁实现</strong></p><p>深度学习框架中的优化器早已构建了动量法。<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Overview_cn.html">paddle.optimizer</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Momentum</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.005</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/28.svg" alt="svg"></p><h4 id="3-2-4-小结"><a href="#3-2-4-小结" class="headerlink" title="3.2.4 小结"></a>3.2.4 小结</h4><ul><li>动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。</li><li>对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。</li><li>动量法可以防止在随机梯度下降的优化过程停滞的问题。</li><li>动量法的实现非常简单，但它需要我们存储额外的状态向量（动量$\mathbf{v}$）。</li><li>由于对过去的数据进行了指数降权，有效梯度数为$\frac{1} {1-\beta}$。</li></ul><h3 id="3-3-AdaGrad"><a href="#3-3-AdaGrad" class="headerlink" title="3.3 AdaGrad"></a>3.3 AdaGrad</h3><p>🎨<code>sec_adagrad</code></p><p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。在动量法一节里我们看到，当$x1$和$x2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。</p><p><strong>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能</strong>。本节我们介绍$AdaGrad$算法，<strong>它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率</strong>，从而避免统一的学习率难以适应所有维度的问题。这有两个好处：首先，我们不再需要决定梯度何时算足够大。其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。</p><h5 id="1）稀疏特征和学习率"><a href="#1）稀疏特征和学习率" class="headerlink" title="1）稀疏特征和学习率"></a><strong>1）稀疏特征和学习率</strong></h5><p>假设我们正在训练一个语言模型。为了获得良好的准确性，我们大多希望在训练的过程中降低学习率，原因在于稀疏特征（即只在偶尔出现的特征）的影响，这对自然语言来说很常见。例如，我们看到“预先条件”这个词比“学习”这个词的可能性要小得多。但是，它在计算广告学和个性化协同过滤等其他领域也很常见。只有在这些不常见的特征出现时，与其相关的参数才会得到有意义的更新。鉴于学习率下降，我们可能最终会面临这样的情况：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。</p><p>解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率【AdaGrad算法的核心思想】。</p><h5 id="2）AdaGrad算法"><a href="#2）AdaGrad算法" class="headerlink" title="2）AdaGrad算法"></a><strong>2）AdaGrad算法</strong></h5><p>AdaGrad优化算法在每次使用一个批量的数据进行参数更新的时候，计算所有参数的梯度，使用变量$\mathbf{s}_t$来累加过去的小批量随机梯度$\mathbf{g}_t^2$，接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{g}_t & = \partial_{\mathbf{w} } l(y_t, f(\mathbf{x}_t, \mathbf{w})), \\    \mathbf{s}_t & = \mathbf{s}_{t-1} + \mathbf{g}_t^2, \\    \mathbf{w}_t & = \mathbf{w}_{t-1} - \frac{\eta} {\sqrt{\mathbf{s}_t + \epsilon} } \cdot \mathbf{g}_t.\end{aligned}</script><p>在这里，操作是按照坐标顺序应用，这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。与之前一样，$\eta$是学习率，$\epsilon$是一个为维持数值稳定性而添加的常数，用来确保我们不会除以$0$。因为有可能 $\mathbf{s}$ 的值为 0，那么 0 出现在分母就会出现无穷大的情况。这样不同的参数由于梯度不同，他们对应的 $\mathbf{s}$ 大小也就不同，所以上面的公式得到的学习率也就不同，这也就实现了自适应的学习率。最后，我们初始化$\mathbf{s}_0 = \mathbf{0}$。</p><p>需要强调的是，小批量随机梯度按元素平方的累加变量$\large s_{t}$出现在学习率的分母项中。因此：</p><ul><li>如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快。</li><li>如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。</li></ul><p>我们使用自适应的学习率就可以帮助算法在梯度大的参数方向减缓学习速率，而在梯度小的参数方向加快学习速率，这就导致了神经网络的训练速度的加快。然而，由于累加变量$\large s_{t}$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率较小，可能较难找到一个有用的解。在深度学习中，我们可能希望更慢地降低学习率，这引出了许多AdaGrad算法的变体，我们将在后续章节中讨论它们。</p><p>眼下让我们先看看它在二次凸问题中的表现如何。我们仍然同一函数为例：</p><script type="math/tex; mode=display">f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.</script><p>我们将使用与之前相同的学习率来实现AdaGrad算法，即$\eta = 0.4$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span> <span class="comment"># 为维持数值稳定性而添加的常数</span></span><br><span class="line">    g1, g2 = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2 <span class="comment"># 自变量梯度</span></span><br><span class="line">    <span class="comment"># 累加变量 s1 s2 变量 x1 x2</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span> </span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(adagrad_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-2.382563, x2: -0.158591</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/29.svg" alt="svg"></p><p>可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。我们将学习率提高到$2$，可以看到更好的表现，这说明学习率降低的情况可能相当剧烈，我们需要确保参数能够适当地收敛。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(adagrad_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.002295, x2: -0.000000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/30.svg" alt="svg"></p><h4 id="3）实现"><a href="#3）实现" class="headerlink" title="3）实现"></a><strong>3）实现</strong></h4><p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。不同点在于，这里需要使用<strong><em>更大的学习率</em></strong>来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adagrad_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;adagrad算法&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] += paddle.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;learning_rate&#x27;</span>] * p.grad / paddle.sqrt(s + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch11</span>(<span class="params">trainer_fn, states, hyperparams, data_iter,</span></span><br><span class="line"><span class="params">               feature_dim, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    w = paddle.normal(mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>, shape=(feature_dim, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: ppl.linreg(X, w, b), ppl.squared_loss</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    animator = ppl.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">0</span>, num_epochs], ylim=[<span class="number">0.22</span>, <span class="number">0.35</span>])</span><br><span class="line">    n, timer = <span class="number">0</span>, ppl.Timer()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer_fn([w, b], states, hyperparams) <span class="comment"># 优化算法</span></span><br><span class="line">            n += X.shape[<span class="number">0</span>] <span class="comment"># 每次增加b个样本</span></span><br><span class="line">            <span class="keyword">if</span> n % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                timer.stop()</span><br><span class="line">                animator.add(n/X.shape[<span class="number">0</span>]/<span class="built_in">len</span>(data_iter), <span class="comment"># x轴一轮迭代的时间坐标</span></span><br><span class="line">                             (ppl.evaluate_loss(net, data_iter, loss),))</span><br><span class="line">                timer.start()</span><br><span class="line">    <span class="keyword">return</span> timer.avg(), animator.Y[<span class="number">0</span>][-<span class="number">1</span>] <span class="comment"># 返回每一轮迭代时间的平均值、最终的误差值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adagrad, init_adagrad_states(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter, feature_dim)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.062071673075358075, 0.24227345788478852)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/31.svg" alt="svg"></p><p>我们也可以直接使用深度学习框架中提供的AdaGrad算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adagrad</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/32.svg" alt="svg"></p><p>​        AdaGrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新，这就是Adagrad算法加快深层神经网络的训练速度的核心。<br>类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。</p><h4 id="3-4-RMSProp"><a href="#3-4-RMSProp" class="headerlink" title="3.4 RMSProp"></a>3.4 RMSProp</h4><p>🎨<code>sec_rmsprop</code></p><p>上一节AdaGrad算法中，我们提到，因为调整学习率时分母上的累加变量一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。<br>因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。简言之：冲的太猛了。</p><p>问题在于，Adagrad算法将梯度$\mathbf{g}_t$的平方累加成状态矢量$\mathbf{s}_t = \mathbf{s}_{t-1} + \mathbf{g}_t^2$。因此，由于缺乏规范化，没有约束力，$\mathbf{s}_t$持续增长，几乎上是在算法收敛时呈线性递增。</p><p>RMSProp是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。</p><h5 id="1）RMSProp算法"><a href="#1）RMSProp算法" class="headerlink" title="1）RMSProp算法"></a><strong>1）RMSProp算法</strong></h5><p>RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。让我们详细写出这些方程式。</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{s}_t & \leftarrow \gamma \mathbf{s}_{t-1} + (1 - \gamma) \mathbf{g}_t^2, \\    \mathbf{x}_t & \leftarrow \mathbf{x}_{t-1} - \frac{\eta} {\sqrt{\mathbf{s}_t + \epsilon} } \odot \mathbf{g}_t.\end{aligned}</script><p>常数$\epsilon &gt; 0$通常设置为$10^{-6}$，以确保我们不会因除以零或步长过大而受到影响。鉴于这种扩展，我们现在可以自由控制学习率$\eta$，而不考虑基于每个坐标应用的缩放。</p><h5 id="2）实现"><a href="#2）实现" class="headerlink" title="2）实现"></a><strong>2）实现</strong></h5><p>和之前一样，我们使用二次函数$f(\mathbf{x})=0.1x_1^2+2x_2^2$来观察RMSProp算法的轨迹。回想在 :numref:<code>sec_adagrad</code>一节中，当我们使用学习率为0.4的AdaGrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。RMSProp算法中不会发生这种情况，因为$\eta$是单独控制的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cProfile <span class="keyword">import</span> label <span class="comment"># 性能分析模块</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop_2d</span>(<span class="params">x1, x2, s1, s2</span>):</span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = gamma * s1 + (<span class="number">1</span> - gamma) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = gamma * s2 + (<span class="number">1</span> - gamma) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_2d</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">ppl.show_trace_2d(f_2d, ppl.train_2d(rmsprop_2d))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">epoch 20, x1:-0.010599, x2: 0.000000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/33.svg" alt="svg"></p><p>接下来，我们在深度网络中实现RMSProp算法。在这里，$\mathbf{s}$累加了过去的$1/(1-\gamma) = 10$次平方梯度观测值的平均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rmsprop_state</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = paddle.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;RMSProp算法&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    gamma, eps = hyperparams[<span class="string">&#x27;gamma&#x27;</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] = gamma * s + (<span class="number">1</span> - gamma) * paddle.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / paddle.sqrt(s + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    rmsprop, init_rmsprop_state(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter, feature_dim</span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.07110411326090495, 0.24332575956980387)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/34.svg" alt="svg"></p><p>我们可直接使用深度学习框架中提供的RMSProp算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.RMSProp</span><br><span class="line">ppl.train_concise_ch11(</span><br><span class="line">    trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/35.svg" alt="svg"></p><h4 id="3-5-Adadelta"><a href="#3-5-Adadelta" class="headerlink" title="3.5 Adadelta"></a>3.5 Adadelta</h4><p>🎨<code>sec_adadelta</code></p><p>除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期较难找到有用解的问题做了改进：主要区别在于前者减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准。</p><h5 id="1）Adadelta算法"><a href="#1）Adadelta算法" class="headerlink" title="1）Adadelta算法"></a><strong>1）Adadelta算法</strong></h5><p>Adadelta使用两个状态变量，$\mathbf{s}_t$来接收小批量随机梯度按元素平方的指数加权移动平均变量：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{s}_t & = \rho \mathbf{s}_{t-1} + (1 - \rho) \mathbf{g}_t^2.\end{aligned}</script><p>与之前算法的不同点在于，额外增加了一个状态变量：$\Delta\mathbf{x}_t$，我们将$\Delta \mathbf{x}_{0}$初始化为$0$，使用$\Delta \mathbf{x}_{t-1}$【上一次更新的长度】来计算自变量的变化量，$\epsilon$（例如$10^{-5}$这样的小值）是为了保持数字稳定性而加入的：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{g}_t' & = \frac{\sqrt{\Delta\mathbf{x}_{t-1} + \epsilon} } {\sqrt{ {\mathbf{s}_t + \epsilon} }} \odot \mathbf{g}_t, \\\end{aligned}</script><p>接着我们使用重新缩放的梯度$\mathbf{g}_t’$执行更新，即：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{x}_t  & = \mathbf{x}_{t-1} - \mathbf{g}_t'. \\\end{aligned}</script><p>我们使用$\large \Delta x_{t}$来记录自变量变化量$\large {g}’_{t}$按元素平方的指数加权移动平均，即：</p><script type="math/tex; mode=display">\begin{aligned}    \Delta \mathbf{x}_t & = \rho \Delta\mathbf{x}_{t-1} + (1 - \rho) {\mathbf{g}_t'}^2,\end{aligned}</script><p>AdaDelta算法跟RMSProp算法的不同之处在于使用$\large \sqrt{\Delta x_{t-1} }$来替代学习率$\large \eta$。</p><h5 id="2）实现-1"><a href="#2）实现-1" class="headerlink" title="2）实现"></a><strong>2）实现</strong></h5><p>Adadelta需要为每个变量维护两个状态变量，即$\mathbf{s}_t$和$\Delta\mathbf{x}_t$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adadelta_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w, s_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    delta_w, delta_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adadelta</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    rho, eps = hyperparams[<span class="string">&#x27;rho&#x27;</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            s[:] = rho * s + (<span class="number">1</span> - rho) * paddle.square(p.grad)</span><br><span class="line">            g = (paddle.sqrt(delta + eps) / paddle.sqrt(s + eps)) * p.grad</span><br><span class="line">            p[:] -= g</span><br><span class="line">            delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        p.grad.zero_()</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adadelta, init_adadelta_states(feature_dim), </span><br><span class="line">    &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter, feature_dim)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0.08289804458618164, 0.24341165928045908)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/36.svg" alt="svg"></p><p>我们也可以简洁实现adadelta算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adadelta</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)<span class="comment"># 没有学习率这个超参数了</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/37.svg" alt="svg"></p><blockquote><p>初始化没有数据支撑，就是随便下降，所以下降的比较缓慢。。</p></blockquote><h4 id="3-6-Adam"><a href="#3-6-Adam" class="headerlink" title="3.6 Adam"></a>3.6 Adam</h4><p>🎨<code>sec_adam</code></p><p>由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这就是当前广泛应用的算法。</p><h5 id="1）Adam算法"><a href="#1）Adam算法" class="headerlink" title="1）Adam算法"></a><strong>1）Adam算法</strong></h5><p>Adam算法的关键组成部分之一是：它使用使用了动量法中的动量变量$\mathbf{v}_t$和RMSProp算法中的小批量随机梯度按元素平方的指数加权移动平均变量$\mathbf{s}_t$，即：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{v}_t & \leftarrow \beta_1 \mathbf{v}_{t-1} + (1 - \beta_1) \mathbf{g}_t, \\    \mathbf{s}_t & \leftarrow \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2.\end{aligned}</script><p>这里$\beta_1$和$\beta_2$是非负加权参数。他们的常见设置是$\beta_1 = 0.9$和$\beta_2 = 0.999$。注意，如果我们初始化$\mathbf{v}_0 = \mathbf{s}_0 = 0$，就会获得一个相当大的初始偏差，原因在于每个最新数据值，都依赖于以前的数据结果。我们可以通过使用<strong>偏差修正（bias correction）</strong> 来解决这个问题，使用$\hat{\mathbf{v} }_t$和$\hat{\mathbf{s} }_t$，公式为：</p><script type="math/tex; mode=display">\hat{\mathbf{v} }_t = \frac{\mathbf{v}_t} {1 - \beta_1^t} \text{ and } \hat{\mathbf{s} }_t = \frac{\mathbf{s}_t} {1 - \beta_2^t}.</script><p>通过公式发现，随着$t$增加，$\beta^t$接近于0，所以当$t$很大的时候，偏差修正几乎没有作用，不过在开始学习阶段，偏差修正可以帮助我们更好预测。有了正确的估计，我们现在可以写出更新方程。首先，我们以非常类似于RMSProp算法的方式重新缩放梯度以获得</p><script type="math/tex; mode=display">\mathbf{g}_t' = \frac{\eta \hat{\mathbf{v} }_t} {\sqrt{\hat{\mathbf{s} }_t} + \epsilon}.</script><p>与RMSProp不同，我们的更新使用动量$\hat{\mathbf{v} }_t$而不是梯度本身。此外，由于使用$\frac{1} {\sqrt{\hat{\mathbf{s} }_t} + \epsilon}$而不是$\frac{1} {\sqrt{\hat{\mathbf{s} }_t + \epsilon} }$进行缩放，两者会略有差异。前者在实践中效果略好一些，因此与RMSProp算法有所区分。通常，我们选择$\epsilon = 10^{-6}$，这是为了在数值稳定性和逼真度之间取得良好的平衡。最后，我们简单更新：</p><script type="math/tex; mode=display">\mathbf{x}_t \leftarrow \mathbf{x}_{t-1} - \mathbf{g}_t'.</script><h5 id="2）实现-2"><a href="#2）实现-2" class="headerlink" title="2）实现"></a><strong>2）实现</strong></h5><p>我们将时间步$t$存储在<code>hyperparams</code>字典中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w, v_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    s_w, s_b = paddle.zeros((feature_dim, <span class="number">1</span>)), paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            s[:] = beta2 * s + (<span class="number">1</span> - beta2) * paddle.square(p.grad)</span><br><span class="line">            v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (paddle.sqrt(s_bias_corr) + eps)</span><br><span class="line">        p.grad.zero_()</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>现在，我们用以上Adam算法来训练模型，这里我们使用$\eta = 0.01$的学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_iter, feature_dim = ppl.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_ch11(</span><br><span class="line">    adam, init_adam_states(feature_dim),</span><br><span class="line">    &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, data_iter, feature_dim)</span><br></pre></td></tr></table></figure><pre><code>(0.07531051635742188, 0.24483665378888447)</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/38.svg" alt="svg"></p><p>此外，我们可以用深度学习框架自带算法应用Adam算法，这里我们只需要传递配置参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.Adam</span><br><span class="line">ppl.train_concise_ch11(trainer, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/39.svg" alt="svg"></p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong><em>小结</em></strong></h4><ul><li>AdaGrad算法会在单个坐标层面动态降低学习率。</li><li>AdaGrad算法利用梯度的大小作为调整速率的手段：用较小的学习率来补偿带有较大梯度的坐标。</li><li>如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。</li><li>AdaGrad算法对于稀疏特征特别有效，在此情况下由于不常出现的问题，学习率需要更慢地降低。</li><li>AdaGrad算法在迭代过程中不断调整学习率，并让目标函数自变量中每个元素都分别拥有自己的学习率。</li><li>使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</li><li>RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。</li><li>RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。</li><li>Adadelta没有学习率参数。相反，它使用参数本身的变化率来调整学习率。</li><li>Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。</li><li>Adam算法使用了偏差修正。</li><li>和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。</li></ul><h2 id="四、提高模型泛化能力—正则化方法"><a href="#四、提高模型泛化能力—正则化方法" class="headerlink" title="四、提高模型泛化能力—正则化方法"></a>四、提高模型泛化能力—正则化方法</h2><p>降低偏差可以提高模型在训练数据上的表现。但实际上，评价一个机器学习模型的性能，不仅要看其在训练集上的表现，还要评估它在未观测到的数据上的表现，这就要求模型具有良好的泛化能力。本节介绍的正则化方法能够提高模型泛化能力，减少训练误差和测试误差的差距。下面将介绍几种常见的正则化方法。</p><h3 id="4-1-L2正则与L1正则"><a href="#4-1-L2正则与L1正则" class="headerlink" title="4.1 L2正则与L1正则"></a>4.1 L2正则与L1正则</h3><p>正则化方法是在目标函数或代价函数后面加上一个正则项，对参数进行约束，来限制模型的学习能力，是应对过拟合的常用方法。回想一下，在多项式回归的例子中，我们可以通过调整拟合多项式的阶数来限制模型的容量，也就是限制参数值的选择范围。</p><script type="math/tex; mode=display">L(\mathbf{w}, b) 满足 ||\mathbf{w}||^2 ≤ \lambda</script><p>这里我们使得权重的平方和小于一个值，以此来限制参数值以达到控制模型复杂度的目的，因为偏置对模型复杂度的影响较小，所以我们通常不限制。</p><h4 id="L2正则"><a href="#L2正则" class="headerlink" title="L2正则"></a><strong>L2正则</strong></h4><p>L2参数正则化方法也叫做权重衰减。<strong>权重衰减（weight decay）</strong> 是最广泛使用的正则化的技术之一。$L_2$范数惩罚项是模型权重参数每个元素的平方和与一个正的常数的乘积。我们通过正则化常数$\lambda$来描述这种权衡，$L_2$范数正则化在模型原损失函数的基础上添加$L_2$范数惩罚项，从而得到训练得到所需的最小化函数。当参数越多或取值越大时，该惩罚项越大：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda} {2} \|\mathbf{w}\|^2</script><p>对于$\lambda = 0$，我们恢复了原来的损失函数。对于$\lambda &gt; 0$，我们限制$| \mathbf{w} |$的大小。这里我们仍然除以$2$：当我们取一个二次函数的导数时，$2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。</p><p>根据之前章节所讲的，我们根据估计值与观测值之间的差异来更新$\mathbf{w}$。然而，加入了正则化之后，我们同时也在试图将$\mathbf{w}$的大小进一步缩小。这就是为什么这种方法有时被称为权重衰减。我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的$\lambda$值对应较少约束的$\mathbf{w}$，而较大的$\lambda$值对$\mathbf{w}$的约束更大。</p><h4 id="L1正则"><a href="#L1正则" class="headerlink" title="L1正则"></a><strong>L1正则</strong></h4><p>L1正则化也是一种常见的正则化方法，它使模型的参数尽可能稀疏化。L1正则化被定义为各个参数的绝对值之和：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \|\mathbf{w}\|</script><p>此外，你可能会问为什么我们首先使用$L_2$范数，而不是$L_1$范数。事实上，这个选择在整个统计领域中都是有效的和受欢迎的。使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为稳定。相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，而将其他权重清除为零。这称为<strong>特征选择（feature selection）</strong>，这可能是其他场景下需要的。</p><h4 id="1）从零开始实现"><a href="#1）从零开始实现" class="headerlink" title="1）从零开始实现"></a><strong>1）从零开始实现</strong></h4><p>下面我们将从头开始实现权重衰减，只需将$L_2$的平方惩罚添加到原始目标函数中。首先，我们像以前一样生成一些数据，生成公式如下：</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.01^2).</script><p>我们选择标签是关于输入的线性函数。标签同时被均值为0，标准差为0.01高斯噪声破坏。为了使过拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，并使用一个只包含20个样本的小训练集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.tensor <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_examples, <span class="built_in">len</span>(w)))<span class="comment">#均值为0，标准差为0.01，n个样本，w长度的特征</span></span><br><span class="line">    y = paddle.matmul(X, w) + b</span><br><span class="line">    y += paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)<span class="comment">#噪音</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment">#reshape(-1,1)转换成1列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_array, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构造数据迭代器</span></span><br><span class="line">    dataset = TensorDataset(data_array)<span class="comment"># 由张量列表定义的数据集</span></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=is_train) <span class="comment"># 之后从DataLoader中随机挑选b个样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># print(&quot;linreg: &quot;, X, w, b)</span></span><br><span class="line">    <span class="keyword">return</span> paddle.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(&quot;squared_loss: &quot;, y_hat, y)</span></span><br><span class="line">    loss = (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span> <span class="comment"># loss=1/2(y_hat-y)^2</span></span><br><span class="line">    <span class="comment">#print(&quot;loss&gt;&gt;&gt;&quot;, loss)</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>) <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line">true_w, true_b = paddle.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = load_array(train_data, batch_size)</span><br><span class="line">test_data = synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = load_array(test_data, batch_size)</span><br></pre></td></tr></table></figure><h5 id="【初始化模型参数】"><a href="#【初始化模型参数】" class="headerlink" title="【初始化模型参数】"></a><strong>【初始化模型参数】</strong></h5><p>首先，我们将定义一个函数来随机初始化模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">    w.stop_gradient = <span class="literal">False</span></span><br><span class="line">    b = paddle.zeros((<span class="number">1</span>, ))</span><br><span class="line">    b.stop_gradient = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><h5 id="【定义-L-2-范数惩罚】"><a href="#【定义-L-2-范数惩罚】" class="headerlink" title="【定义$L_2$范数惩罚】"></a><strong>【定义$L_2$范数惩罚】</strong></h5><p>实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> paddle.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><h5 id="【定义训练代码实现】"><a href="#【定义训练代码实现】" class="headerlink" title="【定义训练代码实现】"></a><strong>【定义训练代码实现】</strong></h5><p>下面的代码将模型拟合训练数据集，并在测试数据集上进行评估。已知，线性网络和平方损失没有变化，唯一的变化是损失现在包括了惩罚项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib_inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        matplotlib_inline.backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line">        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = set_axes( self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):<span class="comment">#lambd超参数</span></span><br><span class="line">    w, b = init_params()<span class="comment">#初始化</span></span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: linreg(X, w, b), squared_loss<span class="comment">#线性回归，均方损失</span></span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.03</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">                w -= lr * w.grad / X.shape[<span class="number">0</span>]</span><br><span class="line">                b -= lr * b.grad / batch_size</span><br><span class="line">                w.clear_grad()</span><br><span class="line">                b.clear_grad()</span><br><span class="line">                w.stop_gradient = <span class="literal">False</span></span><br><span class="line">                b.stop_gradient = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w的L2范数是：&quot;</span>, paddle.norm(w).item())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>①【忽略正则化直接训练】</strong></p><p>我们现在用<code>lambd = 0</code>禁用权重衰减后运行这个代码。这里训练误差有了减少，但测试误差没有减少，这意味着出现了严重的过拟合。</p><blockquote><p>粉色：测试误差</p><p>蓝色：训练误差</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/40.svg" alt="svg"></p><p><strong>②【使用权重衰减】</strong></p><p>下面，我们使用权重衰减来运行代码，这正是我们期望从正则化中得到的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/41.svg" alt="svg"></p><h4 id="2）简洁实现"><a href="#2）简洁实现" class="headerlink" title="2）简洁实现"></a><strong>2）简洁实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.set_value(paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=param.shape))</span><br><span class="line"></span><br><span class="line">    loss = paddle.nn.MSELoss()</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    sdg = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters(), weight_decay=<span class="built_in">float</span>(wd))<span class="comment"># 注意：权重衰减的系数要是一个浮点数~</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            sdg.step()</span><br><span class="line">            sdg.clear_grad()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br><span class="line">    <span class="built_in">print</span>(net[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">train_concise(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><pre><code>w的L2范数： 0.11938107758760452Linear(in_features=200, out_features=1, dtype=float32)</code></pre><h3 id="4-2-暂退法"><a href="#4-2-暂退法" class="headerlink" title="4.2 暂退法"></a>4.2 暂退法</h3><p>🎨<code>sec_dropout</code></p><p><strong>暂退法（dropout）</strong> 是通过修改模型本身结构来实现的，计算方便但功能强大。这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代开始时，按照一定的概率随机选择一些神经元删除，即认为这些神经元不存在。</p><p>我们在之前完成了一个带有1个隐藏层和5个隐藏单元的多层感知机。当我们将暂退法应用到隐藏层，我们就可消除掉一些链接。例如下图，输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。这完全是随机的，这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e4051dc691094456a4e8cfa4c8834df3c1e0711a262241ea9998c7b9a18efbf8" width="600" hegiht="" ></center><center>图1：丢弃前后的神经网络 </center><p>按照这样的网络计算梯度，进行梯度更新，删除的神经元不更新。在下一次迭代时，在随机选择一些神经元，重复上面的做法，直到训练结束。如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。这样的参数更新不再依赖于某些共同作用的隐藏层节点之间的关系，能够有效地防止过拟合。通常，我们在测试时不用暂退法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.vision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">layer</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(layer) == nn.Linear:</span><br><span class="line">        new_weight = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=layer.weight.shape)</span><br><span class="line">        <span class="comment">#print(new_weight)</span></span><br><span class="line">        layer.weight.set_value(new_weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><pre><code>Sequential(  (0): Flatten()  (1): Linear(in_features=784, out_features=256, dtype=float32)  (2): ReLU()  (3): Dropout(p=0.2, axis=None, mode=upscale_in_train)  (4): Linear(in_features=256, out_features=256, dtype=float32)  (5): ReLU()  (6): Dropout(p=0.5, axis=None, mode=upscale_in_train)  (7): Linear(in_features=256, out_features=10, dtype=float32))</code></pre><p>接下来，我们对模型进行训练和测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">trainer = paddle.optimizer.SGD(lr, net.parameters())</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">model=paddle.Model(net)</span><br><span class="line">model.prepare(trainer,loss,paddle.metric.Accuracy(topk=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)))</span><br><span class="line">model.fit(train_iter,epochs=num_epochs,batch_size=batch_size,verbose=<span class="number">1</span>)<span class="comment"># 1开启一个可视化的进程</span></span><br><span class="line">eval_result = model.evaluate(test_iter, batch_size=<span class="number">256</span>,verbose=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#evaluate(eval_data, batch_size=1, log_freq=10, verbose=2, num_workers=0, callbacks=None)</span></span><br><span class="line"><span class="built_in">print</span>(eval_result)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The loss value printed in the log is the current step, and the metric is the average value of previous steps.</span></span><br><span class="line"><span class="string">Epoch 1/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.4857 - acc_top1: 0.5263 - acc_top2: 0.7206 - acc_top3: 0.8382 - 48ms/step         </span></span><br><span class="line"><span class="string">Epoch 2/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.5302 - acc_top1: 0.7777 - acc_top2: 0.9209 - acc_top3: 0.9783 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 3/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3104 - acc_top1: 0.8161 - acc_top2: 0.9399 - acc_top3: 0.9815 - 50ms/step         </span></span><br><span class="line"><span class="string">Epoch 4/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3551 - acc_top1: 0.8371 - acc_top2: 0.9505 - acc_top3: 0.9846 - 52ms/step         </span></span><br><span class="line"><span class="string">Epoch 5/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3623 - acc_top1: 0.8453 - acc_top2: 0.9541 - acc_top3: 0.9854 - 50ms/step         </span></span><br><span class="line"><span class="string">Epoch 6/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3932 - acc_top1: 0.8542 - acc_top2: 0.9579 - acc_top3: 0.9870 - 51ms/step         </span></span><br><span class="line"><span class="string">Epoch 7/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.4121 - acc_top1: 0.8600 - acc_top2: 0.9604 - acc_top3: 0.9876 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 8/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.2441 - acc_top1: 0.8646 - acc_top2: 0.9617 - acc_top3: 0.9882 - 49ms/step         </span></span><br><span class="line"><span class="string">Epoch 9/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.3528 - acc_top1: 0.8673 - acc_top2: 0.9632 - acc_top3: 0.9888 - 48ms/step         </span></span><br><span class="line"><span class="string">Epoch 10/10</span></span><br><span class="line"><span class="string">step 235/235 [==============================] - loss: 0.2424 - acc_top1: 0.8722 - acc_top2: 0.9654 - acc_top3: 0.9891 - 50ms/step         </span></span><br><span class="line"><span class="string">Eval begin...</span></span><br><span class="line"><span class="string">step 40/40 [==============================] - loss: 0.1212 - acc_top1: 0.8572 - acc_top2: 0.9616 - acc_top3: 0.9878 - 48ms/step         </span></span><br><span class="line"><span class="string">Eval samples: 10000</span></span><br><span class="line"><span class="string">&#123;&#x27;loss&#x27;: [0.121200696], &#x27;acc_top1&#x27;: 0.8572, &#x27;acc_top2&#x27;: 0.9616, &#x27;acc_top3&#x27;: 0.9878&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">acc_top1：只拿一个的准确率</span></span><br><span class="line"><span class="string">acc_top2：拿两个的准确率</span></span><br><span class="line"><span class="string">acc_top3：拿三个的准确率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="4-3-小结"><a href="#4-3-小结" class="headerlink" title="4.3 小结"></a>4.3 小结</h3><ul><li>L2正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li><li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li><li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li><li>暂退法仅在训练期间使用。</li><li>暂退法将一些输出项随机置0来控制模型复杂度。常用在隐藏层的输出上。</li><li>0.1,0.9,0.5是常用的丢弃法超参数的值。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_模型选择与调优策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习3.1-模型选择与调优策略（上）</title>
      <link href="/2022/12/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
      <url>/2022/12/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a><strong>模型选择</strong></h1><p>🎨​<code>sec_model_selection</code></p><p>学习本节，希望你能够掌握以下知识点：</p><ol><li>影响模型选择的多种因素；</li><li>对实验中出现的现象进行观察，并判断系统当前所处的状态</li></ol><hr><h2 id="一、影响模型选择的多种因素"><a href="#一、影响模型选择的多种因素" class="headerlink" title="一、影响模型选择的多种因素"></a><strong>一、影响模型选择的多种因素</strong></h2><p>在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为<strong>模型选择（model selection）</strong>。</p><p>可供选择的候选模型可以是有着<strong>不同超参数的同类模型</strong>。<br>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。</p><h3 id="1-发现模式"><a href="#1-发现模式" class="headerlink" title="1.发现模式"></a><strong>1.发现模式</strong></h3><p>作为机器学习科学家，我们的目标是发现<strong>模式（pattern）</strong>。但是，我们如何才能确定模型是真正发现了一种泛化的模式，而不是简单地记住了数据呢？</p><p>例如，我们想要在患者的基因数据与痴呆状态之间寻找模式，其中标签是从集合 $\{\text{痴呆}, \text{轻度认知障碍}, \text{健康}\}$ 中提取的。因为基因可以唯一确定每个个体（不考虑双胞胎），所以在这个任务中是有可能记住整个数据集的。我们不想让模型只会做这样的事情：“那是鲍勃！我记得他！他有痴呆症！”。原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。只有当模型真正发现了一种泛化模式时，才会作出有效的预测。</p><p>更正式地说，<strong>我们的目标是发现某些模式，这些模式捕捉到了我们训练集潜在总体的规律。</strong><br>如果成功做到了这点，即使是对以前从未遇到过的个体，模型也可以成功地评估风险。如何发现可以泛化的模式是机器学习的根本问题。</p><h3 id="2-训练误差和泛化误差"><a href="#2-训练误差和泛化误差" class="headerlink" title="2.训练误差和泛化误差"></a><strong>2.训练误差和泛化误差</strong></h3><p>为了进一步讨论这一现象，我们需要了解训练误差和泛化误差。</p><p><strong>训练误差（training error）</strong> 是指，模型在训练数据集上计算得到的误差。<strong>泛化误差（generalization error）</strong> 是指，模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。参数的选择依据了最小化训练误差。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。</p><p><strong>由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。</strong><br>下面的示例有助于更好地说明这种情况：假设一个大学生正在努力准备期末考试。一个勤奋的学生会努力做好练习，并利用往年的考试题目来测试自己的能力。尽管如此，在过去的考试题目上取得好成绩并不能保证他会在真正考试时发挥出色。例如，学生可能试图通过死记硬背考题的答案来做准备，他甚至可以完全记住过去考试的答案；另一名学生可能会通过试图理解给出某些答案的原因来做准备。在大多数情况下，后者会考得更好。<strong>机器学习模型应关注降低泛化误差</strong>。</p><h3 id="3-欠拟合和过拟合"><a href="#3-欠拟合和过拟合" class="headerlink" title="3.欠拟合和过拟合"></a><strong>3.欠拟合和过拟合</strong></h3><p>当我们比较训练和验证误差时，我们要注意两种常见的情况。</p><p>第一种情况是训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为<strong>欠拟合（underfitting）</strong>。</p><p>另一方面，当我们的训练误差明显低于验证误差时要小心，这表明严重的<strong>过拟合（overfitting）</strong>，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。注意，过拟合并不总是一件坏事，特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/94c736d6cf914aa28587a908985e5b3f96f5a1183c9a40c688192ef8b31d6cc4" width="700" hegiht="" ></center><center>图1：欠拟合、正常拟合和过拟合 </center><h3 id="4-训练集、验证集和测试集"><a href="#4-训练集、验证集和测试集" class="headerlink" title="4.训练集、验证集和测试集"></a><strong>4.训练集、验证集和测试集</strong></h3><p>为了确定候选模型中的最佳模型，我们通常会使用验证集。原则上，在我们确定所有的超参数之前，我们不希望用到测试集。从严格意义上讲，测试集只在所有超参数和模型参数选定后使用一次。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们过拟合了测试数据，我们又该怎么知道呢？因此，我们决不能依靠测试数据进行模型选择。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</p><p>解决此问题的常见做法是将我们的数据分成三份，除了训练和测试数据集之外，可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择，也就是增加一个<strong>验证数据集（validation dataset）</strong>。我们每次实验报告的准确度都是验证集准确度，而不是测试集准确度。</p><p><strong>【$K$折交叉验证】</strong></p><p>那么如果说根本没有那么多的训练数据呢？当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用$K$折交叉验证。<br>这里，原始训练数据被分成$K$（常用的$K$为5或10）个不重叠的子集，然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。。在这$K$次训练和验证中，每次用来验证模型的子数据集都不同。最后对这$K$次训练误差和验证误差分别求平均。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8b852abae99840019ce272b35005e5c10a867ab72b9044608a02ed3b2a4a0bb9" width="500" hegiht="" ></center><center>图2：$K$折交叉验证 </center><p>这里的$K$同样是一个可以用来估计真实参数值得超参数，可以在训练过程中进行调整，经验证集确定最终值。</p><p><strong>【独立同分布假设】</strong></p><p>一个在训练集上训练好的模型，在测试集上也应当具有较好的效果。机器学习的一个前提假设就是训练集和测试集是独立同分布的。这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。</p><p>有时候我们即使轻微违背独立同分布假设，模型仍将继续运行得非常好。比如，我们有许多有用的工具已经应用于现实，如人脸识别、语音识别和语言翻译。毕竟，几乎所有现实的应用都至少涉及到一些违背独立同分布假设的情况。有些违背独立同分布假设的行为肯定会带来麻烦。比如，我们试图只用来自大学生的人脸数据来训练一个人脸识别系统，然后想要用它来监测疗养院中的老人。这不太可能有效，因为大学生看起来往往与老年人有很大的不同。</p><h3 id="5-偏差和方差的平衡"><a href="#5-偏差和方差的平衡" class="headerlink" title="5.偏差和方差的平衡"></a><strong>5.偏差和方差的平衡</strong></h3><p>在机器学习中：<strong>偏差（Bias）</strong> 指的是模型对样本的预测值和真实值的误差。它度量了期望预测和真实标签的偏离程度，反映的是模型本身的精确度，即模型本身的表达能力。<strong>方差（Variance）</strong> 指的是模型对样本数据（由噪声引起的）的微小波动做出的反应。它度量了用不同训练集得到的输出结果与模型输出期望之间的误差，即模型的波动情况，它刻画了学习性能随训练集变动而产生的变化，即数据扰动造成的影响。由此可见，偏差和方差从不同的两个角度刻画估计量的误差。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e5154bbe58694434a182ed26cd9e76884f76d989bb1e4a3995c661237825caa5" width="350" hegiht="" ></center><center>图3：方差与偏差 </center><p>假设靶心区域是真是标签所在的区域，蓝点表示模型对不同数据集的样本输出的预测值。当方差较低的时候，蓝点比较集中；而方差较高的时候，蓝点比较分散。当偏差较低的时候，蓝点更接近与靶心，表示预测效果比较好，反之离靶心比较远，预测效果变差。理想情况下，方差和偏差都应当尽可能地低，即全部落在靶心位置，此时体现了模型良好的表达能力。</p><p>给定了一个学习目标，在训练的开始阶段，由于训练较少，学习不足，模型拟合能力不强，预测值和真是标签差距很大，即偏差很大。而模型无法较好地表达数据，数据集的搅动也无法产生明显的变化，即方差很小，此时是欠拟合状态；随着训练的进行，模型的学习能力不断增强，开始能够捕捉训练数据搅动带来的影响，在充分训练后，轻微的搅动都会导致模型发生明显的变动，此时已经能够学习训练数据集本身特定的、而非所有数据集的通用的特性，说明模型偏差较小，而方差较大，这是过拟合的表现。</p><h2 id="二、小结"><a href="#二、小结" class="headerlink" title="二、小结"></a><strong>二、小结</strong></h2><ul><li>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</li><li>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</li><li>验证集可以用于模型选择，但不能过于随意地使用它。</li><li>我们应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。</li><li>梯度消失的原因在于神经网络的隐藏层过多以及使用了不合适的激活函数（例如softmax函数）；梯度爆炸的原因在于神经网络的隐藏层过多以及初始化的权重过大。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
            <tag> 深度学习基础_模型选择与调优策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习2.3-多层感知机的搭建与实现</title>
      <link href="/2022/12/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
      <url>/2022/12/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机【多层连接】"><a href="#多层感知机【多层连接】" class="headerlink" title="多层感知机【多层连接】"></a><strong>多层感知机【多层连接】</strong></h1><p>⏳<code>sec_mlp</code></p><p>学习本节，希望你能够掌握以下知识点：</p><ol><li>神经网络的工作原理：前向传播、反向传播和梯度下降的过程；</li><li>一定范围内深层网络比浅层网络能力更强；</li><li>掌握搭建深层神经网络的方法。</li></ol><hr><h2 id="一、从浅层到深层"><a href="#一、从浅层到深层" class="headerlink" title="一、从浅层到深层"></a><strong>一、从浅层到深层</strong></h2><p>在之前的课程中，我们主要讲述了浅层神经网络的相关细节，本节将浅层网络的知识扩展到深层网络，<strong>深度神经网络(Deep Neural Networks, DNN)</strong> 是深度学习的基础。</p><h3 id="假设的提出"><a href="#假设的提出" class="headerlink" title="假设的提出"></a><strong>假设的提出</strong></h3><p>仿射变换是一种带有偏置项的线性变换。首先，回想一下softmax回归的模型架构，该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作。如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。但是，仿射变换中的线性是一个很强的假设。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e327ddb9f1d54a88844382662ae02f84a01eefed58a54b558532fe5b5d9cb196" width="500" hegiht="" ></center><center>图1：softmax模型假设 </center><p>线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正），或者导致模型输出的减小（如果对应的权重为负）。我们不能完全否定这种结论，有时这是有道理的。</p><h4 id="1）适用于线性假设"><a href="#1）适用于线性假设" class="headerlink" title="1）适用于线性假设"></a><strong>1）适用于线性假设</strong></h4><p>例如，如果我们试图预测一个人是否会偿还贷款。我们可以认为，在其他条件不变的情况下，收入较高的申请人比收入较低的申请人更有可能偿还贷款。但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。</p><p>收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。处理这一问题的一种方法是<strong>对我们的数据进行预处理</strong>，使线性变得更合理，如使用收入的对数作为我们的特征。</p><h4 id="2）部分适用于线性假设"><a href="#2）部分适用于线性假设" class="headerlink" title="2）部分适用于线性假设"></a><strong>2）部分适用于线性假设</strong></h4><p>例如，我们想要根据体温预测死亡率。对于体温高于37摄氏度的人来说，温度越高风险越大，然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。<br>在这种情况下，我们也可以通过一些巧妙的<strong>预处理</strong>来解决问题。例如，我们可以使用与37摄氏度的距离作为特征。</p><h4 id="3）不适用于线性假设"><a href="#3）不适用于线性假设" class="headerlink" title="3）不适用于线性假设"></a><strong>3）不适用于线性假设</strong></h4><p>但是，如何对猫和狗的图像进行分类呢？增加位置$(13, 17)$处像素的强度是否总是增加（或降低）图像描绘狗的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。</p><p>与我们前面的例子相比，这里的线性很荒谬，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。</p><h4 id="4）总结"><a href="#4）总结" class="headerlink" title="4）总结"></a><strong>4）总结</strong></h4><p>我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。<br>对于深度神经网络，我们会使用观测数据作用于隐藏层，生成可以<strong>自动</strong>学习特征的线性预测器。</p><h3 id="在网络中加入隐藏层"><a href="#在网络中加入隐藏层" class="headerlink" title="在网络中加入隐藏层"></a><strong>在网络中加入隐藏层</strong></h3><p><strong>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。</strong></p><p>要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。<br>这种架构通常称为<strong>多层感知机（multilayer perceptron,MLP）</strong>。下图就一个单隐藏层的多层感知机，有4个输入，3个输出，其隐藏层包含5个隐藏单元。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/82525390c2a54d7d84614a1bbc4ae76c7ff9adcc511e4f41b367c7ab09b373d9" width="500" hegiht="" ></center><center>图2：多层感知机模型 </center><p>输入层不涉及任何计算，使用此网络产生输出只需要实现隐藏层和输出层的计算，因此，这个多层感知机中的层数为2，注意，这两个层都是全连接的，每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。然而，我们之前讲过，具有全连接层的多层感知机的参数开销可能会高得令人望而却步，因此在不改变输入或输出大小的情况下，可能需要在参数节约和模型有效性之间进行权衡。</p><blockquote><p>多层感知机的层数不计算输入层；</p><p>隐藏层就是对特征进行进一步的提取，将输入层提取出来的特征在进行特征的提取，一步步的细分。</p></blockquote><h2 id="二、从线性到非线性"><a href="#二、从线性到非线性" class="headerlink" title="二、从线性到非线性"></a><strong>二、从线性到非线性</strong></h2><p>同之前的章节一样，我们通过矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$来表示$n$个样本的小批量，其中每个样本具有$d$个输入特征。</p><p>对于具有$h$个隐藏单元的单隐藏层多层感知机，用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出，称为<strong>隐藏表示（hidden representations）</strong>。<br>在数学或代码中，$\mathbf{H}$也被称为<strong>隐藏层变量（hidden-layer variable）</strong> 或<strong>隐藏变量（hidden variable）</strong>。<br>因为隐藏层和输出层都是全连接的，所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。<br>形式上，我们按如下方式计算单隐藏层多层感知机的输出$\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\end{aligned}</script><p>然而这样的仿射函数的叠加并不能为我们带来好处，我们之前的线性模型已经能够表示任何仿射函数。因此，为了发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用<strong>非线性的激活函数</strong> 。<strong>非线性的激活函数可以使得我们的神经网络逼近任何非线性函数，从而应用于更多的非线性模型</strong>。激活函数的输出（例如，$\sigma(\cdot)$）被称为<strong>活性值（activations）</strong>。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\\end{aligned}</script><p>我们应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p><p>为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如：<br>$<br>\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$和$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})<br>$<br>，一层叠一层，从而产生更有表达能力的模型。</p><h3 id="【激活函数】"><a href="#【激活函数】" class="headerlink" title="【激活函数】"></a><strong>【激活函数】</strong></h3><p>⏳<code>subsec_activation_functions</code></p><p><strong>激活函数（activation function）</strong> 通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的 。由于激活函数是深度学习的基础，下面(简要介绍一些常见的激活函数。</p><p><strong>1.ReLU函数</strong></p><p>最受欢迎的激活函数是<strong>修正线性单元（Rectified linear unit，ReLU）</strong>，因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的非线性变换，即给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0).</script><p>通俗地说，<strong>ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素</strong>。为了直观感受一下，我们可以画出ReLU函数的曲线图。正如从图中所看到，激活函数是分段线性的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.fluid.layers.nn <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x.stop_gradient = <span class="literal">False</span> <span class="comment"># 开启参数更新</span></span><br><span class="line">y = paddle.nn.functional.relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/3.png" alt="png"></p><p>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。注意，当输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左侧的导数，即<strong>当输入为0时导数为0</strong>。我们可以忽略这种情况，因为输入可能永远都不会是0。这里引用一句古老的谚语，“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”，这个观点正好适用于这里。</p><p>下面我们绘制ReLU函数的导数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x.stop_gradient = <span class="literal">False</span></span><br><span class="line">y = paddle.nn.functional.relu(x)</span><br><span class="line">y.backward(paddle.ones_like(x), retain_graph=<span class="literal">True</span>)<span class="comment"># 计算给定的 Tensors 的反向梯度。</span></span><br><span class="line">plt.plot(x, x.grad)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/4.png" alt="png"></p><p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过，这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。</p><p><strong>2.sigmoid函数</strong></p><p>对于一个定义域在$\mathbb{R}$中的输入，sigmoid函数将输入变换为区间(0, 1)上的输出。因此，sigmoid通常称为<strong>挤压函数（squashing function）</strong>：它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：</p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.</script><p>在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模，因此，这一领域的先驱可以一直追溯到人工神经元的发明者，他们专注于阈值单元。阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/bd6d64cdcd454138bd990075d605ee9780161ef443cc42c09701c3ed7d7bddd4" width="300" hegiht="" ></center><center>图3：单位阶跃函数 </center><p>当人们逐渐关注到到基于梯度的学习时，sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。</p><p>下面，我们绘制sigmoid函数。注意，当输入接近0时，sigmoid函数接近线性变换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y = paddle.nn.functional.sigmoid(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/5.png" alt="png"></p><p>sigmoid函数的导数为下面的公式：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).</script><p>sigmoid函数的导数图像如下所示。<br>注意，当输入为0时，sigmoid函数的导数达到最大值0.25；<br>而输入在任一方向上越远离0点时，导数越接近0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x.stop_gradient = <span class="literal">False</span></span><br><span class="line">y = paddle.nn.functional.sigmoid(x)</span><br><span class="line">y.backward(paddle.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, x.grad)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/6.png" alt="png"></p><p><strong>3.tanh函数</strong></p><p>与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。tanh函数的公式如下：</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.</script><p>我们知道，sigmoid函数、tanh函数他们都是数值越是靠近正无穷或者负无穷，对应的梯度越小，参数更新也就越慢，因此我们总是希望输入接近0，这样可以使得梯度更大，参数更新越快。在实际应用中，使输入尽可能限定在零值附近，从而提高梯度下降算法运算速度。因此，普遍情况下，tanh比sigmoid在隐含层作为激活函数更好。</p><p>下面我们绘制tanh函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y = paddle.nn.functional.tanh(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/7.png" alt="png"></p><p>tanh函数的导数是：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).</script><p>tanh函数的导数图像如下所示。当输入接近0时，tanh函数的导数接近最大值1。与我们在sigmoid函数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, step=<span class="number">0.1</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x.stop_gradient = <span class="literal">False</span></span><br><span class="line">y = paddle.nn.functional.tanh(x)</span><br><span class="line">y.backward(paddle.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, x.grad)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/8.png" alt="png"></p><h2 id="三、多类分类"><a href="#三、多类分类" class="headerlink" title="三、多类分类"></a><strong>三、多类分类</strong></h2><p>现在我们正式来看一下多层感知机的定义。多层感知机（MLP，Multilayer Perceptron）也叫<strong>人工神经网络（ANN，Artificial Neural Network）</strong>，除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构。<br>之前我们实现的softmax模型，实质上是将输入的值直接映射到（0,1）的区间内，最后将概率最大的值作为最终的预测结果,也是一个线性模型。现在在基本的softmax结构中加入隐藏层，这个时候就形成了多层感知机的结构。每一个隐藏层都需要有对应的激活函数【不加入激活函数，那么层与层之间仍然是线性连接的，那么就失去了加入隐藏层的意义，所以每一个隐藏层都需要加入一个激活函数；输出层加不加入激活函数我们根据情况自己来定】，最终我们通过softmax输出结果。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/ef62b4c8072b4528b7e9c6905a605e1bcb4688a36ead4c42be7ccbddeef30784" width="500" hegiht="" ></center><center>图3：多类分类模型 </center><p>在多层感知机中，我们新加入了两个超参数【我们自己决定的】的值，一个是隐藏层数，一个是每层隐藏层的神经单元个数。<br>如果要做单隐藏层的神经网络，那么相对应的隐藏层的需要更多的神经单元；如果想要把隐藏层做的多一点，则大部分情况下隐藏层的神经单元数量会依次减少，我们希望慢慢实现压缩的这个操作。是把模型做的“宽”一点还是“深”一点，这个需要根据实际的情况来判断，因此这边超参数是可调的。</p><blockquote><p>隐藏层的功能就是提取特征，我们希望提取的越多越好，后面才能够把这些特征整合到一块，然后慢慢的进行训练和预测，所以为了不错过某些特征，靠近输入层的的神经单元数一般会更多一些，慢慢的往上对特征进行整合和压缩，越来越少。</p></blockquote><h2 id="四、网络的工作原理"><a href="#四、网络的工作原理" class="headerlink" title="四、网络的工作原理"></a><strong>四、网络的工作原理</strong></h2><p>神经网络算法的核心三步是：正向传播、反向传播和梯度下降。神经网络先要经历正向传播的过程，然后再经历反向传播的过程。正向传播的本质就是根据输入的样本向量$x$经过神经网络得出预测值$\hat y$的过程。只有在正向传播得到了$\hat y$之后，损失函数$L$才能够计算。而反向传播的本质就是从最终输出的损失函数开始反向回退，根据求导的链式法则最终求出所有参数的偏导数的过程。</p><h3 id="4-1-深层网络的前向传播过程"><a href="#4-1-深层网络的前向传播过程" class="headerlink" title="4.1 深层网络的前向传播过程"></a><strong>4.1 深层网络的前向传播过程</strong></h3><p><strong>前向传播（forward propagation）</strong> 就是从输入向量开始顺着网络向后计算的过程，从层的角度来看，正向传播就是将前一层的信息进行加工再传递给下一层。<br>我们将一步步研究单隐藏层神经网络的机制，为了简单起见，我们假设输入样本是 $\mathbf{x}\in \mathbb{R}^d$。这里的中间变量是：</p><script type="math/tex; mode=display">\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}+{b}.</script><p>其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$是隐藏层的权重参数，${b}$是隐藏层神经单元的偏置项。将中间变量$\mathbf{z}\in \mathbb{R}^h$通过激活函数$\phi$后，我们得到长度为$h$的隐藏激活向量：</p><script type="math/tex; mode=display">\mathbf{h}= \phi (\mathbf{z}).</script><p>隐藏变量$\mathbf{h}$也是一个中间变量。假设输出层的权重为$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，我们可以得到输出层变量，它是一个长度为$q$的向量：</p><script type="math/tex; mode=display">\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}+{b}.</script><p> 假设损失函数为$l$，样本标签为$y$，我们可以计算单个数据样本的损失项，</p><script type="math/tex; mode=display">L = l(\mathbf{o}, y).</script><p>根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为</p><script type="math/tex; mode=display">s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right).</script><p>最后，模型在给定数据样本上的正则化损失为【这个$s$也可以叫做惩罚项】：</p><script type="math/tex; mode=display">J = L + s.</script><p>在下面的讨论中，我们将$J$称为<strong>目标函数（objective function）</strong>。</p><h3 id="4-2-深层网络的反向传播过程"><a href="#4-2-深层网络的反向传播过程" class="headerlink" title="4.2 深层网络的反向传播过程"></a><strong>4.2 深层网络的反向传播过程</strong></h3><p><strong>反向传播（backward propagation）</strong> 指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式法则，按相反的顺序从输出层到输入层遍历网络。<br>该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，其中输入和输出$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$是任意形状的张量。利用链式法则，我们可以计算$\mathsf{Z}$关于$\mathsf{X}$的导数</p><script type="math/tex; mode=display">\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).</script><p>在这里，我们使用$\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单，它只是矩阵-矩阵乘法。对于高维张量，我们使用适当的对应项。运算符$\text{prod}$指代了所有的这些符号。</p><p>回想一下，在计算图中的单隐藏层简单网络的参数是$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$。反向传播的目的是计算梯度$\partial J/\partial \mathbf{W}^{(1)}$和$\partial J/\partial \mathbf{W}^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。<em><u>计算的顺序与前向传播中执行的顺序相反</u></em>，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J=L+s$相对于损失项$L$和正则项$s$的梯度。</p><script type="math/tex; mode=display">\frac{\partial J}{\partial L} = 1 \; \text{and} \; \frac{\partial J}{\partial s} = 1.</script><p>接下来，我们根据链式法则计算目标函数关于输出层变量$\mathbf{o}$的梯度：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{o}}= \text{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)= \frac{\partial L}{\partial \mathbf{o}}\in \mathbb{R}^q.</script><p>接下来，我们计算正则化项相对于两个参数的梯度：</p><script type="math/tex; mode=display">\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}\; \text{and} \;\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.</script><p>现在我们可以计算最接近输出层的模型参数的梯度$\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$。使用链式法则得出：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.</script><p>为了获得关于$\mathbf{W}^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度$\partial J/\partial \mathbf{h} \in \mathbb{R}^h$由下式给出：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{h}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{h}}\right)= {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}.</script><p>由于激活函数$\phi$是按元素计算的，计算中间变量$\mathbf{z}$的梯度$\partial J/\partial \mathbf{z} \in \mathbb{R}^h$需要使用按元素乘法运算符，我们用$\odot$表示：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{z}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{h}}, \frac{\partial \mathbf{h}}{\partial \mathbf{z}}\right)= \frac{\partial J}{\partial \mathbf{h}} \odot \phi'\left(\mathbf{z}\right).</script><p>最后，我们可以得到最接近输入层的模型参数的梯度$\partial J/\partial \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$。根据链式法则，我们得到：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{W}^{(1)}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{z}}, \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(1)}}\right)= \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.</script><h3 id="4-3-计算图"><a href="#4-3-计算图" class="headerlink" title="4.3 计算图"></a><strong>4.3 计算图</strong></h3><p>计算图就是将计算过程图形化表示出来。是一种描述方程的“语言”，既然是图，则有节点（变量），边（操作（简单函数））。</p><p>绘制计算图有助于我们可视化计算中操作符和变量的依赖关系。 其中正方形表示变量，圆圈表示操作符。左下角表示输入，右上角表示输出。注意显示数据流的箭头方向主要是向右和向上的。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/5a195c49c65b495bb6b58b276f20fd8a377e92b9a7a54326857b6328dfc648b8" width="600" hegiht="" ></center><center>图4：前向传播计算图 </center><p>每经过一次前向传播和反向传播之后，参数就更新一次，然后用新的参数再次循环上面的过程。这就是神经网络训练的整个过程。</p><p>因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。这也是训练比单纯的预测需要更多的内存（显存）的原因之一。此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。因此，使用更大的批量来训练更深层次的网络更容易导致内存不足的问题。</p><h2 id="五、多层感知机的从零开始实现"><a href="#五、多层感知机的从零开始实现" class="headerlink" title="五、多层感知机的从零开始实现"></a><strong>五、多层感知机的从零开始实现</strong></h2><p>⏳<code>sec_mlp_scratch</code></p><p>为了与之前softmax回归获得的结果进行比较，我们将继续使用<code>Fashion-MNIST</code>图像分类数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据，需要下载</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.fluid.layers.nn <span class="keyword">import</span> shape</span><br><span class="line"><span class="keyword">from</span> paddle.vision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h3 id="【1-初始化模型参数】"><a href="#【1-初始化模型参数】" class="headerlink" title="【1.初始化模型参数】"></a><strong>【1.初始化模型参数】</strong></h3><p>回想一下，Fashion-MNIST中的每个图像由$28 \times 28 = 784$个灰度像素值组成。所有图像共分为10个类别。忽略像素之间的空间结构，我们可以将每个图像视为具有784个输入特征和10个类的简单分类数据集。首先，我们将[<strong>实现一个具有单隐藏层的多层感知机，<br>它包含256个隐藏单元</strong>]。注意，我们可以将这两个变量都视为超参数。通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。</p><p>我们用几个张量来表示我们的参数【张量就是一种通用量格式，和数组一样，让计算更加便捷】。注意，对于每一层我们都要记录一个权重矩阵和一个偏置向量。跟以前一样，我们要为损失关于这些参数的梯度分配内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span> <span class="comment"># 分别为输入层、隐藏层、输出层</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#返回符合标准正态分布（均值为0，标准差为1的正态随机分布）的随机Tensor，形状为 shape，数据类型为 dtype。</span></span><br><span class="line"><span class="comment"># w跟输入、输出有关，b跟输出有关</span></span><br><span class="line">W1 = paddle.randn(shape=(num_inputs, num_hiddens)) </span><br><span class="line">W1.stop_gradient = <span class="literal">False</span></span><br><span class="line">b1 = paddle.zeros(shape=(num_hiddens, ))</span><br><span class="line">b1.stop_gradient = <span class="literal">False</span></span><br><span class="line">W2 = paddle.randn(shape=(num_hiddens, num_outputs)) </span><br><span class="line">W2.stop_gradient = <span class="literal">False</span></span><br><span class="line">b2 = paddle.zeros(shape=(num_outputs, ))</span><br><span class="line">b2.stop_gradient = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure><h3 id="【2-激活函数】"><a href="#【2-激活函数】" class="headerlink" title="【2.激活函数】"></a><strong>【2.激活函数】</strong></h3><p>为了确保我们对模型的细节了如指掌，我们将[<strong>实现ReLU激活函数</strong>]，而不是直接调用内置的<code>relu</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = paddle.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> paddle.maximum(X, a)</span><br></pre></td></tr></table></figure><h3 id="【3-建立模型】"><a href="#【3-建立模型】" class="headerlink" title="【3.建立模型】"></a><strong>【3.建立模型】</strong></h3><p>因为我们忽略了空间结构，所以我们使用<code>reshape</code>将每个二维图像转换为一个长度为<code>num_inputs</code>的向量。只需几行代码就可以实现我们的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)<span class="comment"># 这里“@”代表矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br></pre></td></tr></table></figure><h3 id="【4-损失函数】"><a href="#【4-损失函数】" class="headerlink" title="【4.损失函数】"></a><strong>【4.损失函数】</strong></h3><p>由于我们已经从零实现过softmax函数，因此在这里我们直接使用高级API中的内置函数来计算softmax和交叉熵损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h3 id="【5-优化算法】"><a href="#【5-优化算法】" class="headerlink" title="【5.优化算法】"></a><strong>【5.优化算法】</strong></h3><p>我们选择使用随机梯度下降算法，在这里学习率设置为0.1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.SGD(parameters=params, learning_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h3 id="【6-训练】"><a href="#【6-训练】" class="headerlink" title="【6.训练】"></a><strong>【6.训练】</strong></h3><p>幸运的是，多层感知机的训练过程与softmax回归的训练过程完全相同。将迭代周期数设置为10。</p><h5 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h5><ul><li>迭代周期<br>训练的时间越长，学习的时间也就有越长，学习到的特征也就越明显，越有利于后续的测试和判断。</li><li>学习率<br>影响到下降的力度</li><li>批量大小<br>不宜太大，也不宜太小，要适中。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;——————————————————————————————————————————————————————————————————————————————&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;精确度&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    y = y.reshape(shape=[y.shape[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;———————————————————————————————————————————————————————————————————————————————&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;累加器&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;———————————————————————————————————————————————————————————————————————————————&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;评估精度&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;———————————————————————————————————————————————————————————————————————————————&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;训练&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.train()</span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, paddle.optimizer.Optimizer):</span><br><span class="line">            updater.clear_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;———————————————————————————————————————————————————————————————————————————————&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;训练过程+精度&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train: &quot;</span>, train_metrics)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test: &quot;</span>, test_acc)</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.03</span><span class="comment"># 迭代周期和学习率</span></span><br><span class="line">updater = paddle.optimizer.SGD(parameters=params, learning_rate=lr)</span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure><pre><code>train:  (0.05181671899954478, 0.6512166666666667)test:  0.6777train:  (0.02631197537581126, 0.7511)test:  0.7285train:  (0.02050904820760091, 0.7680166666666667)test:  0.7596train:  (0.016668232532342276, 0.7784666666666666)test:  0.7592train:  (0.014135322586695353, 0.78325)test:  0.7612</code></pre><p>为了对学习到的模型进行评估，我们将[<strong>在一些测试数据上应用这个模型</strong>]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> paddle.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">net, test_iter, n=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = get_fashion_mnist_labels(y)</span><br><span class="line">    preds = get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(net, test_iter)</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/9.png" alt="png"></p><h2 id="六、小结"><a href="#六、小结" class="headerlink" title="六、小结"></a><strong>六、小结</strong></h2><ul><li>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</li><li>常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。</li><li>多层感知机使用隐藏层和激活函数来得到非线性模型。其中，隐藏层数和每层隐藏层的大小作为超参数。</li><li>对于相同的分类问题，多层感知机的实现与softmax回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。</li></ul><p><strong>扩展：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;累加器&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        <span class="comment"># 倘若这里的n为2，则此时self.data以[0.0, 0.0]这样的列表展现出来</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line">        <span class="comment">#print(self.data)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="comment"># 这里的self是刚刚初始化self.data，*args接收非关键字的传参可以是元组，或者是字符串。</span></span><br><span class="line">        <span class="comment">#print(*args)</span></span><br><span class="line">        <span class="comment">#for a, b in zip(self.data, args):</span></span><br><span class="line">        <span class="comment">#    print(a, b)</span></span><br><span class="line">        <span class="comment"># self.data是[0.0, 0.0],倘若args接收的传参为(4, 5),</span></span><br><span class="line">        <span class="comment"># 那么for a, b in zip(self.data, args) 表示 a = 0.0,b = 4,</span></span><br><span class="line">        <span class="comment"># 然后执行a + float(b)，得到结果4.0，此时self.data = [4.0, 0.0],</span></span><br><span class="line">        <span class="comment"># 然后同样再来一次for循环，a = 0.0, b = 5,然后执行a + float(b) 得到结果5.0,</span></span><br><span class="line">        <span class="comment"># 最后self.data = [4.0, 5.0]</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line">        <span class="comment">#print(self.data)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):<span class="comment"># 重置</span></span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):<span class="comment"># 返回与指定键相关联的值</span></span><br><span class="line">        <span class="comment">#print(&quot;这个方法被调用&quot;)# 只有单独取某个值时，即metric[1]，此函数才会被调用</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">metric.add(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习基础_深度学习总览与模型搭建 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习2.2-神经网络中的分类任务</title>
      <link href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/"/>
      <url>/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a><strong>softmax回归</strong></h1><p>:label:<code>sec_softmax</code></p><p>学习本节，希望你能够掌握以下知识点：</p><ol><li>分类任务与线性回归任务的异同点；</li><li>独热编码的概念；</li><li>读取图像分类数据集的流程；</li><li>实现线性回归模型的方法；</li></ol><hr><p>回归可以用于预测多少的问题。比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。事实上，我们也对<strong>分类问题</strong>感兴趣：不是问“多少”，而是问“哪一个”：</p><ul><li>某个电子邮件是否属于垃圾邮件文件夹？</li><li>某个用户可能注册或不注册订阅服务？</li><li>某个图像描绘的是驴、狗、猫、还是鸡？</li><li>某人接下来最有可能看哪部电影？</li></ul><p>通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题：</p><ul><li>我们只对样本的“硬性”类别感兴趣，即属于哪个类别；</li><li>我们希望得到“软性”类别，即得到属于每个类别的概率。</li></ul><p>这两者的界限往往很模糊。其中的一个原因是：<strong>即使我们只关心硬类别，我们仍然使用软类别的模型</strong>。</p><p>刚才我们所了解的是机器学习在<strong>回归问题</strong>中的应用，也就是<strong>给定一个事物的多组已知数据（其中每组已知数据可以包含⼀个特征或多个特征）以及相应的另一个特征的值，要求推断出一个模型，能根据新的观测数据推断出未知特征的值</strong>。而机器学习解决的另大一类问题，是<strong>分类问题</strong>，也就是<strong>给定一个对象的一组已知数据，推断出所属的类别。</strong></p><h2 id="一、类别的表示"><a href="#一、类别的表示" class="headerlink" title="一、类别的表示"></a><strong>一、类别的表示</strong></h2><p>:label:<code>subsec_classification-problem</code></p><p>我们从一个图像分类问题开始。假设每次输入是一个$2\times2$的灰度图像。我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_1, x_2, x_3, x_4$。<br>此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。</p><p>接下来，我们要选择如何表示标签。我们有两个明显的选择：最直接的想法是选择$y \in \{1, 2, 3\}$，<br>其中整数分别代表$\{\text{猫}, \text{鸡}, \text{狗}\}$。这是在计算机上存储此类信息的有效方法。如果类别间有一些自然顺序，比如说我们试图预测$\{\text{婴儿}, \text{儿童}, \text{青少年}, \text{青年人}, \text{中年人}, \text{老年人}\}$，那么将这个问题转变为回归问题，并且保留这种格式是有意义的。</p><p><strong>但是一般的分类问题并不与类别之间的自然顺序有关。</strong></p><h3 id="【独热编码】"><a href="#【独热编码】" class="headerlink" title="【独热编码】"></a><strong>【独热编码】</strong></h3><p>幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：<strong>独热编码（one-hot encoding）</strong>。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d573ca3cf022401d857374423477b6beaab8d07acca441b3be5fcd449caa9919" width="500" hegiht="" ></center><center>图1：独热编码 </center><p>在我们的例子中，标签$y$将是一个三维向量，<br>其中$(1, 0, 0)$对应于“猫”、$(0, 1, 0)$对应于“鸡”、$(0, 0, 1)$对应于“狗”：</p><script type="math/tex; mode=display">y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.</script><h2 id="二、建立模型"><a href="#二、建立模型" class="headerlink" title="二、建立模型"></a><strong>二、建立模型</strong></h2><h3 id="【1-网络架构】"><a href="#【1-网络架构】" class="headerlink" title="【1.网络架构】"></a><strong>【1.网络架构】</strong></h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，我们需要和输出一样多的<strong>仿射函数（affine function）</strong>。每个输出对应于它自己的仿射函数。（仿射函数，即最高次数为1的多项式函数。常数项为零的仿射函数称为线性函数。）</p><p><strong>在线性模型的分类问题中，需要拥有和输出一样多的仿射函数，每个输出对应它自己的仿射函数</strong></p><p>在我们的例子中，由于我们有4个特征和3个可能的输出类别，我们将需要12个标量来表示权重（带下标的$w$），3个标量来表示偏置（带下标的$b$）。<br>下面我们为每个输入计算三个<strong>未规范化的预测（logit）</strong>：$o_1$、$o_2$和$o_3$。</p><script type="math/tex; mode=display">\begin{aligned}o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\end{aligned}</script><p>我们可以用神经网络图来描述这个计算过程。与线性回归一样，$softmax$ 回归也是一个单层神经网络。由于计算每个输出$o_1$、$o_2$和$o_3$取决于所有输入$x_1$、$x_2$、$x_3$和$x_4$。<strong>所以softmax回归的输出层也是全连接层。</strong></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/04eb6a3ec6ab42b9a58659506650eb4a5aaeb0af56ef4292ad916ebecd183528" width="500" hegiht="" ></center><center>图2：softmax分类模型 </center><p>为了更简洁地表达模型，我们仍然使用线性代数符号。通过向量形式表达为$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$。<br>由此，我们已经将所有权重放到一个$3 \times 4$矩阵中。对于给定数据样本的特征$\mathbf{x}$，我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置$\mathbf{b}$得到的。</p><h3 id="【2-softmax函数】"><a href="#【2-softmax函数】" class="headerlink" title="【2.softmax函数】"></a><strong>【2.softmax函数】</strong></h3><p>:label:<code>subsec_softmax_operation</code></p><p>现在我们将优化参数以最大化观测数据的概率。为了得到预测结果，我们将设置一个阈值，以选择具有最大概率的标签。<br>我们希望模型的输出$\hat{y}_j$可以视为属于类$j$的概率，然后选择具有最大输出值的类别$\operatorname*{argmax}_j y_j$作为我们的预测。<br>例如，如果$\hat{y}_1$、$\hat{y}_2$和$\hat{y}_3$分别为0.1、0.8和0.1，那么我们预测的类别是2，在我们的例子中代表“鸡”。</p><p><strong>然而我们能否将未规范化的预测$o$直接视作我们感兴趣的输出呢？答案是否定的。</strong><br>因为将线性层的输出直接视为概率时存在一些问题：一方面，我们没有限制这些输出数字的总和为1。另一方面，根据输入的不同，它们可以为负值。这些违反了概率基本公理。<br><strong>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。</strong></p><p>社会科学家邓肯·卢斯于1959年在<strong>选择模型（choice model）</strong> 的理论基础上发明的softmax函数正是这样做的：<br><strong>softmax函数将未规范化的预测变换为非负并且总和为1，同时要求模型保持可导。</strong><br>具体流程如下：</p><p><strong>我们首先对每个未规范化的预测求幂（指数函数的性质），这样可以确保输出非负。<br>为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和（类似于概率）。</strong></p><script type="math/tex; mode=display">\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{Z})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}</script><p>这里，对于所有经过softmax处理的 $\hat{\mathbf{y}}$ 的值总有$0 \leq \hat{y}_j \leq 1$。因此，$\hat{\mathbf{y}}$可以视为一个正确的概率分布。<br>softmax运算不会改变未规范化的预测$\mathbf{Z}$之间的顺序，只会确定分配给每个类别的概率。因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。</p><script type="math/tex; mode=display">\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.</script><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个<strong>线性模型（linear model）</strong>。</p><blockquote><p>softmax回归是在对数几率函数的基础上转换成的多分类问题。</p></blockquote><h3 id="【3-小批量样本的矢量化】"><a href="#【3-小批量样本的矢量化】" class="headerlink" title="【3.小批量样本的矢量化】"></a><strong>【3.小批量样本的矢量化】</strong></h3><p>:label:<code>subsec_softmax_vectorization</code></p><p><strong>为了提高计算效率并且充分利用GPU，我们通常会针对小批量数据执行矢量计算。</strong><br>假设我们读取了一个批量的样本$\mathbf{X}$，其中特征维度（输入数量）为 $d$，批量大小为 $n$。此外，假设我们在输出中有 $q$个类别。那么小批量特征为 $\mathbf{X} \in \mathbb{R}^{n \times d}$，权重为 $\mathbf{W} \in \mathbb{R}^{d \times q}$，偏置为 $\mathbf{b} \in \mathbb{R}^{1\times q}$。softmax回归的矢量计算表达式为：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}</script><p>相对于一次处理一个样本，小批量样本的矢量化加快了$\mathbf{X}和\mathbf{W}$的矩阵-向量乘法。由于$\mathbf{X}$中的每一行代表一个数据样本，那么softmax运算可以<strong>按行（rowwise）</strong> 执行：<strong>对于$\mathbf{O}$的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。</strong></p><p>$\mathbf{X} \mathbf{W} + \mathbf{b}$的求和会使用广播，小批量的未规范化预测$\mathbf{O}$和输出概率$\hat{\mathbf{Y}}$都是形状为$n \times q$的矩阵。</p><h3 id="【4-损失函数——交叉熵损失】"><a href="#【4-损失函数——交叉熵损失】" class="headerlink" title="【4.损失函数——交叉熵损失】"></a><strong>【4.损失函数——交叉熵损失】</strong></h3><p>现在让我们考虑整个结果分布的情况，即观察到的不仅仅是一个结果。对于标签$\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如$(0.1, 0.2, 0.7)$，而不是仅包含二元项的向量$(0, 0, 1)$。这里我们引出：<strong><em>交叉熵损失</em>(cross-entropy loss)</strong>，它是分类问题最常用的损失之一。</p><p>熵可以用来判断数据集的混乱程度。在这个公式中，$H$代表熵，$pi$代表某个$i$类别出现的概率。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/67441076efb74cee8b18d2ff3cc38d22f4e2fd3a4caa4990ad4decefded16b8e" width="200" hegiht="" ></center><p>如果某个类别出现的概率为1或0，那么$H$为0，这个时候我们就认为数据集没有混乱；相反，如果数据集中的样本包含多个类别，那么熵就会增大，在$（0,1）$中移动；最后，如果一个数据集中，一半属于一个类别，而另一半属于另一个类别，那么$H$为1，此时我们认为数据集非常混乱。</p><p>从另一个角度来说，熵也衡量掌握了信息的多少。 如果⼀个数据集所有的样本都属于同⼀个类别，也就是其中一个类别出现的概率为1或0，这说明我们对这个数据集里的数据所属的类别非常确定，也就是说明我们掌握了所有信息； 而如果一个数据集里的样本，一半属于一个类别，而另一半属于另一个类别，这时如果从数据集里任意抽取一个样本，我们完全无法猜测它属于什么类别。也就是说，我们没有掌握任何信息。 因此，我们目标既可以说是减少分类后的数据集的熵，也可以说是增加信息。这就要用到信息增益的概念了。</p><p>那究竟什么是信息增益呢？先要引入一个概念——条件熵，如果熵可以代表整个数据集的混乱程度，那么条件熵就可以用来描述某个特征的混乱程度。信息增益和两者的关系式可以表示为：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4372317ac62c45fa823bd3dff015fb1abbdd6a5c34e243d8bc84931c590323a9" width="250" hegiht="" ></center><p>因为信息增益 = 总的信息熵 - 某个特征的信息熵，也就是说明，一个数据集的熵是一定的，哪个特征的熵越小，最后的信息增益效果越好。现在我们得到的概率向量为$(0.1,0.2,0.7)$，那么就意味着狗对应的信息增益效果最好，使用其用来做所谓“全局”的特征选择。</p><h3 id="【5-模型预测和评估】"><a href="#【5-模型预测和评估】" class="headerlink" title="【5.模型预测和评估】"></a><strong>【5.模型预测和评估】</strong></h3><p>在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。通常我们使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。在接下来的实验中，我们将使用<strong>精度（accuracy）</strong> 来评估模型的性能。精度等于正确预测数与预测总数之间的比率。</p><h2 id="三、图像分类数据集"><a href="#三、图像分类数据集" class="headerlink" title="三、图像分类数据集"></a><strong>三、图像分类数据集</strong></h2><p>🔖<code>sec_fashion_mnist</code></p><p>本章我们使用<code>Fashion-MNIST</code>数据集来演示分类模型实现的基本流程。Fashion-MNIST数据集包含了10个类别的图像，分别是：t-shirt（T恤），trouser（牛仔裤），pullover（套衫），dress（裙子），coat（外套），sandal（凉鞋），shirt（衬衫），sneaker（运动鞋），bag（包），ankle boot（短靴）。每个输入图像的高度和宽度均为28像素，由灰度图像组成，其通道数为1。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/41ea426f24874702876699bf01c4cd52ae5db1a9a3ab4153b2aeba5e0a3180ad" width="400" hegiht="" ></center><center>图3：服装分类数据集 </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mod</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle.vision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br></pre></td></tr></table></figure><h3 id="1-读取数据集"><a href="#1-读取数据集" class="headerlink" title="1.读取数据集"></a><strong>1.读取数据集</strong></h3><p>我们可以[<strong>通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中</strong>]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line">trans = transforms.ToTensor() <span class="comment"># 图像预处理tensor(将读入进来的数据转换为张量的形式)</span></span><br><span class="line"><span class="comment"># 转换为张量： 一个是将通道数提前；一个是将0~255的取值转换为0~1的取值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 神经网络输入的数据必须是张量这种数值形式存在的</span></span><br><span class="line"><span class="comment"># 一个标量：一维张量</span></span><br><span class="line"><span class="comment"># 一个数组：二维张量</span></span><br><span class="line"><span class="comment"># 一张图片；三维张量</span></span><br><span class="line"><span class="comment"># 多张图片：四维张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是调用他API接口，将数据导入进来，并下载到内存当中</span></span><br><span class="line">mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)<span class="comment">#训练集 </span></span><br><span class="line">mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)<span class="comment">#测试集</span></span><br></pre></td></tr></table></figure><p>Fashion-MNIST由10个类别的图像组成，每个类别由<strong>训练数据集（train dataset）</strong> 中的6000张图像和<strong>测试数据集（test dataset）</strong> 中的1000张图像组成。因此，训练集和测试集分别包含60000和10000张图像。测试数据集不会用于训练，只用于评估模型性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test), mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"><span class="comment"># 打印结果：[1,28,28]</span></span><br><span class="line"><span class="comment"># 1：表示单通道，灰色图片为单通道，彩色图片为红绿蓝三通道，如果为彩色图片，这里即为3</span></span><br><span class="line"><span class="comment"># 28：高和宽</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>]<span class="comment"># 第一个样本</span></span><br><span class="line"><span class="comment"># 一个样本包含两个部分: 图片+所属类别（标签）</span></span><br><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>] <span class="comment"># 图片 ---&gt; x</span></span><br><span class="line">mnist_train[<span class="number">0</span>][<span class="number">1</span>] <span class="comment"># 标签 ---&gt; y</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">此模型就是将x，y传入神经网络，</span></span><br><span class="line"><span class="string">机器自动学习x和y，</span></span><br><span class="line"><span class="string">然后当我们传入其他的x时，</span></span><br><span class="line"><span class="string">会返回给我们y</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>我们现在可以创建一个函数来可视化这些样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">返回Fashion-MNIST数据集的文本标签</span></span><br><span class="line"><span class="string">就是将数字标签利用数组转换为文本标签</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">    </span><br><span class="line"><span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> paddle.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以下是训练数据集中前[<strong>几个样本的图像及其相应的标签</strong>]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(DataLoader(mnist_train, batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape((<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure><h3 id="2-读取小批量"><a href="#2-读取小批量" class="headerlink" title="2.读取小批量"></a><strong>2.读取小批量</strong></h3><p>为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为<code>batch_size</code>。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span> <span class="comment"># 根据CPU来选择</span></span><br><span class="line"><span class="comment"># DataLoader返回一个迭代器，该迭代器根据 batch_sampler 给定的顺序迭代一次给定的 dataset</span></span><br><span class="line"><span class="comment"># 每次读取256个样本</span></span><br><span class="line">train_iter = DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure><h3 id="3-整合组件"><a href="#3-整合组件" class="headerlink" title="3.整合组件"></a><strong>3.整合组件</strong></h3><p>现在我们[<strong>定义<code>load_data_fashion_mnist</code>函数</strong>]，用于获取和读取Fashion-MNIST数据集。这个函数返回训练集和验证集的数据迭代器。此外，这个函数还接受一个可选参数<code>resize</code>，用来将图像大小调整为另一种形状，方便适应更多的图形。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:<span class="comment"># 适用于更多尺寸的图像</span></span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)<span class="comment">#训练集</span></span><br><span class="line">    mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)<span class="comment">#测试集</span></span><br><span class="line">    <span class="keyword">return</span> (DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure><p>下面，我们通过指定<code>resize</code>参数来测试<code>load_data_fashion_mnist</code>函数的图像大小调整功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">64</span>, resize=<span class="number">32</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter():</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X[<span class="number">0</span>],y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>我们现在已经准备好使用Fashion-MNIST数据集，便于下面的章节调用来评估各种分类算法。</p><h2 id="四、softmax回归的从零开始实现"><a href="#四、softmax回归的从零开始实现" class="headerlink" title="四、softmax回归的从零开始实现"></a><strong>四、softmax回归的从零开始实现</strong></h2><p>🔖<code>sec_softmax_scratch</code></p><p>(就像我们从零开始实现线性回归一样，我们认为softmax回归也是重要的基础，因此你应该知道实现softmax回归的细节。本节我们将使用刚刚在 :numref:<code>sec_fashion_mnist</code>中引入的Fashion-MNIST数据集，并设置数据迭代器的批量大小为256。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core.fromnumeric <span class="keyword">import</span> partition</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> paddle.fluid.layers.nn <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">from</span> paddle.tensor.linalg <span class="keyword">import</span> cross</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle.vision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h3 id="【1-读取数据集】"><a href="#【1-读取数据集】" class="headerlink" title="【1.读取数据集】"></a><strong>【1.读取数据集】</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = vision.datasets.FashionMNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"><span class="comment"># 将训练集和测试集都设置成迭代器，一个方便训练，一个方便测试</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h3 id="【2-初始化模型参数】"><a href="#【2-初始化模型参数】" class="headerlink" title="【2.初始化模型参数】"></a><strong>【2.初始化模型参数】</strong></h3><p>和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。原始数据集中的每个样本都是$28 \times 28$的图像。<br>在本节中，我们[<strong>将展平每个图像，把它们看作长度为784的向量</strong>]，这个做法并不是最好的做法。在后面的章节中，我们将讨论能够利用图像空间结构的特征，但现在我们暂时只把每个像素位置看作一个特征。</p><p>回想一下，在softmax回归中，我们的输出与类别一样多。(<strong>因为我们的数据集有10个类别，所以网络输出维度为10</strong>)。因此，权重将构成一个$784 \times 10$的矩阵，偏置将构成一个$1 \times 10$的行向量。与线性回归一样，我们将使用正态分布初始化我们的权重<code>W</code>，偏置初始化为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span> <span class="comment">#将展平每个图像，把它们看作长度为784的向量</span></span><br><span class="line">num_outputs = <span class="number">10</span> <span class="comment">#十个类别</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">W = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=(num_inputs, num_outputs))</span><br><span class="line">W.stop_gradient = <span class="literal">False</span></span><br><span class="line">b = paddle.zeros(shape=[num_outputs])</span><br><span class="line">b.stop_gradient = <span class="literal">False</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W =&quot;</span>, W,<span class="string">&quot;\n\nb =&quot;</span>, b)</span><br></pre></td></tr></table></figure><h3 id="【3-定义模型】"><a href="#【3-定义模型】" class="headerlink" title="【3.定义模型】"></a><strong>【3.定义模型】</strong></h3><p>总结一下，实现softmax是指： 我们首先对每个未规范化的预测求幂（指数函数的性质），这样可以确保输出非负。为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和（类似于概率）。在查看代码之前，我们回顾一下这个表达式：</p><script type="math/tex; mode=display">\mathrm{softmax}(\mathbf{X})_{j} = \frac{\exp(\mathbf{X}_{j})}{\sum_k \exp(\mathbf{X}_{k})}.</script><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e1a4fcb3fe154a4fb263300f51880367c51b0b949d334fe0aa906051e55909c4" width="600" hegiht="" ></center><center>图4：softmax的实现 </center><p>softmax直白来说就是将原来输出值通过softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义softmax操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = paddle.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment"># dim=1代表对行求和，keepdim表示是否需要保持输出的维度与输入一样</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure><p><strong>验证softmax函数</strong>：softmax运算本质就是将每个元素变成非负数，且每一行和为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">5</span>))<span class="comment">#均值为0，方差为1，三行五列的矩阵x</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原来的x = &#x27;</span>,x)</span><br><span class="line">x_1 = softmax(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nsoftmax后的 = &#x27;</span>,x_1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nsoftmax后的每一行的值 = &#x27;</span>,x_1.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>定义softmax操作后，我们可以实现softmax回归模型。下面的代码定义了输入如何通过网络映射到输出。注意，将数据传递到模型之前，我们使用<code>reshape</code>函数将每张原始图像展平为向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(paddle.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line">        <span class="comment"># reshape(-1,1)表示（任意行，1列）,这里传递的是batch值</span></span><br></pre></td></tr></table></figure><h3 id="【4-定义损失函数】"><a href="#【4-定义损失函数】" class="headerlink" title="【4.定义损失函数】"></a><strong>【4.定义损失函数】</strong></h3><p>接下来，我们实现交叉熵损失函数。这可能是深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量。</p><p>回顾一下，交叉熵采用真实标签的预测概率的负对数似然。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a40b4658e3df457491c1101f9821168f600a89aa463941e0827ccfbdae2365a3" width="200" hegiht="" ></center><h4 id="交叉熵损失函数计算案例"><a href="#交叉熵损失函数计算案例" class="headerlink" title="交叉熵损失函数计算案例"></a><strong>交叉熵损失函数计算案例</strong></h4><p>假设有一个3分类问题，某个样例的正确答案是（1， 0， 0）：</p><ul><li>甲模型经过softmax回归之后的预测答案是（0.5， 0.2， 0.3）</li><li>乙模型经过softmax回归之后的预测答案是（0.7， 0.1， 0.2）</li></ul><p>带入公式可知，乙模型的预测结果要优于甲模型。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e3ed5ff9a22e449aac14d978409a62f3a679f5049e3b411da0f78d81220a25fa" width="400" hegiht="" ></center><center> 图5 甲乙模型的交叉熵损失 </center><p>下面，我们<strong>创建一个数据样本<code>y_hat</code>，其中包含2个样本在3个类别的预测概率，以及它们对应的标签<code>y</code>。</strong>有了<code>y</code>，我们知道在第一个样本中，第一类是正确的预测；而在第二个样本中，第三类是正确的预测。然后(<strong>使用<code>y</code>作为<code>y_hat</code>中概率的索引</strong>)，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面定义了两个样本，0和2表示在第0位和第2位是真实的类别</span></span><br><span class="line"><span class="comment"># 2个样本的正确的标签索引值[1,0,0][0,0,1]</span></span><br><span class="line">y = paddle.to_tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">y_hat = paddle.to_tensor([[<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.6</span>], [<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.7</span>]])<span class="comment">#2个样本在3个类别的预测概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面使用y作为y_hat中概率的索引，即告诉y_hat也是第0位和第2位</span></span><br><span class="line"><span class="comment"># 过程为：取出y的第0位为0，y的第一位为2，所以去除y_hat的第一位的第0位和第1位的第2位，顺着取即可</span></span><br><span class="line">y_hat[[<span class="number">0</span>,<span class="number">1</span>], y]</span><br></pre></td></tr></table></figure><p>现在我们只需一行代码就可以[<strong>实现交叉熵损失函数</strong>]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个交叉熵损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    y = y.reshape(shape=[y.shape[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">return</span> - paddle.log(y_hat[<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat))), y])</span><br><span class="line"></span><br><span class="line"><span class="comment">#调用函数测试</span></span><br><span class="line">cross_entropy(y_hat, y)<span class="comment">#分别为样本1和样本2的损失 </span></span><br></pre></td></tr></table></figure><h3 id="【5-分类精度】"><a href="#【5-分类精度】" class="headerlink" title="【5.分类精度】"></a><strong>【5.分类精度】</strong></h3><p>给定预测概率分布<code>y_hat</code>，当我们必须输出<strong>硬预测（hard prediction）</strong> 时，我们通常选择预测概率最高的类。当预测与标签分类<code>y</code>一致时，即是正确的。</p><p>分类精度即正确预测数量与总预测数量之比。为了计算精度，我们执行以下操作。首先，如果<code>y_hat</code>是矩阵，那么假定第二个维度存储每个类的预测分数。我们使用<code>argmax</code>获得每行中最大元素的索引来获得预测类别。<br>然后我们将预测类别与真实<code>y</code>元素进行比较。由于等式运算符“<code>==</code>”对数据类型很敏感，因此我们将<code>y_hat</code>的数据类型转换为与<code>y</code>的数据类型一致。结果是一个包含0（错）和1（对）的张量。最后，我们求和会得到正确预测的数量。</p><h4 id="预测类别与真实类别y进行比较"><a href="#预测类别与真实类别y进行比较" class="headerlink" title="预测类别与真实类别y进行比较"></a><strong>预测类别与真实类别y进行比较</strong></h4><p>我们将继续使用之前定义的变量<code>y_hat</code>和<code>y</code>分别作为预测的概率分布和标签。<br>可以看到，第一个样本的预测类别是2（该行的最大元素为0.6，索引为2），这与实际标签0不一致。<br>第二个样本的预测类别是2（该行的最大元素为0.5，索引为2），这与实际标签2一致。<br>因此，这两个样本的分类精度率为0.5。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求精度的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>): </span><br><span class="line">    y = y.reshape(shape=[y.shape[<span class="number">0</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;y&#x27;</span>,y,y_hat)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在y_hat是一个二维矩阵且每一行&gt;1个 的情况下执行</span></span><br><span class="line">    <span class="comment"># 就是确保y_hat有两个或以上样本，这样才能得出哪个预测正确，哪个预测错误</span></span><br><span class="line">    <span class="comment"># 确保y_hat的每个样本有两个或以上的类别，这样才能有比较，方便和y比较，看高的是否相同</span></span><br><span class="line">    <span class="comment"># 判断一下符不符合精度计算的要求</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)<span class="comment">#每一行中预测值最大的作为y_hat类别</span></span><br><span class="line">        <span class="built_in">print</span>(y_hat)<span class="comment"># [2,2,1]</span></span><br><span class="line">        <span class="built_in">print</span>(y)<span class="comment"># [0,2,1]</span></span><br><span class="line">    <span class="comment"># cmp函数用于比较2个对象</span></span><br><span class="line">    cmp = y_hat == y<span class="comment"># 所以答对了两个</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试：求出预测正确的数量与概率</span></span><br><span class="line"><span class="built_in">print</span>(y_hat)</span><br><span class="line"><span class="built_in">print</span>(accuracy(y_hat, y),accuracy(y_hat, y)/<span class="built_in">len</span>(y))</span><br></pre></td></tr></table></figure><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><strong>扩展</strong></h4><p>刚刚我们是对一组数据进行了精度的评估，同样，对于任意数据迭代器<code>data_iter</code>可访问的数据集，我们可以评估在任意模型<code>net</code>的精度。</p><p>这里定义一个实用程序类<code>Accumulator</code>，用于对多个变量进行累加。在<code>evaluate_accuracy</code>函数中，我们在<code>Accumulator</code>实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            y = paddle.squeeze(y)</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>由于我们使用随机权重初始化<code>net</code>模型，因此该模型的精度应接近于随机猜测。例如在有10个类别情况下的精度为0.1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluate_accuracy(net, test_iter)</span><br></pre></td></tr></table></figure><h3 id="【6-训练】"><a href="#【6-训练】" class="headerlink" title="【6.训练】"></a><strong>【6.训练】</strong></h3><p>我们定义一个函数来训练一个迭代周期。 请注意，updater是更新模型参数的常用函数，它接受批量大小作为参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> sgd([W, b], lr, batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter():</span><br><span class="line">        <span class="comment"># print(&quot;X, y: &quot;, X, y)</span></span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, paddle.optimizer.Optimizer):</span><br><span class="line">            updater.clear_grad()</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            <span class="comment"># print(W, b)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            W, b = updater(W, b, X.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;updated W, b:&quot;</span>, W, b)</span><br><span class="line">            metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]<span class="comment">#所有的loss累加/总样本数，分类正确的/总样本数</span></span><br></pre></td></tr></table></figure><p>现在，我们[<strong>训练模型10个迭代周期</strong>]。<br>请注意，迭代周期（<code>num_epochs</code>）和学习率（<code>lr</code>）都是可调节的超参数。<br>通过更改它们的值，我们可以提高模型的分类精度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=(num_inputs, num_outputs))</span><br><span class="line">W.stop_gradient = <span class="literal">False</span></span><br><span class="line">b = paddle.zeros(shape=[num_outputs])</span><br><span class="line">b.stop_gradient = <span class="literal">False</span></span><br><span class="line"><span class="comment">#print(&quot;W, b:&quot;, W, b)</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.2</span></span><br><span class="line">num_epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter():</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        <span class="comment"># print(&quot;X, y: &quot;, X.shape, y.shape, y_hat.shape)</span></span><br><span class="line">        l = cross_entropy(y_hat, y)</span><br><span class="line">        <span class="comment"># print(&quot;loss: &quot;, l.shape)</span></span><br><span class="line">        <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            W -= lr * W.grad / X.shape[<span class="number">0</span>]</span><br><span class="line">            b -= lr * b.grad / batch_size</span><br><span class="line">            W.clear_grad()</span><br><span class="line">            b.clear_grad()</span><br><span class="line">            W.stop_gradient = <span class="literal">False</span></span><br><span class="line">            b.stop_gradient = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># print(&quot;updated W, b:&quot;, W, b)</span></span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="built_in">print</span>(metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>])<span class="comment">#所有的loss累加/总样本数，分类正确的/总样本数</span></span><br></pre></td></tr></table></figure><h3 id="【7-预测】"><a href="#【7-预测】" class="headerlink" title="【7.预测】"></a><strong>【7.预测】</strong></h3><p>现在训练已经完成，我们的模型已经准备好对图像进行分类预测。给定一系列图像，我们将比较它们的实际标签（文本输出的第一行）和模型预测（文本输出的第二行）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> paddle.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">10</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = get_fashion_mnist_labels(y)</span><br><span class="line">    preds = get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><h2 id="五、小结"><a href="#五、小结" class="headerlink" title="五、小结"></a><strong>五、小结</strong></h2><ul><li>softmax回归是一个多类分类模型。</li><li>softmax运算获取一个向量并将其映射为概率。</li><li>softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。</li><li>交叉熵是一个衡量两个概率分布之间差异的很好的度量。</li><li>Fashion-MNIST是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。</li><li>我们将高度$h$像素，宽度$w$像素图像的形状记为$h \times w$或（$h$,$w$）。</li><li>数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。</li><li>借助softmax回归，我们可以训练多分类的模型。</li><li>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习基础_深度学习总览与模型搭建 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习2.1-线性回归模型的实现</title>
      <link href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
      <url>/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h1><p>:label:<code>sec_linear_regression</code></p><p>学习本节，希望你能够掌握以下知识点：</p><ol><li>线性回归的基本元素：线性模型、损失函数、基础优化算法；</li><li>正态分布的基本概念；</li><li>实现线性回归模型的方法；</li></ol><hr><p><strong>回归（regression）</strong> 是能为一个或多个自变量与因变量之间关系建模的一类方法。<br>在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p><p>机器学习领域中的大多数任务通常都与<strong>预测（prediction）</strong> 有关。<br>当我们想预测一个数值时，就会涉及到回归问题。常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、预测需求（零售销量等）。但不是所有的预测都是回归问题。在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。</p><h2 id="一、线性回归的基本元素"><a href="#一、线性回归的基本元素" class="headerlink" title="一、线性回归的基本元素"></a>一、<strong>线性回归的基本元素</strong></h2><p><strong>线性回归（linear regression）</strong> 可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行。</p><p>线性回归基于几个简单的假设：<br><strong>首先，假设自变量$\mathbf{x}$和因变量$y$之间的关系是线性的，<br>即$y$可以表示为$\mathbf{x}$中元素的加权和，这里通常允许包含观测值的一些噪声；<br>其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</strong></p><p>为了解释线性回归，我们举一个实际的例子：我们希望根据与房屋相关的信息来估算房屋价格。</p><p>这里的相关信息包括$：x1:卧室个数，x2:卫生间个数，x3:居住面积$。<br>那么我们可以得到这样一个式子：$y=w1x1+w2x2+w3x3+b$。</p><h3 id="【线性模型】"><a href="#【线性模型】" class="headerlink" title="【线性模型】"></a><strong>【线性模型】</strong></h3><p>:label:<code>subsec_linear_model</code></p><p>线性假设如下面的式子：</p><script type="math/tex; mode=display">y=w1x1+w2x2+w3x3+b</script><p><strong>权重（weight）</strong>$w$决定了每个特征对我们预测值的影响。<br>$b$称为<strong>偏置（bias）</strong>、<strong>偏移量（offset）</strong> 或<strong>截距（intercept）</strong>。<br>偏置是指当所有特征都取值为0时，预测值应该为多少（原先已经存在）。在现实的任何情况下我们都需要偏置项。如果没有偏置项，我们模型的表达能力将受到限制。</p><p><strong>给定一个数据集，我们的目标是寻找模型的权重$w$和偏置$b$，使得根据模型做出的预测大体符合数据里的真实价格</strong>。</p><p>而在<strong>机器学习(machine learning)</strong> 领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。<br>当我们的输入包含$d$个特征时，我们将预测结果$\hat{y}$（通常使用“尖角”符号表示$y$的估计值）表示为：</p><script type="math/tex; mode=display">\hat{y} = w_1  x_1 + ... + w_d  x_d + b.</script><p>将所有特征放到向量$\mathbf{x} \in \mathbb{R}^d$中，并将所有权重放到向量$\mathbf{w} \in \mathbb{R}^d$中，我们可以用<strong>点积(scalar product)</strong> 形式来简洁地表达模型：</p><script type="math/tex; mode=display">\hat{y} = \mathbf{w}· \mathbf{x} + b.</script><p>无论我们使用什么手段来观察特征$\mathbf{X}$和标签$\mathbf{y}$，都可能会出现少量的<strong>观测误差(observation error)</strong> 。因此，即使确信特征与标签的潜在关系是线性的，我们也会加入一个噪声项来考虑观测误差带来的影响。</p><p>在开始寻找最好的<strong>模型参数（model parameters）</strong> $\mathbf{w}$和$b$之前，我们还需要两个东西：</p><ul><li>（1）一种模型质量的度量方式；</li><li>1（2）一种能够更新模型以提高模型预测质量的方法。</li></ul><h3 id="【损失函数——一种模型质量的度量方式】"><a href="#【损失函数——一种模型质量的度量方式】" class="headerlink" title="【损失函数——一种模型质量的度量方式】"></a><strong>【损失函数——一种模型质量的度量方式】</strong></h3><p>在我们开始考虑如何用模型<strong>拟合（fit）</strong> 数据之前，我们需要确定一个拟合程度的度量。<strong>损失函数（loss function）能够量化目标的实际值与预测值之间的差距</strong>。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。</p><p>回归问题中最常用的损失函数是<strong>平方误差函数(Squared error function)</strong>。当样本的预测值为$\hat{y}$，其相应的真实标签为$y$时，平方误差可以定义为以下公式：</p><script type="math/tex; mode=display">Loss = \frac{1}{2} \left(\hat{y} - y\right)^2.</script><p>常数$\frac{1}{2}$不会带来本质的差别，但这样在形式上稍微简单一些（因为当我们对损失函数求导后常数系数为1）。</p><p>我们为一维情况下的回归问题绘制图像：</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/98cc0446e71b414c8d18e3095faf83f84f910421aac34ba09121c306e9649f99" width="400" hegiht="" ></center>     <center>图1：一维线性回归 </center><p>由于平方误差函数中的二次方项，估计值$\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集$n$个样本上的损失均值（也等价于求和）。</p><script type="math/tex; mode=display">L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}· \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.</script><p>在训练模型时，我们希望寻找一组参数（$\mathbf{w}^<em>, b^</em>$），这组参数能最小化在所有训练样本上的总损失。如下式：</p><script type="math/tex; mode=display">\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).</script><h3 id="【基础优化算法——一种能够更新模型以提高模型预测质量的方法】"><a href="#【基础优化算法——一种能够更新模型以提高模型预测质量的方法】" class="headerlink" title="【基础优化算法——一种能够更新模型以提高模型预测质量的方法】"></a><strong>【基础优化算法——一种能够更新模型以提高模型预测质量的方法】</strong></h3><p>我们用到一种名为<strong>梯度下降（gradient descent）</strong> 的方法，这种方法几乎可以优化所有深度学习模型。它<strong>通过不断地在损失函数递减的方向上更新参数来降低误差</strong>。</p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。<br>但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做<strong>小批量随机梯度下降（minibatch stochastic gradient descent）</strong>。</p><p>在每次迭代中，我们首先随机抽样一个小批量$b$，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数$\eta$，并从当前参数的值中减掉。我们用下面的数学公式来表示这一更新过程（$\partial$表示偏导数）：</p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).</script><p>$b$表示每个小批量中的样本数，这也称为<strong>批量大小（batch size）</strong>。$\eta$表示<strong>学习率【步长，一次学习跨越的长度】（learning rate）</strong>。<br>批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为<strong>超参数（hyperparameter）</strong>。</p><p><strong>调参（hyperparameter tuning）</strong> 是选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的<strong>验证数据集（validation dataset）</strong> 上评估得到的。</p><p>在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后），我们记录下模型参数的估计值，表示为$\hat{\mathbf{w}}, \hat{b}$。<br>但是，<strong>即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值</strong>。</p><script type="math/tex; mode=display">w0 \rightarrow w优</script><p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为<strong>泛化（generalization）</strong>。</p><p>总结一下，算法的步骤如下：</p><ul><li>（1）初始化模型参数的值，如随机初始化；</li><li>（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</li></ul><h2 id="二、正态分布与平方损失"><a href="#二、正态分布与平方损失" class="headerlink" title="二、正态分布与平方损失"></a>二、<strong>正态分布与平方损失</strong></h2><p>:label:<code>subsec_normal_distribution_and_squared_loss</code></p><p>正态分布和线性回归之间的关系很密切。<br><strong>正态分布（normal distribution）</strong>，也称为<strong>高斯分布（Gaussian distribution）</strong>，最早由德国数学家高斯（Gauss）应用于天文学研究。</p><p>简单的说，若随机变量$x$具有均值$\mu$和方差$\sigma^2$ ，其正态分布概率密度函数如下：</p><script type="math/tex; mode=display">p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).</script><p>下面我们定义一个Python函数来演示正态分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">x, mu, sigma</span>):</span><br><span class="line">    p = <span class="number">1</span> / math.sqrt(<span class="number">2</span> * math.pi * sigma**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> p * np.exp(-<span class="number">0.5</span> / sigma**<span class="number">2</span> * (x - mu)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import MutableMapping/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import Iterable, Mapping/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import Sized</code></pre><p>我们现在可视化正态分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X, Y=<span class="literal">None</span>, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>), axes=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        legend = []</span><br><span class="line"></span><br><span class="line">    set_figsize(figsize)</span><br><span class="line">    axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> plt.gca()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果X有一个轴，输出True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X, <span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>)</span><br><span class="line">                <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>], <span class="string">&quot;__len__&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">        X = [X]</span><br><span class="line">    <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X, Y = [[]] * <span class="built_in">len</span>(X), X</span><br><span class="line">    <span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">        Y = [Y]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">        X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">    axes.cla()</span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">            axes.plot(x, y, fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            axes.plot(y, fmt)</span><br><span class="line">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line"></span><br><span class="line">x = np.arange(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">0.01</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><pre><code>[-7.   -6.99 -6.98 ...  6.97  6.98  6.99]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均值和标准差对</span></span><br><span class="line">params = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">plot(x, [normal(x, mu, sigma) <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params], xlabel=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;p(x)&#x27;</span>, figsize=(<span class="number">4.5</span>, <span class="number">2.5</span>),</span><br><span class="line">         legend=[<span class="string">f&#x27;mean <span class="subst">&#123;mu&#125;</span>, std <span class="subst">&#123;sigma&#125;</span>&#x27;</span> <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params])</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`  from ipykernel import kernelapp as app/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  if isinstance(obj, collections.Iterator):/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  return list(data) if isinstance(data, collections.MappingView) else data</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/1.svg" alt="svg"></p><p>就像我们所看到的，<strong>均值会产生沿$x$轴的偏移，方差将会影响分布以及峰值</strong>。</p><p><strong>均方误差(Mean squared error function)</strong> 损失函数可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式:</p><script type="math/tex; mode=display">y = \mathbf{w}· \mathbf{x} + b + \epsilon,</script><p>其中，$\epsilon \sim \mathcal{N}(0, \sigma^2)$。</p><h3 id="正态分布-vs-标准正态分布"><a href="#正态分布-vs-标准正态分布" class="headerlink" title="正态分布 vs 标准正态分布"></a>正态分布 vs 标准正态分布</h3><p>在正态分布分布中，根据其概率密度函数，可以知道$μ$决定其位置，而$σ$决定幅度，整体形状呈钟型。而标准正态分布是正态分布的一种，满足$μ = 0,σ = 1$的条件的正态分布即为标准正态分布。简单来说幅度限定$（σ = 1），y$轴对称的正态分布就是标准正态分布。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/0bb4ff9ae7b74f55baade1a40dd89b46949f87aeef8e47d7bc326a0bce934ded" width="500" hegiht="" ></center><center>图2：正态分布 </center><p>这里需要注意的是，以上仅为假设。噪声不见得服从高斯分布，但是对服从高斯分布的噪声，其估计量具有更容易推导的统计性质，也能进行小样本的统计假设的检验。</p><h2 id="三、线性回归的从零开始实现"><a href="#三、线性回归的从零开始实现" class="headerlink" title="三、线性回归的从零开始实现"></a>三、<strong>线性回归的从零开始实现</strong></h2><p>:label:<code>sec_linear_scratch</code></p><p>在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。</p><p>在这一节中，(<strong>我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器</strong>)。虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。</p><p>在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h3 id="【1-生成数据集】"><a href="#【1-生成数据集】" class="headerlink" title="【1.生成数据集】"></a><strong>【1.生成数据集】</strong></h3><p>为了简单起见，我们将[<strong>根据带有噪声的线性模型构造一个人造数据集。</strong>]<br>我们的<strong>任务是使用这个有限样本的数据集来恢复这个模型的参数</strong>。</p><p>我们将使用低维数据，这样可以很容易地将其可视化。在下面的代码中，我们生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。我们的合成数据集是一个矩阵$\mathbf{X}\in \mathbb{R}^{1000 \times 2}$。</p><p><strong>我们使用线性模型参数$\mathbf{w} = [2, -3.4]^\top$、$b = 4.2$<br>和噪声项$\epsilon$生成数据集及其标签：</strong></p><script type="math/tex; mode=display">\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.</script><p>你可以将$\epsilon$视为模型预测和标签时的潜在观测误差。在这里我们认为标准假设成立，即$\epsilon$服从均值为0的正态分布。为了简化问题，我们将标准差设为0.01。</p><p>下面的代码生成<strong>合成数据集(Synthetic datasets)</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    X = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))<span class="comment">#均值为0，方差为1，n个样本，w长度的特征</span></span><br><span class="line">    y = paddle.matmul(X, w) + b</span><br><span class="line">    y += paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment">#reshape(-1,1)转换成1列</span></span><br><span class="line"></span><br><span class="line">true_w = paddle.to_tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>注意，[<strong><code>features</code>中的每一行都包含一个二维数据样本，<br><code>labels</code>中的每一行都包含一维标签值（一个标量）</strong>]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制散点图</span></span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>features: Tensor(shape=[2], dtype=float32, place=Place(cpu), stop_gradient=True,       [ 0.88939798, -0.09182443]) label: Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,       [6.27633619])</code></pre><p><img src="https://picbed.dai2yutou.space/article_img/深度学习/2.svg" alt="svg"></p><h3 id="【2-读取数据集】"><a href="#【2-读取数据集】" class="headerlink" title="【2.读取数据集】"></a><strong>【2.读取数据集】</strong></h3><p>回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据。</p><p>在下面的代码中，我们[<strong>定义一个<code>data_iter</code>函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为<code>batch_size</code>的小批量</strong>]。<br>每个小批量包含一组特征和标签。【特征就是x，标签就是y，分真实标签和预测标签】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)<span class="comment"># 随机打乱</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = paddle.to_tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])<span class="comment"># 随机读取的样本</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices] <span class="comment"># yield--构造生成器，不同于return，此处函数会继续向下执行</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。</p><p>我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。每个批量的特征维度显示批量大小和输入特征数。同样的，批量的标签形状与<code>batch_size</code>相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[10, 2], dtype=float32, place=Place(cpu), stop_gradient=True,       [[-0.49911308, -0.39810586],        [-0.76750195,  0.92735165],        [ 0.50302643, -0.39648017],        [ 1.14350212,  0.45846194],        [-0.58703244,  0.41355067],        [ 0.90497136, -0.58438981],        [-0.09798826,  1.66836739],        [ 0.07398646, -0.60611689],        [ 0.23126918, -0.46856895],        [-0.59221536,  0.55128318]])  Tensor(shape=[10, 1], dtype=float32, place=Place(cpu), stop_gradient=True,       [[ 4.56645775],        [-0.48575446],        [ 6.56141758],        [ 4.92852592],        [ 1.62627256],        [ 8.01213646],        [-1.67052901],        [ 6.39333963],        [ 6.26368427],        [ 1.13665283]])</code></pre><h3 id="【3-初始化模型参数】"><a href="#【3-初始化模型参数】" class="headerlink" title="【3.初始化模型参数】"></a><strong>【3.初始化模型参数】</strong></h3><p>[<strong>在我们开始用小批量随机梯度下降优化我们的模型参数之前</strong>]，<br>(<strong>我们需要先有一些参数</strong>)。在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。每次更新都需要计算损失函数关于模型参数的梯度。有了这个梯度，我们就可以向减小损失的方向更新每个参数。</p><p>在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">w.stop_gradient = <span class="literal">False</span> <span class="comment">#更新参数</span></span><br><span class="line">b = paddle.zeros(shape=[<span class="number">1</span>])</span><br><span class="line">b.stop_gradient = <span class="literal">False</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w, b: &quot;</span>, w, b)</span><br></pre></td></tr></table></figure><pre><code>w, b:  Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[-0.00806116],        [ 0.00138983]]) Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [0.])</code></pre><h3 id="【4-定义模型】"><a href="#【4-定义模型】" class="headerlink" title="【4.定义模型】"></a><strong>【4.定义模型】</strong></h3><p>接下来，我们必须[<strong>定义模型，将模型的输入和参数同模型的输出关联起来。</strong>]<br>回想一下，要计算线性模型的输出，我们只需计算输入特征$\mathbf{X}$和模型权重$\mathbf{w}$的矩阵-向量乘法后加上偏置$b$。<br>注意，上面的$\mathbf{X·w}$是一个向量，而$b$是一个标量。我们可以通过<strong>广播机制(broadcast mechanism)</strong>：广播机制的本质就是<strong>张量(tensor<br>)</strong> 自动扩展，当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># print(&quot;linreg: &quot;, X, w, b)</span></span><br><span class="line">    <span class="keyword">return</span> paddle.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h3 id="【5-定义损失函数】"><a href="#【5-定义损失函数】" class="headerlink" title="【5.定义损失函数】"></a><strong>【5.定义损失函数】</strong></h3><p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。这里我们使用平方损失函数。<br>在实现中，我们需要将真实值<code>y</code>的形状转换为和预测值<code>y_hat</code>的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="comment">#print(&quot;squared_loss: &quot;, y_hat, y)</span></span><br><span class="line">    loss = (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span> <span class="comment"># loss=1/2(y_hat-y)^2</span></span><br><span class="line">    <span class="comment">#print(&quot;loss&gt;&gt;&gt;&quot;, loss)</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="【6-定义优化算法】"><a href="#【6-定义优化算法】" class="headerlink" title="【6.定义优化算法】"></a><strong>【6.定义优化算法】</strong></h3><p>这里我们介绍小批量随机梯度下降。</p><p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。<br>接下来，朝着减少损失的方向更新我们的参数。</p><p>下面的函数实现小批量随机梯度下降更新。<br>该函数接受模型参数集合、学习速率和批量大小作为输入。每一步更新的大小由学习速率<code>lr</code>决定。<br>因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（<code>batch_size</code>）<br>来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):<span class="comment"># params存储参数w、b</span></span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># print(&quot;param.grad: &quot;, param.grad)</span></span><br><span class="line">            param -= lr * param.grad / batch_size<span class="comment"># 梯度下降法公式</span></span><br><span class="line">            <span class="comment"># print(&quot;param: &quot;, param)</span></span><br><span class="line">            param.clear_grad()<span class="comment"># 清除当前梯度值</span></span><br></pre></td></tr></table></figure><h3 id="【7-训练】"><a href="#【7-训练】" class="headerlink" title="【7.训练】"></a><strong>【7.训练】</strong></h3><p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的[<strong>训练过程</strong>]部分了。理解这段代码至关重要，因为从事深度学习后，你会一遍又一遍地看到几乎相同的训练过程。<br>在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测；计算完损失后，我们开始反向传播，存储每个参数的梯度；最后，我们调用优化算法<code>sgd</code>来更新模型参数。</p><p>概括一下，我们将执行以下循环：</p><ul><li>初始化参数</li><li>重复以下训练，直到完成<ul><li>计算梯度$\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$</li><li>更新参数$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$</li></ul></li></ul><p>在每个<strong>迭代周期（epoch）</strong> 中，我们使用<code>data_iter</code>函数遍历整个数据集，并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，分别设为3和0.03。设置超参数很棘手，需要通过反复试验进行调整。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">lr = <span class="number">0.3</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)</span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        <span class="comment"># print(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;, l)</span></span><br><span class="line">        sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="comment"># print(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&quot;, w.grad, b.grad)</span></span><br><span class="line">        <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">            <span class="comment"># for param in [w, b]:</span></span><br><span class="line">            <span class="comment"># print(&quot;param.grad: &quot;, param.grad)</span></span><br><span class="line">            w -= lr * w.grad / batch_size</span><br><span class="line">            b -= lr * b.grad / batch_size</span><br><span class="line">            <span class="comment"># print(&quot;param: &quot;, param)</span></span><br><span class="line">            w.clear_grad()</span><br><span class="line">            b.clear_grad()</span><br><span class="line">            w.stop_gradient = <span class="literal">False</span></span><br><span class="line">            b.stop_gradient = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># print(&quot;===========&quot;, w, b)</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py:523: UserWarning: [93mWarning:tensor.grad will return the tensor value of the gradient. This is an incompatible upgrade for tensor.grad API.  It&#39;s return type changes from numpy.ndarray in version 2.0 to paddle.Tensor in version 2.1.0.  If you want to get the numpy value of the gradient, you can use :code:`x.grad.numpy()` [0m  warnings.warn(warning_msg)epoch 1, loss 0.000051epoch 2, loss 0.000050epoch 3, loss 0.000049</code></pre><p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。因此，我们可以通过输出比较<strong>真实参数</strong>和<strong>通过训练学到的参数</strong>来评估训练的成功程度。</p><p>注意，我们不应该想当然地认为我们能够完美地求解参数。在机器学习中，<strong>我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[ 2.00002027],        [-3.39890456]]) Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [4.20090818])w的估计误差: Tensor(shape=[2], dtype=float32, place=Place(cpu), stop_gradient=False,       [-0.00002027, -0.00109553])b的估计误差: Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [-0.00090837])</code></pre><h2 id="四、线性回归的简洁实现"><a href="#四、线性回归的简洁实现" class="headerlink" title="四、线性回归的简洁实现"></a>四、<strong>线性回归的简洁实现</strong></h2><p>:label:<code>sec_linear_concise</code></p><h3 id="【深度学习框架】"><a href="#【深度学习框架】" class="headerlink" title="【深度学习框架】"></a><strong>【深度学习框架】</strong></h3><p>在过去的几年里，出于对深度学习强烈的兴趣，许多公司、学者和业余爱好者开发了各种成熟的开源框架。深度学习框架有助于建模者聚焦业务场景和模型设计本身，省去大量而繁琐的代码编写工作，其优势主要表现在如下两个方面：</p><ul><li>节省编写大量底层代码的精力：深度学习框架屏蔽了底层实现，用户只需关注模型的逻辑结构，同时简化了计算逻辑，降低了深度学习入门门槛；</li><li>省去了部署和适配环境的烦恼：深度学习框架具备灵活的移植性，可将代码部署到CPU、GPU或移动端上，选择具有分布式性能的深度学习框架会使模型训练更高效。</li></ul><p>在构建模型的过程中，每一步所需要完成的任务均可以拆分成个性化和通用化两个部分。深度学习框架的本质是自动实现建模过程中相对通用的模块，建模者只实现模型中个性化的部分，这样可以在“节省投入”和“产出强大”之间达到一个平衡。</p><ul><li>个性化部分：往往是指定模型由哪些逻辑元素组合，由建模者完成；</li><li>通用部分：聚焦这些元素的算法实现，由深度学习框架完成。</li></ul><center><img src="https://ai-studio-static-online.cdn.bcebos.com/13b074198a924cd8904389d4a4f5f58ae1791bc2aa77427786190682d2273589" width="700" hegiht="" ></center><center>图3：深度学习框架设计思路 </center><p>无论是计算机视觉任务还是自然语言处理任务，使用的深度学习模型结构都是类似的，只是在每个环节指定的实现算法不同。因此，多数情况下，算法实现只是相对有限的一些选择，如常见的Loss函数不超过十种、常用的网络配置也就十几种、常用优化算法不超过五种等等，这些特性使得<strong>基于框架建模更像一个编写“模型配置”的过程</strong>。</p><h3 id="【飞桨-PaddlePaddle-深度学习平台】"><a href="#【飞桨-PaddlePaddle-深度学习平台】" class="headerlink" title="【飞桨(PaddlePaddle)深度学习平台】"></a><strong>【飞桨(PaddlePaddle)深度学习平台】</strong></h3><p>百度出品的深度学习平台飞桨(PaddlePaddle)是主流深度学习框架中一款完全国产化的产品，与Google TensorFlow、Facebook Pytorch齐名。相比国内其他产品，飞桨是一个功能完整的深度学习平台，也是唯一成熟稳定、具备大规模推广条件的深度学习开源开放平台。飞桨源于产业实践，始终致力于与产业深入融合，与合作伙伴一起帮助越来越多的行业完成AI赋能，并已广泛应用于智慧城市、智能制造、智慧金融、泛交通、泛互联网、智慧农业等领域。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/6093ab75759b4f2d88ddbd3c2344d5c194ba1f3290a14e93b45ac9882c73cc0d" width="700" hegiht="" ></center><center>图4：飞桨在各领域的应用 </center><ul><li>飞桨官方网站： <a href="https://www.paddlepaddle.org.cn/">https://www.paddlepaddle.org.cn/</a></li><li>飞桨GitHub： <a href="https://github.com/paddlepaddle">https://github.com/paddlepaddle</a></li><li>官方API文档：<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html">https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html</a></li></ul><p>在本节中，我们将介绍如何通过使用深度学习框架来简洁地实现线性回归模型。</p><h3 id="【1-生成数据集】-1"><a href="#【1-生成数据集】-1" class="headerlink" title="【1.生成数据集】"></a><strong>【1.生成数据集】</strong></h3><p>与:label:numref:<code>sec_linear_scratch</code>中类似，我们首先生成数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    X = paddle.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = paddle.matmul(X, w) + b</span><br><span class="line">    y += paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = paddle.to_tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)<span class="comment">#生成特征值和标签</span></span><br></pre></td></tr></table></figure><h3 id="【2-读取数据集】-1"><a href="#【2-读取数据集】-1" class="headerlink" title="【2.读取数据集】"></a><strong>【2.读取数据集】</strong></h3><p>我们可以[<strong>调用框架中现有的API来读取数据</strong>]。<br>我们将<code>features</code>和<code>labels</code>作为API的参数传递，并通过数据迭代器指定<code>batch_size</code>。<br>此外，布尔值<code>is_train</code>表示是否希望数据迭代器对象在每个迭代周期内打乱数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_array, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构造数据迭代器</span></span><br><span class="line">    dataset = TensorDataset(data_array)<span class="comment"># 由张量列表定义的数据集，就是将数据转换为张量</span></span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=is_train) <span class="comment"># 之后从DataLoader中随机挑选batch_size个样本</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span><span class="comment">#10表示每次要读取10个样本</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)<span class="comment">#调用函数</span></span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))<span class="comment">#Iterator--迭代器，next不断调用并返回下一个值</span></span><br></pre></td></tr></table></figure><pre><code>&lt;paddle.fluid.dataloader.dataset.TensorDataset object at 0x7f5f52aaf790&gt;[Tensor(shape=[10, 2], dtype=float32, place=Place(cpu), stop_gradient=True,        [[-0.54224837, -0.39263305],         [-1.01299679,  0.94332039],         [ 0.47301245,  0.70776784],         [ 0.73045009, -1.13905752],         [ 1.54503405,  0.48081467],         [ 0.65195972, -1.31317592],         [ 0.11326745, -0.05683252],         [-0.95777512,  0.04747771],         [-0.47813359, -0.18815866],         [-0.20791157,  0.94915169]]), Tensor(shape=[10, 1], dtype=float32, place=Place(cpu), stop_gradient=True,        [[ 4.44495058],         [-1.02339399],         [ 2.72681808],         [ 9.53551197],         [ 5.67309809],         [ 9.94829559],         [ 4.62414885],         [ 2.12689281],         [ 3.88397408],         [ 0.55416900]])]</code></pre><h3 id="【3-定义模型】"><a href="#【3-定义模型】" class="headerlink" title="【3.定义模型】"></a><strong>【3.定义模型】</strong></h3><p>全连接层在<code>Linear</code>类中定义。值得注意的是，我们将两个参数传递到<code>nn.Linear</code>中。第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># 线性回归是单层神经网络，Sequential是一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行</span></span><br><span class="line">net = paddle.nn.Sequential(paddle.nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="【4-初始化模型参数】"><a href="#【4-初始化模型参数】" class="headerlink" title="【4.初始化模型参数】"></a><strong>【4.初始化模型参数】</strong></h3><blockquote><p>通常情况下，此步骤可以不用写，框架中初始化了</p></blockquote><p>在使用<code>net</code>之前，我们需要初始化模型参数。如在线性回归模型中的权重和偏置。<br>深度学习框架通常有预定义的方法来初始化参数，我们先不使用。在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = paddle.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">w.stop_gradient = <span class="literal">False</span></span><br><span class="line">b = paddle.zeros(shape=[<span class="number">1</span>])</span><br><span class="line">b.stop_gradient = <span class="literal">False</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w, b: &quot;</span>, w, b)</span><br></pre></td></tr></table></figure><pre><code>w, b:  Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,       [[-0.00905993],        [-0.00498687]]) Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [0.])</code></pre><h3 id="【5-定义损失函数】-1"><a href="#【5-定义损失函数】-1" class="headerlink" title="【5.定义损失函数】"></a><strong>【5.定义损失函数】</strong></h3><p>计算均方误差使用的是<code>MSELoss</code>类，也称为$L_2$范数。<br>默认情况下，它返回所有样本损失的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = paddle.nn.MSELoss() <span class="comment"># 均方误差</span></span><br></pre></td></tr></table></figure><h3 id="【6-定义优化算法】-1"><a href="#【6-定义优化算法】-1" class="headerlink" title="【6.定义优化算法】"></a><strong>【6.定义优化算法】</strong></h3><p>小批量随机梯度下降算法是一种优化神经网络的标准工具，<br>当我们(实例化一个<code>SGD</code>实例时，我们要指定优化的参数（可通过<code>net.parameters()</code>从我们的模型中获得）以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置<code>lr</code>值，这里设置为0.03。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = paddle.optimizer.SGD(learning_rate=<span class="number">0.03</span>, parameters = net.parameters())</span><br><span class="line"><span class="comment"># learning_rate为学习率【超参数】  parameters为神经网络参数</span></span><br></pre></td></tr></table></figure><h3 id="【7-训练】-1"><a href="#【7-训练】-1" class="headerlink" title="【7.训练】"></a><strong>【7.训练】</strong></h3><p>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。当我们需要更复杂的模型时，高级API的优势将大大增加。当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。</p><p>回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（<code>train_data</code>），不停地从中获取一个小批量的输入和相应的标签。对于每一个小批量，我们会进行以下步骤:</p><ul><li>通过调用<code>net(X)</code>生成预测并计算损失<code>l</code>（前向传播）。</li><li>通过进行反向传播来计算梯度。</li><li>通过调用优化器来更新模型参数。</li></ul><p>为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span><span class="comment"># 训练十轮，迭代周期【超参数】</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter():<span class="comment"># 将x和y分别从数据迭代器中拿出来</span></span><br><span class="line">        l = loss(net(X), y)<span class="comment"># 计算损失函数</span></span><br><span class="line">        trainer.clear_grad()<span class="comment"># 清除梯度  防止每一小步的梯度之间相互干扰，之前位置的梯度不会对后续位置的梯度产生影响，每次计算的都是当前的梯度</span></span><br><span class="line">        l.backward()<span class="comment"># 反向传播</span></span><br><span class="line">        <span class="comment"># 最小化loss，更新参数</span></span><br><span class="line">        trainer.step()</span><br><span class="line">        </span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    train_cost = l.numpy()[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss&#x27;</span>, train_cost)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.00017633408epoch 2, loss 9.7692595e-05epoch 3, loss 9.7552016e-05epoch 4, loss 9.749315e-05epoch 5, loss 9.774655e-05epoch 6, loss 9.733056e-05epoch 7, loss 9.7501295e-05epoch 8, loss 9.8026896e-05epoch 9, loss 9.82324e-05epoch 10, loss 9.7922566e-05</code></pre><p>下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。要访问参数，我们首先从<code>net</code>访问所需的层，然后读取该层的权重和偏置。正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight<span class="comment"># 第0层的参数，即输入层，真实参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br></pre></td></tr></table></figure><pre><code>w的估计误差： Tensor(shape=[2], dtype=float32, place=Place(cpu), stop_gradient=False,       [-0.00050569, -0.00013828])b的估计误差： Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,       [0.00014687])</code></pre><h2 id="五、小结"><a href="#五、小结" class="headerlink" title="五、小结"></a><strong>五、小结</strong></h2><ul><li>机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。</li><li>矢量化使数学表达上更简洁，同时运行的更快。</li><li>线性回归模型也是一个简单的神经网络。</li><li>梯度下降法通过不断沿着反梯度方向更新参数求解。</li><li>小批量随机梯度下降是深度学习默认的求解算法。</li><li>两个重要的超参数是批量大小和学习率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习基础_深度学习总览与模型搭建 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习1.1-深度学习概论</title>
      <link href="/2022/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
      <url>/2022/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习概论"><a href="#深度学习概论" class="headerlink" title="深度学习概论"></a><strong>深度学习概论</strong></h1><p>学习本节，希望你能够掌握以下知识点：</p><ol><li>人工智能、机器学习与深度学习的关系；</li><li>深度学习崛起的三个理由；</li><li>常见的深度学习网络模型；</li><li>机器学习的基本概念：建立模型、损失函数、优化算法。 </li></ol><hr><h2 id="人工智能-amp-机器学习-amp-深度学习"><a href="#人工智能-amp-机器学习-amp-深度学习" class="headerlink" title="人工智能&amp;机器学习&amp;深度学习"></a><strong>人工智能&amp;机器学习&amp;深度学习</strong></h2><p>在研究深度学习之前，先从三个概念的正本清源开始。概括来说，人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的，三者的关系：人工智能 &gt; 机器学习 &gt; 深度学习。</p><p>人工智能是最早出现的，范围也最广；随后出现的是机器学习；最内层的是深度学习，也是当今人工智能大爆炸的核心驱动力。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/01793714803c4b1a9a76727a0e1afcb8a97a438362d44915b95070eaf219bc36" width="500" hegiht="500" ></center><center>图1：人工智能&机器学习&深度学习的关系 </center><p>简单来说，机器学习是实现人工智能的方法，深度学习是实现机器学习的技术之一。也可以说，机器学习是人工智能的子集，而深度学习是机器学习的子集。接下来我们分别看一看这三者具体包含了什么。</p><h3 id="【人工智能】"><a href="#【人工智能】" class="headerlink" title="【人工智能】"></a><strong>【人工智能】</strong></h3><p><strong>人工智能(Artificial Intelligence, AI)</strong> 是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。【此定义只是针对现状，不知未来如何】</p><p>人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以<strong>人类智能</strong>相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理等。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/624e19151f724144ab1d67cfbb6a799d5d81b823afc646f7a26d7d05a7936aba" width="700" hegiht="" ></center><center>图2：人工智能研究领域 </center><p>2006 至今，大数据分析的需求，神经网络又被重视，成为深度学习理论的基础。</p><p>到目前为止，人工智能在人们日常生活中各个领域的普及度日趋升高。相对于初期发展阶段而言，人工智能的实用性、应用价值均产生了较为明显的变化。<a href="https://www.bilibili.com/video/BV19a411t7vG?spm_id_from=333.337.search-card.all.click">i am AI</a></p><h3 id="【机器学习】"><a href="#【机器学习】" class="headerlink" title="【机器学习】"></a><strong>【机器学习】</strong></h3><p>卡内基梅隆大学的Tom Michael Mitchell 教授在 1997 年出版的书籍《机器学习》中对<strong>机器学习(Machine Learning)</strong> 做了非常专业的定义：如果一个程序可以在任务T上，随着经验E的增加，效果P也可以随之增加，则称这个程序可以从经验中学习。以下棋为例，设计出的程序可以随着对弈盘数的增加而不断修正自己的下棋策略，使得获胜率不断提高，就认为这个程序可以在经验中学习。</p><p>机器学习是从人工智能中产生的一个重要学科分支，是实现智能化的关键。</p><p>但传统的机器学习在使用过程中存在着不可避免的问题。传统机器学习最关键的问题是必须依赖给定数据的表示，而实际上，在大多数任务中我们很难知道应该提取哪些特征。例如我们想要在一堆动物的图片中辨认出猫，试图通过胡须、耳朵、尾巴等元素的存在与否来辨认，但如果照片中存在很多遮挡物，或是猫的姿势发生了改变等，都会影响到机器识别。</p><p>找不到一个合理的方法提取数据，这就使得问题变得棘手。</p><p>深度学习采用深层网络结构，具备了强大的特征学习能力，从而解决了机器学习的核心问题。</p><h3 id="【深度学习】"><a href="#【深度学习】" class="headerlink" title="【深度学习】"></a><strong>【深度学习】</strong></h3><p>机器学习算法理论在上个世纪90年代发展成熟，在许多领域都取得了成功，但平静的日子只延续到2010年左右。随着大数据的涌现和计算机算力提升，深度学习模型异军突起，极大改变了机器学习的应用格局。今天，多数机器学习任务都可以使用深度学习模型解决，<strong>尤其在语音、计算机视觉和自然语言处理等领域，深度学习模型的效果比传统机器学习算法有显著提升</strong>。</p><p>2016年，AlphaGo 击败韩国围棋冠军李世石，在媒体报道中，曾多次提及“深度学习”这个概念。而新版本的AlphaGoZero，更充分地运用了深度学习法，不再从人类棋手的以往棋谱记录中开始训练，而是完全靠自己的学习算法，通过自我对弈来学会下棋。经过一段时间的自我学习，它就击败了曾打败李世石的以及曾完胜柯洁的AlphaGo版本。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d9a67ba8091b4ab8aa757a7b92b4e06acadbaa2efdcf4537aabb883bee735260" width="500" hegiht="" ></center><center>图3：AlphaGoZero </center><p>深度学习作为目前机器学习领域最受关注的分支，是用于实现人工智能的关键技术，相比较于传统的机器学习，深度学习不再需要人工的方式提取特征，而是自动从简单特征中提取，组合更复杂的特征，从数据中学习到复杂的特征表达形式并使用这些组合特征解决问题。</p><p>通过以上描述可以简单理解为，深度学习是基于多层神经网络的、以海量数据为输入的规则自学习方法。深度学习可以获得更好的方法来表示数据的特征，同时由于模型的层次深、表达能力强，因此有能力处理大规模数据。对于图像、语音这种直接特征不明显的问题，深度模型能够在大规模训练数据上取得更好的效果。</p><p>早期的深度学习可以理解为传统神经网络的拓展，二者的相同之处在于，深度学习采用了与神经网络相似的分层结构：包括输入层、隐藏层、输出层的多层网络。关于神经网络，我们会在之后的章节展开论述。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/520d0ff0b6a24a3db6ada98d294571e9d04a46ec881d48dba21b2689d150647c" width="400" hegiht="" ></center><center>图4：神经网络 </center><h2 id="深度学习基本概念"><a href="#深度学习基本概念" class="headerlink" title="深度学习基本概念"></a><strong>深度学习基本概念</strong></h2><h3 id="深度学习崛起的时代背景"><a href="#深度学习崛起的时代背景" class="headerlink" title="深度学习崛起的时代背景"></a><strong>深度学习崛起的时代背景</strong></h3><p>谈到深度学习的历史不得不追溯到神经网络技术。深度学习崛起之前，神经网络曾经历两次高潮与两次低谷。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/71f3874a1720415ba188a0f09a8c8990cd7f8e9caee44d4180ce03921ad1589d" width="700" hegiht="" ></center><center>图5：神经网络的历史 </center><p>纵使神经网络又一次进入寒冬，但杰弗里·辛顿等人仍然没有放弃。2006年，辛顿在论文 <em>A Fast Learning Algorithm for Deep Belief Nets</em> 中介绍了一种成功训练多层神经网络的方法，他将这种神经网络成为深度信念网络。</p><p>辛顿提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型得到的特征数据相对于原始数据有更本质的代表性，这将大大提高分类识别的能力；②对于深度神经网络很难通过训练达到最优的问题，可以采取逐层训练的方法解决，将上一层训练好的结果作为下层训练过程中的初始化参数。</p><p>由此，神经网络实现了最新的一次突破——深度学习。</p><p>深度学习的诞生伴随着更优化的算法、更高性能的计算能力(GPU)和更大的数据集的时代背景。使用GPU集群可以将原来一个月才能训练出的网络加速到几小时就能完成，除了硬件的飞速发展为其提供条件以外，深度学习还得到了充分的燃料：大数据。相较传统的神经网络，尽管在算法上我们确实简化了深度架构的训练，但最重要的进展还是我们有了成功训练这些算法的资源。可以说，人工智能只有在数据的驱动下才能实现深度学习，不断迭代模型，变得越来越智能。</p><p>因此，想要持续发展深度学习技术，算法、硬件和大数据缺一不可。</p><h3 id="常见的深度学习网络结构"><a href="#常见的深度学习网络结构" class="headerlink" title="常见的深度学习网络结构"></a><strong>常见的深度学习网络结构</strong></h3><p>深度学习可以应用在各大领域中，根据应用的情况不同，深度神经网络的形态也各不相同。常见的深度学习模型主要有<strong>全连接神经网络(Fully Connected Netural Network,FCN)</strong> 、<strong>卷积神经网络(convolutional neural network, CNN)</strong> 和<strong>循环神经网络(Recurrent neural network,RNN)</strong> 。他们均有着自身的特点，并在不同的场景中发挥着重要的作用。</p><h4 id="【全连接神经网络】"><a href="#【全连接神经网络】" class="headerlink" title="【全连接神经网络】"></a><strong>【全连接神经网络】</strong></h4><blockquote><p>非常适用于简单的模型，在数学建模的过程中用到的比较多。</p></blockquote><p>全连接神经网络是一种连接方式较为简单的人工神经网络结构，全连接层的每一个节点都与上一层的所有节点相连。<strong>多层感知机(Multi-Layer Perception,MLP)</strong> 就属于全连接网络，MLP 网络是可以应用于几乎所有任务的多功能学习方法，包括分类、回归，甚至是无监督学习。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/41e0c82504b049d5bb9334247c9503bd367e92aaf4b548bba06db891d92ee1de" width="400" hegiht="" ></center><center>图6：全连接神经网络 </center><p>然而由于全连接层所有的输出和输入都是相连的，一般全连接层的参数是最多的，这需要相当数量的存储和计算空间。参数的冗杂问题使得单纯的FC组成的常规神经网络很少会被用于较为复杂的场景中。常规神经网络一般用于依赖所有特征的简单场景，比如我们接下来会介绍的房价预测模型使用的就是相对标准的全连接神经网络。</p><h4 id="【卷积神经网络】"><a href="#【卷积神经网络】" class="headerlink" title="【卷积神经网络】"></a><strong>【卷积神经网络】</strong></h4><blockquote><p>可以减少全连接神经网络的参数量；</p><p>主要针对参数与参数之间、特征与特征之间有相互连结的数据；</p><p>专门用来处理计算机视觉问题。</p></blockquote><p>卷积神经网络是一种专门用来处理具有类似网络结构的数据的神经网络，如图像数据。与FC不同的地方在于，CNN的上下层神经元并不都能直接连接，而是通过<strong>卷积核(convolution kernel)</strong> 作为中介，通过核的共享，大大减少了隐藏层的参数。</p><p>简单的CNN是一系列层，并且每个层都通过一个可微函数将一个量转化为另一个量，这些层主要包括卷积层(Convolutional layer)、池化层(Pooling layer) 和全连接层(FC layer) 。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/b5fe80963f054e8daa3c0f03014339f48ffd35c76fee4d43926d03c2c7564d3b" width="400" hegiht="" ></center><center>图7：卷积神经网络 </center><h4 id="【循环神经网络】"><a href="#【循环神经网络】" class="headerlink" title="【循环神经网络】"></a><strong>【循环神经网络】</strong></h4><blockquote><p> 专门用来处理自然语言方面的问题。</p></blockquote><p>循环神经网络也是常用的深度学习模型之一，就像CNN是专门用于处理网格化数据的神经网络，RNN是一种用于处理序列数据的神经网络。如音频中含有时间成分，因此音频可以被表示为一维时间序列；语言中的单词都是逐个出现的，因此语言的表示方式也是序列数据。RNN在机器翻译、语音识别等领域中均有非常好的表现。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/771862d38d94418dac47b6a19a335c6020293de0d2594ce6bd6521ce774bc15d" width="400" hegiht="" ></center><center>图8：循环神经网络 </center><h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a><strong>机器学习基础</strong></h2><p>在了解了深度学习的概念之后，本节将会以机器学习的相关概念作为切入点，来使用代码实现基本的神经网络模型，这些实验方法在深度学习中也同样使用。</p><h3 id="机器学习一般过程"><a href="#机器学习一般过程" class="headerlink" title="机器学习一般过程"></a><strong>机器学习一般过程</strong></h3><p><strong>机器学习</strong>有三个主要的组成部分：经验Experience（E）、任务Task（T）、任务完成效果的衡量指标Performance measure（P）。有了这三个概念，机器学习的定义可以表述为：在有了经验E的帮助后，机器完成任务T的衡量指标P会变得更好。</p><p>本节从构造模型的角度将机器学习理解为：<strong>从数据中产生模型的过程</strong>。机器学习的过程如下：输入训练数据，利用特定的机器学习方法建立估计函数。在训练得到函数后，可将测试数据输入该函数，该函数的输出即预测结果。</p><h3 id="机器学习的方法"><a href="#机器学习的方法" class="headerlink" title="机器学习的方法"></a><strong>机器学习的方法</strong></h3><p><strong>（1）有监督学习（supervised learning）</strong></p><p>从给定的有标注的训练数据集中学习出一个函数（模型参数），当新的数据到来时可以根据这个函数预测结果。 常见任务包括<strong>分类</strong>与<strong>回归</strong>。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/d0a76f00ccad4c69a463f2b08130daf9c0d03e4340ee41cab6d48b554f789dd5" width="600" hegiht="" ></center><center>图9：有监督学习 </center><p><strong>（2）无监督学习（unsupervised learning）</strong></p><p>使用没有标注的训练数据集，需要根据样本间的统计规律对样本集进行分析，常见任务如<strong>聚类</strong>等。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/500f4125efef4dd28357cddb05b2d0683c4ad0ae629948e886c66ba5bf679c71" width="600" hegiht="" ></center><center>图10：无监督学习 </center><p><strong>（3）半监督学习（Semi-supervised learning）</strong></p><p>结合（少量的）<strong>标注训练数据</strong>和（大量的）<strong>未标注数据</strong>来进行数据的分类学习。</p><p><strong>（4）强化学习（Reinforcement Learning）</strong></p><p>外部环境对输出只给出评价信息而非正确答案，学习机通过强化受奖励的动作来改善自身的性能。</p><p>如：让计算机学着去玩Flappy Bird</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/25feff85710d4ee996fde5bc921b387e17e7185322e0445082a54ea4af355282" width="400" hegiht="" ></center><center>图11：Flappy Bird 游戏 </center><p>我们不需要设置具体的策略，比如先飞到上面，再飞到下面，我们只是需要给算法定一个“小目标”！比如当计算机玩的好的时候，我们就给它一定的奖励，它玩的不好的时候，就给它一定的惩罚，在这个算法框架下，它就可以越来越好，超过人类玩家的水平。</p><h3 id="机器学习的准备"><a href="#机器学习的准备" class="headerlink" title="机器学习的准备"></a><strong>机器学习的准备</strong></h3><p>机器学习的实现可以分成两步：<strong>训练和预测</strong>，类似于归纳和演绎：</p><p><strong>归纳</strong>： 从具体案例中抽象一般规律，机器学习中的“训练”亦是如此。从一定数量的样本（已知模型输入$X$和模型输出$Y$）中，学习输出$Y$与输入$X$的关系（可以想象成是某种表达式）。</p><p><strong>演绎</strong>： 从一般规律推导出具体案例的结果，机器学习中的“预测”亦是如此。基于训练得到的$Y$与$X$之间的关系，如出现新的输入$X$，计算出输出$Y$。通常情况下，如果通过模型计算的输出和真实场景的输出一致，则说明模型是有效的。</p><h4 id="【特征工程】"><a href="#【特征工程】" class="headerlink" title="【特征工程】"></a><strong>【特征工程】</strong></h4><center><img src="https://ai-studio-static-online.cdn.bcebos.com/194042801ccc44a2813758d7c58abbd19d564785361f44708a4751c2b4440ce7" width="500" hegiht="" ></center><center>图13：人看到的图像 </center><center><img src="https://ai-studio-static-online.cdn.bcebos.com/faf29e181a5a48abb2d5671e2abbacb0db378c4f242f466a8b1c9cb9cd305fa5" width="500" hegiht="" ></center><center>图14：计算机看到的图像 </center><p>特征工程，是指用一系列工程化的方式从原始数据中筛选出更好的数据特征，以提升模型的训练效果。业内有一句广为流传的话是：数据和特征决定了机器学习的上限，而模型和算法是在逼近这个上限而已。由此可见，好的数据和特征是模型和算法发挥更大的作用的前提。</p><p>特征工程通常包括数据预处理、特征选择、降维等环节。</p><p><strong>1.数据预处理</strong></p><p>真实世界的数据通常包含噪声、缺失值，并且可能采用无法直接用于机器学习模型的不可用格式（例如：图像）。当收集到真实数据后，往往不能直接使用，要根据数据的具体情况来进行针对性的处理。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/3a07e61dee96471eb52e59726f8e30717a151c0c32f94156a9361e996e6535f1" width="400" hegiht="" ></center><center>图15：数据预处理 </center><p>1) 数据清洗<br>对各种脏数据进行对应方式的处理，得到标准、干净、连续的数据，提供给数据统计、数据挖掘等使用。数据的完整性、数据的合法性、数据的唯一性、数据的权威性以及数据的一致性等。<br>2) 数据采样<br>要避免数据的不平衡（数据集的类别分布不均）。<br>3) 数据集拆分<br>训练数据集、验证数据集、测试数据集。$K$-折交叉验证法：把训练样例分成$k$份，然后进行$K$次交叉验证过程，每次使用不同的一份样本作为验证集合，其余$k-1$份样本合并作为训练集合。</p><p><strong>2.特征选择</strong></p><p>请看下面这张图片并思考，所有特征都有用吗？</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f258b1e2ca2745dbb729055e99904dd0d11f22146e5c455bb9a917ba717ca015" width="600" hegiht="" ></center><center>图16：特征选择 </center><p>特征选择方法比较简单粗暴，直接将不重要的特征删除。特征选择方法主要包括三大类：<strong>过滤法(Filter)</strong>、<strong>包装法(Wrapper)</strong> 和<strong>嵌入法(Embedded)</strong>。</p><ol><li>过滤法：根据发散性或者相关性对各个特征进行评分，通过设定阈值或者待选择阈值的个数来选择特征。</li><li>包装法：根据目标函数(通常是预测效果评分)每次选择若干特征，或者排除若干特征。包装法的思路是将最终要用的学习器的性能作为特征子集的评价准则。</li><li>嵌入法：是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。</li></ol><p><strong>3.特征降维</strong></p><p>特征选择完成后，可能由于特征矩阵过大，导致计算量大、训练时间长，因此降低特征矩阵维度也是必不可少的。特征降维主要是通过映射变换方法，将高维特征向量空间映射到低维特征向量空间中去，通过这种方法产生的的特征都不在原始数据中。</p><p><strong>4.特征编码</strong></p><p>数据集中经常会出现字符串信息，例如男女、高中低等，这类信息不能直接用于算法计算，需要将这些数据转化为数值形式进行编码，便于后期进行建模。</p><blockquote><p><strong>独热编码(one-hot)</strong>：图中的Elevator和Renovation都是定类型数据。除去缺失值，Elevator分类有电梯和无电梯两种，因此可用01和10表示。Renovation分为有精装，简装，毛坯和其它四种，可用0001/0010/0100/1000表示。</p></blockquote><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f4e28298e89f4d749dd4f68fe6c6fbb2bf5b6cb759fc4d809cae0b9e0ca2527e" width="800" hegiht="" ></center><center>图17：独热编码 </center><h4 id="【数据建模】"><a href="#【数据建模】" class="headerlink" title="【数据建模】"></a><strong>【数据建模】</strong></h4><p>世界上的可能关系千千万，漫无目标的试探$Y$~$X$之间的关系显然是十分低效的。因此假设空间先圈定了一个模型能够表达的关系可能。主要的数据模型有两类：<strong>分类问题</strong>和<strong>回归问题</strong>。</p><p><strong>分类问题</strong>：分类问题是监督学习的一个核心问题，它从数据中学习一个分类决策函数或分类模型，对新的输入进行输出预测，输出变量取有限个离散值。分类在我们日常生活中很常见。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/2e64f52ed15247faac6a3a38c80cfb7075bc5055b8c440d1be78c1c6ffb1772b" width="500" hegiht="" ></center><center>图18：分类 </center><p><strong>回归问题</strong>：回归分析用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量值随之发生变化。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/18b70cc6988f4b919760c85a1c13e9546e0a56e394a54f3098161f9bf94616d0" width="400" hegiht="" ></center><center>图19：回归 </center><p>在把一个问题建模的时候一定要考虑好需求，让你的模型更好的与现实问题相对应。</p><h4 id="【模型评估】"><a href="#【模型评估】" class="headerlink" title="【模型评估】"></a><strong>【模型评估】</strong></h4><p>所谓模型评估，即对模型的泛化能力(性能)进行评估，一方面可以从实验角度进行比较，如交叉验证等；另一方面可以利用具体的性能评价标准，如测试集准确率等。通常来说, 模型的好坏不仅取决于算法和数据，还取决于任务需求。因此，不同的任务往往对应不同的评价指标。</p><p><strong>1.性能评价指标-分类</strong></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/538c58a0539a477f84d8af43fa4a5e0b813327f8fe184d23bdd9808094de8aa7" width="400" hegiht="" ></center><center>图20：分类评价 </center><p><strong>准确率(Accuracy)</strong>：分类任务中，分类正确（包括正负样本）的记录个数占总记录个数的比。</p><p><strong>精确率(Precision)</strong>：分类正确的正样本个数占分类器所有的正样本个数的比例。</p><p><strong>召回率(Recall)</strong>：也叫查全率，是指在分类中样本中的<strong>正例</strong>有多少被预测正确了，通常，准确率高时，召回率偏低；召回率高时，准确率偏低。</p><p><strong>F1-Score</strong>：精确率与召回率的调和平均值，它的值更接近于Precision与Recall中较小的值。</p><p>例如，对于地震的预测，我们希望的是召回率非常高，也就是说每次地震我们都希望预测出来；而对于嫌疑人的定罪我们希望是非常准确的，即使有时候放过了一些罪犯（召回率低）。</p><p><strong>2.性能评价指标-回归</strong></p><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：平均绝对误差就是指预测值与真实值之间平均相差多大。平均绝对误差能更好地反映预测值误差的实际情况。</p><p><strong>均方误差（Mean Squared Error，MSE）</strong>：观测值与真值偏差的平方和与观测次数的比值。这也是线性回归中最常用的损失函数，线性回归过程中尽量让该损失函数最小。那么模型之间的对比也可以用它来比较。MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><p><strong>交叉验证（Cross-Validation）</strong>：交叉验证，有的时候也称作<strong>循环估计（Rotation Estimation）</strong>，是一种统计学上将数据样本切割成较小子集的实用方法。在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为<strong>PRESS(predicted Error Sum of Squares)</strong>。</p><p>交叉验证的基本思想是把在某种意义下将原始<strong>数据(dataset)</strong> 进行分组,一部分做为<strong>训练集(train set)</strong>，另一部分做为<strong>验证集(validation set)</strong>。首先用训练集对分类器进行训练，再利用验证集来测试训练得到的<strong>模型(model)</strong>，以此来做为评价分类器的性能指标。</p><p>无论分类还是回归模型，都可以利用交叉验证进行模型评估。</p><hr><p><strong>补充：</strong></p><p><strong>性能评价指标-分类举例</strong></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4b80e0d2361846b3b1350ff62dfe689a61c46fa73eb043998d46127eeb064a57" width="600" hegiht="" ></center><p><strong>准确率(Accuracy)</strong>：分类任务中，<strong>分类正确</strong>的记录个数占<strong>总</strong>记录个数的比。<script type="math/tex">公式：(TP+TN)/(TP+TN+FP+FN)</script></p><p><strong>精确率(Precision)</strong>：<strong>分类正确</strong>的正样本个数占<strong>分类器(预测出来的)所有</strong>的正样本个数的比例。—-&gt;猜对了多少？</p><script type="math/tex; mode=display">公式：FP/(TP+FP)</script><p><strong>召回率(Recall)</strong>：也叫查全率，是指在分类中<strong>样本正例</strong>有多少被<strong>预测正确</strong>了。—-&gt;找出了多少？</p><script type="math/tex; mode=display">公式：TP/(TP+FN)</script><p><strong>F1-Score</strong>：精确率与召回率的<strong>调和平均值</strong><script type="math/tex">（F1=2·Precision·Recall/（Precision+Recall））</script>，它的值更接近于Precision与Recall中较小的值。</p><hr><p><strong>精确率和召回率二者是此消彼长的关系</strong></p><p>这句话可以这么理解：我们做核酸检测，希望能够达到比较高的召回率，也就是样本中的正例尽可能预测出来，所以正例会偏多，不想放过可疑的。但是这个时候，会加大预测失误的风险，因此分类正确的指标就会下降。</p><blockquote><p><strong>宁可错杀一万，不可放过一人———&gt; 召回率</strong></p><p><strong>宁可错过一万，不可误杀一人———&gt; 精确率</strong></p></blockquote><hr><p><strong>假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本。系统查找出50个正样本，其中只有40个是真正的正样本，计算上述各指标。</strong></p><ul><li>TP：将正类预测为正类的样本数 40</li><li>FN：将正类预测为负类的样本数 20</li><li>FP：将负类预测为正类的样本数 10</li><li>TN：将负类预测为负类的样本数 30</li></ul><p><strong>由此可得：</strong></p><ul><li>准确率：（40+30）/100 = 70%</li><li>精确率：40/（40+10） = 80%</li><li>召回率：40/（40+20） = 67%</li></ul><p><strong>①高召回率：假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本。系统查找出55个正样本，其中只有42个是真正的正样本，计算上述各指标。</strong></p><ul><li>TP：将正类预测为正类的样本数 ？42</li><li>FN：将正类预测为负类的样本数 ？18</li><li>FP：将负类预测为正类的样本数 ？13</li><li>TN：将负类预测为负类的样本数 ？27</li></ul><p><strong>由此可得：</strong></p><ul><li>准确率：？69%</li><li>精确率：？76%</li><li>召回率：？70%</li></ul><p><strong>②高精确率：假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本。系统查找出30个正样本，其中只有25个是真正的正样本，计算上述各指标。</strong></p><ul><li>TP：将正类预测为正类的样本数 ？</li><li>FN：将正类预测为负类的样本数 ？</li><li>FP：将负类预测为正类的样本数 ？</li><li>TN：将负类预测为负类的样本数 ？</li></ul><p><strong>由此可得：</strong></p><ul><li>准确率：？</li><li>精确率：？</li><li>召回率：？</li></ul><h2 id="机器学习的方法论"><a href="#机器学习的方法论" class="headerlink" title="机器学习的方法论"></a><strong>机器学习的方法论</strong></h2><p>机器学习的方法论和人类科研的过程有着异曲同工之妙，下面以“机器从牛顿第二定律实验中学习知识”为例，来更加深入理解机器学习（有监督学习）的方法论本质，即在“机器思考”的过程中确定模型的三个关键要素：假设、评价、优化。</p><p><strong>案例：机器从牛顿第二定律实验中学习知识</strong></p><blockquote><p>牛顿第二定律是艾萨克·牛顿在1687年于《自然哲学的数学原理》一书中提出的，其常见表述：物体加速度的大小跟作用力成正比，跟物体的质量成反比，与物体质量的倒数成正比。牛顿第二运动定律和第一、第三定律共同组成了牛顿运动定律，阐述了经典力学中基本的运动规律。</p></blockquote><p>在中学课本中，牛顿第二定律有两种实验设计方法：倾斜滑动法和水平拉线法。通过实验，我们可以获取大量数据样本和观测结果。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/afdf03064fd24298954b03072d9db7d2480482aae65744f3894f3dab61abae42" width="600" hegiht="" ></center><center>图21：牛顿第二定律实验 </center><p>观察实验数据不难猜测，物体的加速度$a$和作用力$F$之间的关系应该是线性关系。因此我们提出假设 $a = w \cdot F$，其中，$a$代表加速度，$F$代表作用力，$w$是待确定的参数。<br>通过大量实验数据的训练，确定参数$w$是物体质量的倒数$(1/m)$，即得到完整的模型公式$a = F \cdot (1/m)$。</p><p>这个有趣的案例演示了机器学习的基本过程，但其中有一个关键点的实现尚不清晰，即：<strong>如何确定模型参数$（w=1/m）$？</strong></p><p>在牛顿第二定律的案例中，基于对数据的观测，我们提出了线性<strong>假设(hypothesis)</strong>，即作用力和加速度是线性关系，用线性方程表示之后我们使用已有的数据来验证假设的可行性，衡量模型预测值和真实值差距的评价函数也被称为<strong>损失函数(loss function)</strong>。最小化损失是模型的优化目标，实现损失最小化的方法称为<strong>优化算法(optimization)</strong>，也称为寻解算法（找到使得损失函数最小的参数解），直到模型学习到一个参数$w4，使得损失函数的值最小。</p><p>由此可见，<strong>模型假设、损失函数和优化算法</strong>是构成模型的三个关键要素。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/fb15753624dd4f13813391a82dc30210e3abf09f01494b678198bbaf24a17035" width="600" hegiht="" ></center><center>图22：模型构成三要素 </center><ul><li>模型假设：世界上的可能关系千千万，漫无目标的试探$Y$~$X$之间的关系显然是十分低效的。因此假设空间先圈定了一个模型能够表达的关系可能，如蓝色圆圈所示。机器还会进一步在假设圈定的圆圈内寻找最优的$Y$~$X$关系，即确定参数$w$。</li><li>损失函数：寻找最优之前，我们需要先定义什么是最优，即评价一个$Y$~$X$关系的好坏的指标。通常衡量该关系是否能很好的拟合现有观测样本，将拟合的误差最小作为优化目标。</li><li>优化算法：设置了评价指标后，就可以在假设圈定的范围内，将使得评价指标最优（损失函数最小/最拟合已有观测样本）的$Y$~$X$关系找出来，这个寻找最优解的方法即为优化算法。最笨的优化算法即按照参数的可能，穷举每一个可能取值来计算损失函数，保留使得损失函数最小的参数作为最终结果。</li></ul><p>机器执行学习任务的框架体现了其学习的本质是 <strong>“参数估计”（Learning is parameter estimation）</strong>。</p><h2 id="传统机器学习VS深度学习"><a href="#传统机器学习VS深度学习" class="headerlink" title="传统机器学习VS深度学习"></a><strong>传统机器学习VS深度学习</strong></h2><p>相比传统的机器学习算法，深度学习做出了哪些改进呢？其实<strong>两者在理论结构上是一致的，即：模型假设、损失函数和优化算法，其根本差别在于假设的复杂度</strong>。这种变换已经无法用数学公式表达，因此研究者们借鉴了人脑神经元的结构，设计出基于<strong>神经网络的模型</strong>。</p><p><strong>传统方法</strong>：人工特征提取+分类器</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/49b9cc4ff48342c790bd31968837a68d2e1ccbb25941437c8f99fe8f42e2a8e5" width="500" hegiht="" ></center><center>图23：传统机器学习下的鸢尾花分类 </center><ol><li>收集并标注几百张鸢尾花的图片；</li><li>观察不同的鸢尾花图片，并绞尽脑汁选择或设计一些特征形状，颜色，纹理；</li><li>根据设计的特征，提取图像的特征，并选择分类器进行训练和测试；</li><li>结果不好的话，回到第二步。</li></ol><p><strong>深度学习方法</strong>：提供图像和标签，通过网络自己去学习特征</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4972b98ead7d49f9a23058bcbe28043316300e20c8504546a798892b9fe610ac" width="500" hegiht="" ></center><center>图24：深度学习下的鸢尾花分类 </center><ol><li>收集并标注几千张图像</li><li>确定网络结构，交给机器绞尽脑汁完成任务</li></ol><h2 id="深度学习结构三要素"><a href="#深度学习结构三要素" class="headerlink" title="深度学习结构三要素"></a><strong>深度学习结构三要素</strong></h2><p>数字识别是计算机从纸质文档、照片或其他来源接收、理解并识别可读的数字的能力，目前比较受关注的是手写数字识别。手写数字识别是一个典型的图像分类问题，已经被广泛应用于汇款单号识别、手写邮政编码识别等领域，大大缩短了业务处理时间，提升了工作效率和质量。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/04e594dbf02d4c18af3622292e303b3f96720fba0c8b43899c9580f52c51d843" width="500" hegiht="" ></center><center>图25：手写数字识别流程 </center><ol><li>建立模型：选择什么样的网络结构/选择多少层数，每层选择多少神经元</li><li>损失函数：选择常用损失函数，平方误差，交叉熵，…</li><li>优化学习：梯度下降/反向传播算法</li></ol><h3 id="【建立模型】"><a href="#【建立模型】" class="headerlink" title="【建立模型】"></a><strong>【建立模型】</strong></h3><p>借鉴了人脑神经元的结构，设计出的神经网络模型可以更好的拟合复杂的数据。</p><p><strong>生物神经元</strong></p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e01411e04291412d8a38d0a11bd71652846ec038ed8c4c9f8bd2946c05a9e15d" width="500" hegiht="" ></center><center>图26：生物神经元 </center><ul><li>每个神经元都是一个多输入单输出的信息处理单元；</li><li>神经元具有空间整合特性和阈值特性（兴奋和抑制，超过阈值为兴奋，低于是抑制）；</li><li>神经元输入分兴奋性输入和抑制性输入两种类型；</li><li>神经元输入与输出间有固定的时滞，主要取决于突触延搁（计算耗时）。 </li></ul><p><strong>M-P神经元</strong></p><p>按照生物神经元，我们建立M-P模型。为了使得建模更加简单，以便于进行形式化表达，我们忽略时间整合作用、不应期等复杂因素，并把神经元的突触时延和强度当成常数。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/7ba722dcc4e14db3a722f67613edfb218d00269d6d754a0897c2f7895b3e9caf" width="600" hegiht="" ></center><center>图27：M-P神经元 </center><p>神经网络中每个节点称为神经元，由两部分组成：</p><ul><li>加权和：将所有输入加权求和。</li><li>非线性变换（激活函数）：加权和的结果经过一个非线性函数变换，让神经元计算具备非线性的能力。</li></ul><p>大量这样的节点按照不同的层次排布，形成多层的结构连接起来，即称为神经网络，神经元不同的连接方式构成不同的网络结构。每个神经元都有自己的权重和偏置参数。</p><h3 id="【损失函数】"><a href="#【损失函数】" class="headerlink" title="【损失函数】"></a><strong>【损失函数】</strong></h3><p>损失函数是模型优化的目标，用于在众多的参数取值中识别最理想的取值，<strong>损失函数的计算在训练过程的代码中，每一轮模型训练的过程都相同</strong>，分如下三步：</p><ol><li>先根据输入数据正向计算预测输出；</li><li>再根据预测值和真实值计算损失；</li><li>最后根据损失反向传播梯度并更新参数。</li></ol><h3 id="【优化学习】"><a href="#【优化学习】" class="headerlink" title="【优化学习】"></a><strong>【优化学习】</strong></h3><p>借用古代炼丹的一些名词，我们可以把训练模型中的数据比做炼丹药材，模型比做炼丹炉，火候比做优化器。那么我们知道，同样的药材同样的炼丹炉，但是火候不一样的话，炼出来的丹药千差万别，同样的对于深度学习中训练模型而言，有时候模型性能不好，也许不是数据或者模型本身的原因，而是优化器的原因。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/e3a02a4a011f4e7db14336c5b5808d0bc20f632d5efb46d1b72e858a79852d9b" width="500" hegiht="" ></center><center>图28：优化学习 </center><p>深度学习中的优化问题通常指的是：寻找神经网络上的一组参数，它能显著地降低目标函数。在深度学习中，常见的优化算法包括传统的 SGD，Momentum SGD，AdaGrad，RMSProp 和 Adam 等，我们可以使用一张图来简单看一下这些优化算法各自的性质，后面我们会在优化算法一章中展开讲解。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/335feff01a2346fd897d18139294eeb5dc447180f3a9491da461c12db1ae7e52" width="500" hegiht="" ></center><center>图29：优化算法 </center>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习基础_深度学习总览与模型搭建 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> paddle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🎈如何使用nvm管理node？</title>
      <link href="/2022/12/17/nvm/"/>
      <url>/2022/12/17/nvm/</url>
      
        <content type="html"><![CDATA[<h1 id="使用nvm管理node-js"><a href="#使用nvm管理node-js" class="headerlink" title="使用nvm管理node.js"></a>使用nvm管理node.js</h1><h3 id="1、nvm是什么"><a href="#1、nvm是什么" class="headerlink" title="1、nvm是什么"></a>1、nvm是什么</h3><p>（1）nvm(Node.js version manager) 是一个命令行应用，可以协助您快速地 更新、安装、使用、卸载 本机的全局 node.js 版本。<br>（2）有时候，我们可能同时在进行多个项目开发，而多个项目所使用的node版本又是不一样的，或者是要用最新的node版本进行试验和学习。这种情况下，对于维护多个版本的node将会是一件非常麻烦的事情，而nvm就是为解决这个问题而产生的，他可以在同一台电脑上进行多个node版本之间的切换，而这正是nvm的价值所在。</p><h3 id="2、使用-nvm-管理版本（nvm常用命令）"><a href="#2、使用-nvm-管理版本（nvm常用命令）" class="headerlink" title="2、使用 nvm 管理版本（nvm常用命令）"></a>2、使用 nvm 管理版本（nvm常用命令）</h3><h5 id="最常用的命令"><a href="#最常用的命令" class="headerlink" title="最常用的命令"></a>最常用的命令</h5><ul><li><strong>nvm install [node版本号]</strong>   //安装指定版本的node</li><li><strong>nvm ls</strong> // 查看已安装版本</li><li><strong>nvm use [node版本号]</strong>   //切换到指定版本的node</li></ul><blockquote><p><strong>nvm install latest</strong> 安装最新版本node.js<br><strong>nvm use 版本号</strong> 使用某一具体版本，例如 ：nvm use 14.3.0<br><strong>nvm list</strong> 列出当前已安装的所有版本<br><strong>nvm ls</strong> 列出当前已安装的所有版本<br><strong>nvm uninstall 版本号</strong> 卸载某一具体版本，例如：nvm use 14.3.0<br><strong>nvm ls-remote</strong> Mac版本中,列出全部可以安装的node版本<br><strong>nvm ls available</strong> windows版本,列出全部可以安装的node版本<br><strong>nvm current</strong> 显示当前的版本<br><strong>nvm alias</strong> 给不同的版本号添加别名<br><strong>nvm unalias</strong> 删除已定义的别名<br><strong>nvm reinstall-packages</strong> 在当前版本node环境下，重新全局安装指定版本号的npm包</p><p>nvm arch [32|64]： 显示node是运行在32位还是64位模式。指定32或64来覆盖默认体系结构。<br>-nvm install [arch]：该可以是node.js版本或最新稳定版本latest。（可选[arch]）指定安装32位或64位版本（默认为系统arch）。设置[arch]为all以安装32和64位版本。在命令后面添加–insecure，可以绕过远端下载服务器的SSL验证。<br>nvm list [available]：列出已经安装的node.js版本。可选的available，显示可下载版本的部分列表。这个命令可以简写为nvm ls [available]。<br>nvm on： 启用node.js版本管理。<br>nvm off： 禁用node.js版本管理(不卸载任何东西)<br>nvm proxy [url]： 设置用于下载的代理。留[url]空白，以查看当前的代理。设置[url]为none删除代理。<br>nvm node_mirror [url]：设置node镜像，默认为<a href="https://nodejs.org/dist/.。可以设置为淘宝的镜像https://npm.taobao.org/mirrors/node/">https://nodejs.org/dist/.。可以设置为淘宝的镜像https://npm.taobao.org/mirrors/node/</a><br>nvm npm_mirror [url]：设置npm镜像，默认为<a href="https://github.com/npm/npm/archive/。可以设置为淘宝的镜像https://npm.taobao.org/mirrors/npm/">https://github.com/npm/npm/archive/。可以设置为淘宝的镜像https://npm.taobao.org/mirrors/npm/</a><br>nvm uninstall ： 卸载指定版本的nodejs。<br>nvm use [version] [arch]： 切换到使用指定的nodejs版本。可以指定32/64位[arch]。<br>-nvm use ：将继续使用所选版本，但根据提供的值切换到32/64位模式<br>nvm root [path]： 设置 nvm 存储node.js不同版本的目录 ,如果未设置，将使用当前目录。<br>-nvm version： 显示当前运行的nvm版本，可以简写为nvm v</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> node.js </tag>
            
            <tag> nvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🐌博客搭建学习笔记</title>
      <link href="/2022/12/17/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/12/17/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="snowman-vscode中对博客项目的操作"><a href="#snowman-vscode中对博客项目的操作" class="headerlink" title=":snowman:vscode中对博客项目的操作"></a>:snowman:vscode中对博客项目的操作</h1><p><code>hexo cl;hexo s</code>跑博客项目。</p><hr><h2 id="1、将本地博客部署到github"><a href="#1、将本地博客部署到github" class="headerlink" title="1、将本地博客部署到github"></a>1、将本地博客部署到github</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy  // Git BASH终端</span><br><span class="line">hexo clean; hexo generate; hexo deploy  // VSCODE终端</span><br><span class="line">注意：需要输入密码。</span><br></pre></td></tr></table></figure><blockquote><ul><li>hexo clean：删除之前生成的文件，若未生成过静态文件，可忽略此命令。</li><li>hexo generate：生成静态文章，可以用<code>hexo g</code>缩写</li><li>hexo deploy：部署文章，可以用<code>hexo d</code>缩写</li></ul></blockquote><hr>              </div>            </details><h2 id="2、Nunjucks-Error的错误解决方案"><a href="#2、Nunjucks-Error的错误解决方案" class="headerlink" title="2、Nunjucks Error的错误解决方案"></a>2、<code>Nunjucks Error</code>的错误解决方案</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p><a href="[Nunjucks Error错误处理 – 软件学习笔记 (sixiwenwu.com">参考博客</a>](<a href="http://sixiwenwu.com/86/">http://sixiwenwu.com/86/</a>))</p><p>我们MarkDown记录一些笔记或者啥的难免会用到公式，当我们上传到博客时可能会出现<code>Nunjucks Error</code>的错误信息，这时需要从报错信息上寻找解决方法。</p><h4 id="报错信息"><a href="#报错信息" class="headerlink" title="报错信息"></a>报错信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FATAL Something&#x27;s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html</span><br><span class="line">Nunjucks Error: _posts/深度学习3.2-模型选择与调优策略（下）.md [Line 469, Column 85] expected variable end</span><br></pre></td></tr></table></figure><h4 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h4><p>官网对于Nunjucks的描述：<a href="https://hexo.io/docs/troubleshooting.html">https://hexo.io/docs/troubleshooting.html</a></p><p><img src="https://s1.ax1x.com/2020/07/13/UYla2q.png" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hexo使用Nunjucks来渲染帖子（Swig在旧版本中使用，它们共享类似的语法）。内容被包装或将被解析，可能会导致问题。您可以通过使用原始标签插件、单反引号或三反引号包装它来跳过解析。</span><br><span class="line">或者，可以通过渲染器的选项（如果支持）、API 或前言禁用 Nunjucks 标签。&#123;&#123; &#125;&#125;&#123;% %&#125;`&#123;&#123; &#125;&#125;`</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;</span><br><span class="line">Hello &#123;&#123; world &#125;&#125;</span><br><span class="line">&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>对于正文中除了代码块（单行代码也不行）以外的所有地方，在<code>%与{}</code>之间添加空格，如：<code>{ % % } 或 { { } }</code>或者使用代码块表示。</p>              </div>            </details><h1 id="snowman-Hexo博客"><a href="#snowman-Hexo博客" class="headerlink" title=":snowman:Hexo博客"></a>:snowman:Hexo博客</h1><p>Hexo框架默认的根目录就是source文件夹。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">menu:   //博客的菜单栏</span><br><span class="line">  首页: / || fas fa-home</span><br><span class="line">  归档: /archives/ || fas fa-archive</span><br><span class="line">  标签: /tags/ || fas fa-tags</span><br><span class="line">  分裂: /categories/ || fas fa-folder-open</span><br><span class="line">  列表||fas fa-list:</span><br><span class="line">    音乐: /music/ || fas fa-music</span><br><span class="line">    电影: /movies/ || fas fa-video</span><br><span class="line">  友链: /link/ || fas fa-link</span><br><span class="line">  关于: /about/ || fas fa-heart</span><br></pre></td></tr></table></figure><p>归档、标签、分类页框架会自动渲染出来，但我们需要添加自定义页：音乐、电影、友链、关于。</p><hr><p><code>_config.butterfly.ymlqw</code>文件中<code>inject</code>模块中可以引用外部的<code>css</code>文件和<code>js</code>文件</p><hr><h2 id="1、关于博客中的图标："><a href="#1、关于博客中的图标：" class="headerlink" title="1、关于博客中的图标："></a>1、关于博客中的图标：</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p><strong>1、iconfont阿里巴巴矢量图标</strong></p><p><details class="folding-tag" red><summary> 第一种方法 </summary>              <div class='content'>              <p>直接在阿里巴巴矢量图标网站选择想要i的图标添加到购物车，然后在添加到<code>my_blog</code>项目中，将此项目的css文件引用到bolg源代码的inject中即可。</p><p>引用图标用<em><code>iconfont icon-图标名</code></em>，在项目中可见</p>              </div>            </details></p><p><details class="folding-tag" red><summary> 第二种方法 </summary>              <div class='content'>              <div class="tabs" id="icon图标"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#icon图标-1">标签语法</button></li><li class="tab"><button type="button" data-href="#icon图标-2">参数配置</button></li><li class="tab"><button type="button" data-href="#icon图标-3">示例源码</button></li><li class="tab"><button type="button" data-href="#icon图标-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="icon图标-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon [icon-xxxx],[font-size] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="icon图标-2"><ol><li><code>icon-xxxx</code>：表示图标<code>font-class</code>,可以在自己的阿里矢量图标库项目的<code>font-class</code>引用方案内查询并复制。</li><li><code>font-size</code>：表示图标大小，直接填写数字即可，单位为<code>em</code>。图标大小默认值为<code>1em</code>。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="icon图标-3"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon icon-rat_zi %&#125;&#123;% icon icon-rat,2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-ox_chou,3 %&#125;&#123;% icon icon-ox,4 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-tiger_yin,5 %&#125;&#123;% icon icon-tiger,6 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="icon图标-4"><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rat_zi"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rat"></use></svg><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-ox_chou"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-ox"></use></svg><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-tiger_yin"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-tiger"></use></svg><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>              </div>            </details></p><p><p><strong>2、使用电脑自带图标表情</strong></p></p><p><p><em><code>开始键+；</code></em>即可</p></p><p><p><strong>3、Font Awesome图标</strong></p></p><p><p>我们可以上 <a href="http://fontawesome.dashgame.com/">http://fontawesome.dashgame.com/</a> 网站上选自己喜欢的图标，在 font-awesome.css 中查看引用id，然后在需要的地方使用。【font-awesome.css文件已经下载到本地，并在inject中引用】<br>比如我想将侧边栏文章日期前的小时钟图标换为日历图标，在网站上搜一下<code>calendar</code>，找到图标<code>calendar</code>，去font-awesome.css中查找其id为<code>fa-calendar</code>，在ejs中替换默认的<code>fa-clock-o</code>。</p></p><p><p>[点击直接查看图标引用的格式]（<a href="https://wpmore.cn/resources/font-awesome/）">https://wpmore.cn/resources/font-awesome/）</a></p></p><p><p><strong>4、动态图标</strong></p></p><p><p>使用<code>fa-spin</code>类来使任意图标旋转，``即可使fa-tags图标旋转。</p></p><p><p>具体到我的博客，我将主页侧边栏的“友情链接“中“我的CSDN博客”前的小足球改为旋转的了。<br>在主题的配置文件_config.yml中，在图标引用中加上<code>fa-spin</code>类：</p><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">links:</span><br><span class="line">  - title: &quot;我的CSDN博客&quot;</span><br><span class="line">    url: http://blog.csdn.net/masikkk</span><br><span class="line">    intro: &quot;我的CSDN博客&quot;</span><br><span class="line">    icon: &quot;fa fa-soccer-ball-o fa-spin&quot;</span><br></pre></td></tr></table></figure></p><hr>              </div>            </details><h2 id="2、博客内容写作"><a href="#2、博客内容写作" class="headerlink" title="2、博客内容写作"></a>2、博客内容写作</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <h3 id="标签外挂"><a href="#标签外挂" class="headerlink" title="标签外挂"></a>标签外挂</h3><p>:yum:<a href="https://www.fomal.cc/posts/2013454d.html">查阅链接点这里 | Fomalhaut🥝</a></p><blockquote><p>除了标准的mackdown语法以外，butterfly还提供了额外的一些组件标签（非标准mackdown语法，butterfly以外的地方使用可能不生效且会报错）</p></blockquote><h3 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h3><blockquote><p>markdown允许在页面中定义一些额外属性（相当于变量），Hexo框架会读取每个md文件的front-matter传给butterfly主题框架使用。每个页面或文章的一些相关信息设置需要用到。如文章分类，创建时间以及文章顶部标题都需要在 front-matter中配置。</p><p>front-matter 定义在md文件顶部，由 —- 符号包括。、</p></blockquote><h3 id="page-matter"><a href="#page-matter" class="headerlink" title="page matter"></a>page matter</h3><p>由 <code>hexo new page [页面名称]</code> 命令创建的页面中的front-matter都属于 page matter,</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">updated:</span><br><span class="line">type:</span><br><span class="line">comments:</span><br><span class="line">description:</span><br><span class="line">keywords:</span><br><span class="line">top_img:</span><br><span class="line">mathjax:</span><br><span class="line">katex:</span><br><span class="line">aside:</span><br><span class="line">aplayer:</span><br><span class="line">highlight_shrink:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/2.png" alt="2"></p><p><strong>post matter</strong></p><p>使用 <code>hexo new post [文章名称]</code> 创建的md文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">updated:</span><br><span class="line">tags:</span><br><span class="line">categories:</span><br><span class="line">keywords:</span><br><span class="line">description:</span><br><span class="line">top_img:</span><br><span class="line">comments:</span><br><span class="line">cover:</span><br><span class="line">toc:</span><br><span class="line">toc_number:</span><br><span class="line">toc_style_simple:</span><br><span class="line">copyright:</span><br><span class="line">copyright_author:</span><br><span class="line">copyright_author_href:</span><br><span class="line">copyright_url:</span><br><span class="line">copyright_info:</span><br><span class="line">mathjax:</span><br><span class="line">katex:</span><br><span class="line">aplayer:</span><br><span class="line">highlight_shrink:</span><br><span class="line">aside:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p><img src="https://picbed.dai2yutou.space/article_img/3.png" alt="3"></p>              </div>            </details><h2 id="3、找图片"><a href="#3、找图片" class="headerlink" title="3、找图片"></a>3、找图片</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p>下面这篇博客的图片就很美嘿嘿😍</p><p><a href="https://www.fomal.cc/box/gallery/">一些好看的图片 | Fomalhaut🥝</a></p>              </div>            </details><h2 id="4、博客中添加自定义html页面"><a href="#4、博客中添加自定义html页面" class="headerlink" title="4、博客中添加自定义html页面"></a>4、博客中添加自定义html页面</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <ol><li><p><strong>在博客的根目录的<code>source</code>文件夹下，新建一个文件夹用来存放要部署的<code>html</code>文件</strong></p></li><li><p><strong>然后在博客根目录的配置文件<code>_config.yml</code>文件里，设置跳过渲染：</strong></p><ul><li><p>单个文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 跳过渲染</span><br><span class="line">skip_render: </span><br><span class="line">  - &quot;xxxx.html&quot;</span><br></pre></td></tr></table></figure></li><li><p>如果只创建了一个文件夹，要跳过它目录下所有文件的渲染</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 跳过文件夹下所有文件</span><br><span class="line">skip_render: </span><br><span class="line">  - &quot;文件夹名/*&quot;</span><br></pre></td></tr></table></figure></li><li><p>如果父文件夹下还有子文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 跳过子文件夹</span><br><span class="line">skip_render: </span><br><span class="line">  - &quot;文件夹名/子文件夹名/*&quot;</span><br></pre></td></tr></table></figure></li><li><p>或更简单粗暴的方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 跳过文件夹下所有子文件夹和文件</span><br><span class="line">skip_render: </span><br><span class="line">  - &quot;文件夹名/**&quot;   </span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>如何引用此页面展示</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在导航栏中引用方法:</span><br><span class="line">直接添加html文件的路径即可</span><br><span class="line">🔐项目: /HTML/新年倒计时/index.html</span><br></pre></td></tr></table></figure></li><li><p><strong>单个特殊文件添加不应用主题模板渲染的标记</strong></p><p>例如我们的网站如果要使用百度统计，往往需要在根目录放一个html格式的验证文件，这个文件默认也会经过用主题模板渲染，避免渲染的办法就是在文件头部添加如下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">layout: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li></ol>              </div>            </details><h2 id="5、Hexo中使用emoji表情"><a href="#5、Hexo中使用emoji表情" class="headerlink" title="5、Hexo中使用emoji表情"></a>5、Hexo中使用emoji表情</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p>在博客写文章时不免用到表情，因为我经常是用markdown写好后，粘贴复制到这里的，那么博客就需要支持markdown中输入的表情，本博客对此已进行魔改，<br>当使用到表情时，可到如下大佬链接查询：</p><div class="tag link"><a class="link-card" title="银河小徐" href="https://blog.xaoxu.cn/archives/hexo-use-emoji"><div class="left"><img src="https://blog-cos.xaoxu.cn/images/site-img/avatar.png"/></div><div class="right"><p class="text">银河小徐</p><p class="url">https://blog.xaoxu.cn/archives/hexo-use-emoji</p></div></a></div><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p>输入对应的emoji编码就行了<br>例如：输入笑脸对应的 emoji 编码 :smile: 就可以得到 😄</p>              </div>            </details><h2 id="6、文章置顶滚动栏的文章添加"><a href="#6、文章置顶滚动栏的文章添加" class="headerlink" title="6、文章置顶滚动栏的文章添加"></a>6、文章置顶滚动栏的文章添加</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p>在文章的front_matter中添加swiper_index配置项即可。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章标题</span><br><span class="line">date: 创建日期</span><br><span class="line">updated: 更新日期</span><br><span class="line">cover: 文章封面</span><br><span class="line">description: 文章描述</span><br><span class="line"><span class="section">swiper<span class="emphasis">_index: 1 #置顶轮播图顺序，非负整数，数字越大越靠前</span></span></span><br><span class="line"><span class="emphasis"><span class="section">---</span></span></span><br></pre></td></tr></table></figure>              </div>            </details><h2 id="7、文章页局部-html-代码不渲染"><a href="#7、文章页局部-html-代码不渲染" class="headerlink" title="7、文章页局部 html 代码不渲染"></a>7、文章页局部 html 代码不渲染</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p>在你的 md 文章页中，部分内容不想经过 Hexo 渲染，则包一层 raw 标签：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;&quot;</span>&gt;</span></span>你的一些代码...<span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span>你的一些代码...<span class="language-xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure><p>这样标签内的代码就不会被框架渲染了~</p>              </div>            </details><h1 id="snowman-GitHub"><a href="#snowman-GitHub" class="headerlink" title=":snowman:GitHub"></a>:snowman:GitHub</h1><h2 id="1-图床文件推送到远程仓库"><a href="#1-图床文件推送到远程仓库" class="headerlink" title="1.图床文件推送到远程仓库"></a>1.图床文件推送到远程仓库</h2><details class="folding-tag" blue><summary> 点击查看 </summary>              <div class='content'>              <p><a href="https://www.fomal.cc/posts/d7fb1ba1.html">如何建立基于Github的仓库的图床？？？</a><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 将更改提交</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;更新图片&quot;</span><br><span class="line"># 推送至github仓库</span><br><span class="line">git push</span><br></pre></td></tr></table></figure></p><p>已经将图床部署到<code>Vercel</code>总，当在引用图床中的图片时，只需要<code>https://picbed.dai2yutou.space/图片所在文件夹/图片</code>即可。</p><p>例如：<a href="https://picbed.dai2yutou.space/web_img/web_background1.jpg">https://picbed.dai2yutou.space/web_img/web_background1.jpg</a></p>              </div>            </details>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇文章</title>
      <link href="/2022/12/09/hello-world/"/>
      <url>/2022/12/09/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第一篇文章"><a href="#这是我的第一篇文章" class="headerlink" title="这是我的第一篇文章"></a>这是我的第一篇文章</h2><p>本文章是用来测试的，顺便纪念一下第一次基于hexo搭建的博客吧！</p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/card_author.css"/>
      <url>/css/card_author.css</url>
      
        <content type="html"><![CDATA[/************************************************* 个人资料卡片 *************************************************/#aside-content>.card-widget:first-child {  background: #fff url(https://image.anzhiy.cn/adminuploads/1/2022/10/26/6358a07bf21fc.webp) top -1% center no-repeat !important;  background-size: 100% 64% !important;  position: relative;}[data-theme=dark] #aside-content>.card-widget:first-child {  background: #121212 !important}#aside-content>.card-widget.card-info>#card-info-btn {  border-radius: 10px;}.card-widget .author-info-top {  margin: 15px auto 0;  display: flex;  justify-content: center;}.card-widget .card-info-avatar {  display: inline-block;  position: relative;}.card-info-avatar .avatar-img {  width: 100px;  height: 100px;  display: block;}.avatar-img img {  border: 4px solid #f7f9fe;  border-radius: 50%;}.card-info-avatar .author-status-box {  position: absolute;  bottom: 0;  left: calc(100% - 28px);  width: 28px;  height: 28px;  border: 1px solid #d0d7de;  border-radius: 2em;  background-color: #fff;  transition: .3s;  overflow: hidden;}.card-info-avatar .author-status-box .author-status {  display: flex;  align-items: center;  justify-content: center;  height: 28px;  padding: 0 5px;}.card-info-avatar .author-status-box:hover {  width: 105px}.card-info-avatar .author-status-box:hover .author-status span {  width: 105px;  margin-left: 4px;}.card-info-avatar .author-status-box .author-status span {  width: 0;  font-size: 12px;  height: 100%;  overflow: hidden;  text-overflow: ellipsis;  white-space: nowrap;  transition: .3s;}div#author-info__sayhi {  text-align: left;  background: rgba(255, 255, 255, 0.2);  color: var(--font-color);  border: 1px solid #e3e8f7;  font-size: 12px;  margin-right: auto;  padding: 5px 8px;  border-radius: 12px;  width: fit-content;  display: inline;  margin-left: -1px;}.card-widget .author-info__name {  line-height: 1.5em;  margin: 10px 0 4px;}#aside-content .card-info .author-info__description {  margin-top: -.42em;}.card-widget.card-info .banner-button-group {  margin: 15px 0 0;  display: flex;}#aside-content .card-info .banner-button {  height: 40px;  width: 100%;  margin-right: unset !important;  border-radius: 20px;  justify-content: center;}.card-widget.card-info .banner-button-group .banner-button {  padding: 20px 12px;  background: #3b70fc;  border-radius: 12px;  color: #fff;  display: flex;  align-items: center;  z-index: 1;  transition: all .3s ease 0s;  cursor: pointer;}#aside-content>div.animate__fadeIn.card-info.card-widget.wow>h1 {  font-weight: bold;  font-size: 1.57em;}#aside-content>div.animate__fadeIn.card-info.card-widget.wow>div.banner-button-group>a>span {  color: #fff;  font-weight: unset;}.card-widget.card-info .banner-button-group .banner-button .fa-solid.fa-circle-chevron-right {  font-size: 1.3rem;  margin-right: 10px;}.card-widget.card-info .banner-button-group .banner-button:hover {  background: #1856fb;  color: #fff;}@media screen and (max-width: 900px) {  #aside-content>.card-widget:first-child {    margin-top: 20px;  }}/************************************************* 个人资料卡片 *************************************************/]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/color.css"/>
      <url>/css/color.css</url>
      
        <content type="html"><![CDATA[/* 颜色 */:root {    --xiaoyutou-theme-op: #4259ef23;    --xiaoyutou-white: #fff;    --xiaoyutou-black: #000;    --xiaoyutou-none: rgba(0, 0, 0, 0);    --xiaoyutou-gray: #999999;    --xiaoyutou-yellow: #ffc93e;    --xiaoyutou-border-radius: 8px;    --xiaoyutou-main: var(--xiaoyutou-theme);    --xiaoyutou-main-op: var(--xiaoyutou-theme-op);    --xiaoyutou-shadow-theme: 0 8px 12px -3px var(--xiaoyutou-theme-op);    --xiaoyutou-shadow-main: 0 8px 12px -3px var(--xiaoyutou-main-op);    --xiaoyutou-shadow-blue: 0 8px 12px -3px rgba(40, 109, 234, 0.2);    --xiaoyutou-shadow-white: 0 8px 12px -3px rgba(255, 255, 255, 0.2);    --xiaoyutou-shadow-black: 0 0 12px 4px rgba(0, 0, 0, 0.05);    --xiaoyutou-shadow-yellow: 0px 38px 77px -26px rgba(255, 201, 62, 0.12);    --xiaoyutou-shadow-red: 0 8px 12px -3px #ee7d7936;    --xiaoyutou-shadow-green: 0 8px 12px -3px #87ee7936;    --xiaoyutou-shadow-border: 0 8px 16px -4px #2c2d300c;    --xiaoyutou-logo-color: linear-gradient(215deg, #4584ff 30%, #ff7676 70%);    --style-border: 1px solid var(--xiaoyutou-card-border);    --xiaoyutou-blue-main: #3b70fc;    --style-border-hover: 1px solid var(--xiaoyutou-main);    --style-border-dashed: 1px dashed var(--xiaoyutou-theme-op);    --style-border-avatar: 4px solid var(--xiaoyutou-background);    --style-border-always: 1px solid var(--xiaoyutou-card-border);    --xiaoyutou-white-acrylic1: #fefeff !important;    --xiaoyutou-white-acrylic2: #fcfdff !important;    --xiaoyutou-black-acrylic2: #08080a !important;    --xiaoyutou-black-acrylic1: #0b0b0e !important;  }    [data-theme="light"] {    --xiaoyutou-theme: #3b70fc;    --xiaoyutou-theme-op: #4259ef23;    --xiaoyutou-blue: #3b70fc;    --xiaoyutou-red: #d8213c;    --xiaoyutou-pink: #ff7c7c;    --xiaoyutou-green: #57bd6a;    --xiaoyutou-fontcolor: #363636;    --xiaoyutou-background: #f7f9fe;    --xiaoyutou-reverse: #000;    --xiaoyutou-maskbg: rgba(255, 255, 255, 0.6);    --xiaoyutou-maskbgdeep: rgba(255, 255, 255, 0.85);    --xiaoyutou-hovertext: var(--xiaoyutou-theme);    --xiaoyutou-ahoverbg: #f7f7fa;    --xiaoyutou-lighttext: var(--xiaoyutou-main);    --xiaoyutou-secondtext: rgba(60, 60, 67, 0.6);    --xiaoyutou-scrollbar: rgba(60, 60, 67, 0.4);    --xiaoyutou-card-btn-bg: #edf0f7;    --xiaoyutou-post-blockquote-bg: #fafcff;    --xiaoyutou-post-tabs-bg: #f2f5f8;    --xiaoyutou-secondbg: #edf0f7;    --xiaoyutou-shadow-nav: 0 5px 12px -5px rgba(102, 68, 68, 0.05);    --xiaoyutou-card-bg: #fff;    --xiaoyutou-shadow-lightblack: 0 5px 12px -5px rgba(102, 68, 68, 0);    --xiaoyutou-shadow-light2black: 0 5px 12px -5px rgba(102, 68, 68, 0);    --xiaoyutou-card-border: #c0c6d8;  }    [data-theme="dark"] {    --xiaoyutou-theme: #3eb8be;    --xiaoyutou-theme-op: #0084ff23;    --xiaoyutou-blue: #0084ff;    --xiaoyutou-red: #ff3842;    --xiaoyutou-pink: #ff7c7c;    --xiaoyutou-green: #57bd6a;    --xiaoyutou-fontcolor: #f7f7fa;    --xiaoyutou-background: #18171d;    --xiaoyutou-reverse: #fff;    --xiaoyutou-maskbg: rgba(0, 0, 0, 0.6);    --xiaoyutou-maskbgdeep: rgba(0, 0, 0, 0.85);    --xiaoyutou-hovertext: #0a84ff;    --xiaoyutou-ahoverbg: #fff;    --xiaoyutou-lighttext: #f2b94b;    --xiaoyutou-secondtext: #a1a2b8;    --xiaoyutou-scrollbar: rgba(200, 200, 223, 0.4);    --xiaoyutou-card-btn-bg: #30343f;    --xiaoyutou-post-blockquote-bg: #000;    --xiaoyutou-post-tabs-bg: #121212;    --xiaoyutou-secondbg: #30343f;    --xiaoyutou-shadow-nav: 0 5px 20px 0px rgba(28, 28, 28, 0.4);    --xiaoyutou-card-bg: #1d1b26;    --xiaoyutou-shadow-lightblack: 0 5px 12px -5px rgba(102, 68, 68, 0);    --xiaoyutou-shadow-light2black: 0 5px 12px -5px rgba(102, 68, 68, 0);    --xiaoyutou-card-border: #42444a;  }]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/css.css"/>
      <url>/css/css.css</url>
      
        <content type="html"><![CDATA[/* _config.butterfly.yml中的inject中引用 *//*    1.此部分修改的是页面的页头和页脚显示透明，更美观*//* 页脚与头图透明 */#footer {    background: transparent !important;}#page-header {  background: transparent !important;}/* 白天模式遮罩透明 */#footer::before {background: transparent !important;}#page-header::before {background: transparent !important;}/* 夜间模式遮罩透明 */[data-theme="dark"] #footer::before {background: transparent !important;}[data-theme="dark"] #page-header::before {background: transparent !important;}/*    2.此部分代码是对首先分类磁贴1.0在夜间模式下的外观的修改*//* 小冰分类分类磁铁黑夜模式适配 *//* 一般状态 */[data-theme="dark"] .magnet_link_context {    background: #1e1e1e;    color: antiquewhite;}/* 鼠标悬浮状态 */[data-theme="dark"] .magnet_link_context:hover {    background: #3ecdf1;    color: #f2f2f2;}/*    3.设置字体*/@font-face {    font-family: "仓耳渔阳体 W05";font-weight: 400;src: url("//at.alicdn.com/wf/webfont/SE6Q2VLzYKil/roZwi8MDONv1PXqk_XQPA.woff2") format("woff2"),    url("//at.alicdn.com/wf/webfont/SE6Q2VLzYKil/gLjn4lJ4jMiKoKfSC3atz.woff") format("woff");    font-display: swap;}/*    4.导航栏的魔改*//* 一级菜单居中 */#nav .menus_items {    position: absolute !important;    width: fit-content !important;    left: 50% !important;    transform: translateX(-50%) !important;  }  /* 子菜单横向展示 */  #nav .menus_items .menus_item:hover .menus_item_child {    display: flex !important;  }  /* 调整子菜单栏的菜单的居中，这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */  .menus_items .menus_item:nth-child(2) .menus_item_child {    left: -80px;  }/*  5.此部分代码作用为夜间模式菜单栏的小字的发光效果、    颜色可以自己指定，在下面rgb处修改即可*/[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px rgb(179, 71, 241) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px rgb(179, 71, 241) !important;}/*  6.此部分代码为星空的css代码*//* 背景宇宙星光  */#universe{  display: block;  position: fixed;  margin: 0;  padding: 0;  border: 0;  outline: 0;  left: 0;  top: 0;  width: 100%;  height: 100%;  pointer-events: none;  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */  z-index: -1;}/*   7.侧边栏个人信息卡片动态渐变色*/#aside-content > .card-widget.card-info {  background: linear-gradient(    -45deg,    #e8d8b9,    #eccec5,    #a3e9eb,    #bdbdf0,    #eec1ea  );  box-shadow: 0 0 5px rgb(66, 68, 68);  position: relative;  background-size: 400% 400%;  -webkit-animation: Gradient 10s ease infinite;  -moz-animation: Gradient 10s ease infinite;  animation: Gradient 10s ease infinite !important;}@-webkit-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@-moz-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}/* 黑夜模式适配 */[data-theme="dark"] #aside-content > .card-widget.card-info {  background: #191919ee;}/* 个人信息Follow me按钮 */#aside-content > .card-widget.card-info > #card-info-btn {  background-color: #3eb8be;  border-radius: 8px;}/*\   8.博客宽屏适配*//* 全局宽度 */.layout {  max-width: 1400px !important;}/* 侧边卡片栏宽度 */.aside-content {  max-width: 318px;  min-width: 300px;}/* 平板尺寸自适应(不启用侧边栏宽度限制) */@media screen and (max-width: 900px) {  .aside-content {    max-width: none !important;    padding: 0 5px 0 5px;  }}/*   9.个人信息卡片背景图 *//* [data-theme="light"] #aside-content > .card-widget.card-info {  background-image: url(https://picbed.dai2yutou.space/web_img/6.png);  background-repeat: no-repeat;  background-attachment: inherit;  background-size: 100%;}[data-theme="dark"] #aside-content > .card-widget.card-info {  background-image: url(https://picbed.dai2yutou.space/web_img/6.png);  background-repeat: no-repeat;  background-attachment: inherit;  background-size: 100%;} *//*   10.页面样式调节 */:root {  --trans-light: rgba(255, 255, 255, 1);  --trans-dark: rgba(25, 25, 25, 1);  --border-style: 1px solid black;  /* --backdrop-filter: blur(5px) saturate(150%); */}/* 首页文章卡片 */#recent-posts > .recent-post-item {  background: var(--trans-light);  /* backdrop-filter: var(--backdrop-filter); */  border-radius: 25px;  border: var(--border-style);}/* 首页侧栏卡片 */#aside-content .card-widget {  background: var(--trans-light);  /* backdrop-filter: var(--backdrop-filter); */  border-radius: 18px;  border: var(--border-style);}/* 文章页、归档页、普通页面 */div#post,div#page,div#archive {  background: var(--trans-light);  /* backdrop-filter: var(--backdrop-filter); */  border: var(--border-style);  border-radius: 20px;}/* 导航栏 */#page-header.nav-fixed #nav {  background: rgba(255, 255, 255, 0.75);  /* backdrop-filter: var(--backdrop-filter); */}[data-theme="dark"] #page-header.nav-fixed #nav {  background: rgba(0, 0, 0, 0.7) !important;}/* 夜间模式遮罩 */[data-theme="dark"] #recent-posts > .recent-post-item,[data-theme="dark"] #aside-content .card-widget,[data-theme="dark"] div#post,[data-theme="dark"] div#archive,[data-theme="dark"] div#page {  background: var(--trans-dark);}/* 夜间模式页脚页头遮罩透明 */[data-theme="dark"] #footer::before {  background: transparent !important;}[data-theme="dark"] #page-header::before {  background: transparent !important;}/* 阅读模式 */.read-mode #aside-content .card-widget {  background: rgba(158, 204, 171, 0.5) !important;}.read-mode div#post {  background: rgba(158, 204, 171, 0.5) !important;}/* 夜间模式下的阅读模式 */[data-theme="dark"] .read-mode #aside-content .card-widget {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}[data-theme="dark"] .read-mode div#post {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}/* 更改首页轮播图样式 */@media screen and (min-width:600px) {  .blog-slider {      height: 250px !important;  }  .blog-slider__img {      height: 240px !important;      width: 240px !important;  }}/*  定义加载动画头像*/.loading-img {  background: url(https://www.dai2yutou.space/img/avatar.jpg) no-repeat center center;  background-size: cover;}/* 侧边栏卡片公告栏欢迎信息 */#welcome-info {  background-color: #3eb8be;  border-radius: 18px;  padding: 8px;}[data-theme="dark"] #welcome-info {  background: #212121;}/* 标签页面 */#aside-content .card-tag-cloud a {  border: 1px solid;  line-height: 1.5;  border-radius: 6px;  margin: 3px;  padding: 0 5px;}.tag-cloud-list a {  border: 1px solid;  line-height: 1.5;  border-radius: 6px;  padding: 5px 15px;  font-size: 1.2rem;  margin: 5px;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/font-awesome.css"/>
      <url>/css/font-awesome.css</url>
      
        <content type="html"><![CDATA[/*! *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License) *//* FONT PATH * -------------------------- */@font-face {  font-family: 'FontAwesome';  src: url('../fonts/fontawesome-webfont.eot?v=4.7.0');  src: url('../fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');  font-weight: normal;  font-style: normal;}.fa {  display: inline-block;  font: normal normal normal 14px/1 FontAwesome;  font-size: inherit;  text-rendering: auto;  -webkit-font-smoothing: antialiased;  -moz-osx-font-smoothing: grayscale;}/* makes the font 33% larger relative to the icon container */.fa-lg {  font-size: 1.33333333em;  line-height: 0.75em;  vertical-align: -15%;}.fa-2x {  font-size: 2em;}.fa-3x {  font-size: 3em;}.fa-4x {  font-size: 4em;}.fa-5x {  font-size: 5em;}.fa-fw {  width: 1.28571429em;  text-align: center;}.fa-ul {  padding-left: 0;  margin-left: 2.14285714em;  list-style-type: none;}.fa-ul > li {  position: relative;}.fa-li {  position: absolute;  left: -2.14285714em;  width: 2.14285714em;  top: 0.14285714em;  text-align: center;}.fa-li.fa-lg {  left: -1.85714286em;}.fa-border {  padding: .2em .25em .15em;  border: solid 0.08em #eeeeee;  border-radius: .1em;}.fa-pull-left {  float: left;}.fa-pull-right {  float: right;}.fa.fa-pull-left {  margin-right: .3em;}.fa.fa-pull-right {  margin-left: .3em;}/* Deprecated as of 4.4.0 */.pull-right {  float: right;}.pull-left {  float: left;}.fa.pull-left {  margin-right: .3em;}.fa.pull-right {  margin-left: .3em;}.fa-spin {  -webkit-animation: fa-spin 2s infinite linear;  animation: fa-spin 2s infinite linear;}.fa-pulse {  -webkit-animation: fa-spin 1s infinite steps(8);  animation: fa-spin 1s infinite steps(8);}@-webkit-keyframes fa-spin {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  100% {    -webkit-transform: rotate(359deg);    transform: rotate(359deg);  }}@keyframes fa-spin {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  100% {    -webkit-transform: rotate(359deg);    transform: rotate(359deg);  }}.fa-rotate-90 {  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";  -webkit-transform: rotate(90deg);  -ms-transform: rotate(90deg);  transform: rotate(90deg);}.fa-rotate-180 {  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";  -webkit-transform: rotate(180deg);  -ms-transform: rotate(180deg);  transform: rotate(180deg);}.fa-rotate-270 {  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";  -webkit-transform: rotate(270deg);  -ms-transform: rotate(270deg);  transform: rotate(270deg);}.fa-flip-horizontal {  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";  -webkit-transform: scale(-1, 1);  -ms-transform: scale(-1, 1);  transform: scale(-1, 1);}.fa-flip-vertical {  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";  -webkit-transform: scale(1, -1);  -ms-transform: scale(1, -1);  transform: scale(1, -1);}:root .fa-rotate-90,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-flip-horizontal,:root .fa-flip-vertical {  filter: none;}.fa-stack {  position: relative;  display: inline-block;  width: 2em;  height: 2em;  line-height: 2em;  vertical-align: middle;}.fa-stack-1x,.fa-stack-2x {  position: absolute;  left: 0;  width: 100%;  text-align: center;}.fa-stack-1x {  line-height: inherit;}.fa-stack-2x {  font-size: 2em;}.fa-inverse {  color: #ffffff;}/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen   readers do not read off random characters that represent icons */.fa-glass:before {  content: "\f000";}.fa-music:before {  content: "\f001";}.fa-search:before {  content: "\f002";}.fa-envelope-o:before {  content: "\f003";}.fa-heart:before {  content: "\f004";}.fa-star:before {  content: "\f005";}.fa-star-o:before {  content: "\f006";}.fa-user:before {  content: "\f007";}.fa-film:before {  content: "\f008";}.fa-th-large:before {  content: "\f009";}.fa-th:before {  content: "\f00a";}.fa-th-list:before {  content: "\f00b";}.fa-check:before {  content: "\f00c";}.fa-remove:before,.fa-close:before,.fa-times:before {  content: "\f00d";}.fa-search-plus:before {  content: "\f00e";}.fa-search-minus:before {  content: "\f010";}.fa-power-off:before {  content: "\f011";}.fa-signal:before {  content: "\f012";}.fa-gear:before,.fa-cog:before {  content: "\f013";}.fa-trash-o:before {  content: "\f014";}.fa-home:before {  content: "\f015";}.fa-file-o:before {  content: "\f016";}.fa-clock-o:before {  content: "\f017";}.fa-road:before {  content: "\f018";}.fa-download:before {  content: "\f019";}.fa-arrow-circle-o-down:before {  content: "\f01a";}.fa-arrow-circle-o-up:before {  content: "\f01b";}.fa-inbox:before {  content: "\f01c";}.fa-play-circle-o:before {  content: "\f01d";}.fa-rotate-right:before,.fa-repeat:before {  content: "\f01e";}.fa-refresh:before {  content: "\f021";}.fa-list-alt:before {  content: "\f022";}.fa-lock:before {  content: "\f023";}.fa-flag:before {  content: "\f024";}.fa-headphones:before {  content: "\f025";}.fa-volume-off:before {  content: "\f026";}.fa-volume-down:before {  content: "\f027";}.fa-volume-up:before {  content: "\f028";}.fa-qrcode:before {  content: "\f029";}.fa-barcode:before {  content: "\f02a";}.fa-tag:before {  content: "\f02b";}.fa-tags:before {  content: "\f02c";}.fa-book:before {  content: "\f02d";}.fa-bookmark:before {  content: "\f02e";}.fa-print:before {  content: "\f02f";}.fa-camera:before {  content: "\f030";}.fa-font:before {  content: "\f031";}.fa-bold:before {  content: "\f032";}.fa-italic:before {  content: "\f033";}.fa-text-height:before {  content: "\f034";}.fa-text-width:before {  content: "\f035";}.fa-align-left:before {  content: "\f036";}.fa-align-center:before {  content: "\f037";}.fa-align-right:before {  content: "\f038";}.fa-align-justify:before {  content: "\f039";}.fa-list:before {  content: "\f03a";}.fa-dedent:before,.fa-outdent:before {  content: "\f03b";}.fa-indent:before {  content: "\f03c";}.fa-video-camera:before {  content: "\f03d";}.fa-photo:before,.fa-image:before,.fa-picture-o:before {  content: "\f03e";}.fa-pencil:before {  content: "\f040";}.fa-map-marker:before {  content: "\f041";}.fa-adjust:before {  content: "\f042";}.fa-tint:before {  content: "\f043";}.fa-edit:before,.fa-pencil-square-o:before {  content: "\f044";}.fa-share-square-o:before {  content: "\f045";}.fa-check-square-o:before {  content: "\f046";}.fa-arrows:before {  content: "\f047";}.fa-step-backward:before {  content: "\f048";}.fa-fast-backward:before {  content: "\f049";}.fa-backward:before {  content: "\f04a";}.fa-play:before {  content: "\f04b";}.fa-pause:before {  content: "\f04c";}.fa-stop:before {  content: "\f04d";}.fa-forward:before {  content: "\f04e";}.fa-fast-forward:before {  content: "\f050";}.fa-step-forward:before {  content: "\f051";}.fa-eject:before {  content: "\f052";}.fa-chevron-left:before {  content: "\f053";}.fa-chevron-right:before {  content: "\f054";}.fa-plus-circle:before {  content: "\f055";}.fa-minus-circle:before {  content: "\f056";}.fa-times-circle:before {  content: "\f057";}.fa-check-circle:before {  content: "\f058";}.fa-question-circle:before {  content: "\f059";}.fa-info-circle:before {  content: "\f05a";}.fa-crosshairs:before {  content: "\f05b";}.fa-times-circle-o:before {  content: "\f05c";}.fa-check-circle-o:before {  content: "\f05d";}.fa-ban:before {  content: "\f05e";}.fa-arrow-left:before {  content: "\f060";}.fa-arrow-right:before {  content: "\f061";}.fa-arrow-up:before {  content: "\f062";}.fa-arrow-down:before {  content: "\f063";}.fa-mail-forward:before,.fa-share:before {  content: "\f064";}.fa-expand:before {  content: "\f065";}.fa-compress:before {  content: "\f066";}.fa-plus:before {  content: "\f067";}.fa-minus:before {  content: "\f068";}.fa-asterisk:before {  content: "\f069";}.fa-exclamation-circle:before {  content: "\f06a";}.fa-gift:before {  content: "\f06b";}.fa-leaf:before {  content: "\f06c";}.fa-fire:before {  content: "\f06d";}.fa-eye:before {  content: "\f06e";}.fa-eye-slash:before {  content: "\f070";}.fa-warning:before,.fa-exclamation-triangle:before {  content: "\f071";}.fa-plane:before {  content: "\f072";}.fa-calendar:before {  content: "\f073";}.fa-random:before {  content: "\f074";}.fa-comment:before {  content: "\f075";}.fa-magnet:before {  content: "\f076";}.fa-chevron-up:before {  content: "\f077";}.fa-chevron-down:before {  content: "\f078";}.fa-retweet:before {  content: "\f079";}.fa-shopping-cart:before {  content: "\f07a";}.fa-folder:before {  content: "\f07b";}.fa-folder-open:before {  content: "\f07c";}.fa-arrows-v:before {  content: "\f07d";}.fa-arrows-h:before {  content: "\f07e";}.fa-bar-chart-o:before,.fa-bar-chart:before {  content: "\f080";}.fa-twitter-square:before {  content: "\f081";}.fa-facebook-square:before {  content: "\f082";}.fa-camera-retro:before {  content: "\f083";}.fa-key:before {  content: "\f084";}.fa-gears:before,.fa-cogs:before {  content: "\f085";}.fa-comments:before {  content: "\f086";}.fa-thumbs-o-up:before {  content: "\f087";}.fa-thumbs-o-down:before {  content: "\f088";}.fa-star-half:before {  content: "\f089";}.fa-heart-o:before {  content: "\f08a";}.fa-sign-out:before {  content: "\f08b";}.fa-linkedin-square:before {  content: "\f08c";}.fa-thumb-tack:before {  content: "\f08d";}.fa-external-link:before {  content: "\f08e";}.fa-sign-in:before {  content: "\f090";}.fa-trophy:before {  content: "\f091";}.fa-github-square:before {  content: "\f092";}.fa-upload:before {  content: "\f093";}.fa-lemon-o:before {  content: "\f094";}.fa-phone:before {  content: "\f095";}.fa-square-o:before {  content: "\f096";}.fa-bookmark-o:before {  content: "\f097";}.fa-phone-square:before {  content: "\f098";}.fa-twitter:before {  content: "\f099";}.fa-facebook-f:before,.fa-facebook:before {  content: "\f09a";}.fa-github:before {  content: "\f09b";}.fa-unlock:before {  content: "\f09c";}.fa-credit-card:before {  content: "\f09d";}.fa-feed:before,.fa-rss:before {  content: "\f09e";}.fa-hdd-o:before {  content: "\f0a0";}.fa-bullhorn:before {  content: "\f0a1";}.fa-bell:before {  content: "\f0f3";}.fa-certificate:before {  content: "\f0a3";}.fa-hand-o-right:before {  content: "\f0a4";}.fa-hand-o-left:before {  content: "\f0a5";}.fa-hand-o-up:before {  content: "\f0a6";}.fa-hand-o-down:before {  content: "\f0a7";}.fa-arrow-circle-left:before {  content: "\f0a8";}.fa-arrow-circle-right:before {  content: "\f0a9";}.fa-arrow-circle-up:before {  content: "\f0aa";}.fa-arrow-circle-down:before {  content: "\f0ab";}.fa-globe:before {  content: "\f0ac";}.fa-wrench:before {  content: "\f0ad";}.fa-tasks:before {  content: "\f0ae";}.fa-filter:before {  content: "\f0b0";}.fa-briefcase:before {  content: "\f0b1";}.fa-arrows-alt:before {  content: "\f0b2";}.fa-group:before,.fa-users:before {  content: "\f0c0";}.fa-chain:before,.fa-link:before {  content: "\f0c1";}.fa-cloud:before {  content: "\f0c2";}.fa-flask:before {  content: "\f0c3";}.fa-cut:before,.fa-scissors:before {  content: "\f0c4";}.fa-copy:before,.fa-files-o:before {  content: "\f0c5";}.fa-paperclip:before {  content: "\f0c6";}.fa-save:before,.fa-floppy-o:before {  content: "\f0c7";}.fa-square:before {  content: "\f0c8";}.fa-navicon:before,.fa-reorder:before,.fa-bars:before {  content: "\f0c9";}.fa-list-ul:before {  content: "\f0ca";}.fa-list-ol:before {  content: "\f0cb";}.fa-strikethrough:before {  content: "\f0cc";}.fa-underline:before {  content: "\f0cd";}.fa-table:before {  content: "\f0ce";}.fa-magic:before {  content: "\f0d0";}.fa-truck:before {  content: "\f0d1";}.fa-pinterest:before {  content: "\f0d2";}.fa-pinterest-square:before {  content: "\f0d3";}.fa-google-plus-square:before {  content: "\f0d4";}.fa-google-plus:before {  content: "\f0d5";}.fa-money:before {  content: "\f0d6";}.fa-caret-down:before {  content: "\f0d7";}.fa-caret-up:before {  content: "\f0d8";}.fa-caret-left:before {  content: "\f0d9";}.fa-caret-right:before {  content: "\f0da";}.fa-columns:before {  content: "\f0db";}.fa-unsorted:before,.fa-sort:before {  content: "\f0dc";}.fa-sort-down:before,.fa-sort-desc:before {  content: "\f0dd";}.fa-sort-up:before,.fa-sort-asc:before {  content: "\f0de";}.fa-envelope:before {  content: "\f0e0";}.fa-linkedin:before {  content: "\f0e1";}.fa-rotate-left:before,.fa-undo:before {  content: "\f0e2";}.fa-legal:before,.fa-gavel:before {  content: "\f0e3";}.fa-dashboard:before,.fa-tachometer:before {  content: "\f0e4";}.fa-comment-o:before {  content: "\f0e5";}.fa-comments-o:before {  content: "\f0e6";}.fa-flash:before,.fa-bolt:before {  content: "\f0e7";}.fa-sitemap:before {  content: "\f0e8";}.fa-umbrella:before {  content: "\f0e9";}.fa-paste:before,.fa-clipboard:before {  content: "\f0ea";}.fa-lightbulb-o:before {  content: "\f0eb";}.fa-exchange:before {  content: "\f0ec";}.fa-cloud-download:before {  content: "\f0ed";}.fa-cloud-upload:before {  content: "\f0ee";}.fa-user-md:before {  content: "\f0f0";}.fa-stethoscope:before {  content: "\f0f1";}.fa-suitcase:before {  content: "\f0f2";}.fa-bell-o:before {  content: "\f0a2";}.fa-coffee:before {  content: "\f0f4";}.fa-cutlery:before {  content: "\f0f5";}.fa-file-text-o:before {  content: "\f0f6";}.fa-building-o:before {  content: "\f0f7";}.fa-hospital-o:before {  content: "\f0f8";}.fa-ambulance:before {  content: "\f0f9";}.fa-medkit:before {  content: "\f0fa";}.fa-fighter-jet:before {  content: "\f0fb";}.fa-beer:before {  content: "\f0fc";}.fa-h-square:before {  content: "\f0fd";}.fa-plus-square:before {  content: "\f0fe";}.fa-angle-double-left:before {  content: "\f100";}.fa-angle-double-right:before {  content: "\f101";}.fa-angle-double-up:before {  content: "\f102";}.fa-angle-double-down:before {  content: "\f103";}.fa-angle-left:before {  content: "\f104";}.fa-angle-right:before {  content: "\f105";}.fa-angle-up:before {  content: "\f106";}.fa-angle-down:before {  content: "\f107";}.fa-desktop:before {  content: "\f108";}.fa-laptop:before {  content: "\f109";}.fa-tablet:before {  content: "\f10a";}.fa-mobile-phone:before,.fa-mobile:before {  content: "\f10b";}.fa-circle-o:before {  content: "\f10c";}.fa-quote-left:before {  content: "\f10d";}.fa-quote-right:before {  content: "\f10e";}.fa-spinner:before {  content: "\f110";}.fa-circle:before {  content: "\f111";}.fa-mail-reply:before,.fa-reply:before {  content: "\f112";}.fa-github-alt:before {  content: "\f113";}.fa-folder-o:before {  content: "\f114";}.fa-folder-open-o:before {  content: "\f115";}.fa-smile-o:before {  content: "\f118";}.fa-frown-o:before {  content: "\f119";}.fa-meh-o:before {  content: "\f11a";}.fa-gamepad:before {  content: "\f11b";}.fa-keyboard-o:before {  content: "\f11c";}.fa-flag-o:before {  content: "\f11d";}.fa-flag-checkered:before {  content: "\f11e";}.fa-terminal:before {  content: "\f120";}.fa-code:before {  content: "\f121";}.fa-mail-reply-all:before,.fa-reply-all:before {  content: "\f122";}.fa-star-half-empty:before,.fa-star-half-full:before,.fa-star-half-o:before {  content: "\f123";}.fa-location-arrow:before {  content: "\f124";}.fa-crop:before {  content: "\f125";}.fa-code-fork:before {  content: "\f126";}.fa-unlink:before,.fa-chain-broken:before {  content: "\f127";}.fa-question:before {  content: "\f128";}.fa-info:before {  content: "\f129";}.fa-exclamation:before {  content: "\f12a";}.fa-superscript:before {  content: "\f12b";}.fa-subscript:before {  content: "\f12c";}.fa-eraser:before {  content: "\f12d";}.fa-puzzle-piece:before {  content: "\f12e";}.fa-microphone:before {  content: "\f130";}.fa-microphone-slash:before {  content: "\f131";}.fa-shield:before {  content: "\f132";}.fa-calendar-o:before {  content: "\f133";}.fa-fire-extinguisher:before {  content: "\f134";}.fa-rocket:before {  content: "\f135";}.fa-maxcdn:before {  content: "\f136";}.fa-chevron-circle-left:before {  content: "\f137";}.fa-chevron-circle-right:before {  content: "\f138";}.fa-chevron-circle-up:before {  content: "\f139";}.fa-chevron-circle-down:before {  content: "\f13a";}.fa-html5:before {  content: "\f13b";}.fa-css3:before {  content: "\f13c";}.fa-anchor:before {  content: "\f13d";}.fa-unlock-alt:before {  content: "\f13e";}.fa-bullseye:before {  content: "\f140";}.fa-ellipsis-h:before {  content: "\f141";}.fa-ellipsis-v:before {  content: "\f142";}.fa-rss-square:before {  content: "\f143";}.fa-play-circle:before {  content: "\f144";}.fa-ticket:before {  content: "\f145";}.fa-minus-square:before {  content: "\f146";}.fa-minus-square-o:before {  content: "\f147";}.fa-level-up:before {  content: "\f148";}.fa-level-down:before {  content: "\f149";}.fa-check-square:before {  content: "\f14a";}.fa-pencil-square:before {  content: "\f14b";}.fa-external-link-square:before {  content: "\f14c";}.fa-share-square:before {  content: "\f14d";}.fa-compass:before {  content: "\f14e";}.fa-toggle-down:before,.fa-caret-square-o-down:before {  content: "\f150";}.fa-toggle-up:before,.fa-caret-square-o-up:before {  content: "\f151";}.fa-toggle-right:before,.fa-caret-square-o-right:before {  content: "\f152";}.fa-euro:before,.fa-eur:before {  content: "\f153";}.fa-gbp:before {  content: "\f154";}.fa-dollar:before,.fa-usd:before {  content: "\f155";}.fa-rupee:before,.fa-inr:before {  content: "\f156";}.fa-cny:before,.fa-rmb:before,.fa-yen:before,.fa-jpy:before {  content: "\f157";}.fa-ruble:before,.fa-rouble:before,.fa-rub:before {  content: "\f158";}.fa-won:before,.fa-krw:before {  content: "\f159";}.fa-bitcoin:before,.fa-btc:before {  content: "\f15a";}.fa-file:before {  content: "\f15b";}.fa-file-text:before {  content: "\f15c";}.fa-sort-alpha-asc:before {  content: "\f15d";}.fa-sort-alpha-desc:before {  content: "\f15e";}.fa-sort-amount-asc:before {  content: "\f160";}.fa-sort-amount-desc:before {  content: "\f161";}.fa-sort-numeric-asc:before {  content: "\f162";}.fa-sort-numeric-desc:before {  content: "\f163";}.fa-thumbs-up:before {  content: "\f164";}.fa-thumbs-down:before {  content: "\f165";}.fa-youtube-square:before {  content: "\f166";}.fa-youtube:before {  content: "\f167";}.fa-xing:before {  content: "\f168";}.fa-xing-square:before {  content: "\f169";}.fa-youtube-play:before {  content: "\f16a";}.fa-dropbox:before {  content: "\f16b";}.fa-stack-overflow:before {  content: "\f16c";}.fa-instagram:before {  content: "\f16d";}.fa-flickr:before {  content: "\f16e";}.fa-adn:before {  content: "\f170";}.fa-bitbucket:before {  content: "\f171";}.fa-bitbucket-square:before {  content: "\f172";}.fa-tumblr:before {  content: "\f173";}.fa-tumblr-square:before {  content: "\f174";}.fa-long-arrow-down:before {  content: "\f175";}.fa-long-arrow-up:before {  content: "\f176";}.fa-long-arrow-left:before {  content: "\f177";}.fa-long-arrow-right:before {  content: "\f178";}.fa-apple:before {  content: "\f179";}.fa-windows:before {  content: "\f17a";}.fa-android:before {  content: "\f17b";}.fa-linux:before {  content: "\f17c";}.fa-dribbble:before {  content: "\f17d";}.fa-skype:before {  content: "\f17e";}.fa-foursquare:before {  content: "\f180";}.fa-trello:before {  content: "\f181";}.fa-female:before {  content: "\f182";}.fa-male:before {  content: "\f183";}.fa-gittip:before,.fa-gratipay:before {  content: "\f184";}.fa-sun-o:before {  content: "\f185";}.fa-moon-o:before {  content: "\f186";}.fa-archive:before {  content: "\f187";}.fa-bug:before {  content: "\f188";}.fa-vk:before {  content: "\f189";}.fa-weibo:before {  content: "\f18a";}.fa-renren:before {  content: "\f18b";}.fa-pagelines:before {  content: "\f18c";}.fa-stack-exchange:before {  content: "\f18d";}.fa-arrow-circle-o-right:before {  content: "\f18e";}.fa-arrow-circle-o-left:before {  content: "\f190";}.fa-toggle-left:before,.fa-caret-square-o-left:before {  content: "\f191";}.fa-dot-circle-o:before {  content: "\f192";}.fa-wheelchair:before {  content: "\f193";}.fa-vimeo-square:before {  content: "\f194";}.fa-turkish-lira:before,.fa-try:before {  content: "\f195";}.fa-plus-square-o:before {  content: "\f196";}.fa-space-shuttle:before {  content: "\f197";}.fa-slack:before {  content: "\f198";}.fa-envelope-square:before {  content: "\f199";}.fa-wordpress:before {  content: "\f19a";}.fa-openid:before {  content: "\f19b";}.fa-institution:before,.fa-bank:before,.fa-university:before {  content: "\f19c";}.fa-mortar-board:before,.fa-graduation-cap:before {  content: "\f19d";}.fa-yahoo:before {  content: "\f19e";}.fa-google:before {  content: "\f1a0";}.fa-reddit:before {  content: "\f1a1";}.fa-reddit-square:before {  content: "\f1a2";}.fa-stumbleupon-circle:before {  content: "\f1a3";}.fa-stumbleupon:before {  content: "\f1a4";}.fa-delicious:before {  content: "\f1a5";}.fa-digg:before {  content: "\f1a6";}.fa-pied-piper-pp:before {  content: "\f1a7";}.fa-pied-piper-alt:before {  content: "\f1a8";}.fa-drupal:before {  content: "\f1a9";}.fa-joomla:before {  content: "\f1aa";}.fa-language:before {  content: "\f1ab";}.fa-fax:before {  content: "\f1ac";}.fa-building:before {  content: "\f1ad";}.fa-child:before {  content: "\f1ae";}.fa-paw:before {  content: "\f1b0";}.fa-spoon:before {  content: "\f1b1";}.fa-cube:before {  content: "\f1b2";}.fa-cubes:before {  content: "\f1b3";}.fa-behance:before {  content: "\f1b4";}.fa-behance-square:before {  content: "\f1b5";}.fa-steam:before {  content: "\f1b6";}.fa-steam-square:before {  content: "\f1b7";}.fa-recycle:before {  content: "\f1b8";}.fa-automobile:before,.fa-car:before {  content: "\f1b9";}.fa-cab:before,.fa-taxi:before {  content: "\f1ba";}.fa-tree:before {  content: "\f1bb";}.fa-spotify:before {  content: "\f1bc";}.fa-deviantart:before {  content: "\f1bd";}.fa-soundcloud:before {  content: "\f1be";}.fa-database:before {  content: "\f1c0";}.fa-file-pdf-o:before {  content: "\f1c1";}.fa-file-word-o:before {  content: "\f1c2";}.fa-file-excel-o:before {  content: "\f1c3";}.fa-file-powerpoint-o:before {  content: "\f1c4";}.fa-file-photo-o:before,.fa-file-picture-o:before,.fa-file-image-o:before {  content: "\f1c5";}.fa-file-zip-o:before,.fa-file-archive-o:before {  content: "\f1c6";}.fa-file-sound-o:before,.fa-file-audio-o:before {  content: "\f1c7";}.fa-file-movie-o:before,.fa-file-video-o:before {  content: "\f1c8";}.fa-file-code-o:before {  content: "\f1c9";}.fa-vine:before {  content: "\f1ca";}.fa-codepen:before {  content: "\f1cb";}.fa-jsfiddle:before {  content: "\f1cc";}.fa-life-bouy:before,.fa-life-buoy:before,.fa-life-saver:before,.fa-support:before,.fa-life-ring:before {  content: "\f1cd";}.fa-circle-o-notch:before {  content: "\f1ce";}.fa-ra:before,.fa-resistance:before,.fa-rebel:before {  content: "\f1d0";}.fa-ge:before,.fa-empire:before {  content: "\f1d1";}.fa-git-square:before {  content: "\f1d2";}.fa-git:before {  content: "\f1d3";}.fa-y-combinator-square:before,.fa-yc-square:before,.fa-hacker-news:before {  content: "\f1d4";}.fa-tencent-weibo:before {  content: "\f1d5";}.fa-qq:before {  content: "\f1d6";}.fa-wechat:before,.fa-weixin:before {  content: "\f1d7";}.fa-send:before,.fa-paper-plane:before {  content: "\f1d8";}.fa-send-o:before,.fa-paper-plane-o:before {  content: "\f1d9";}.fa-history:before {  content: "\f1da";}.fa-circle-thin:before {  content: "\f1db";}.fa-header:before {  content: "\f1dc";}.fa-paragraph:before {  content: "\f1dd";}.fa-sliders:before {  content: "\f1de";}.fa-share-alt:before {  content: "\f1e0";}.fa-share-alt-square:before {  content: "\f1e1";}.fa-bomb:before {  content: "\f1e2";}.fa-soccer-ball-o:before,.fa-futbol-o:before {  content: "\f1e3";}.fa-tty:before {  content: "\f1e4";}.fa-binoculars:before {  content: "\f1e5";}.fa-plug:before {  content: "\f1e6";}.fa-slideshare:before {  content: "\f1e7";}.fa-twitch:before {  content: "\f1e8";}.fa-yelp:before {  content: "\f1e9";}.fa-newspaper-o:before {  content: "\f1ea";}.fa-wifi:before {  content: "\f1eb";}.fa-calculator:before {  content: "\f1ec";}.fa-paypal:before {  content: "\f1ed";}.fa-google-wallet:before {  content: "\f1ee";}.fa-cc-visa:before {  content: "\f1f0";}.fa-cc-mastercard:before {  content: "\f1f1";}.fa-cc-discover:before {  content: "\f1f2";}.fa-cc-amex:before {  content: "\f1f3";}.fa-cc-paypal:before {  content: "\f1f4";}.fa-cc-stripe:before {  content: "\f1f5";}.fa-bell-slash:before {  content: "\f1f6";}.fa-bell-slash-o:before {  content: "\f1f7";}.fa-trash:before {  content: "\f1f8";}.fa-copyright:before {  content: "\f1f9";}.fa-at:before {  content: "\f1fa";}.fa-eyedropper:before {  content: "\f1fb";}.fa-paint-brush:before {  content: "\f1fc";}.fa-birthday-cake:before {  content: "\f1fd";}.fa-area-chart:before {  content: "\f1fe";}.fa-pie-chart:before {  content: "\f200";}.fa-line-chart:before {  content: "\f201";}.fa-lastfm:before {  content: "\f202";}.fa-lastfm-square:before {  content: "\f203";}.fa-toggle-off:before {  content: "\f204";}.fa-toggle-on:before {  content: "\f205";}.fa-bicycle:before {  content: "\f206";}.fa-bus:before {  content: "\f207";}.fa-ioxhost:before {  content: "\f208";}.fa-angellist:before {  content: "\f209";}.fa-cc:before {  content: "\f20a";}.fa-shekel:before,.fa-sheqel:before,.fa-ils:before {  content: "\f20b";}.fa-meanpath:before {  content: "\f20c";}.fa-buysellads:before {  content: "\f20d";}.fa-connectdevelop:before {  content: "\f20e";}.fa-dashcube:before {  content: "\f210";}.fa-forumbee:before {  content: "\f211";}.fa-leanpub:before {  content: "\f212";}.fa-sellsy:before {  content: "\f213";}.fa-shirtsinbulk:before {  content: "\f214";}.fa-simplybuilt:before {  content: "\f215";}.fa-skyatlas:before {  content: "\f216";}.fa-cart-plus:before {  content: "\f217";}.fa-cart-arrow-down:before {  content: "\f218";}.fa-diamond:before {  content: "\f219";}.fa-ship:before {  content: "\f21a";}.fa-user-secret:before {  content: "\f21b";}.fa-motorcycle:before {  content: "\f21c";}.fa-street-view:before {  content: "\f21d";}.fa-heartbeat:before {  content: "\f21e";}.fa-venus:before {  content: "\f221";}.fa-mars:before {  content: "\f222";}.fa-mercury:before {  content: "\f223";}.fa-intersex:before,.fa-transgender:before {  content: "\f224";}.fa-transgender-alt:before {  content: "\f225";}.fa-venus-double:before {  content: "\f226";}.fa-mars-double:before {  content: "\f227";}.fa-venus-mars:before {  content: "\f228";}.fa-mars-stroke:before {  content: "\f229";}.fa-mars-stroke-v:before {  content: "\f22a";}.fa-mars-stroke-h:before {  content: "\f22b";}.fa-neuter:before {  content: "\f22c";}.fa-genderless:before {  content: "\f22d";}.fa-facebook-official:before {  content: "\f230";}.fa-pinterest-p:before {  content: "\f231";}.fa-whatsapp:before {  content: "\f232";}.fa-server:before {  content: "\f233";}.fa-user-plus:before {  content: "\f234";}.fa-user-times:before {  content: "\f235";}.fa-hotel:before,.fa-bed:before {  content: "\f236";}.fa-viacoin:before {  content: "\f237";}.fa-train:before {  content: "\f238";}.fa-subway:before {  content: "\f239";}.fa-medium:before {  content: "\f23a";}.fa-yc:before,.fa-y-combinator:before {  content: "\f23b";}.fa-optin-monster:before {  content: "\f23c";}.fa-opencart:before {  content: "\f23d";}.fa-expeditedssl:before {  content: "\f23e";}.fa-battery-4:before,.fa-battery:before,.fa-battery-full:before {  content: "\f240";}.fa-battery-3:before,.fa-battery-three-quarters:before {  content: "\f241";}.fa-battery-2:before,.fa-battery-half:before {  content: "\f242";}.fa-battery-1:before,.fa-battery-quarter:before {  content: "\f243";}.fa-battery-0:before,.fa-battery-empty:before {  content: "\f244";}.fa-mouse-pointer:before {  content: "\f245";}.fa-i-cursor:before {  content: "\f246";}.fa-object-group:before {  content: "\f247";}.fa-object-ungroup:before {  content: "\f248";}.fa-sticky-note:before {  content: "\f249";}.fa-sticky-note-o:before {  content: "\f24a";}.fa-cc-jcb:before {  content: "\f24b";}.fa-cc-diners-club:before {  content: "\f24c";}.fa-clone:before {  content: "\f24d";}.fa-balance-scale:before {  content: "\f24e";}.fa-hourglass-o:before {  content: "\f250";}.fa-hourglass-1:before,.fa-hourglass-start:before {  content: "\f251";}.fa-hourglass-2:before,.fa-hourglass-half:before {  content: "\f252";}.fa-hourglass-3:before,.fa-hourglass-end:before {  content: "\f253";}.fa-hourglass:before {  content: "\f254";}.fa-hand-grab-o:before,.fa-hand-rock-o:before {  content: "\f255";}.fa-hand-stop-o:before,.fa-hand-paper-o:before {  content: "\f256";}.fa-hand-scissors-o:before {  content: "\f257";}.fa-hand-lizard-o:before {  content: "\f258";}.fa-hand-spock-o:before {  content: "\f259";}.fa-hand-pointer-o:before {  content: "\f25a";}.fa-hand-peace-o:before {  content: "\f25b";}.fa-trademark:before {  content: "\f25c";}.fa-registered:before {  content: "\f25d";}.fa-creative-commons:before {  content: "\f25e";}.fa-gg:before {  content: "\f260";}.fa-gg-circle:before {  content: "\f261";}.fa-tripadvisor:before {  content: "\f262";}.fa-odnoklassniki:before {  content: "\f263";}.fa-odnoklassniki-square:before {  content: "\f264";}.fa-get-pocket:before {  content: "\f265";}.fa-wikipedia-w:before {  content: "\f266";}.fa-safari:before {  content: "\f267";}.fa-chrome:before {  content: "\f268";}.fa-firefox:before {  content: "\f269";}.fa-opera:before {  content: "\f26a";}.fa-internet-explorer:before {  content: "\f26b";}.fa-tv:before,.fa-television:before {  content: "\f26c";}.fa-contao:before {  content: "\f26d";}.fa-500px:before {  content: "\f26e";}.fa-amazon:before {  content: "\f270";}.fa-calendar-plus-o:before {  content: "\f271";}.fa-calendar-minus-o:before {  content: "\f272";}.fa-calendar-times-o:before {  content: "\f273";}.fa-calendar-check-o:before {  content: "\f274";}.fa-industry:before {  content: "\f275";}.fa-map-pin:before {  content: "\f276";}.fa-map-signs:before {  content: "\f277";}.fa-map-o:before {  content: "\f278";}.fa-map:before {  content: "\f279";}.fa-commenting:before {  content: "\f27a";}.fa-commenting-o:before {  content: "\f27b";}.fa-houzz:before {  content: "\f27c";}.fa-vimeo:before {  content: "\f27d";}.fa-black-tie:before {  content: "\f27e";}.fa-fonticons:before {  content: "\f280";}.fa-reddit-alien:before {  content: "\f281";}.fa-edge:before {  content: "\f282";}.fa-credit-card-alt:before {  content: "\f283";}.fa-codiepie:before {  content: "\f284";}.fa-modx:before {  content: "\f285";}.fa-fort-awesome:before {  content: "\f286";}.fa-usb:before {  content: "\f287";}.fa-product-hunt:before {  content: "\f288";}.fa-mixcloud:before {  content: "\f289";}.fa-scribd:before {  content: "\f28a";}.fa-pause-circle:before {  content: "\f28b";}.fa-pause-circle-o:before {  content: "\f28c";}.fa-stop-circle:before {  content: "\f28d";}.fa-stop-circle-o:before {  content: "\f28e";}.fa-shopping-bag:before {  content: "\f290";}.fa-shopping-basket:before {  content: "\f291";}.fa-hashtag:before {  content: "\f292";}.fa-bluetooth:before {  content: "\f293";}.fa-bluetooth-b:before {  content: "\f294";}.fa-percent:before {  content: "\f295";}.fa-gitlab:before {  content: "\f296";}.fa-wpbeginner:before {  content: "\f297";}.fa-wpforms:before {  content: "\f298";}.fa-envira:before {  content: "\f299";}.fa-universal-access:before {  content: "\f29a";}.fa-wheelchair-alt:before {  content: "\f29b";}.fa-question-circle-o:before {  content: "\f29c";}.fa-blind:before {  content: "\f29d";}.fa-audio-description:before {  content: "\f29e";}.fa-volume-control-phone:before {  content: "\f2a0";}.fa-braille:before {  content: "\f2a1";}.fa-assistive-listening-systems:before {  content: "\f2a2";}.fa-asl-interpreting:before,.fa-american-sign-language-interpreting:before {  content: "\f2a3";}.fa-deafness:before,.fa-hard-of-hearing:before,.fa-deaf:before {  content: "\f2a4";}.fa-glide:before {  content: "\f2a5";}.fa-glide-g:before {  content: "\f2a6";}.fa-signing:before,.fa-sign-language:before {  content: "\f2a7";}.fa-low-vision:before {  content: "\f2a8";}.fa-viadeo:before {  content: "\f2a9";}.fa-viadeo-square:before {  content: "\f2aa";}.fa-snapchat:before {  content: "\f2ab";}.fa-snapchat-ghost:before {  content: "\f2ac";}.fa-snapchat-square:before {  content: "\f2ad";}.fa-pied-piper:before {  content: "\f2ae";}.fa-first-order:before {  content: "\f2b0";}.fa-yoast:before {  content: "\f2b1";}.fa-themeisle:before {  content: "\f2b2";}.fa-google-plus-circle:before,.fa-google-plus-official:before {  content: "\f2b3";}.fa-fa:before,.fa-font-awesome:before {  content: "\f2b4";}.fa-handshake-o:before {  content: "\f2b5";}.fa-envelope-open:before {  content: "\f2b6";}.fa-envelope-open-o:before {  content: "\f2b7";}.fa-linode:before {  content: "\f2b8";}.fa-address-book:before {  content: "\f2b9";}.fa-address-book-o:before {  content: "\f2ba";}.fa-vcard:before,.fa-address-card:before {  content: "\f2bb";}.fa-vcard-o:before,.fa-address-card-o:before {  content: "\f2bc";}.fa-user-circle:before {  content: "\f2bd";}.fa-user-circle-o:before {  content: "\f2be";}.fa-user-o:before {  content: "\f2c0";}.fa-id-badge:before {  content: "\f2c1";}.fa-drivers-license:before,.fa-id-card:before {  content: "\f2c2";}.fa-drivers-license-o:before,.fa-id-card-o:before {  content: "\f2c3";}.fa-quora:before {  content: "\f2c4";}.fa-free-code-camp:before {  content: "\f2c5";}.fa-telegram:before {  content: "\f2c6";}.fa-thermometer-4:before,.fa-thermometer:before,.fa-thermometer-full:before {  content: "\f2c7";}.fa-thermometer-3:before,.fa-thermometer-three-quarters:before {  content: "\f2c8";}.fa-thermometer-2:before,.fa-thermometer-half:before {  content: "\f2c9";}.fa-thermometer-1:before,.fa-thermometer-quarter:before {  content: "\f2ca";}.fa-thermometer-0:before,.fa-thermometer-empty:before {  content: "\f2cb";}.fa-shower:before {  content: "\f2cc";}.fa-bathtub:before,.fa-s15:before,.fa-bath:before {  content: "\f2cd";}.fa-podcast:before {  content: "\f2ce";}.fa-window-maximize:before {  content: "\f2d0";}.fa-window-minimize:before {  content: "\f2d1";}.fa-window-restore:before {  content: "\f2d2";}.fa-times-rectangle:before,.fa-window-close:before {  content: "\f2d3";}.fa-times-rectangle-o:before,.fa-window-close-o:before {  content: "\f2d4";}.fa-bandcamp:before {  content: "\f2d5";}.fa-grav:before {  content: "\f2d6";}.fa-etsy:before {  content: "\f2d7";}.fa-imdb:before {  content: "\f2d8";}.fa-ravelry:before {  content: "\f2d9";}.fa-eercast:before {  content: "\f2da";}.fa-microchip:before {  content: "\f2db";}.fa-snowflake-o:before {  content: "\f2dc";}.fa-superpowers:before {  content: "\f2dd";}.fa-wpexplorer:before {  content: "\f2de";}.fa-meetup:before {  content: "\f2e0";}.sr-only {  position: absolute;  width: 1px;  height: 1px;  padding: 0;  margin: -1px;  overflow: hidden;  clip: rect(0, 0, 0, 0);  border: 0;}.sr-only-focusable:active,.sr-only-focusable:focus {  position: static;  width: auto;  height: auto;  margin: 0;  overflow: visible;  clip: auto;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/progress_bar.css"/>
      <url>/css/progress_bar.css</url>
      
        <content type="html"><![CDATA[/* 此css文件为博客顶部渐变加载条的css文件，已在主题配置文件_config.butterfly.yml的inject中引入，需要得到渐变加载条的效果，还需要js依赖，已在_config.butterfly.yml的inject中引入，js文件是别人的直接拿来用。 */.pace {  -webkit-pointer-events: none;  pointer-events: none;  -webkit-user-select: none;  -moz-user-select: none;  user-select: none;  z-index: 2000;  position: fixed;  margin: auto;  top: 10px;  left: 0;  right: 0;  height: 8px;  border-radius: 8px;  width: 4rem;  background: #eaecf2;  border: 1px #e3e8f7;  overflow: hidden;}.pace-inactive .pace-progress {  opacity: 0;  transition: 0.3s ease-in;}.pace .pace-progress {  -webkit-box-sizing: border-box;  -moz-box-sizing: border-box;  -ms-box-sizing: border-box;  -o-box-sizing: border-box;  box-sizing: border-box;  -webkit-transform: translate3d(0, 0, 0);  -moz-transform: translate3d(0, 0, 0);  -ms-transform: translate3d(0, 0, 0);  -o-transform: translate3d(0, 0, 0);  transform: translate3d(0, 0, 0);  max-width: 200px;  position: absolute;  z-index: 2000;  display: block;  top: 0;  right: 100%;  height: 100%;  width: 100%;  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);  animation: gradient 1.5s ease infinite;  background-size: 200%;}.pace.pace-inactive {  opacity: 0;  transition: 0.3s;  top: -8px;}@keyframes gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/nav_menu.css"/>
      <url>/css/nav_menu.css</url>
      
        <content type="html"><![CDATA[#nav a:hover {  background: var(--xiaoyutou-main);  transition: 0.3s;}#nav-totop:hover .totopbtn i {  opacity: 1;}#nav-totop #percent {  font-size: 12px;  background: var(--xiaoyutou-white);  color: var(--xiaoyutou-main);  width: 25px;  height: 25px;  border-radius: 35px;  display: flex;  justify-content: center;  align-items: center;  transition: 0.3s;}.nav-fixed #nav-totop #percent,.page #nav-totop #percent {  background: var(--font-color);  color: var(--card-bg);  font-size: 13px;}#nav-totop {  width: 35px;}#page-header:not(.is-top-bar) #percent {  transition: 0.3s;}#page-header:not(.is-top-bar) #nav-totop {  width: 0;  opacity: 0;  transition: width 0.3s, opacity 0.2s;  margin-left: 0 !important;}#nav-totop #percent {  font-weight: 700;}#nav-totop:hover #percent {  opacity: 0;  transform: scale(1.5);  font-weight: 700;}#page-header #nav #nav-right div {  margin-left: 0.5rem;  padding: 0;}#nav-totop {  display: flex;  align-items: center;  justify-content: center;  transition: 0.3s;}.nav-button {  cursor: pointer;}div#menus {  display: flex;  align-items: center;}#page-header #nav .nav-button a {  height: 35px;  width: 35px;  display: flex;  align-items: center;  justify-content: center;}#nav .site-page {  padding-bottom: 0px;}#nav *::after {  background-color: transparent !important;}/* 顶栏修改 */#nav .menus_items .menus_item .menus_item_child li a {  padding: 2px 16px;}#nav .menus_items .menus_item .menus_item_child li:hover a {  color: white !important;}#nav .menus_items .menus_item .menus_item_child li {  margin: 6px;  border-radius: 5px;  transition: all 0.3s;  display: inline-block;  margin: 0 3px;}#nav .menus_items .menus_item .menus_item_child:before {  top: -19px;}#site-name,.shuoshuo {  white-space: nowrap;  overflow: hidden;}#site-name {  padding: 0 8px;  position: relative;  display: flex;  align-items: center;  justify-content: center;  transition: 0.3s;}#blog_name #site-name i {  opacity: 0;  position: absolute;}#blog_name #site-name:hover .title {  opacity: 0;}#blog_name #site-name:hover i {  opacity: 1;  transform: scale(1.01);  color: white;}/* 圆角隐藏 */ul.menus_item_child {  border-radius: 5px;}/* 一级菜单居中 */#nav .menus_items {  position: absolute;  width: fit-content;  left: 50%;  transform: translateX(-50%);  display: flex;  flex-direction: row;  justify-content: center;  align-items: center;  height: 60px;}#nav .menus_items .menus_item:hover .menus_item_child {  display: block;  transform: translateX(-50%);  right: auto;  left: auto !important;  padding: 6px 4px;  box-sizing: content-box;  line-height: 35px;}#nav .menus_items .menus_item:hover {  padding: 0 5px 27px 5px !important;  margin-bottom: -14.5px !important;}#nav .menus_items .menus_item .menus_item_child {  top: 44px;}@media screen and (min-width: 768px) {  .page .menus_item:hover>a.site-page {    color: var(--xiaoyutou-white) !important;    background: var(--xiaoyutou-main);    transition: 0.3s;    box-shadow: var(--xiaoyutou-shadow-main);  }}#nav a {  border-radius: 8px;}.page #nav a:hover {  color: var(--xiaoyutou-white) !important;  background: var(--xiaoyutou-main);  transition: 0.3s;  box-shadow: var(--xiaoyutou-shadow-main);}#menus>div.menus_items>div>a {  letter-spacing: 0.3rem;  font-weight: 700;  padding: 0em 0.3em 0em 0.5em;  height: 35px;  line-height: 35px;}#nav .menus_items .menus_item {  padding: 0 5px;  display: flex;  flex-direction: column;  margin: auto;  align-items: center;}#nav div#toggle-menu {  padding: 2px 0 4px 6px;}#nav-totop .totopbtn i {  position: absolute;  display: flex;  opacity: 0;}#page-name::before {  font-size: 18px;  position: absolute;  width: 100%;  height: 100%;  border-radius: 8px;  color: white !important;  top: 0;  left: 0;  content: "回到顶部";  background-color: var(--xiaoyutou-theme);  transition: all 0.3s;  -webkit-transition: all 0.3s;  -moz-transition: all 0.3s;  -ms-transition: all 0.3s;  -o-transition: all 0.3s;  opacity: 0;  box-shadow: 0 0 3px var(--xiaoyutou-theme);  line-height: 45px;  /*如果垂直位置不居中可以微调此值，也可以删了*/}#page-name:hover:before {  opacity: 1;}#name-container {  transition: all 0.3s;  -webkit-transition: all 0.3s;  -moz-transition: all 0.3s;  -ms-transition: all 0.3s;  -o-transition: all 0.3s;}#name-container:hover {  transform: translateX(-50%) scale(1.03);}#page-name {  position: relative;  padding: 10px 30px;}center#name-container {  position: absolute;  width: 100%;  left: 50%;  transform: translateX(-50%);  font-family: "ZhuZiAYuanJWD";}.nav-fixed.nav-visible #name-container {  transition: 0.3s;  transform: translate(-50%, 60px);}.nav-fixed.nav-visible #menus .menus_items {  transform: translate(-50%);  transition: 0.3s;  line-height: 60px;}.nav-fixed #menus .menus_items {  transform: translate(-50%, -60px);  transition: 0.3s;}.nav-fixed #name-container {  top: 15%;  transition: 0.3s;}#name-container {  bottom: 60px;}.mask-name-container {  max-width: 1200px;  width: 50%;  height: 100%;  position: absolute;  overflow: hidden;  left: 50%;  transform: translateX(-50%);}@media screen and (max-width: 992px) {  .mask-name-container {    width: 65%;  }}@media screen and (max-width: 768px) {  .mask-name-container {    display: none;  }}#sidebar #sidebar-menus .menus_items .site-page:hover {  color: var(--xiaoyutou-white);  border-radius: var(--xiaoyutou-border-radius);}#nav .menus_items .menus_item>a>i:last-child {  display: none;}#nav #search-button {  font-size: 1.3em;}@media screen and (min-width: 900px) {  #nav .back-home-button:hover {    box-shadow: var(--xiaoyutou-shadow-main);  }}.back-home-button:hover {  background: var(--xiaoyutou-main);  color: var(--xiaoyutou-white) !important;}.back-home-button {  display: flex;  width: 35px;  height: 35px;  padding: 0 !important;  align-items: center;  justify-content: center;  margin-right: 4px;  transition: 0.3s;  border-radius: 8px;}.back-home-button:hover .back-menu-list-groups {  display: flex;  opacity: 1;  transition: 0.3s;  top: 55px;  pointer-events: auto;  left: 0;}.back-home-button .back-menu-list-groups {  position: absolute;  top: 65px;  left: 0;  background: var(--xiaoyutou-card-bg);  border-radius: 12px;  border: var(--style-border);  flex-direction: column;  font-size: 12px;  color: var(--xiaoyutou-secondtext);  box-shadow: var(--xiaoyutou-shadow-border);  transition: 0s;  opacity: 0;  pointer-events: none;}.back-home-button .back-menu-list-group {  display: flex;  flex-direction: column;}.back-home-button .back-menu-list-group .back-menu-list-title {  margin: 8px 0 0 16px;  transition: 0.3s;}.back-home-button .back-menu-list {  display: flex;  flex-direction: column;}.back-home-button .back-menu-list::before {  position: absolute;  top: -22px;  left: 0px;  width: 100%;  height: 25px;  content: "";}.back-home-button .back-menu-list-group:hover .back-menu-list-title {  color: var(--xiaoyutou-main);}.back-home-button .back-menu-list-groups:hover {  border: var(--style-border-hover);}.back-home-button .back-menu-list .back-menu-item {  display: flex;  align-items: center;  margin: 4px 8px;  padding: 4px 8px !important;  transition: 0.3s;  border-radius: 8px;}.back-home-button .back-menu-list .back-menu-item .back-menu-item-text {  font-size: var(--global-font-size);  margin-left: 0.5rem;  color: var(--xiaoyutou-fontcolor);  white-space: nowrap;}#nav #blog_name {  flex-wrap: nowrap;  height: 60px;  display: flex;  align-items: center;  /* z-index: 102; */  transition: 0.3s;}/* #nav.show{  background: rgba(255, 255, 255, 0.75);  backdrop-filter: var(--backdrop-filter);} */.back-home-button .back-menu-list .back-menu-item .back-menu-item-icon {  width: 24px;  height: 24px;  border-radius: 24px;  background: var(--xiaoyutou-secondbg);}#page-header #nav .back-home-button {  cursor: pointer;  position: relative;}@media screen and (min-width: 1300px) {  #nav a:hover {    transform: scale(1.03);  }}.back-home-button .back-menu-list .back-menu-item:hover .back-menu-item-text {  color: var(--xiaoyutou-white);}.back-menu-item-icon.loading img {  width: 25px;}#page-header #nav #menus .nav-button.long a.totopbtn,#page-header #nav #menus .nav-button.long,#page-header #nav #menus .nav-button.long a.totopbtn span {  width: 70px;}#page-header #nav #menus .nav-button.long a.totopbtn span {  border-radius: 35px;  display: flex;  justify-content: center;  align-items: center;  transition: 0.3s;  white-space: nowrap;}#page-header #nav #menus .nav-button.long a.totopbtn:hover {  border-radius: 35px;  height: 30px;}#nav #search-button {  padding-left: 0;}#page-header #nav .nav-button {  margin-left: 0.5rem;  padding: 0;}#page-header:not(.is-top-bar) #nav-totop a {  display: none;}#search-button a.site-page.social-icon.search span {  display: none;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/card_author.js"/>
      <url>/js/card_author.js</url>
      
        <content type="html"><![CDATA[// 这里是信息卡片美化js的代码var card_author = {    getTimeState: function() {        var e = (new Date).getHours()          , t = "";        return 0 <= e && e <= 5 ? t = "晚安😴" : 5 < e && e <= 10 ? t = "早上好👋" : 10 < e && e <= 14 ? t = "中午好👋" : 14 < e && e <= 18 ? t = "下午好👋" : 18 < e && e <= 24 && (t = "晚上好👋"),        t    },    sayhi: function() {        var e = document.getElementById("author-info__sayhi");        e && (e.innerHTML = card_author.getTimeState() + "！我是")    },}card_author.sayhi();]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/console.js"/>
      <url>/js/console.js</url>
      
        <content type="html"><![CDATA[var now1 = new Date();function createtime1() {    var grt = new Date("12/03/2022 00:00:00"); //此处修改你的建站时间或者网站上线时间    now1.setTime(now1.getTime() + 250);    var days = (now1 - grt) / 1000 / 60 / 60 / 24;    var dnum = Math.floor(days);    var ascll = [        `欢迎来到小漁头|小戴の小家`,        `来日方长，何惧车遥马慢💑`,        `                ██╗  ██╗██╗ █████╗  ██████╗ ██╗   ██╗██╗   ██╗████████╗ ██████╗ ██╗   ██╗        ╚██╗██╔╝██║██╔══██╗██╔═══██╗╚██╗ ██╔╝██║   ██║╚══██╔══╝██╔═══██╗██║   ██║         ╚███╔╝ ██║███████║██║   ██║ ╚████╔╝ ██║   ██║   ██║   ██║   ██║██║   ██║         ██╔██╗ ██║██╔══██║██║   ██║  ╚██╔╝  ██║   ██║   ██║   ██║   ██║██║   ██║        ██╔╝ ██╗██║██║  ██║╚██████╔╝   ██║   ╚██████╔╝   ██║   ╚██████╔╝╚██████╔╝        ╚═╝  ╚═╝╚═╝╚═╝  ╚═╝ ╚═════╝    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝  ╚═════╝                                                                                                                                `,        "小站已经苟活",        dnum,        "天啦!",        "©2022 By 小漁头|小戴",    ];    setTimeout(        console.log.bind(            console,            `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,            "color:#39c5bb",            "",            "color:#39c5bb",            "color:#39c5bb",            "",            "color:#39c5bb",            ""        )    );}createtime1();function createtime2() {    var ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为「小笨蛋」`, `Photo captured: `, ` 🤪 `];    setTimeout(        console.log.bind(            console,            `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}`,            "color:white; background-color:#10bcc0",            "",            "",            'background:url("https://unpkg.zhimg.com/anzhiyu-assets@latest/image/common/tinggge.gif") no-repeat;font-size:450%'        )    );    setTimeout(console.log.bind(console, "%c WELCOME %c 欢迎光临，小笨蛋", "color:white; background-color:#23c682", ""));    setTimeout(        console.warn.bind(            console,            "%c ⚡ Powered by xiaoyutou %c 你正在访问小漁头|小戴の小家",            "color:white; background-color:#f0ad4e",            ""        )    );    setTimeout(console.log.bind(console, "%c W23-12 %c 系统监测到你已打开控制台", "color:white; background-color:#4f90d9", ""));    setTimeout(        console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中", "color:white; background-color:#d9534f", "")    );}createtime2();console.clear();    //清楚控制台// 重写部分console方法console.log = function () { };console.error = function () { };console.warn = function () { };]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/countup.js"/>
      <url>/js/countup.js</url>
      
        <content type="html"><![CDATA[// 关于页面countup.jsvar CountUp = function (target, startVal, endVal, decimals, duration, options) {    var self = this;    self.version = function () {      return "1.9.2";    };    self.options = {      useEasing: true,      useGrouping: true,      separator: ",",      decimal: ".",      easingFn: easeOutExpo,      formattingFn: formatNumber,      prefix: "",      suffix: "",      numerals: [],    };    if (options && typeof options === "object") {      for (var key in self.options) {        if (options.hasOwnProperty(key) && options[key] !== null) {          self.options[key] = options[key];        }      }    }    if (self.options.separator === "") {      self.options.useGrouping = false;    } else {      self.options.separator = "" + self.options.separator;    }    var lastTime = 0;    var vendors = ["webkit", "moz", "ms", "o"];    for (var x = 0; x < vendors.length && !window.requestAnimationFrame; ++x) {      window.requestAnimationFrame = window[vendors[x] + "RequestAnimationFrame"];      window.cancelAnimationFrame =        window[vendors[x] + "CancelAnimationFrame"] || window[vendors[x] + "CancelRequestAnimationFrame"];    }    if (!window.requestAnimationFrame) {      window.requestAnimationFrame = function (callback, element) {        var currTime = new Date().getTime();        var timeToCall = Math.max(0, 16 - (currTime - lastTime));        var id = window.setTimeout(function () {          callback(currTime + timeToCall);        }, timeToCall);        lastTime = currTime + timeToCall;        return id;      };    }    if (!window.cancelAnimationFrame) {      window.cancelAnimationFrame = function (id) {        clearTimeout(id);      };    }    function formatNumber(num) {      num = num.toFixed(self.decimals);      num += "";      var x, x1, x2, x3, i, l;      x = num.split(".");      x1 = x[0];      x2 = x.length > 1 ? self.options.decimal + x[1] : "";      if (self.options.useGrouping) {        x3 = "";        for (i = 0, l = x1.length; i < l; ++i) {          if (i !== 0 && i % 3 === 0) {            x3 = self.options.separator + x3;          }          x3 = x1[l - i - 1] + x3;        }        x1 = x3;      }      if (self.options.numerals.length) {        x1 = x1.replace(/[0-9]/g, function (w) {          return self.options.numerals[+w];        });        x2 = x2.replace(/[0-9]/g, function (w) {          return self.options.numerals[+w];        });      }      return self.options.prefix + x1 + x2 + self.options.suffix;    }    function easeOutExpo(t, b, c, d) {      return (c * (-Math.pow(2, (-10 * t) / d) + 1) * 1024) / 1023 + b;    }    function ensureNumber(n) {      return typeof n === "number" && !isNaN(n);    }    self.initialize = function () {      if (self.initialized) {        return true;      }      self.error = "";      self.d = typeof target === "string" ? document.getElementById(target) : target;      if (!self.d) {        self.error = "[CountUp] target is null or undefined";        return false;      }      self.startVal = Number(startVal);      self.endVal = Number(endVal);      if (ensureNumber(self.startVal) && ensureNumber(self.endVal)) {        self.decimals = Math.max(0, decimals || 0);        self.dec = Math.pow(10, self.decimals);        self.duration = Number(duration) * 1000 || 2000;        self.countDown = self.startVal > self.endVal;        self.frameVal = self.startVal;        self.initialized = true;        return true;      } else {        self.error = "[CountUp] startVal (" + startVal + ") or endVal (" + endVal + ") is not a number";        return false;      }    };    self.printValue = function (value) {      var result = self.options.formattingFn(value);      if (self.d.tagName === "INPUT") {        this.d.value = result;      } else {        if (self.d.tagName === "text" || self.d.tagName === "tspan") {          this.d.textContent = result;        } else {          this.d.innerHTML = result;        }      }    };    self.count = function (timestamp) {      if (!self.startTime) {        self.startTime = timestamp;      }      self.timestamp = timestamp;      var progress = timestamp - self.startTime;      self.remaining = self.duration - progress;      if (self.options.useEasing) {        if (self.countDown) {          self.frameVal = self.startVal - self.options.easingFn(progress, 0, self.startVal - self.endVal, self.duration);        } else {          self.frameVal = self.options.easingFn(progress, self.startVal, self.endVal - self.startVal, self.duration);        }      } else {        if (self.countDown) {          self.frameVal = self.startVal - (self.startVal - self.endVal) * (progress / self.duration);        } else {          self.frameVal = self.startVal + (self.endVal - self.startVal) * (progress / self.duration);        }      }      if (self.countDown) {        self.frameVal = self.frameVal < self.endVal ? self.endVal : self.frameVal;      } else {        self.frameVal = self.frameVal > self.endVal ? self.endVal : self.frameVal;      }      self.frameVal = Math.round(self.frameVal * self.dec) / self.dec;      self.printValue(self.frameVal);      if (progress < self.duration) {        self.rAF = requestAnimationFrame(self.count);      } else {        if (self.callback) {          self.callback();        }      }    };    self.start = function (callback) {      if (!self.initialize()) {        return;      }      self.callback = callback;      self.rAF = requestAnimationFrame(self.count);    };    self.pauseResume = function () {      if (!self.paused) {        self.paused = true;        cancelAnimationFrame(self.rAF);      } else {        self.paused = false;        delete self.startTime;        self.duration = self.remaining;        self.startVal = self.frameVal;        requestAnimationFrame(self.count);      }    };    self.reset = function () {      self.paused = false;      delete self.startTime;      self.initialized = false;      if (self.initialize()) {        cancelAnimationFrame(self.rAF);        self.printValue(self.startVal);      }    };    self.update = function (newEndVal) {      if (!self.initialize()) {        return;      }      newEndVal = Number(newEndVal);      if (!ensureNumber(newEndVal)) {        self.error = "[CountUp] update() - new endVal is not a number: " + newEndVal;        return;      }      self.error = "";      if (newEndVal === self.frameVal) {        return;      }      cancelAnimationFrame(self.rAF);      self.paused = false;      delete self.startTime;      self.startVal = self.frameVal;      self.endVal = newEndVal;      self.countDown = self.startVal > self.endVal;      self.rAF = requestAnimationFrame(self.count);    };    if (self.initialize()) {      self.printValue(self.startVal);    }  };]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[/*    此部分代码是黑夜霓虹灯的代码【大标题和个人信息的动态霓虹灯】    _config.butterfly.yml中的inject中引用*/// 霓虹灯效果// 颜色数组var arr = ["#39c5bb", "#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 1200);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/reward.js"/>
      <url>/js/reward.js</url>
      
        <content type="html"><![CDATA[// 关于页面的赞赏名单js文件function reward(){    Swal.fire({      title: '<strong>您正在为 <u>Peach</u> 充电</strong>',      html: '<b>请选择您的付款方式</b>',      icon: 'info',      showCancelButton: true,      confirmButtonText:        '<i class="fa-brands fa-alipay"></i> 支付宝',      cancelButtonText:        '<i class="fa-brands fa-weixin"></i> 微信支付',      confirmButtonColor: '#1677FF',      cancelButtonColor: '#2AAE67',    }).then((result) => {      if (result.isConfirmed) {        Swal.fire({          title: '感谢您',          html: '请打开支付宝 <b>[扫一扫]</b> 以充电',          imageUrl: '/img/alipay.jpg',          imageWidth: 175,          imageHeight: 225,          imageAlt: 'Custom image'        }).then((result) => {          Swal.fire(            '充电成功',            '感谢您的支持',            'success'          )        })      } else if (        result.dismiss === Swal.DismissReason.cancel      ) {        Swal.fire({          title: '感谢您',          html: '请打开微信 <b>[扫一扫]</b> 以充电',          imageUrl: '/img/wechat.jpg',          imageWidth: 175,          imageHeight: 225,          imageAlt: 'Custom image'        }).then((result) => {          Swal.fire(            '充电成功',            '感谢您的支持',            'success'          )        })      }    })  }]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/runtime.js"/>
      <url>/js/runtime.js</url>
      
        <content type="html"><![CDATA[/*    此部分代码事用来计算本博客当前运行了多么长时间，    以及博客footer的一些东西    在_config.yml中的footer_beautif中引用*/var now = new Date;function createtime() {     now.setTime(now.getTime() + 1e3);     var e = new Date("12/03/2022 00:00:00");    var t = Math.trunc(234e8 + (now - e) / 1e3 * 17);    var a = (t / 1496e5).toFixed(6);    var o = new Date("12/03/2022 00:00:00");    var r = (now - o) / 1e3 / 60 / 60 / 24;    var i = Math.floor(r);    var n = (now - o) / 1e3 / 60 / 60 - 24 * i;    var s = Math.floor(n); 1 == String(s).length && (s = "0" + s);     var l = (now - o) / 1e3 / 60 - 1440 * i - 60 * s, g = Math.floor(l); 1 == String(g).length && (g = "0" + g);     var d = (now - o) / 1e3 - 86400 * i - 3600 * s - 60 * g, b = Math.round(d); 1 == String(b).length && (b = "0" + b);     let c = "";     // c = s < 18 && s >= 9 ? `<img class='boardsign' src='https://img.shields.io/badge/F%E5%B0%8F%E5%B1%8B-%E7%A7%91%E7%A0%94%E6%91%B8%E9%B1%BC%E4%B8%AD%F0%9F%90%9F-1ade23?style=social&logo=Buy%20Me%20A%20Coffee' title='努力学习中~'><br> 本站居然运行了 ${i} 天 ${s} 小时 ${g} 分 ${b} 秒 <i id="heartbeat" class='fas fa-heartbeat'></i> <br> 旅行者 1 号当前距离地球 ${t} 千米，约为 ${a} 个天文单位 🚀 <br> I wish you to become your own sun, no need to rely on who's light.✨ ` : `<img class='boardsign' src='https://img.shields.io/badge/F%E5%B0%8F%E5%B1%8B-%E4%B8%8B%E7%8F%AD%E4%BC%91%E6%81%AF%E5%95%A6%F0%9F%8C%99-6adea8?style=social&logo=coffeescript' title='正在开开心心地玩耍~'><br> 本站居然运行了 ${i} 天 ${s} 小时 ${g} 分 ${b} 秒 <i id="heartbeat" class='fas fa-heartbeat'></i> <br> 旅行者 1 号当前距离地球 ${t} 千米，约为 ${a} 个天文单位 🚀 <br> I wish you to become your own sun, no need to rely on who's light.✨ `, document.getElementById("workboard") && (document.getElementById("workboard").innerHTML = c) } setInterval((() => { createtime() }), 1e3);    c = s < 18 && s >= 9 ? `本站居然运行了 ${i} 天 ${s} 小时 ${g} 分 ${b} 秒 <i id="heartbeat" class='fas fa-heartbeat'></i> <br> 旅行者 1 号当前距离地球 ${t} 千米，约为 ${a} 个天文单位 🚀 <br> I wish you to become your own sun, no need to rely on who's light.✨ ` : `本站居然运行了 ${i} 天 ${s} 小时 ${g} 分 ${b} 秒 <i id="heartbeat" class='fas fa-heartbeat'></i> <br> 旅行者 1 号当前距离地球 ${t} 千米，约为 ${a} 个天文单位 🚀 <br> I wish you to become your own sun, no need to rely on who's light.✨ `, document.getElementById("workboard") && (document.getElementById("workboard").innerHTML = c) } setInterval((() => { createtime() }), 1e3);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/starry_sky.js"/>
      <url>/js/starry_sky.js</url>
      
        <content type="html"><![CDATA[/*    此代码为星空的js代码，    代码用来描述星星在画布上的运动轨迹    _config.butterfly.yml中的inject中引用*/function dark() {    window.requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame || window.msRequestAnimationFrame;    var n, e, i, h, t = .05, s = document.getElementById("universe"), o = !0, a = "180,184,240", r = "226,225,142", d = "226,225,224", c = [];    function f() {        n = window.innerWidth, e = window.innerHeight, i = .216 * n, s.setAttribute("width", n), s.setAttribute("height", e)    }    function u() {        h.clearRect(0, 0, n, e);        for (var t = c.length, i = 0; i < t; i++) {            var s = c[i]; s.move(), s.fadeIn(), s.fadeOut(), s.draw()        }    }    function y() {        this.reset = function () {            this.giant = m(3),                this.comet = !this.giant && !o && m(10),                this.x = l(0, n - 10), this.y = l(0, e),                this.r = l(1.1, 2.6), this.dx = l(t, 6 * t) + (this.comet + 1 - 1) * t * l(50, 120) + 2 * t,                this.dy = -l(t, 6 * t) - (this.comet + 1 - 1) * t * l(50, 120),                this.fadingOut = null,                this.fadingIn = !0, this.opacity = 0,                this.opacityTresh = l(.2, 1 - .4 * (this.comet + 1 - 1)),                this.do = l(5e-4, .002) + .001 * (this.comet + 1 - 1)        },            this.fadeIn = function () {                this.fadingIn && (this.fadingIn = !(this.opacity > this.opacityTresh),                    this.opacity += this.do)            },            this.fadeOut = function () {                this.fadingOut && (this.fadingOut = !(this.opacity < 0),                    this.opacity -= this.do / 2, (this.x > n || this.y < 0) && (this.fadingOut = !1, this.reset()))            },            this.draw = function () {                if (h.beginPath(), this.giant) {                    h.fillStyle = "rgba(" + a + "," + this.opacity + ")", h.arc(this.x, this.y, 2, 0, 2 * Math.PI, !1)                }                else if (this.comet) {                    h.fillStyle = "rgba(" + d + "," + this.opacity + ")",                        h.arc(this.x, this.y, 1.5, 0, 2 * Math.PI, !1);                    for (var t = 0; t < 30; t++)h.fillStyle = "rgba(" + d + "," + (this.opacity - this.opacity / 20 * t) + ")", h.rect(this.x - this.dx / 4 * t, this.y - this.dy / 4 * t - 2, 2, 2), h.fill()                } else h.fillStyle = "rgba(" + r + "," + this.opacity + ")", h.rect(this.x, this.y, this.r, this.r); h.closePath(), h.fill()            },            this.move = function () {                this.x += this.dx, this.y += this.dy, !1 === this.fadingOut && this.reset(), (this.x > n - n / 4 || this.y < 0) && (this.fadingOut = !0)            }, setTimeout(function () { o = !1 }, 50)    }    function m(t) {        return Math.floor(1e3 * Math.random()) + 1 < 10 * t    }    function l(t, i) {        return Math.random() * (i - t) + t    }    f(), window.addEventListener("resize", f, !1), function () {        h = s.getContext("2d"); for (var t = 0; t < i; t++)c[t] = new y, c[t].reset(); u()    }(),        function t() {            document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark' && u(), window.requestAnimationFrame(t)        }()};dark()]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友链</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[<style>    .not-home-page{        /*height: 100px!important;*/    }    #page-site-info{        /*display: none;*/    }    #page{        /*background-color: rgba(255, 255, 255, 0.2)!important;*/        /*border: none!important;*/    }    .flink-list-item{        /*!*box-shadow: 0 0 10px #eeeeee; /\* 设置水平阴影 \*!  *!*/        /*!*box-shadow: 0 -10px 10px #eeeeee; /\* 设置垂直阴影 \*!*!*/        /*background-color: rgba(255, 255, 255, 1)!important;*/        /*border: none!important;*/    }</style>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/txmap.js"/>
      <url>/js/txmap.js</url>
      
        <content type="html"><![CDATA[//get请求$.ajax({    type: 'get',    url: 'https://apis.map.qq.com/ws/location/v1/ip',    data: {        key: 'GKLBZ-SMFWQ-57F5E-4OCX5-OAUX7-IABAI',        output: 'jsonp',    },    dataType: 'jsonp',    success: function (res) {        ipLoacation = res;    }})function getDistance(e1, n1, e2, n2) {    const R = 6371    const { sin, cos, asin, PI, hypot } = Math    let getPoint = (e, n) => {        e *= PI / 180        n *= PI / 180        return { x: cos(n) * cos(e), y: cos(n) * sin(e), z: sin(n) }    }    let a = getPoint(e1, n1)    let b = getPoint(e2, n2)    let c = hypot(a.x - b.x, a.y - b.y, a.z - b.z)    let r = asin(c / 2) * 2 * R    return Math.round(r);}function showWelcome() {    // 淄博张店山东理工大学经纬度为 36.811771(维度), 118.039688(经度)    // 日照岚山区秦海家园经纬度为 35.12561(维度),119.371948(经度)    //参数请看文档：https://lbs.qq.com/service/webService/webServiceGuide/webServiceIp    let dist = getDistance(118.039688,36.811771,ipLoacation.result.location.lng, ipLoacation.result.location.lat); //这里换成自己的经纬度    let pos = ipLoacation.result.ad_info.nation;    let ip;    let posdesc;    //根据国家、省份、城市信息自定义欢迎语    switch (ipLoacation.result.ad_info.nation) {        case "日本":            posdesc = "よろしく，一起去看樱花吗";            break;        case "美国":            posdesc = "Let us live in peace!";            break;        case "英国":            posdesc = "想同你一起夜乘伦敦眼";            break;        case "俄罗斯":            posdesc = "干了这瓶伏特加！";            break;        case "法国":            posdesc = "C'est La Vie";            break;        case "德国":            posdesc = "Die Zeit verging im Fluge.";            break;        case "澳大利亚":            posdesc = "一起去大堡礁吧！";            break;        case "加拿大":            posdesc = "拾起一片枫叶赠予你";            break;        case "中国":            pos = ipLoacation.result.ad_info.province + " " + ipLoacation.result.ad_info.city + " " + ipLoacation.result.ad_info.district;            ip = ipLoacation.result.ip;            switch (ipLoacation.result.ad_info.province) {                case "北京市":                    posdesc = "北——京——欢迎你~~~";                    break;                case "天津市":                    posdesc = "讲段相声吧。";                    break;                case "河北省":                    posdesc = "山势巍巍成壁垒，天下雄关。铁马金戈由此向，无限江山。";                    break;                case "山西省":                    posdesc = "展开坐具长三尺，已占山河五百余。";                    break;                case "内蒙古自治区":                    posdesc = "天苍苍，野茫茫，风吹草低见牛羊。";                    break;                case "辽宁省":                    posdesc = "我想吃烤鸡架！";                    break;                case "吉林省":                    posdesc = "状元阁就是东北烧烤之王。";                    break;                case "黑龙江省":                    posdesc = "很喜欢哈尔滨大剧院。";                    break;                case "上海市":                    posdesc = "众所周知，中国只有两个城市。";                    break;                case "江苏省":                    switch (ipLoacation.result.ad_info.city) {                        case "南京市":                            posdesc = "这是我挺想去的城市啦。";                            break;                        case "苏州市":                            posdesc = "上有天堂，下有苏杭。";                            break;                        default:                            posdesc = "散装是必须要散装的。";                            break;                    }                    break;                case "浙江省":                    posdesc = "东风渐绿西湖柳，雁已还人未南归。";                    break;                case "河南省":                    switch (ipLoacation.result.ad_info.city) {                        case "郑州市":                            posdesc = "豫州之域，天地之中。";                            break;                        case "南阳市":                            posdesc = "臣本布衣，躬耕于南阳。此南阳非彼南阳！";                            break;                        case "驻马店市":                            posdesc = "峰峰有奇石，石石挟仙气。嵖岈山的花很美哦！";                            break;                        case "开封市":                            posdesc = "刚正不阿包青天。";                            break;                        case "洛阳市":                            posdesc = "洛阳牡丹甲天下。";                            break;                        default:                            posdesc = "可否带我品尝河南烩面啦？";                            break;                    }                    break;                case "安徽省":                    posdesc = "蚌埠住了，芜湖起飞。";                    break;                case "福建省":                    posdesc = "井邑白云间，岩城远带山。";                    break;                case "江西省":                    posdesc = "落霞与孤鹜齐飞，秋水共长天一色。";                    break;                case "山东省":                    posdesc = "遥望齐州九点烟，一泓海水杯中泻。";                    break;                case "湖北省":                    posdesc = "来碗热干面！";                    break;                case "湖南省":                    posdesc = "74751，长沙斯塔克。";                    break;                case "广东省":                    posdesc = "老板来两斤福建人。";                    break;                case "广西壮族自治区":                    posdesc = "桂林山水甲天下。";                    break;                case "海南省":                    posdesc = "朝观日出逐白浪，夕看云起收霞光。";                    break;                case "四川省":                    posdesc = "康康川妹子。";                    break;                case "贵州省":                    posdesc = "茅台，学生，再塞200。";                    break;                case "云南省":                    posdesc = "玉龙飞舞云缠绕，万仞冰川直耸天。";                    break;                case "西藏自治区":                    posdesc = "躺在茫茫草原上，仰望蓝天。";                    break;                case "陕西省":                    posdesc = "来份臊子面加馍。";                    break;                case "甘肃省":                    posdesc = "羌笛何须怨杨柳，春风不度玉门关。";                    break;                case "青海省":                    posdesc = "牛肉干和老酸奶都好好吃。";                    break;                case "宁夏回族自治区":                    posdesc = "大漠孤烟直，长河落日圆。";                    break;                case "新疆维吾尔自治区":                    posdesc = "驼铃古道丝绸路，胡马犹闻唐汉风。";                    break;                case "台湾省":                    posdesc = "我在这头，大陆在那头。";                    break;                case "香港特别行政区":                    posdesc = "永定贼有残留地鬼嚎，迎击光非岁玉。";                    break;                case "澳门特别行政区":                    posdesc = "性感荷官，在线发牌。";                    break;                default:                    posdesc = "带我去你的城市逛逛吧！";                    break;            }            break;        default:            posdesc = "带我去你的国家逛逛吧。";            break;    }    //根据本地时间切换欢迎语    let timeChange;    let date = new Date();    if (date.getHours() >= 5 && date.getHours() < 11) timeChange = "<span>上午好</span>，一日之计在于晨！";    else if (date.getHours() >= 11 && date.getHours() < 13) timeChange = "<span>中午好</span>，该摸鱼吃午饭了。";    else if (date.getHours() >= 13 && date.getHours() < 15) timeChange = "<span>下午好</span>，懒懒地睡个午觉吧！";    else if (date.getHours() >= 15 && date.getHours() < 16) timeChange = "<span>三点几啦</span>，一起饮茶呀！";    else if (date.getHours() >= 16 && date.getHours() < 19) timeChange = "<span>夕阳无限好！</span>";    else if (date.getHours() >= 19 && date.getHours() < 24) timeChange = "<span>晚上好</span>，夜生活嗨起来！";    else timeChange = "夜深了，早点休息，少熬夜。";    try {        //自定义文本和需要放的位置        document.getElementById("welcome-info").innerHTML =            `<b><center>🎉 欢迎信息 🎉</center>&emsp;&emsp;欢迎来自 <span style="color:white">${pos}</span> 的小伙伴，${timeChange}您现在距离站长约 <span style="color:white">${dist}</span> 公里，当前的IP地址为： <span style="color:white">${ip}</span>， ${posdesc}</b>`;    } catch (err) {        console.log("Pjax无法获取#welcome-info元素🙄🙄🙄")    }}window.onload = showWelcome;// 如果使用了pjax在加上下面这行代码document.addEventListener('pjax:complete', showWelcome);// 二叉排序树算法]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>项目</title>
      <link href="/my_project/index.html"/>
      <url>/my_project/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>恋爱小屋💕</title>
      <link href="/love/index.html"/>
      <url>/love/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于本站</title>
      <link href="/web_site/index.html"/>
      <url>/web_site/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>唠嗑</title>
      <link href="/%E5%94%A0%E5%97%91/index.html"/>
      <url>/%E5%94%A0%E5%97%91/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>博客装修日志</title>
      <link href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/index.html"/>
      <url>/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/index.html</url>
      
        <content type="html"><![CDATA[<div class="timeline blue"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>博客装修日志⏰ </p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-5-10</p></div></div><div class='timeline-item-content'><ol><li><p>ios添加到主屏幕webapp</p><p>参考：<a href="https://blog.zhheo.com/p/b737e93d.html">让网页支持iOS添加到主屏幕全屏应用，webapp启动图生成工具</a></p><p>修改代码位置：</p><p><code>themes/butterfly/layout/includes/head.pug</code></p><p><code>themes/butterfly/source/manifest.json</code></p><p>图片存储位置：<code>/themes/butterfly/source/img/pwa/</code></p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-4-3</p></div></div><div class='timeline-item-content'><ol><li><p>对标签页进行魔改</p><p>参考：<a href="https://blog.leonus.cn/2022/tags.html">Hexo博客标签的魔改</a></p><p>修改代码：<code>[blogroot]/source/css/css.css</code>的347行；<strong><code>butterfly\scripts\helpers\page.js</code></strong>的52行</p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-3-20</p></div></div><div class='timeline-item-content'><ol><li><strong>关闭旧版首页分类磁铁</strong></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-3-20</p></div></div><div class='timeline-item-content'><ol><li><p><strong>关闭旧版首页分类磁铁</strong></p></li><li><p><strong>欢迎信息显示地理位置</strong></p><p>参考：<a href="https://www.fomal.cc/posts/d739261b.html">https://www.fomal.cc/posts/d739261b.html</a></p><p>使用了腾讯的<a href="https://lbs.qq.com/dev/console/application/mine">腾讯位置服务</a></p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-3-2</p></div></div><div class='timeline-item-content'><ol><li><p>关于页面美化</p><p>参考博客：<a href="https://opeach.cn/posts/dab4.html">Peach</a>、<a href="https://anzhiy.cn/posts/e62b.html">安知鱼</a></p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-2-9</p></div></div><div class='timeline-item-content'><ol><li><p>个人信息卡片美化</p><p>参考：<a href="https://blog.shineyu.cn/card-author-tutorial.html">Shine</a></p><p>删除了之前的背景图，在<code>BLOG/source/css.css</code> 中 215 行</p><p>修改了主题文件<code>card_author.pug</code></p><p>新建了<code>source/js/card_author.js</code>文件和<code>source/css/card_author.css</code></p></li><li><p>开启了pjax</p><p>在<code>_config.butterfly.yml</code>文件的843行</p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-2-6</p></div></div><div class='timeline-item-content'><ol><li><strong>卸载了Live2D</strong></li><li><strong>关闭自动夜间模式</strong></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-2-3</p></div></div><div class='timeline-item-content'><ol><li><p><strong>导航栏样式修改</strong></p><p>如果需要修改；</p><p>修改方法：在博客根目录下的css文件中新建了nav_menu文件夹，对导航栏样式魔改</p><p>魔改前：</p><p><a href="https://picbed.dai2yutou.space/web_img/11.png">魔改前样式</a></p><p>魔改后：</p><p><a href="ttps://picbed.dai2yutou.space/web_img/12.png">魔改后样式</a></p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-1-20</p></div></div><div class='timeline-item-content'><ol><li><p><strong>对控制台样式自定义</strong></p><p><strong>参考博客</strong>：<a href="https://anzhiy.cn/posts/3444.html">♪(^∇^*)欢迎肥来！console的高级用法 | 安知鱼 (anzhiy.cn)</a></p><p><strong>方法</strong>：新建立了一个<code>console.js</code>文件放在了博客根目录下的<code>js</code>文件夹下，并在站点主题配置文件<code>inject</code>中引入，具体可见<code>_config.butterfly.yml</code>中第920行</p></li><li><p><strong>顶部渐变加载条</strong></p><p><strong>参考博客</strong>：<a href="https://www.chuckle.top/article/13d6481a.html">给Butterfly加上顶部加载条 | 轻笑Chuckle</a></p><p><strong>方法</strong>：新建了一个progress_bar.css文件放在了博客根目录下的css文件夹中，并在站点主题配置文件<code>inject</code>中引入，然后需要inject中引入必要的js依赖。</p></li><li><p><strong>Heo同款加载动画</strong></p><p><strong>参考博客</strong>：<a href="https://anzhiy.cn/posts/52d8.html">Heo同款loading动画 | 安知鱼 (anzhiy.cn)</a></p><p><strong>方法</strong>：参考博客，注意定义满屏加载的头像的css代码在[BlogRoot]/source/css/css.css文件的第329行，需要修改头像时改变background的地址即可。</p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-1-7</p></div></div><div class='timeline-item-content'><ol><li><p><strong>安装了PDF插件</strong></p><p>参考博客：<a href="https://www.fomal.cc/posts/2d7ac914.html">博客魔改教程总结(四) | Fomalhaut🥝</a></p><p>使用方法：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 1.本地文件:在md文件路径下创建一个同名文件夹，其内放pdf文件名为xxx.pdf的文件</span></span><br><span class="line">&#123;% pdf ./文件名/xxx.pdf %&#125;</span><br><span class="line"><span class="section"># 2.在线链接(推荐第二种)</span></span><br><span class="line">&#123;% pdf https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/pdf/小作文讲义.pdf %&#125;</span><br></pre></td></tr></table></figure></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-1-5</p></div></div><div class='timeline-item-content'><ol><li><p><strong>博客宽屏适配</strong></p><p>配置信息放在了<code>BLOG/source/css.css</code>中188行</p><p>参考博客：<a href="https://www.fomal.cc/posts/d739261b.html">博客魔改教程总结(四) | Fomalhaut🥝</a></p></li><li><p><strong>文章三栏</strong>【暂时改回来了】</p><p>配置信息放在了<code>_config.butterfly.yml</code>中1047行，注意看魔改方法，改了许多多地方</p><p>参考博客：<a href="https://www.fomal.cc/posts/d739261b.html">博客魔改教程总结(四) | Fomalhaut🥝</a></p></li><li><p><strong>信息卡片背景图</strong></p><p>配置信息放在了<code>BLOG/source/css.css</code>中215行</p><p>参考博客：<a href="https://www.fomal.cc/posts/d739261b.html">博客魔改教程总结(四) | Fomalhaut🥝</a></p></li><li><p><strong>页面样式调节</strong>、</p><p>配置信息放在了<code>BLOG/source/css.css</code>中233行</p><p>参考博客：<a href="https://www.fomal.cc/posts/5389e93f.html">博客魔改教程总结(五) | Fomalhaut🥝</a></p></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-1-4</p></div></div><div class='timeline-item-content'><ul><li><p>外挂标签引入</p><p>配置信息放在了<code>_config.yml</code>中340行。</p><p>参考博客：<a href="https://www.fomal.cc/posts/5389e93f.html">博客魔改教程总结(二) | Fomalhaut🥝</a></p></li><li><p>文章置顶滚动栏</p><p>配置信息放在了_config.butterfly.yml中1027行</p><p>参考博客：<a href="https://www.fomal.cc/posts/eec9786.html">博客魔改教程总结(一) | Fomalhaut🥝</a></p></li></ul></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-12-30</p></div></div><div class='timeline-item-content'><ol><li><p>[Hexo中使用emoji表情]</p><blockquote><p>Hexo对博客实施了Github emoji 渲染到静态的 html 页面中</p><p>具体参考博客：<a href="https://blog.xaoxu.cn/archives/hexo-use-emoji">https://blog.xaoxu.cn/archives/hexo-use-emoji</a></p></blockquote></li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2022-12-3 [博客诞生]</p></div></div><div class='timeline-item-content'><ol><li>安装Hexo博客，配置node.js</li><li>博客上传到Github，博客部署到Vercel</li><li>更换主题为butterfly</li><li>完成butterfly的基础配置</li><li>安装插件侧栏电子时钟、Live2D 看板娘</li><li>装修主菜单栏</li><li>装修博客的footer<blockquote><p>参考博客：<br><a href="https://www.fomal.cc/">Fomalhaut🥝</a></p></blockquote></li></ol></div></div></div>]]></content>
      
    </entry>
    
    
  
</search>
