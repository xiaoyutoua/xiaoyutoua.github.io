<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习7.1-图像分割任务的实现 | 小漁头|小戴</title><meta name="author" content="小漁头&amp;小戴"><meta name="copyright" content="小漁头&amp;小戴"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文是深度学习的第十七篇，本文介绍其他有关计算机视觉中图像分割的概念等，介绍转置卷积。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习7.1-图像分割任务的实现">
<meta property="og:url" content="http://blog.dai2yutou.space/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="小漁头|小戴">
<meta property="og:description" content="本文是深度学习的第十七篇，本文介绍其他有关计算机视觉中图像分割的概念等，介绍转置卷积。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picbed.dai2yutou.space/web_img/19.png">
<meta property="article:published_time" content="2023-01-04T06:59:29.000Z">
<meta property="article:modified_time" content="2023-03-30T12:13:10.095Z">
<meta property="article:author" content="小漁头&amp;小戴">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="paddle">
<meta property="article:tag" content="深度学习高级_计算机视觉之图像分割">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picbed.dai2yutou.space/web_img/19.png"><link rel="shortcut icon" href="/img/basketball.png"><link rel="canonical" href="http://blog.dai2yutou.space/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 小漁头&小戴","link":"链接: ","source":"来源: 小漁头|小戴","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习7.1-图像分割任务的实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-30 20:13:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/css.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/at.alicdn.com/t/c/font_3829236_a49e40pee5.css"><link rel="stylesheet" href="/css/font-awesome.css"><link rel="stylesheet" href="/css/progress_bar.css"><link rel="stylesheet" href="/css/nav_menu.css"><link rel="stylesheet" href="/css/color.css"><link rel="apple-touch-icon" href="/img/apple-touch-icon.jpg"><meta name="apple-mobile-web-app-title" content="小漁头🏀"><link rel="bookmark" href="/img/apple-touch-icon.jpg"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/apple-touch-icon.jpg" ><link rel="stylesheet" href="/css/card_author.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (ture) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picbed.dai2yutou.space/web_img/19.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小漁头|小戴</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习7.1-图像分割任务的实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-04T06:59:29.000Z" title="发表于 2023-01-04 14:59:29">2023-01-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-30T12:13:10.095Z" title="更新于 2023-03-30 20:13:10">2023-03-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习7.1-图像分割任务的实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、图像分割"><a href="#一、图像分割" class="headerlink" title="一、图像分割"></a>一、图像分割</h1><p>:label:<code>sec_semantic_segmentation</code></p>
<p><strong>图像分割（image segmentation）</strong> 任务的定义是：根据某些规则将图片分成若干个特定的、具有独特性质的区域，并提出感兴趣目标的技术和过程。目前图像分割任务发展出了以下几个子领域：<strong>语义分割（semantic segmentation）</strong>、<strong>实例分割（instance segmentation）</strong> 以及2018年兴起的新领域<strong>全景分割（panoptic segmentation）</strong>。</p>
<p>而想要理清三个子领域的区别就不得不提到关于图像分割中 things 和 stuff 的区别：图像中的内容可以按照是否有固定形状分为 things 类别和 stuff 类别，其中，人，车等有固定形状的物体属于 things 类别（可数名词通常属于 things）；天空，草地等没有固定形状的物体属于 stuff 类别（不可数名词属于 stuff）。</p>
<p>语义分割更注重「类别之间的区分」，而实例分割更注重「个体之间的区分」，如下图所示，语义分割会重点将前景里的人群和背景里树木、天空和草地分割开，但是它不区分人群的单独个体，如图中的人全部标记为红色，导致右边黄色框中的人无法辨别是一个人还是不同的人；而实例分割会重点将人群里的每一个人分割开，但是不在乎草地、树木和天空的分割。</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/dab09d5c8bec4632801d73dfde33b2f34aa54a394edd43b2b9003d7463af1ee2" width = "800"  div align=center" width = "800"></center>
<center><br>图1：图像分割不同的领域</br></center>

<p><br></br></p>
<p>全景分割可以说是语义分割和实例分割的结合，每个 stuff 类别与 things 类别都被分割开，可以看到，things 类别的不同个体也被彼此分割开了。</p>
<h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a><strong>语义分割</strong></h2><p>在之前讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨语义分割问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中每一个<strong>像素</strong>的内容：其语义区域的标注和预测是像素级的。</p>
<p>语义分割提供有关道路上自由空间的信息，以及检测车道标记和交通标志等信息。 </p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/320c215536fe4f7489796ea3adffd3f1910171e2ff1b422eb24382d36e5e43b2" width = "600"  div align=center" width = "800"></center>
<center><br>图2：驾驶场景分割</br></center>

<p><br></br></p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/076d232f7da04178a4d4644602919462eeeff9f9a0614ea6948c46f7998fe432" width = "600"  div align=center" width = "800"></center>
<center><br>图3：人像分割</br></center>

<p><br></br></p>
<p> 面部的语义分割通常涉及诸如皮肤、头发、眼睛、鼻子、嘴巴和背景等的分类。 面部分割在计算机视觉的许多面部应用中是有用的，例如性别、表情、年龄和种族的估计。</p>
<h2 id="Pascal-VOC2012-语义分割数据集"><a href="#Pascal-VOC2012-语义分割数据集" class="headerlink" title="Pascal VOC2012 语义分割数据集"></a><strong>Pascal VOC2012 语义分割数据集</strong></h2><p>最重要的语义分割数据集之一是<a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC2012</a>。下面我们深入了解一下这个数据集。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/9921bc85efee4507a62bc2e34df1fd8260a04eb408ed4fdcb8163c6af3900ce7" width = "800"  div align=center" width = "800"></center>
<center><br>图4：VOC2012 语义分割数据集</br></center>

<hr>
<p>VOC2012数据集有多种用途，里面的数据有些可以用于分割、动作识别等。共有二十个类别：</p>
<p>Person:person</p>
<p>Animal: bird, cat, cow, dog, horse, sheep</p>
<p>Vehicle:aeroplane, bicycle, boat, bus, car, motorbike, train</p>
<p>Indoor: bottle, chair, dining table, potted plant, sofa, tv&#x2F;monitor</p>
<p>主要有四个大类别，分别是人、常见动物、交通车辆、室内家具用品。主要为图像分类、对象检测识别、图像分割三类任务服务。其中：</p>
<p>（1）<strong>Annotations文件夹</strong>：里面是图像对应的XML标注信息。每张图像对应一个XML文件。XML文件声明了图像数据的来源、大小等信息。</p>
<p>（2）<strong>ImageSets文件夹</strong>：</p>
<ul>
<li>Action：存放人的行为动作的图像信息；</li>
<li>Layout：存放的是具有人体部位的数据（人的head、hand、feet等等）；</li>
<li>Main：存放的是目标检测的数据，总共分为20类；</li>
<li>Segmentation：是可以用于分割的图像数据，包含val.txt、train.txt等。</li>
</ul>
<p>（3）<strong>JPEGImages</strong>：所有的原始图像文件，格式必须是JPG格式。</p>
<p>（4）<strong>SegmentationClass</strong>：类别分割label图。注意Class的意思就是对所有图像中的相同类别使用一个颜色，比如，飞机使用枣红色，那么所有图像中飞机的分割label标注就是枣红色。</p>
<p>（5）<strong>SegmentationObject</strong>：实例分割label图。一种图中识别出所有的object之后，对object使用不同的颜色进行填充。</p>
<hr>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a><strong>加载数据</strong></h3><p>进入路径<code>VOCdevkit/VOC2012</code>之后，我们可以看到数据集的不同组件。<code>ImageSets/Segmentation</code>路径包含用于训练和测试样本的文本文件，而<code>JPEGImages</code>和<code>SegmentationClass</code>路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。<br>下面将<code>read_voc_images</code>函数定义为将所有输入的图像和标签读入内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tar -xf /home/aistudio/data/data150715/VOCtrainval_11-May-<span class="number">2012.</span>tar</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">voc_dir = <span class="string">&quot;VOCdevkit/VOC2012&quot;</span>	<span class="comment"># 数据集解压后的目录</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, </span><br><span class="line">                            <span class="string">&#x27;ImageSets&#x27;</span>, </span><br><span class="line">                            <span class="string">&#x27;Segmentation&#x27;</span>, </span><br><span class="line">                            <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="comment"># 读入图片 &#x27;JPEGImages&#x27;</span></span><br><span class="line">        feature_load = np.asarray(vision.image_load(os.path.join(voc_dir, </span><br><span class="line">                                                <span class="string">&#x27;JPEGImages&#x27;</span>, </span><br><span class="line">                                                <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.jpg&#x27;</span>))).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        features.append(feature_load)</span><br><span class="line">        <span class="comment"># 把像素标签也存成图片 &#x27;SegmentationClass&#x27;</span></span><br><span class="line">        label_load = np.asarray(vision.image_load(os.path.join(voc_dir, </span><br><span class="line">                                <span class="string">&#x27;SegmentationClass&#x27;</span>, </span><br><span class="line">                                <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.png&#x27;</span>)).convert(<span class="string">&#x27;RGB&#x27;</span>)).transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        labels.append(label_load)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = train_features[<span class="number">0</span>:n] + train_labels[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment"># 参数img的格式为(channels,imagesize,imagesize)，</span></span><br><span class="line"><span class="comment"># show_images在现实的时候输入的是(imagesize,imagesize,channels)</span></span><br><span class="line">imgs = [img.transpose((<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">ppl.show_images(imgs, <span class="number">2</span>, n, scale=<span class="number">2.5</span>);</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/75.png" alt="png"></p>
<p>接下来，我们列举RGB颜色值和类名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>, <span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了<code>voc_colormap2label</code>函数来构建从RGB到VOC类别索引的映射，而<code>voc_label_indices</code>函数将VOC标签中的RGB值映射到它们的类别索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = paddle.zeros((<span class="number">256</span> ** <span class="number">3</span>, ), dtype=paddle.int64) <span class="comment"># 占位</span></span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        <span class="comment"># 把RGB换算成一个整数（使与类别的整数值对应）</span></span><br><span class="line">        idx = (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]</span><br><span class="line">        colormap2label[idx] = i		<span class="comment"># i为RGB类别索引，idx为RGB类别转换为的一个数</span></span><br><span class="line">        <span class="comment">#print(idx, i)</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label <span class="comment"># 返回一个字典</span></span><br><span class="line">voc_colormap2label()</span><br></pre></td></tr></table></figure>

<pre><code>Tensor(shape=[16777216], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [0, 0, 0, ..., 0, 0, 0])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colormap = train_labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="comment">#print(colormap)</span></span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span></span><br><span class="line">           + colormap[:, :, <span class="number">2</span>]) <span class="comment"># 将RGB值换成一个下标idx,将idx直接给字典即每个像素对应的标号找到。</span></span><br><span class="line">    <span class="comment">#print(idx)</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure>

<p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用了idx来代替RGB</span></span><br><span class="line">colormap2label = voc_colormap2label()</span><br><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], colormap2label)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>Tensor(shape=[10, 10], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]) aeroplane
</code></pre>
<h3 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h3><p>在之前的实验，我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。</p>
<p>为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># rect为随机裁剪后的一个的框，get_param返回指定模型或模块对象的指定参数的名称或值</span></span><br><span class="line">    rect = vision.transforms.RandomCrop(</span><br><span class="line">            size=(height, width))._get_param(img=feature,output_size=(height, width))</span><br><span class="line">    <span class="comment"># crop 在图像分割时，假设对图片裁剪后对标号也要做相应裁剪，才能使标号还能对应住图片。</span></span><br><span class="line">    feature = vision.transforms.functional.crop(feature, *rect)</span><br><span class="line">    label = vision.transforms.functional.crop(label, *rect)</span><br><span class="line">    <span class="comment"># print(feature.shape, label.shape)</span></span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = []</span><br><span class="line"><span class="built_in">print</span>(train_features[<span class="number">0</span>].shape) <span class="comment"># 图像尺寸不一致</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), </span><br><span class="line">                            train_labels[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line">imgs = [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">ppl.show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n, scale=<span class="number">2.5</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>, <span class="number">281</span>, <span class="number">500</span>)	<span class="comment"># 第一张图片的尺寸大小</span></span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/76.png" alt="png"></p>
<h3 id="自定义语义分割数据集类"><a href="#自定义语义分割数据集类" class="headerlink" title="自定义语义分割数据集类"></a><strong>自定义语义分割数据集类</strong></h3><p>我们通过继承高级API提供的<code>Dataset</code>类，自定义了一个语义分割数据集类<code>VOCSegDataset</code>。通过实现<code>__getitem__</code>函数，我们可以任意访问数据集中索引为<code>idx</code>的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的<code>filter</code>函数移除掉。此外，我们还定义了<code>normalize_image</code>函数，从而对输入图像的RGB三个通道的值分别做标准化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VOCSegDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span><br><span class="line">        self.transform = vision.transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        self.features = [self.normalize_image(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> self.<span class="built_in">filter</span>(features)]</span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        self.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入图像的RGB三个通道的值分别做标准化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize_image</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transform(img / <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 比较图像的尺寸和随机裁剪所指定的输出尺寸，如小于则剔除。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.shape[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">            img.shape[<span class="number">2</span>] &gt;= self.crop_size[<span class="number">1</span>])]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机裁剪特征和标签图像，返回索引为idx的输入图像及其每个像素的类别索引。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        feature, label = voc_rand_crop(</span><br><span class="line">            self.features[idx].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), </span><br><span class="line">            self.labels[idx].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)),</span><br><span class="line">            *self.crop_size</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)), self.colormap2label))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure>

<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a><strong>读取数据集</strong></h3><p>我们通过自定义的<code>VOCSegDataset</code>类来分别创建训练集和测试集的实例。假设我们指定随机裁剪的输出图像的形状为$320\times 480$，下面我们可以查看训练集和测试集所保留的样本个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line"><span class="comment">#由于数据集过大，整体运行会超显存，请在运行到这一步的时候重启内核。</span></span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir)	<span class="comment"># true表示开始训练</span></span><br></pre></td></tr></table></figure>

<pre><code>read 1114 examples
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure>

<pre><code>read 1078 examples
</code></pre>
<p>设批量大小为64，我们定义训练集的迭代器。<br>打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签<code>Y</code>是一个三维数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter = paddle.io.DataLoader(voc_train, </span><br><span class="line">                                    batch_size=batch_size, </span><br><span class="line">                                    shuffle=<span class="literal">True</span>, </span><br><span class="line">                                    drop_last=<span class="literal">True</span>) <span class="comment"># 扔掉不足batch的样本</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[64, 320, 480, 3]	64批量大小 329×480像素 3的通道数</span></span><br><span class="line"><span class="string">[64, 320, 480]	#由于y每一个像素点位置对应的是类别的取值，而原图对应的是RGB3个数值，所以这里是单通道</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>


<h3 id="整合所有组件"><a href="#整合所有组件" class="headerlink" title="[整合所有组件]"></a>[<strong>整合所有组件</strong>]</h3><p>最后，我们定义以下<code>load_data_voc</code>函数来下载并读取Pascal VOC2012语义分割数据集。它返回训练集和测试集的数据迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_voc</span>(<span class="params">batch_size, crop_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class="line">    voc_dir =  <span class="string">&quot;VOCdevkit/VOC2012&quot;</span></span><br><span class="line">    train_iter = paddle.io.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), </span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    test_iter = paddle.io.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir), </span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></figure>

<h1 id="二、转置卷积"><a href="#二、转置卷积" class="headerlink" title="二、转置卷积"></a>二、转置卷积</h1><p>:label:<code>sec_transposed_conv</code></p>
<p>到目前为止，我们所见到的卷积神经网络层，例如卷积层（ :numref:<code>sec_conv_layer</code>）和池化层（ :numref:<code>sec_pooling</code>），通常会减少下采样输入图像的空间维度（高和宽）。然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。】</p>
<p>例如，<strong>输出像素所处的通道维可以保有输入像素在同一位置上的分类结果</strong>。为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增加<strong>上采样</strong>中间层特征图的空间维度。</p>
<p>在算法中常用的上采样方法就是双线性插值以及转置卷积。在本节中，我们将介绍<strong>转置卷积（transposed convolution）</strong> ，用于逆转下采样导致的空间尺寸减小，使空间尺寸放大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> ppl</span><br></pre></td></tr></table></figure>

<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h2><p>让我们暂时忽略通道，从基本的转置卷积开始，设步幅为1且没有填充。假设我们有一个$n_h \times n_w$的输入张量和一个$k_h \times k_w$的卷积核。以步幅为1滑动卷积核窗口，每行	$n_w$次，每列$n_h$次，共产生$n_h n_w$个中间结果。每个中间结果都是一个$(n_h + k_h - 1) \times (n_w + k_w - 1)$的张量，初始化为0。为了计算每个中间张量，输入张量中的每个元素都要乘以卷积核，从而使所得的$k_h \times k_w$张量替换中间张量的一部分。请注意，每个中间张量被替换部分的位置与输入张量中元素的位置相对应。最后，所有中间结果相加以获得最终结果。</p>
<p>下图解释了如何为$2\times 2$的输入张量计算卷积核为$2\times 2$的转置卷积：</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/4d0c75a3740d49b4b795d856fb4e652d021d20fcf72e495a8d2b7f12deb74b4b" width = "800"  div align=center" width = "800"></center>
<center><br>图5：转置卷积</br></center>

<p>这将是一个一对多（one-to-many）的映射关系。我们可以对输入矩阵<code>X</code>和卷积核矩阵<code>K</code>(<strong>实现基本的转置卷积运算</strong>)<code>trans_conv</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trans_conv</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = paddle.zeros((X.shape[<span class="number">0</span>] + h - <span class="number">1</span>, X.shape[<span class="number">1</span>] + w - <span class="number">1</span>)) <span class="comment"># 输出形状公式</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i:i+h, j:j+w] += X[i,j] * K </span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<p>与通过卷积核“减少”输入元素的常规卷积（在 :numref:<code>sec_conv_layer</code>中）相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出。此实现是基本的二维转置卷积运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">K = paddle.to_tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">trans_conv(X, K)</span><br></pre></td></tr></table></figure>


<pre><code>Tensor(shape=[3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[0. , 0. , 1. ],
        [0. , 4. , 6. ],
        [4. , 12., 9. ]])
</code></pre>
<p>或者，当输入<code>X</code>和卷积核<code>K</code>都是四维张量时，我们可以使用高级API获得相同的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X, K = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)), K.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">class paddle.nn.Conv2DTranspose(</span></span><br><span class="line"><span class="string">                                in_channels, 	#输入通道数</span></span><br><span class="line"><span class="string">                                out_channels, 	#输出通道数</span></span><br><span class="line"><span class="string">                                kernel_size, 	#卷积核的大小</span></span><br><span class="line"><span class="string">                                stride=1, </span></span><br><span class="line"><span class="string">                                padding=0, </span></span><br><span class="line"><span class="string">                                output_padding=0, # 输出形状上一侧额外添加的大小. 默认值: 0</span></span><br><span class="line"><span class="string">                                groups=1, # 二维卷积层的组数</span></span><br><span class="line"><span class="string">                                dilation=1, # 空洞大小</span></span><br><span class="line"><span class="string">                                weight_attr=None, </span></span><br><span class="line"><span class="string">                                bias_attr=None, </span></span><br><span class="line"><span class="string">                                data_format=&#x27;NCHW&#x27;</span></span><br><span class="line"><span class="string">                                )</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>


<pre><code>Tensor(shape=[1, 1, 3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[[[0. , 0. , 1. ],
          [0. , 4. , 6. ],
          [4. , 12., 9. ]]]])
</code></pre>
<h2 id="填充、步幅和多通道"><a href="#填充、步幅和多通道" class="headerlink" title="填充、步幅和多通道"></a><strong>填充、步幅和多通道</strong></h2><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a><strong>填充</strong></h3><p>与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。（你加我就减）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入了填充实际上使得输出尺寸变小</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>


<pre><code>Tensor(shape=[1, 1, 1, 1], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[[[4.]]]])
</code></pre>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a><strong>步幅</strong></h3><p>在转置卷积中，步幅被指定为中间结果（输出），而不是输入。使用上图中相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，具体会体现在输出图像上。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/1bfa009a5c01420da9747628011a0ec961986b86336f43618daf48b205c824ae" width = "600"  div align=center" width = "800"></center>
<center><br>图6：步幅为2下的转置卷积</br></center>

<p>以下代码可以验证步幅为2的转置卷积的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入了步幅实际上使得输出尺寸变大</span></span><br><span class="line">tconv = nn.Conv2DTranspose(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, bias_attr=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.set_value(K)</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>


<pre><code>Tensor(shape=[1, 1, 4, 4], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[[[0., 0., 0., 1.],
          [0., 0., 2., 3.],
          [0., 2., 0., 3.],
          [4., 6., 6., 9.]]]])
</code></pre>
<p>对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\times k_w$的卷积核张量。当指定多个输出通道时，每个输出通道将有一个$c_i\times k_h\times k_w$的卷积核。</p>
<p>常规卷积与转置卷积可以相互转化。如果我们将$\mathsf{X}$代入卷积层$f$来输出$\mathsf{Y}&#x3D;f(\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\mathsf{X}$中通道数的转置卷积层$g$，那$g(Y)$的形状将与$\mathsf{X}$相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = paddle.rand(shape=(<span class="number">1</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">conv = nn.Conv2D(</span><br><span class="line">    <span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">tconv = nn.Conv2DTranspose(</span><br><span class="line">    <span class="number">20</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">tconv(conv(X)).shape == X.shape</span><br></pre></td></tr></table></figure>


<pre><code>True
</code></pre>
<h1 id="三、全卷积网络"><a href="#三、全卷积网络" class="headerlink" title="三、全卷积网络"></a>三、全卷积网络</h1><p>:label:<code>sec_fcn</code></p>
<p>语义分割是对图像中的每个像素分类。<strong>全卷积网络（fully convolutional network，FCN）</strong> 采用卷积神经网络实现了从图像像素到像素类别的变换。与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过转置卷积实现的。</p>
<p>因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> ppl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="1-构造模型"><a href="#1-构造模型" class="headerlink" title="1. 构造模型"></a>1. 构造模型</h2><p>下面我们了解一下全卷积网络模型最基本的设计。全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。</p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/036bdd31b81147f99d2e06665bf99de0dd6b41ed67e44fdd898a1c7f66f2be52" width = "800"  div align=center" width = "800"></center>
<center><br>图7：全卷积网络</br></center>

<p>下面，我们使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征，并将该网络记为<code>pretrained_net</code>。ResNet-18模型的最后几层包括全局平均池化层和全连接层，然而全卷积网络中不需要它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = vision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(pretrained_net.children())</span><br></pre></td></tr></table></figure>

<p>接下来，我们创建一个全卷积网络<code>net</code>。它复制了ResNet-18中大部分的预训练层，除了最后的全局平均池化层和最接近输出的全连接层。<br>给定高度为320和宽度为480的输入，<code>net</code>的前向传播将输入的高和宽减小至原来的$1&#x2F;32$，即10和15。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*<span class="built_in">list</span>(pretrained_net.children())[:-<span class="number">2</span>])</span><br><span class="line">X = paddle.rand(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure>


<pre><code>[1, 512, 10, 15]
</code></pre>
<p>接下来，我们使用$1\times1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）。最后，我们需要将特征图的高度和宽度增加32倍，从而将其变回输入图像的高和宽。</p>
<p>回想一下 ​​<code>sec_padding</code>中卷积层输出形状的计算方法：由于$(320-64+16\times2+32)&#x2F;32&#x3D;10$且$(480-64+16\times2+32)&#x2F;32&#x3D;15$，我们构造一个步幅为$32$的转置卷积层，并将卷积核的高和宽设为$64$，填充为$16$。我们可以看到如果步幅为$s$，填充为$s&#x2F;2$（假设$s&#x2F;2$是整数）且卷积核的高和宽为$2s$，转置卷积核会将输入的高和宽分别放大$s$倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line"><span class="comment"># 添加一个子层 add_sublayer</span></span><br><span class="line">net.add_sublayer(<span class="string">&#x27;final_conv&#x27;</span>, nn.Conv2D(<span class="number">512</span>, num_classes, kernel_size=<span class="number">1</span>))</span><br><span class="line">net.add_sublayer(<span class="string">&#x27;transpose_conv&#x27;</span>, </span><br><span class="line">                nn.Conv2DTranspose(</span><br><span class="line">                                num_classes, </span><br><span class="line">                                num_classes, </span><br><span class="line">                                kernel_size=<span class="number">64</span>, </span><br><span class="line">                                padding=<span class="number">16</span>, </span><br><span class="line">                                stride=<span class="number">32</span></span><br><span class="line">                                )</span><br><span class="line">                )</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure>


<pre><code>[1, 21, 320, 480]
</code></pre>
<h2 id="2-初始化转置卷积层"><a href="#2-初始化转置卷积层" class="headerlink" title="2. 初始化转置卷积层"></a>2. 初始化转置卷积层</h2><p>在图像处理中，我们有时需要将图像放大，即<strong>上采样（upsampling）</strong>。<strong>双线性插值（bilinear interpolation）</strong> 是常用的上采样方法之一，它也经常用于初始化转置卷积层。</p>
<p>为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。首先，将输出图像的坐标$(x,y)$映射到输入图像的坐标$(x’,y’)$上。例如，根据输入与输出的尺寸之比来映射。请注意，映射后的$x′$和$y′$是实数。然后，在输入图像上找到离坐标$(x’,y’)$最近的4个像素。最后，输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x’,y’)$的相对距离来计算。双线性插值的上采样可以通过转置卷积层实现，内核由以下<code>bilinear_kernel</code>函数构造，这里我们在<strong>中心位置</strong>插入了一个值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;利用双线性插值来初始化卷积核参数&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_kernel</span>(<span class="params">in_channels, out_channels, kernel_size</span>):</span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="comment"># 填充个数</span></span><br><span class="line">    <span class="built_in">print</span>(factor)</span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 中心位置</span></span><br><span class="line">    <span class="built_in">print</span>(center)</span><br><span class="line">    <span class="comment"># 表示出kernel_size长度的列向量和行向量</span></span><br><span class="line">    og = (paddle.arange(kernel_size).reshape((-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    paddle.arange(kernel_size).reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;og&quot;</span>, og)</span><br><span class="line">    <span class="comment"># 使用坐标及相对距离计算</span></span><br><span class="line">    filt = (<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor) * (<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> - paddle.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    </span><br><span class="line">    weight = paddle.zeros((in_channels, out_channels, kernel_size, kernel_size))</span><br><span class="line">    weight[<span class="built_in">range</span>(in_channels), <span class="built_in">range</span>(out_channels), :, :] = filt</span><br><span class="line">    <span class="built_in">print</span>(weight.shape)</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure>

<p>让我们用双线性插值的上采样实验它由转置卷积层实现。我们构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.Conv2DTranspose(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">conv_trans.weight.set_value(bilinear_kernel(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># [3, 3, 4, 4]--&gt; [通道，通道，核大小，核大小]--&gt; 4x4就是权重的数量</span></span><br></pre></td></tr></table></figure>

<pre><code>2
1.5
og (Tensor(shape=[4, 1], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [[0],
        [1],
        [2],
        [3]]), Tensor(shape=[1, 4], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [[0, 1, 2, 3]]))
Tensor(shape=[4, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[0.25000000],
        [0.75000000],
        [0.75000000],
        [0.25000000]])
Tensor(shape=[1, 4], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[0.25000000, 0.75000000, 0.75000000, 0.25000000]])
[3, 3, 4, 4]
</code></pre>
<p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>。为了打印图像，我们需要调整通道维的位置。<br>可以看到，转置卷积层将图像的高和宽分别放大了2倍。除了坐标刻度不同，双线性插值放大的图像和在 :numref:<code>sec_bbox</code>中打印出的原图看上去没什么两样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">img = vision.transforms.ToTensor()(ppl.Image.<span class="built_in">open</span>(<span class="string">&#x27;work/catdog.jpg&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># [通道，高，宽]</span></span><br><span class="line">X = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line">Y = conv_trans(X) <span class="comment"># 双线性插值后输出Y--&gt; 高宽放大2倍</span></span><br><span class="line">out_img = Y[<span class="number">0</span>].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).detach()</span><br><span class="line"><span class="built_in">print</span>(out_img.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;********************&quot;</span>)</span><br><span class="line"></span><br><span class="line">ppl.set_figsize()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input image shape:&#x27;</span>, img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).shape)</span><br><span class="line">ppl.plt.imshow(img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output image shape:&#x27;</span>, out_img.shape)</span><br><span class="line">ppl.plt.imshow(out_img)</span><br></pre></td></tr></table></figure>

<pre><code>[3, 561, 728]
[1, 3, 561, 728]
[1122, 1456, 3]
********************
input image shape: [561, 728, 3]
output image shape: [1122, 1456, 3]
</code></pre>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/77.png" alt="svg"></p>
<p>在全卷积网络中，我们用双线性插值的上采样初始化转置卷积层。对于$1\times 1$卷积层，我们使用Xavier初始化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)</span><br><span class="line">net.transpose_conv.weight.set_value(W)</span><br></pre></td></tr></table></figure>


<h2 id="3-读取数据集"><a href="#3-读取数据集" class="headerlink" title="3. 读取数据集"></a>3. 读取数据集</h2><p>我们读取语义分割读取数据集。指定随机裁剪的输出图像的形状为$320\times 480$：高和宽都可以被$32$整除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = <span class="number">8</span>, (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">train_iter, test_iter = ppl.load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></table></figure>

<pre><code>read 1114 examples
read 1078 examples
</code></pre>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><p>现在我们可以训练全卷积网络了。这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为我们使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维。此外，模型基于每个像素的预测类别是否正确来计算准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    <span class="comment"># 先求高的平均，再求宽的平均</span></span><br><span class="line">    <span class="keyword">return</span> F.cross_entropy(inputs, targets, reduction=<span class="string">&#x27;none&#x27;</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, paddle.nn.Layer):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = ppl.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> paddle.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            X = paddle.cast(X.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)), dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            pred = net(X)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(pred.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> pred.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                y_hat = ppl.argmax(pred, axis=<span class="number">1</span>)</span><br><span class="line">            cmp = paddle.cast(y_hat, y.dtype) == y</span><br><span class="line">            metric.add(<span class="built_in">float</span>(cmp.<span class="built_in">sum</span>()), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch_ch13</span>(<span class="params">net, X, y, loss, trainer</span>):</span><br><span class="line">    net.train()</span><br><span class="line">    trainer.clear_grad()</span><br><span class="line">    pred = net(X)</span><br><span class="line">    l = loss(pred.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)), y)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> pred.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 将one-hot转换为数值的形式</span></span><br><span class="line">        y_hat = ppl.argmax(pred, axis=<span class="number">1</span>)</span><br><span class="line">    cmp = paddle.cast(y_hat, y.dtype) == y</span><br><span class="line">    train_acc_sum = <span class="built_in">float</span>(cmp.<span class="built_in">sum</span>())</span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs</span>):</span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = ppl.Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            <span class="comment"># 数据维度需要修改为[N, C, H, W]，以符合模型的输入维度。</span></span><br><span class="line">            <span class="comment"># 模型的输入数据类型为float32</span></span><br><span class="line">            features = paddle.cast(features.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)), dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            l, acc = train_batch_ch13(net, features, labels, loss, trainer)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>,  &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span> &#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, wd = <span class="number">2</span>, <span class="number">0.001</span>, <span class="number">1e-3</span></span><br><span class="line">trainer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters(), weight_decay=wd)</span><br><span class="line">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs)</span><br></pre></td></tr></table></figure>

<pre><code>loss 0.492,  train acc 0.852,  test acc 0.844 
</code></pre>
<h2 id="5-预测"><a href="#5-预测" class="headerlink" title="5. 预测"></a>5. 预测</h2><p>在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">img</span>): <span class="comment"># 返回一个跟原图高宽等同的矩阵：[320, 480]</span></span><br><span class="line">    <span class="comment"># expand_dims 增加一个维度</span></span><br><span class="line">    X = np.expand_dims(test_iter.dataset.normalize_image(img), <span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(X.shape) # [1, 3, 320, 480]</span></span><br><span class="line">    X = paddle.to_tensor(X, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    pred = net(X).argmax(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(pred.shape) # [1, 320, 480]</span></span><br><span class="line">    <span class="keyword">return</span> pred.reshape((pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<p>为了<strong>可视化预测的类别</strong>给每个像素，我们将预测类别映射回它们在数据集中的标注颜色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label2image</span>(<span class="params">pred</span>):</span><br><span class="line">    colormap = paddle.to_tensor(ppl.VOC_COLORMAP)</span><br><span class="line">    <span class="comment"># print(colormap.shape)</span></span><br><span class="line">    X = paddle.cast(pred, dtype=<span class="string">&#x27;int64&#x27;</span>)  <span class="comment"># 转换数据类型</span></span><br><span class="line">    <span class="comment"># print(&quot;label2image: &quot;, X.shape)</span></span><br><span class="line">    <span class="keyword">return</span> colormap[X]</span><br></pre></td></tr></table></figure>

<p>测试数据集中的图像大小和形状各异。由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。</p>
<p>为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播。请注意，这些区域的并集需要完整覆盖输入图像。当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为<code>softmax</code>运算的输入，从而预测类别。</p>
<p>为简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320\times480$的区域用于预测。对于这些测试图像，我们逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">voc_dir = <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span></span><br><span class="line">test_images, test_labels = ppl.read_voc_images(voc_dir, <span class="literal">False</span>)</span><br><span class="line">n, imgs = <span class="number">4</span>, []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    crop_rect = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">    crop_rect2 = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">320</span>, <span class="number">480</span>) <span class="comment"># 从左上角开始截取形状</span></span><br><span class="line">    rect = paddle.vision.transforms.RandomCrop(size=crop_rect)._get_param(</span><br><span class="line">                            img=test_images[i].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)),</span><br><span class="line">                            output_size=crop_rect</span><br><span class="line">                            )</span><br><span class="line">    X = paddle.vision.transforms.functional.crop(test_images[i].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), *crop_rect2)</span><br><span class="line">    X = X.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    pred = label2image(predict(X)) <span class="comment"># 预测图</span></span><br><span class="line">    croped = paddle.vision.transforms.crop(test_labels[i], *rect)</span><br><span class="line">    croped = croped.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># 真实标签</span></span><br><span class="line">    X = X.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) <span class="comment"># 原图</span></span><br><span class="line">    imgs += [X, pred, paddle.vision.transforms.crop(test_labels[i], *crop_rect2).transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))]</span><br><span class="line"><span class="comment"># 第一行是真实图片，第二行是预测结果，第三行是标签图片</span></span><br><span class="line">ppl.show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n, scale=<span class="number">3</span>);</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/78.png" alt="svg"></p>
<h1 id="四、小结"><a href="#四、小结" class="headerlink" title="四、小结"></a>四、小结</h1><ul>
<li>语义分割通过将图像划分为属于不同语义类别的区域，来识别并理解图像中像素级别的内容。</li>
<li>由于语义分割的输入图像和标签在像素上一一对应，输入图像会被随机裁剪为固定尺寸而不是缩放。</li>
<li>与通过卷积核减少输入元素的常规卷积相反，转置卷积通过卷积核广播输入元素，从而产生形状大于输入的输出。</li>
<li>全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。</li>
<li>在全卷积网络中，我们可以将转置卷积层初始化为双线性插值的上采样。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space">小漁头&amp;小戴</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/">http://blog.dai2yutou.space/2023/01/04/深度学习7-1-图像分割任务的实现/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.dai2yutou.space" target="_blank">小漁头|小戴</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/paddle/">paddle</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%AB%98%E7%BA%A7-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">深度学习高级_计算机视觉之图像分割</a></div><div class="post_share"><div class="social-share" data-image="https://picbed.dai2yutou.space/web_img/19.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A06.4-%E5%85%B6%E4%BB%96%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习6.4-其他目标检测模型概述</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-2-%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84KITTI%E9%81%93%E8%B7%AF%E5%88%86%E5%89%B2/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习7.2-基于U-Net的KITTI道路分割</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/01/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07-2-%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84KITTI%E9%81%93%E8%B7%AF%E5%88%86%E5%89%B2/" title="深度学习7.2-基于U-Net的KITTI道路分割"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-04</div><div class="title">深度学习7.2-基于U-Net的KITTI道路分割</div></div></a></div><div><a href="/2022/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" title="深度学习1.1-深度学习概论"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-18</div><div class="title">深度学习1.1-深度学习概论</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="深度学习2.1-线性回归模型的实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.1-线性回归模型的实现</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/" title="深度学习2.2-神经网络中的分类任务"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.2-神经网络中的分类任务</div></div></a></div><div><a href="/2022/12/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="深度学习2.3-多层感知机的搭建与实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-23</div><div class="title">深度学习2.3-多层感知机的搭建与实现</div></div></a></div><div><a href="/2022/12/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03.1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8A%EF%BC%89/" title="深度学习3.1-模型选择与调优策略（上）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-25</div><div class="title">深度学习3.1-模型选择与调优策略（上）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="animate__fadeIn card-info card-widget wow" data-wow-delay="0" data-wow-duration="" data-wow-iteration="" data-wow-offset="" style="visibility: visible; animation-name: fadeIn;"><div class="author-info-top"><div class="card-info-avatar"><a class="avatar-img" data-pjax-state="" href="/about"><img class="entered loaded" alt="avatar" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apple-touch-icon.jpg" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;"/></a><div class="author-status-box"><div class="author-status"><g-emoji class="g-emoji" alias="palm_tree" fallback-src="/img/tree_icon.png">🐟</g-emoji><span>摸鱼中~</span></div></div></div></div><div class="author-info__sayhi" id="author-info__sayhi">晚安😴！我是</div><h1 class="author-info__name">XiaoYutou|XiaoDai</h1><div class="author-info__description">热爱生活点滴，分享时刻精彩。</div><a id="card-info-btn" data-pjax-state="" onclick="pjax.loadUrl(/about/)"><i></i><span style="padding-left:32px;font-weight:600;font-size:large">了解更多<i class="faa-passing animated" style="padding-left:-2px;display:inline-block;vertical-align:middle;"><span style="height:28px;width:28px;fill:currentColor;position:relative;top:-1.5px">💨</span></i></span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xiaoyutoua" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2143191301@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>主域名:<a target="_blank" rel="noopener" href="https://www.dai2yutou.space">小漁头|小戴</a><br><span>技术问题欢迎交流🧐</span><span color="#3eb8be">VX:yuguolong_001</span></center></div><div id="welcome-info"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="toc-text">一、图像分割</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-text">语义分割</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pascal-VOC2012-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">Pascal VOC2012 语义分割数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-text">预处理数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB"><span class="toc-text">自定义语义分割数据集类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6"><span class="toc-text">[整合所有组件]</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-text">二、转置卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-text">基本操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E3%80%81%E6%AD%A5%E5%B9%85%E5%92%8C%E5%A4%9A%E9%80%9A%E9%81%93"><span class="toc-text">填充、步幅和多通道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-text">填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85"><span class="toc-text">步幅</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-text">三、全卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. 构造模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">2. 初始化转置卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">3. 读取数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%AE%AD%E7%BB%83"><span class="toc-text">4. 训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E9%A2%84%E6%B5%8B"><span class="toc-text">5. 预测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%B0%8F%E7%BB%93"><span class="toc-text">四、小结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/07/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%9F%EF%BC%9F%EF%BC%9F/" title="如何学习动态规划？？？"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/28.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何学习动态规划？？？"/></a><div class="content"><a class="title" href="/2023/06/07/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%9F%EF%BC%9F%EF%BC%9F/" title="如何学习动态规划？？？">如何学习动态规划？？？</a><time datetime="2023-06-07T15:58:25.000Z" title="发表于 2023-06-07 23:58:25">2023-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/30/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%BD%AC%E7%9A%84%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="基于时间片轮转的进程管理系统的设计与实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/27.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基于时间片轮转的进程管理系统的设计与实现"/></a><div class="content"><a class="title" href="/2023/05/30/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%BD%AC%E7%9A%84%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="基于时间片轮转的进程管理系统的设计与实现">基于时间片轮转的进程管理系统的设计与实现</a><time datetime="2023-05-30T15:25:36.000Z" title="发表于 2023-05-30 23:25:36">2023-05-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/29/Python%E5%9B%9B%E5%A4%A7%E6%B3%95%E5%AE%9D/" title="Python四大法宝"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/Python/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python四大法宝"/></a><div class="content"><a class="title" href="/2023/05/29/Python%E5%9B%9B%E5%A4%A7%E6%B3%95%E5%AE%9D/" title="Python四大法宝">Python四大法宝</a><time datetime="2023-05-29T05:19:20.000Z" title="发表于 2023-05-29 13:19:20">2023-05-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 小漁头&小戴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.6/translate/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer src="/js/light.js"></script><canvas id="universe"></canvas><script defer src="/js/starry_sky.js"></script><script defer src="/js/console.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script async data-pjax src="/js/card_author.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JzK9w99AgP1g6fso",ck:"JzK9w99AgP1g6fso"})</script><script type="text/javascript" src ="/js/reward.js" ></script><script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.6.16/dist/sweetalert2.all.min.js"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="/js/txmap.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/4.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-02</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">英文水平不高，咋翻译论文？</a><div class="blog-slider__text">英文水平不高，咋翻译论文？</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/web_background2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-17</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">🐌博客搭建学习笔记</a><div class="blog-slider__text">这是再搭建博客已经写文章时遇到的bug和对博客的一些必要操作，不定时更新哦~</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/9.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">Butterfly外挂标签</a><div class="blog-slider__text">本文是撰写博客文章时可能会用到的外挂标签汇总，放到一起，便于查阅和使用</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/3.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-09</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">第一篇文章</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">Hexo发生error：spawn failed错误的解决方法</a><div class="blog-slider__text">Hexo发生error：spawn failed错误的解决方法</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">Hexo博客备份与恢复</a><div class="blog-slider__text">本文旨在解决在不同电脑上都能维护博客或配置、发布的内容丢失可恢复的问题。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-24</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">Echarts社区地址</a><div class="blog-slider__text">一些Echarts图标的开源网站。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = 'b16a1fa0e63c46a4b8f28abfb06ae3fe';
  var gaud_map_key = 'e2b04289e870b005374ee030148d64fd&s=rsv3';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/about/'|| '/about/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.dai2yutou.space/api?xiaoyutoua",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xiaoyutoua')
    }
  </script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '2');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__bounceInRight');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>