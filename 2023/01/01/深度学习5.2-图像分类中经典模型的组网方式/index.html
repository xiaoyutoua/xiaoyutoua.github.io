<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习5.2-图像分类中经典模型的组网方式 | 小漁头|小戴</title><meta name="author" content="小漁头&amp;小戴"><meta name="copyright" content="小漁头&amp;小戴"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文是深度学习的第十篇，本文介绍了五种图像分类的组网方式的深层原理，并给出代码示例加以理解，此代码较为复杂，用来理解模型即可。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习5.2-图像分类中经典模型的组网方式">
<meta property="og:url" content="http://blog.dai2yutou.space/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F/index.html">
<meta property="og:site_name" content="小漁头|小戴">
<meta property="og:description" content="本文是深度学习的第十篇，本文介绍了五种图像分类的组网方式的深层原理，并给出代码示例加以理解，此代码较为复杂，用来理解模型即可。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picbed.dai2yutou.space/web_img/19.png">
<meta property="article:published_time" content="2023-01-01T12:57:29.000Z">
<meta property="article:modified_time" content="2023-03-30T12:12:52.399Z">
<meta property="article:author" content="小漁头&amp;小戴">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="paddle">
<meta property="article:tag" content="深度学习基础_卷积基本概念及经典模型复现">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picbed.dai2yutou.space/web_img/19.png"><link rel="shortcut icon" href="/img/basketball.png"><link rel="canonical" href="http://blog.dai2yutou.space/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 小漁头&小戴","link":"链接: ","source":"来源: 小漁头|小戴","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习5.2-图像分类中经典模型的组网方式',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-30 20:12:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/css.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/at.alicdn.com/t/c/font_3829236_a49e40pee5.css"><link rel="stylesheet" href="/css/font-awesome.css"><link rel="stylesheet" href="/css/progress_bar.css"><link rel="stylesheet" href="/css/nav_menu.css"><link rel="stylesheet" href="/css/color.css"><link rel="apple-touch-icon" href="/img/apple-touch-icon.jpg"><meta name="apple-mobile-web-app-title" content="小漁头🏀"><link rel="bookmark" href="/img/apple-touch-icon.jpg"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/apple-touch-icon.jpg" ><link rel="stylesheet" href="/css/card_author.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (ture) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picbed.dai2yutou.space/web_img/19.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小漁头|小戴</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.dai2yutou.space/"><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><span> 📦归档</span></a></li><li><a class="site-page child" href="/tags/"><span> 🔖标签</span></a></li><li><a class="site-page child" href="/categories/"><span> 📂分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw hide"></i><span> 万花筒</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%94%A0%E5%97%91/"><span> 💭唠嗑</span></a></li><li><a class="site-page child" href="/HTML/%E6%96%B0%E5%B9%B4%E5%80%92%E8%AE%A1%E6%97%B6/index.html"><span> 🔐项目</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/love/"><span> 恋爱小屋</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/"><span> ⏰装修日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习5.2-图像分类中经典模型的组网方式</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-01T12:57:29.000Z" title="发表于 2023-01-01 20:57:29">2023-01-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-30T12:12:52.399Z" title="更新于 2023-03-30 20:12:52">2023-03-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>57分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习5.2-图像分类中经典模型的组网方式"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="图像分类中经典模型的组网方式"><a href="#图像分类中经典模型的组网方式" class="headerlink" title="图像分类中经典模型的组网方式"></a>图像分类中经典模型的组网方式</h1><p>图像分类是根据图像的语义信息对不同类别图像进行区分，是计算机视觉的核心，是物体检测、图像分割、物体跟踪、行为分析、人脸识别等其他高层次视觉任务的基础。</p>
<p>图像分类在许多领域都有着广泛的应用，如：安防领域的人脸识别和智能视频分析等，交通领域的交通场景识别，互联网领域基于内容的图像检索和相册自动归类，医学领域的图像识别等。</p>
<p>上一节主要介绍了卷积神经网络常用的一些基本模块，本节将基于眼疾分类数据集 <a target="_blank" rel="noopener" href="https://ai.baidu.com/broad/introduction">iChallenge-PM</a>，对图像分类领域的经典卷积神经网络进行剖析，介绍如何应用这些基础模块构建卷积神经网络，解决图像分类问题。按照被提出的时间顺序，涵盖如下卷积神经网络：</p>
<ul>
<li>LeNet：Yan LeCun 等人于 1998 年第一次将卷积神经网络应用到图像分类任务上[1]，在手写数字识别任务上取得了巨大成功。</li>
<li>AlexNet：Alex Krizhevsky 等人在 2012 年提出了AlexNet[2], 并应用在大尺寸图片数据集 ImageNet 上，获得了 2012 年 ImageNet 比赛冠军 （ImageNet Large Scale Visual Recognition Challenge，ILSVRC）。</li>
<li>VGG：Simonyan 和 Zisserman 于 2014 年提出了 VGG 网络结构[3]，是当前最流行的卷积神经网络之一，由于其结构简单、应用性极强而深受广大研究者欢迎。</li>
<li>GoogLeNet：Christian Szegedy 等人在 2014 提出了 GoogLeNet[4]，并取得了 2014 年 ImageNet 比赛冠军。</li>
<li>ResNet：Kaiming He 等人在 2015 年提出了 ResNet[5]，通过引入残差模块加深网络层数，在 ImagNet 数据集上的错误率降低到3.6%，超越了人眼识别水平。ResNet 的设计思想深刻地影响了后来的深度神经网络的设计。</li>
</ul>
<blockquote>
<p>各个组网中主要的模型搭建的部分，训练部分的代码都是差不多的！</p>
</blockquote>
<h1 id="一、LeNet"><a href="#一、LeNet" class="headerlink" title="一、LeNet"></a>一、LeNet</h1><p>LeNet 是最早的卷积神经网络之一，它是卷积神经网络的 HelloWorld。1998年，Yann LeCun 第一次将 LeNet 卷积神经网络应用到图像分类上，在手写数字识别任务中取得了巨大成功。<br><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/12e79a7c294e49dea0a0778ffe47161b851054d9e1ea4a5e859ffdb7c55e8366" width = "600"></center>
<center><br>图1：手写数字识别</br></center>

<p><br></br></p>
<p>LeNet 通过连续使用卷积和池化层的组合提取图像特征，其架构如 <strong>图2</strong> 所示，这里展示的是用于MNIST手写体数字识别任务中的 LeNet-5 模型：<br><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/82e4124e2e6a4231bcde17e086bc86ba732d3e81dcd7415f86fb4ef050aa7772" width = "800"></center>
<center><br>图2：LeNet模型网络结构示意图</br></center>

<p><br></br></p>
<ul>
<li><p>第一模块：包含 5×5 的 6 通道卷积和 2×2 的池化。卷积提取图像中包含的特征模式（激活函数使用 Sigmoid），图像尺寸从 28 减小到 24。经过池化层可以降低输出特征图对空间位置的敏感性，图像尺寸减到 12【高宽减半】。</p>
</li>
<li><p>第二模块：和第一模块尺寸相同，通道数由 6 增加为 16。卷积操作使图像尺寸减小到 8，经过池化后变成 4。</p>
</li>
<li><p>第三模块：包含 4×4 的 120 通道卷积。卷积之后的图像尺寸减小到 1，但是通道数增加为 120。将经过第 3 次卷积提取到的特征图输入到全连接层。第一个全连接层的输出神经元的个数是 64，第二个全连接层的输出神经元个数是分类标签的类别数，对于手写数字识别的类别数是 10。然后使用 Softmax 函数即可计算出每个类别的预测概率。</p>
</li>
</ul>
<hr>
<p><strong>【提示】：</strong></p>
<p>卷积层的输出特征图如何当作全连接层的输入使用呢？</p>
<p>卷积层的输出数据格式是$[N, C, H, W]$，在输入全连接层的时候，会自动将数据拉平，</p>
<p>也就是对每个样本，自动将其转化为长度为 $K$ 的向量，</p>
<p>其中 $K &#x3D; C \times H \times W$，一个 mini-batch 的数据维度变成了 $N\times K$ 的二维向量。</p>
<hr>
<h2 id="1-1-LeNet在手写数字识别上的应用"><a href="#1-1-LeNet在手写数字识别上的应用" class="headerlink" title="1.1 LeNet在手写数字识别上的应用"></a><strong>1.1 LeNet在手写数字识别上的应用</strong></h2><p>MNIST 手写数字数据库拥有 60,000 个样本的训练集和 10,000 个样本的测试集。<br><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/a35d80f579d44552b22c19f7d36128d944db99f95a28414099e695d96ef3fd63" width = "500"></center>
<center><br>图3：手写数字识别数据集</br></center>

<p><br></br></p>
<p>LeNet网络的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear</span><br><span class="line"></span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LeNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 下面定义层，如图2所示，一共七个层</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 尺寸的逻辑：输入层将数据拉平[B,C,H,W] -&gt; [B,C*H*W] [1,120,1,1]</span></span><br><span class="line">        <span class="comment"># 输入size是[28,28]，经过三次卷积和两次池化之后，C*H*W等于120</span></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">120</span>, out_features=<span class="number">64</span>)</span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">64</span>, out_features=num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 网络的前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># 尺寸的逻辑：输入层将数据拉平[B,C,H,W] -&gt; [B,C*H*W]</span></span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>]) <span class="comment"># -1：我不知道，机器算，size一致</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>]) <span class="comment"># paddle.reshape(x, shape)</span></span><br><span class="line">x.shape</span><br></pre></td></tr></table></figure>

<p>飞桨会根据实际图像数据的尺寸和卷积核参数自动推断中间层数据的 W 和 H 等，只需要用户表达通道数即可。下面的程序使用随机数作为输入，查看经过 LeNet-5 的每一层作用之后，输出数据的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据形状是 [N, 1, H, W]</span></span><br><span class="line"><span class="comment"># 这里用np.random创建一个随机数组作为输入数据</span></span><br><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line"><span class="comment"># print(&#x27;输入的X是：&#x27;, x)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建LeNet类的实例，指定模型名称和分类的类别数目</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 通过调用LeNet从基类继承的sublayers()函数，查看LeNet中所包含的子层</span></span><br><span class="line">model.sublayers()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="comment"># item是LeNet类中的一个子层</span></span><br><span class="line">    <span class="comment"># 查看经过子层之后的输出数据形状</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="comment"># 查看卷积和全连接层的数据和参数的形状，</span></span><br><span class="line">        <span class="comment"># 其中item.parameters()[0]是权重参数w，item.parameters()[1]是偏置参数b</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="number">0</span>].shape, item.parameters()[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 池化层没有参数</span></span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure>

<p>卷积 Conv2D 的 padding 参数默认为 0，stride 参数默认为 1。</p>
<ul>
<li>当输入形状为 [Bx1x28x28] 时，B 是 batch_size，经过第一层卷积（kernel_size&#x3D;5, out_channels&#x3D;6）和 maxpool 之后，得到形状为 [Bx6x12x12] 的特征图；</li>
<li>经过第二层卷积(kernel_size&#x3D;5, out_channels&#x3D;16)和 maxpool 之后，得到形状为 [Bx16x4x4] 的特征图；</li>
<li>经过第三层卷积(out_channels&#x3D;120, kernel_size&#x3D;4)之后，得到形状为 [Bx120x1x1] 的特征图；</li>
<li>在 FC 层计算之前，将输入特征从卷积得到的四维特征 reshape 到格式为 [B, 120x1x1] 的特征，这也是 LeNet 中第一层全连接层输入 shape 为 120 的原因。</li>
</ul>
<h3 id="【训练】"><a href="#【训练】" class="headerlink" title="【训练】"></a><strong>【训练】</strong></h3><p>下面我们将训练过程与验证过程打包好，最终我们将模型的参数保存下来，方便后续的调用。</p>
<p>具体的流程设计请参考之前的项目：<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/4957701"><strong>使用极简方法实现手写数字识别任务</strong></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># LeNet 识别手写数字</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">from</span> paddle.vision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, opt, train_loader, valid_loader</span>):</span><br><span class="line">    <span class="comment"># 开启0号GPU训练</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start training ... &#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            img = data[<span class="number">0</span>]</span><br><span class="line">            label = data[<span class="number">1</span>] </span><br><span class="line">            <span class="comment"># 计算模型输出</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            <span class="comment"># 计算损失函数</span></span><br><span class="line">            loss_func = paddle.nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">            loss = loss_func(logits, label)</span><br><span class="line">            avg_loss = paddle.mean(loss)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, batch_id, <span class="built_in">float</span>(avg_loss.numpy())))</span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.clear_grad()</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        accuracies = []</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_loader()):</span><br><span class="line">            img = data[<span class="number">0</span>]</span><br><span class="line">            label = data[<span class="number">1</span>] </span><br><span class="line">            <span class="comment"># 计算模型输出</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            pred = F.softmax(logits)</span><br><span class="line">            <span class="comment"># 计算损失函数</span></span><br><span class="line">            loss_func = paddle.nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">            loss = loss_func(logits, label)</span><br><span class="line">            acc = paddle.metric.accuracy(pred, label)</span><br><span class="line">            accuracies.append(acc.numpy())</span><br><span class="line">            losses.append(loss.numpy())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[validation] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(accuracies), np.mean(losses)))</span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型参数</span></span><br><span class="line">    paddle.save(model.state_dict(), <span class="string">&#x27;mnist.pdparams&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 设置迭代轮数</span></span><br><span class="line">EPOCH_NUM = <span class="number">5</span></span><br><span class="line"><span class="comment"># 设置优化器为 Momentum，学习率为 0.001</span></span><br><span class="line">opt = paddle.optimizer.Momentum(</span><br><span class="line">                                    learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                    momentum=<span class="number">0.9</span>, </span><br><span class="line">                                    parameters=model.parameters())</span><br><span class="line"><span class="comment"># 定义数据读取器</span></span><br><span class="line">train_loader = paddle.io.DataLoader(</span><br><span class="line">                                    MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=ToTensor()), </span><br><span class="line">                                    batch_size=<span class="number">10</span>, </span><br><span class="line">                                    shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = paddle.io.DataLoader(</span><br><span class="line">                                    MNIST(mode=<span class="string">&#x27;test&#x27;</span>, </span><br><span class="line">                                    transform=ToTensor()), </span><br><span class="line">                                    batch_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">train(model, opt, train_loader, valid_loader)</span><br></pre></td></tr></table></figure>

<h3 id="【预测】"><a href="#【预测】" class="headerlink" title="【预测】"></a><strong>【预测】</strong></h3><p>下面我们读入一张手写数字，测试模型的实际预测效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入图像读取第三方库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取一张本地的样例图片，转变成模型输入的格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">img_path</span>):</span><br><span class="line">    <span class="comment"># 从img_path中读取图像，并转为灰度图，进行下采样</span></span><br><span class="line">    im = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">    im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = <span class="number">1</span> - im / <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;work/1.png&#x27;</span>).convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">im</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/49.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义预测过程</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">10</span>)</span><br><span class="line">params_file_path = <span class="string">&#x27;mnist.pdparams&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;work/1.png&#x27;</span></span><br><span class="line"><span class="comment"># 加载模型参数</span></span><br><span class="line">param_dict = paddle.load(params_file_path)</span><br><span class="line">model.load_dict(param_dict)</span><br><span class="line"><span class="comment"># 灌入数据</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">tensor_img = load_image(img_path).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">result = model(paddle.to_tensor(tensor_img)).argmax(axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result&#x27;</span>, result)</span><br></pre></td></tr></table></figure>


<pre><code>result Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=False,
       [1])
</code></pre>
<p>通过运行结果可以看出，LeNet 在手写数字识别 MNIST 验证数据集上的准确率在 95% 以上。那么对于其它数据集效果如何呢？我们通过眼疾识别数据集 iChallenge-PM 验证一下。</p>
<h2 id="1-2-LeNet在眼疾识别数据集iChallenge-PM上的应用"><a href="#1-2-LeNet在眼疾识别数据集iChallenge-PM上的应用" class="headerlink" title="1.2 LeNet在眼疾识别数据集iChallenge-PM上的应用"></a><strong>1.2 LeNet在眼疾识别数据集iChallenge-PM上的应用</strong></h2><p><a target="_blank" rel="noopener" href="https://ai.baidu.com/broad/introduction">iChallenge-PM</a> 是百度大脑和中山大学中山眼科中心联合举办的 iChallenge 比赛中，提供的关于病理性近视（Pathologic Myopia，PM）的医疗类数据集，包含 1200 个受试者的眼底视网膜图片，训练、验证和测试数据集各 400 张。下面我们详细介绍 LeNet 在 iChallenge-PM 上的训练过程。</p>
<hr>
<p><strong>说明：</strong></p>
<p>如今近视已经成为困扰人们健康的一项全球性负担，在近视人群中，有超过 35% 的人患有重度近视。近视会拉长眼睛的光轴，也可能引起视网膜或者络网膜的病变。随着近视度数的不断加深，高度近视有可能引发病理性病变，这将会导致以下几种症状：视网膜或者络网膜发生退化、视盘区域萎缩、漆裂样纹损害、Fuchs 斑等。因此，及早发现近视患者眼睛的病变并采取治疗，显得非常重要。</p>
<p>数据可以从AI Studio<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/datasetdetail/19065">下载</a></p>
<hr>
<h3 id="1）数据集准备"><a href="#1）数据集准备" class="headerlink" title="1）数据集准备"></a><strong>1）数据集准备</strong></h3><p><code>/home/aistudio/data/data23828</code> 目录包括如下三个文件，解压缩后存放在 <code>/home/aistudio/work/palm</code> 目录下。</p>
<ul>
<li><strong>training.zip</strong>：包含训练中的图片和标签</li>
<li><strong>validation.zip</strong>：包含验证集的图片</li>
<li><strong>valid_gt.zip</strong>：包含验证集的标签</li>
</ul>
<p>!unzip相关指令可以查看以下链接：<a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-comm-unzip.html">!unzip指令</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果已经解压过，不需要运行此段代码（注释），否则由于文件已经存在，解压时会报错</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Extracting files... ...&#x27;</span>)</span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/training.<span class="built_in">zip</span></span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/validation.<span class="built_in">zip</span></span><br><span class="line">!unzip -q -d /home/aistudio/work/palm /home/aistudio/data/data23828/valid_gt.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;File extraction succeeded.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%cd /home/aistudio/work/palm/PALM-Training400/</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Extracting the training datasets... ...&#x27;</span>)</span><br><span class="line">!unzip -q PALM-Training400.<span class="built_in">zip</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;File extraction succeeded.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<blockquote>
<p><strong>【注意】</strong>：</p>
<p><code>valid_gt.zip</code>（验证集的标签）文件解压缩之后，需要将<code>/home/aistudio/work/palm/PALM-Validation-GT/</code>目录下的<code>PM_Label_and_Fovea_Location.xlsx</code>标签文件转存成<code>.csv</code>格式。</p>
</blockquote>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 转成文件labels.csv</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xlsx_to_csv_pd</span>():</span><br><span class="line">    <span class="comment"># 指定第0列为索引列</span></span><br><span class="line">    data_xls = pd.read_excel(<span class="string">&#x27;/home/aistudio/work/palm/PALM-Validation-GT/PM_Label_and_Fovea_Location.xlsx&#x27;</span>, index_col=<span class="number">0</span>)</span><br><span class="line">    data_xls.to_csv(<span class="string">&#x27;/home/aistudio/labels.csv&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    xlsx_to_csv_pd()</span><br></pre></td></tr></table></figure>

<h3 id="2）查看数据集图片"><a href="#2）查看数据集图片" class="headerlink" title="2）查看数据集图片"></a><strong>2）查看数据集图片</strong></h3><p>iChallenge-PM 中既有病理性近视患者的眼底图片，也有非病理性近视患者的图片，命名规则如下：</p>
<ul>
<li><p>病理性近视（PM）：文件名以P开头</p>
</li>
<li><p>非病理性近视（non-PM）：</p>
<ul>
<li><p>高度近视（high myopia）：文件名以H开头</p>
</li>
<li><p>正常眼睛（normal）：文件名以N开头</p>
</li>
</ul>
</li>
</ul>
<p>我们将病理性患者的图片作为正样本，标签为 1； 非病理性患者的图片作为负样本，标签为 0。下面从数据集中选取两张图片，并将图片显示出来。之后通过 LeNet 提取特征，构建分类器，对正负样本进行分类，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line"><span class="comment"># 文件名以N开头的是正常眼底图片，以P开头的是病变眼底图片</span></span><br><span class="line">file1 = <span class="string">&#x27;N0012.jpg&#x27;</span></span><br><span class="line">file2 = <span class="string">&#x27;P0095.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">img1 = Image.<span class="built_in">open</span>(os.path.join(DATADIR, file1))</span><br><span class="line">img1 = np.array(img1)</span><br><span class="line">img2 = Image.<span class="built_in">open</span>(os.path.join(DATADIR, file2))</span><br><span class="line">img2 = np.array(img2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出读取的图片，绘制子图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">f = plt.subplot(<span class="number">121</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;Normal&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.imshow(img1)</span><br><span class="line">f = plt.subplot(<span class="number">122</span>)</span><br><span class="line">f.set_title(<span class="string">&#x27;PM&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.imshow(img2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/50.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看图片形状</span></span><br><span class="line">img1.shape, img2.shape</span><br></pre></td></tr></table></figure>


<pre><code>((2056, 2124, 3), (2056, 2124, 3))
</code></pre>
<h3 id="3）定义数据读取器"><a href="#3）定义数据读取器" class="headerlink" title="3）定义数据读取器"></a><strong>3）定义数据读取器</strong></h3><p>使用 OpenCV 从磁盘读入图片，将每张图缩放到$224\times224$大小，并且将像素值调整到$[-1, 1]$之间，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对读入的图像数据进行预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_img</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="comment"># 将图片尺寸缩放道 224x224</span></span><br><span class="line">    img = cv2.resize(img, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># 读入的图像数据格式是[H, W, C]，使用转置操作将其变成[C, H, W]</span></span><br><span class="line">    img = np.transpose(img, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    img = img.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将数据范围调整到[-1.0, 1.0]之间</span></span><br><span class="line">    img = img / <span class="number">255.</span></span><br><span class="line">    img = img * <span class="number">2.0</span> - <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>

<br>

<p><strong>①定义训练集数据读取器</strong></p>
<p>训练集读取时通过文件名来确定样本标签，在读取的过程中随机打乱数据的顺序，最终返回以数据迭代器的形式返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; -------定义训练集数据读取器--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_loader</span>(<span class="params">datadir, batch_size=<span class="number">10</span>, mode = <span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># 将datadir目录下的文件列出来，每条文件都要读入</span></span><br><span class="line">    filenames = os.listdir(datadir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            <span class="comment"># 训练时随机打乱数据顺序</span></span><br><span class="line">            random.shuffle(filenames)</span><br><span class="line">        batch_imgs = []</span><br><span class="line">        batch_labels = []</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> filenames:</span><br><span class="line">            filepath = os.path.join(datadir, name)</span><br><span class="line">            img = cv2.imread(filepath)</span><br><span class="line">            img = transform_img(img)</span><br><span class="line">            <span class="keyword">if</span> name[<span class="number">0</span>] == <span class="string">&#x27;H&#x27;</span> <span class="keyword">or</span> name[<span class="number">0</span>] == <span class="string">&#x27;N&#x27;</span>:</span><br><span class="line">                <span class="comment"># H开头的文件名表示高度近似，N开头的文件名表示正常视力</span></span><br><span class="line">                <span class="comment"># 高度近视和正常视力的样本，都不是病理性的，属于负样本，标签为0</span></span><br><span class="line">                label = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> name[<span class="number">0</span>] == <span class="string">&#x27;P&#x27;</span>:</span><br><span class="line">                <span class="comment"># P开头的是病理性近视，属于正样本，标签为1</span></span><br><span class="line">                label = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span>(<span class="string">&#x27;Not excepted file name&#x27;</span>)</span><br><span class="line">            <span class="comment"># 每读取一个样本的数据，就将其放入数据列表中</span></span><br><span class="line">            batch_imgs.append(img)</span><br><span class="line">            batch_labels.append(label)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) == batch_size:</span><br><span class="line">                <span class="comment"># 当数据列表的长度等于batch_size的时候，</span></span><br><span class="line">                <span class="comment"># 把这些数据当作一个mini-batch，并作为数据生成器（yield）的一个输出</span></span><br><span class="line">                imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">                labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line">                batch_imgs = []</span><br><span class="line">                batch_labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch</span></span><br><span class="line">            imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure>

<br>

<p><strong>②定义验证集数据读取器</strong></p>
<p>训练集读取时通过文件名来确定样本标签，验证集则通过<code>labels.csv</code>来读取每个图片对应的标签。</p>
<p>查看解压后的验证集标签数据可以发现，<code>labels.csv</code>文件所包含的内容格式如下，每一行代表一个样本：</p>
<ul>
<li>其中第一列是图片id，第二列是文件名，第三列是图片标签，</li>
<li>第四列和第五列是<strong>正样本区域（Fovea）</strong> 的坐标，与分类任务无关。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; -------定义验证集数据读取器--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">valid_data_loader</span>(<span class="params">datadir, csvfile, batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;valid&#x27;</span></span>):</span><br><span class="line">    filelists = <span class="built_in">open</span>(csvfile).readlines()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reader</span>():</span><br><span class="line">        batch_imgs = []</span><br><span class="line">        batch_labels = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> filelists[<span class="number">1</span>:]:</span><br><span class="line">            line = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            name = line[<span class="number">1</span>]</span><br><span class="line">            label = <span class="built_in">int</span>(line[<span class="number">2</span>])</span><br><span class="line">            <span class="comment"># 根据图片文件名加载图片，并对图像数据作预处理</span></span><br><span class="line">            filepath = os.path.join(datadir, name)</span><br><span class="line">            img = cv2.imread(filepath)</span><br><span class="line">            img = transform_img(img)</span><br><span class="line">            <span class="comment"># 每读取一个样本的数据，就将其放入数据列表中</span></span><br><span class="line">            batch_imgs.append(img)</span><br><span class="line">            batch_labels.append(label)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) == batch_size:</span><br><span class="line">                <span class="comment"># 当数据列表的长度等于batch_size的时候，</span></span><br><span class="line">                <span class="comment"># 把这些数据当作一个mini-batch，并作为数据生成器的一个输出</span></span><br><span class="line">                imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">                labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line">                batch_imgs = []</span><br><span class="line">                batch_labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_imgs) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch</span></span><br><span class="line">            imgs_array = np.array(batch_imgs).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            labels_array = np.array(batch_labels).astype(<span class="string">&#x27;float32&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> imgs_array, labels_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看数据形状</span></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line">train_loader = data_loader(DATADIR, </span><br><span class="line">                           batch_size=<span class="number">10</span>, </span><br><span class="line">                           mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">data_reader = train_loader()</span><br><span class="line">data = <span class="built_in">next</span>(data_reader)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">0</span>].shape, data[<span class="number">1</span>].shape)</span><br><span class="line"></span><br><span class="line">eval_loader = data_loader(DATADIR, </span><br><span class="line">                           batch_size=<span class="number">10</span>, </span><br><span class="line">                           mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line">data_reader = eval_loader()</span><br><span class="line">data = <span class="built_in">next</span>(data_reader)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">0</span>].shape, data[<span class="number">1</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>(10, 3, 224, 224) (10, 1)
(10, 3, 224, 224) (10, 1)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(data_reader)</span><br></pre></td></tr></table></figure>




<h3 id="4）启动训练"><a href="#4）启动训练" class="headerlink" title="4）启动训练"></a><strong>4）启动训练</strong></h3><p>首先我们定义好训练和验证的基本流程，同时我们需要将模型训练 5 个迭代周期后的模型参数保存下来，方便我们后续进行评估，此处是为了模拟真实场景下的评估方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># LeNet 识别眼疾图片</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">DATADIR = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Training400/PALM-Training400&#x27;</span></span><br><span class="line">DATADIR2 = <span class="string">&#x27;/home/aistudio/work/palm/PALM-Validation400&#x27;</span></span><br><span class="line">CSVFILE = <span class="string">&#x27;/home/aistudio/labels.csv&#x27;</span></span><br><span class="line"><span class="comment"># 设置迭代轮数</span></span><br><span class="line">EPOCH_NUM = <span class="number">2</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_pm</span>(<span class="params">model, optimizer</span>):</span><br><span class="line">    <span class="comment"># 开启0号GPU训练</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start training ... &#x27;</span>)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    <span class="comment"># 定义数据读取器，训练数据读取器和验证数据读取器</span></span><br><span class="line">    train_loader = data_loader(DATADIR, batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    valid_loader = valid_data_loader(DATADIR2, CSVFILE)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader()):</span><br><span class="line">            x_data, y_data = data</span><br><span class="line">            img = paddle.to_tensor(x_data)</span><br><span class="line">            label = paddle.to_tensor(y_data)</span><br><span class="line">            <span class="comment"># 运行模型前向计算，得到预测值</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            loss = F.binary_cross_entropy_with_logits(logits, label)</span><br><span class="line">            avg_loss = paddle.mean(loss)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_id % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                iters.append(<span class="built_in">iter</span>)</span><br><span class="line">                train_losses.append(avg_loss.numpy())</span><br><span class="line">                <span class="built_in">iter</span> = <span class="built_in">iter</span> + <span class="number">2</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;, batch_id: &#123;&#125;, loss is: &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, batch_id, <span class="built_in">float</span>(avg_loss.numpy())))</span><br><span class="line">            <span class="comment"># 反向传播，更新权重，清除梯度</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.clear_grad()</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        accuracies = []</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_loader()):</span><br><span class="line">            x_data, y_data = data</span><br><span class="line">            img = paddle.to_tensor(x_data)</span><br><span class="line">            label = paddle.to_tensor(y_data)</span><br><span class="line">            <span class="comment"># 运行模型前向计算，得到预测值</span></span><br><span class="line">            logits = model(img)</span><br><span class="line">            <span class="comment"># 二分类，sigmoid计算后的结果以0.5为阈值分两个类别</span></span><br><span class="line">            <span class="comment"># 计算sigmoid后的预测概率，进行loss计算</span></span><br><span class="line">            pred = F.sigmoid(logits)</span><br><span class="line">            loss = F.binary_cross_entropy_with_logits(logits, label)</span><br><span class="line">            <span class="comment"># 计算预测概率小于0.5的类别</span></span><br><span class="line">            pred2 = pred * (-<span class="number">1.0</span>) + <span class="number">1.0</span></span><br><span class="line">            <span class="comment"># 得到两个类别的预测概率，并沿第一个维度级联</span></span><br><span class="line">            pred = paddle.concat([pred2, pred], axis=<span class="number">1</span>)</span><br><span class="line">            acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            accuracies.append(acc.numpy())</span><br><span class="line">            losses.append(loss.numpy())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[validation] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(accuracies), np.mean(losses)))</span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">        paddle.save(model.state_dict(), <span class="string">&#x27;palm.pdparams&#x27;</span>)</span><br><span class="line">        paddle.save(optimizer.state_dict(), <span class="string">&#x27;palm.pdopt&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> iters, losses</span><br></pre></td></tr></table></figure>

<p>接下来我们定义评估的过程，首先需要将之前保存的模型参数重新载入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义评估过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluation</span>(<span class="params">model, params_file_path</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启0号GPU预估</span></span><br><span class="line">    use_gpu = <span class="literal">True</span></span><br><span class="line">    paddle.device.set_device(<span class="string">&#x27;gpu:0&#x27;</span>) <span class="keyword">if</span> use_gpu <span class="keyword">else</span> paddle.device.set_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start evaluation .......&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载模型参数</span></span><br><span class="line">    model_state_dict = paddle.load(params_file_path)</span><br><span class="line">    model.load_dict(model_state_dict)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    eval_loader = data_loader(DATADIR, </span><br><span class="line">                        batch_size=<span class="number">10</span>, mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    acc_set = []</span><br><span class="line">    avg_loss_set = []</span><br><span class="line">    <span class="keyword">for</span> batch_id, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_loader()):</span><br><span class="line">        x_data, y_data = data</span><br><span class="line">        img = paddle.to_tensor(x_data)</span><br><span class="line">        label = paddle.to_tensor(y_data)</span><br><span class="line">        y_data = y_data.astype(np.int64)</span><br><span class="line">        label_64 = paddle.to_tensor(y_data)</span><br><span class="line">        <span class="comment"># 计算预测和精度</span></span><br><span class="line">        prediction, acc = model(img, label_64)</span><br><span class="line">        <span class="comment"># 计算损失函数值</span></span><br><span class="line">        loss = F.binary_cross_entropy_with_logits(prediction, label)</span><br><span class="line">        avg_loss = paddle.mean(loss)</span><br><span class="line">        acc_set.append(<span class="built_in">float</span>(acc.numpy()))</span><br><span class="line">        avg_loss_set.append(<span class="built_in">float</span>(avg_loss.numpy()))</span><br><span class="line">    <span class="comment"># 求平均精度</span></span><br><span class="line">    acc_val_mean = np.array(acc_set).mean()</span><br><span class="line">    avg_loss_val_mean = np.array(avg_loss_set).mean()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss=&#123;:.4f&#125;, acc=&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(avg_loss_val_mean, acc_val_mean))</span><br></pre></td></tr></table></figure>

<p>对比本章最初定义的 LeNet，发现两个 LeNet 的第一层全连接层的输入特征维度不同，一个是 120，一个是 300000。</p>
<p>这个不同是由输入数据的形状不同引起的，手写数字识别的图像输入形状比较小，第三层卷积之前的特征维度是 [B, 120x1x1]，但是 PALM 数据的输入数据形状较大，形状为 [B, 120x50x50]，120x50x50 等于 300000，所以不同的输入大小，会影响卷积后全连接层的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, Dropout</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LeNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># 创建卷积和池化层块，每个卷积层使用Sigmoid激活函数，后面跟着一个2x2的池化</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 创建第3个卷积层</span></span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">16</span>, out_channels=<span class="number">120</span>, kernel_size=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 创建全连接层，第一个全连接层的输出神经元个数为64</span></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">300000</span>, out_features=<span class="number">64</span>)</span><br><span class="line">        <span class="comment"># 第二个全连接层输出神经元个数为分类标签的类别数</span></span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">64</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 网络的前向计算过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, label=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> self.num_classes == <span class="number">1</span>:</span><br><span class="line">                pred = F.sigmoid(x)</span><br><span class="line">                pred = paddle.concat([<span class="number">1.0</span> - pred, pred], axis=<span class="number">1</span>)</span><br><span class="line">                acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc = paddle.metric.accuracy(x, paddle.cast(label, dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line">            <span class="keyword">return</span> x, acc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">model = LeNet(num_classes=<span class="number">1</span>)</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="number">0</span>].shape, item.parameters()[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure>

<pre><code>conv2d_12 [1, 6, 220, 220] [6, 3, 5, 5] [6]
max_pool2d_8 [1, 6, 110, 110]
conv2d_13 [1, 16, 106, 106] [16, 6, 5, 5] [16]
max_pool2d_9 [1, 16, 53, 53]
conv2d_14 [1, 120, 50, 50] [120, 16, 4, 4] [120]
linear_8 [1, 64] [300000, 64] [64]
linear_9 [1, 1] [64, 1] [1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = LeNet(num_classes=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters())</span><br><span class="line">iters, train_losses = train_pm(model, optimizer=opt)</span><br><span class="line">evaluation(model, params_file_path=<span class="string">&quot;palm.pdparams&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_losses</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;LeNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters, train_losses, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/51.png" alt="png"></p>
<p>通过运行结果可以看出，在眼疾筛查数据集iChallenge-PM上，LeNet的loss很难下降，模型没有收敛。这是因为MNIST数据集的图片尺寸比较小（$28\times28$），但是眼疾筛查数据集图片尺寸比较大（原始图片尺寸约为$2000 \times 2000$，经过缩放之后变成$224 \times 224$），LeNet模型很难进行有效分类。这说明在图片尺寸比较大时，LeNet在图像分类任务上存在局限性。</p>
<h1 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h1><p>通过上面的实际训练可以看到，虽然LeNet在手写数字识别数据集上取得了很好的结果，但在更大的数据集上表现却并不好。自从1998年LeNet问世以来，接下来十几年的时间里，神经网络并没有在计算机视觉领域取得很好的结果，反而一度被其它算法所超越。原因主要有两方面，一是神经网络的计算比较复杂，对当时计算机的算力来说，训练神经网络是件非常耗时的事情；另一方面，当时还没有专门针对神经网络做算法和训练技巧的优化，神经网络的收敛是件非常困难的事情。</p>
<p>随着技术的进步和发展，计算机的算力越来越强大，尤其是在GPU并行计算能力的推动下，复杂神经网络的计算也变得更加容易实施。另一方面，互联网上涌现出越来越多的数据，极大的丰富了数据库。同时也有越来越多的研究人员开始专门针对神经网络做算法和模型的优化，Alex Krizhevsky等人提出的AlexNet以很大优势获得了2012年ImageNet比赛的冠军。这一成果极大的激发了产业界对神经网络的兴趣，开创了使用深度神经网络解决图像问题的途径，随后也在这一领域涌现出越来越多的优秀成果。</p>
<p>AlexNet的具体结构如 <strong>图4</strong> 所示：</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/630059b01a9a4e8c8eded2e7584412daa27bc7c034a8441fabadd713dac29d77" width = "1000"></center>
<center><br>图4：AlexNet模型网络结构示意图</br></center>

<p><br></br></p>
<h2 id="AlexNet的重要构成"><a href="#AlexNet的重要构成" class="headerlink" title="AlexNet的重要构成"></a><strong>AlexNet的重要构成</strong></h2><p>AlexNet与LeNet相比，具有更深的网络结构，包含5层卷积和3层全连接，同时使用了如下三种方法改进模型的训练过程：</p>
<ul>
<li><p>数据增广：深度学习中常用的一种处理方式，通过对训练随机加一些变化，比如平移、缩放、裁剪、旋转、翻转或者增减亮度等，产生一系列跟原始图片相似但又不完全相同的样本，从而扩大训练数据集。通过这种方式，可以随机改变训练样本，避免模型过度依赖于某些属性，能从一定程度上抑制过拟合。</p>
</li>
<li><p>使用Dropout抑制过拟合。</p>
</li>
<li><p>使用ReLU激活函数减少梯度消失现象。</p>
</li>
</ul>
<hr>
<p><strong>说明：</strong></p>
<p>之后会详细介绍数据增广的具体实现方式。</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/91c51b2d7dac42338bf91df0120ad202a2c4f937abe840889f4dce024a3fde1d" width = "500"></center>

<center><br>图5：图像增广技术</br></center>

<p><br></br></p>
<hr>
<h3 id="1）激活函数的改变"><a href="#1）激活函数的改变" class="headerlink" title="1）激活函数的改变"></a><strong>1）激活函数的改变</strong></h3><p>AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。<br><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/cc4b7ab6ebe844aa9a833c4e5bfdd94108bf7f60cee14145807be3f19621e61c" width = "700"></center>
<center><br>图6：激活函数的改变</br></center>

<p><br></br></p>
<p>一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 当sigmoid激活函数的输出非常接近于正无穷或负无穷时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p>
<h3 id="2）容量控制和预处理"><a href="#2）容量控制和预处理" class="headerlink" title="2）容量控制和预处理"></a><strong>2）容量控制和预处理</strong></h3><p>AlexNet通过Dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减。<br>Dropout的意思是每次训练的时候随机损失掉一些神经元，这些神经元被Dropped-out了，换句话讲，这些神经元在正向传播时对下游的启动影响被忽略，反向传播时也不会更新权重。Dropout的效果是，网络对某个神经元的权重变化更不敏感，增加泛化能力，减少过拟合。</p>
<p>AlexNet在眼疾筛查数据集iChallenge-PM上具体实现的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, Dropout</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 AlexNet 网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        <span class="comment"># AlexNet与LeNet一样也会同时使用卷积和池化层提取图像特征</span></span><br><span class="line">        <span class="comment"># 与LeNet不同的是激活函数换成了‘relu’</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">5</span>)</span><br><span class="line">        self.max_pool1 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2D(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.max_pool2 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2D(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = Conv2D(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = Conv2D(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.max_pool5 = MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = Linear(in_features=<span class="number">12544</span>, out_features=<span class="number">4096</span>)</span><br><span class="line">        self.drop_ratio1 = <span class="number">0.5</span></span><br><span class="line">        self.drop1 = Dropout(self.drop_ratio1)</span><br><span class="line">        self.fc2 = Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>)</span><br><span class="line">        self.drop_ratio2 = <span class="number">0.5</span></span><br><span class="line">        self.drop2 = Dropout(self.drop_ratio2)</span><br><span class="line">        self.fc3 = Linear(in_features=<span class="number">4096</span>, out_features=num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.max_pool5(x)</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="comment"># 在全连接之后使用dropout抑制过拟合</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>我们构造一个高度和宽度都为224，3通道的数据来观察每一层输出的形状。 它与AlexNet架构相匹配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = AlexNet()</span><br><span class="line">x = paddle.randn(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> model.sublayers():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = item(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item.parameters())==<span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(item.full_name(), x.shape)</span><br></pre></td></tr></table></figure>

<pre><code>conv2d_18 [1, 96, 56, 56]
max_pool2d_12 [1, 96, 28, 28]
conv2d_19 [1, 256, 28, 28]
max_pool2d_13 [1, 256, 14, 14]
conv2d_20 [1, 384, 14, 14]
conv2d_21 [1, 384, 14, 14]
conv2d_22 [1, 256, 14, 14]
max_pool2d_14 [1, 256, 7, 7]
linear_12 [1, 4096]
dropout_0 [1, 4096]
linear_13 [1, 4096]
dropout_1 [1, 4096]
linear_14 [1, 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = AlexNet()</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">opt = paddle.optimizer.Adam(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                            parameters=model.parameters())</span><br><span class="line"></span><br><span class="line">iters_A, train_losses_A = train_pm(model, optimizer=opt)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;AlexNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_A, train_losses_A, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/52.png" alt="png"></p>
<p>通过运行结果可以发现，在眼疾筛查数据集iChallenge-PM上使用AlexNet，loss能有效下降，经过5个epoch的训练，在验证集上的准确率可以达到94%左右。</p>
<h1 id="三、VGG"><a href="#三、VGG" class="headerlink" title="三、VGG"></a>三、VGG</h1><p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。这也就是为什么有人会问：AlexNet为什么要这么设计？想要把网络设计的更好，我应该怎么去优化我的网络？在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。</p>
<p>VGG是当前最流行的CNN模型之一，2014年由Simonyan和Zisserman提出，其命名来源于论文作者所在的实验室视觉几何组(Visual Geometry Group)。</p>
<p>AlexNet模型通过构造多层网络，取得了较好的效果，但是并没有给出深度神经网络设计的方向。VGG通过使用<strong>一系列大小为3x3的小尺寸卷积核和池化层</strong>构造深度卷积神经网络，并取得了较好的效果。VGG模型因为结构简单、应用性极强而广受研究者欢迎，尤其是它的网络结构设计方法，为构建深度神经网络提供了方向。</p>
<p><strong>图7</strong> 是VGG-16的网络结构示意图，有13层卷积和3层全连接层。VGG网络的设计严格使用$3\times 3$的卷积层和池化层来提取特征，并在网络的最后面使用三层全连接层，将最后一层全连接层的输出作为分类的预测。在VGG中每层卷积将使用ReLU作为激活函数，在全连接层之后添加dropout来抑制过拟合。</p>
<p>VGG的创新之处在于使用两层$3\times 3$卷积层，而不是使用$5 \times 5$的卷积层，为什么这么做呢？</p>
<p>原因是使用小的卷积核能够有效地减少参数的个数，使得训练和测试变得更加有效。由于卷积核比较小，可以堆叠更多的卷积层，加深网络的深度，这对于图像分类任务来说是有利的。VGG模型的成功证明了增加网络的深度，可以更好的学习图像中的特征模式。</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/3b6e1725e5934d2293e03b9c0a83e1d48660137f3c4449ba89bf9766d4380f3a" width = "1000"></center>
<center><br>图7：VGG模型网络结构示意图</br></center>

<p><br></br></p>
<p>VGG在眼疾识别数据集iChallenge-PM上的具体实现如下代码所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># VGG模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="comment"># from paddle.nn import Conv2D, MaxPool2D, BatchNorm, Linear</span></span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, BatchNorm2D, Linear</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义vgg网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        in_channels = [<span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">512</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义第一个block，包含两个卷积</span></span><br><span class="line">        self.conv1_1 = Conv2D(in_channels=in_channels[<span class="number">0</span>], out_channels=in_channels[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv1_2 = Conv2D(in_channels=in_channels[<span class="number">1</span>], out_channels=in_channels[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第二个block，包含两个卷积</span></span><br><span class="line">        self.conv2_1 = Conv2D(in_channels=in_channels[<span class="number">1</span>], out_channels=in_channels[<span class="number">2</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2_2 = Conv2D(in_channels=in_channels[<span class="number">2</span>], out_channels=in_channels[<span class="number">2</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第三个block，包含三个卷积</span></span><br><span class="line">        self.conv3_1 = Conv2D(in_channels=in_channels[<span class="number">2</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3_2 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3_3 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">3</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第四个block，包含三个卷积</span></span><br><span class="line">        self.conv4_1 = Conv2D(in_channels=in_channels[<span class="number">3</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4_2 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv4_3 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">4</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义第五个block，包含三个卷积</span></span><br><span class="line">        self.conv5_1 = Conv2D(in_channels=in_channels[<span class="number">4</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5_2 = Conv2D(in_channels=in_channels[<span class="number">5</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv5_3 = Conv2D(in_channels=in_channels[<span class="number">5</span>], out_channels=in_channels[<span class="number">5</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用Sequential 将全连接层和relu组成一个线性结构（fc + relu）</span></span><br><span class="line">        <span class="comment"># 当输入为224x224时，经过五个卷积块和池化层后，特征维度变为[512x7x7]</span></span><br><span class="line">        self.fc1 = paddle.nn.Sequential(paddle.nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), paddle.nn.ReLU())</span><br><span class="line">        self.drop1_ratio = <span class="number">0.5</span></span><br><span class="line">        self.dropout1 = paddle.nn.Dropout(self.drop1_ratio, mode=<span class="string">&#x27;upscale_in_train&#x27;</span>)</span><br><span class="line">        <span class="comment"># 使用Sequential 将全连接层和relu组成一个线性结构（fc + relu）</span></span><br><span class="line">        self.fc2 = paddle.nn.Sequential(paddle.nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), paddle.nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.drop2_ratio = <span class="number">0.5</span></span><br><span class="line">        self.dropout2 = paddle.nn.Dropout(self.drop2_ratio, mode=<span class="string">&#x27;upscale_in_train&#x27;</span>)</span><br><span class="line">        self.fc3 = paddle.nn.Linear(<span class="number">4096</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = paddle.nn.ReLU()</span><br><span class="line">        self.pool = MaxPool2D(stride=<span class="number">2</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu(self.conv1_1(x))</span><br><span class="line">        x = self.relu(self.conv1_2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv2_1(x))</span><br><span class="line">        x = self.relu(self.conv2_2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv3_1(x))</span><br><span class="line">        x = self.relu(self.conv3_2(x))</span><br><span class="line">        x = self.relu(self.conv3_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv4_1(x))</span><br><span class="line">        x = self.relu(self.conv4_2(x))</span><br><span class="line">        x = self.relu(self.conv4_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.relu(self.conv5_1(x))</span><br><span class="line">        x = self.relu(self.conv5_2(x))</span><br><span class="line">        x = self.relu(self.conv5_3(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = paddle.flatten(x, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        x = self.dropout1(self.relu(self.fc1(x)))</span><br><span class="line">        x = self.dropout2(self.relu(self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = VGG()</span><br><span class="line">model.sublayers()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_V, train_losses_V = train_pm(model, optimizer=opt)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;VGGNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_V, train_losses_V, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>通过运行结果可以发现，在眼疾筛查数据集iChallenge-PM上使用VGG，loss能有效的下降，经过5个epoch的训练，在验证集上的准确率可以达到94%左右。</p>
<h1 id="四、GoogLeNet"><a href="#四、GoogLeNet" class="headerlink" title="四、GoogLeNet"></a>四、GoogLeNet</h1><p>GoogLeNet是2014年ImageNet比赛的冠军，它的主要特点是网络不仅有深度，还在横向上具有“宽度”。由于图像信息在空间尺寸上的巨大差异，如何选择合适的卷积核来提取特征就显得比较困难了。空间分布范围更广的图像信息适合用较大的卷积核来提取其特征；而空间分布范围较小的图像信息则适合用较小的卷积核来提取其特征。为了解决这个问题，GoogLeNet提出了一种被称为Inception模块的方案。如 <strong>图8</strong> 所示：</p>
<hr>
<p><strong>说明：</strong></p>
<ul>
<li>Google的研究人员为了向LeNet致敬，特地将模型命名为GoogLeNet。</li>
<li>Inception一词来源于电影《盗梦空间》（Inception）。</li>
</ul>
<hr>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/ebc171e0281549a9b6aace1113f92fb72df08b947059446ca62a07b9af22e4b4" width = "1000"></center>
<center><br>图8：Inception模块结构示意图</br></center>

<p><br></br></p>
<p>图8(a)是Inception模块的设计思想，使用3个不同大小的卷积核对输入图片进行卷积操作，并附加最大池化，将这4个操作的输出沿着通道这一维度进行拼接，构成的输出特征图将会包含经过不同大小的卷积核提取出来的特征，从而达到捕捉不同尺度信息的效果。</p>
<p>Inception模块采用多通路(multi-path)的设计形式，每个支路使用不同大小的卷积核，最终输出特征图的通道数是每个支路输出通道数的总和，这将会导致输出通道数变得很大，尤其是使用多个Inception模块串联操作的时候，模型参数量会变得非常大。</p>
<p>为了减小参数量，Inception模块使用了图(b)中的设计方式，在每个3x3和5x5的卷积层之前，增加1x1的卷积层来控制输出通道数；在最大池化层后面增加1x1卷积层减小输出通道数。基于这一设计思想，形成了上图(b)中所示的结构。下面这段程序是Inception块的具体实现方式，可以对照图(b)和代码一起阅读。</p>
<hr>
<p><strong>提示：</strong></p>
<p>可能有读者会问，经过3x3的最大池化之后图像尺寸不会减小吗，为什么还能跟另外3个卷积输出的特征图进行拼接？这是因为池化操作可以指定窗口大小$k_h &#x3D; k_w &#x3D; 3$，stride&#x3D;1和padding&#x3D;1，输出特征图尺寸可以保持不变。</p>
<hr>
<p>Inception模块的具体实现如下代码所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GoogLeNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Inception块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c0, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inception模块的实现代码，</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        c1,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        c2,图(b)中第二条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c2[0]是1x1卷积的输出通道数，c2[1]是3x3</span></span><br><span class="line"><span class="string">        c3,图(b)中第三条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c3[0]是1x1卷积的输出通道数，c3[1]是3x3</span></span><br><span class="line"><span class="string">        c4,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 依次创建Inception块每条支路上使用到的操作</span></span><br><span class="line">        self.p1_1 = Conv2D(in_channels=c0,out_channels=c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_1 = Conv2D(in_channels=c0,out_channels=c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = Conv2D(in_channels=c2[<span class="number">0</span>],out_channels=c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p3_1 = Conv2D(in_channels=c0,out_channels=c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = Conv2D(in_channels=c3[<span class="number">0</span>],out_channels=c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.p4_1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = Conv2D(in_channels=c0,out_channels=c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 支路1只包含一个1x1卷积</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 支路2包含 1x1卷积 + 3x3卷积</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        <span class="comment"># 支路3包含 1x1卷积 + 5x5卷积</span></span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        <span class="comment"># 支路4包含 最大池化和1x1卷积</span></span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 将每个支路的输出特征图拼接在一起作为最终的输出结果</span></span><br><span class="line">        <span class="keyword">return</span> paddle.concat([p1, p2, p3, p4], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>GoogLeNet的架构如 <strong>图9</strong> 所示，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3 ×3最大池化层来减小输出高宽。</p>
<ul>
<li>第一模块使用一个64通道的7 × 7卷积层。</li>
<li>第二模块使用2个卷积层:首先是64通道的1 × 1卷积层，然后是将通道增大3倍的3 × 3卷积层。</li>
<li>第三模块串联2个完整的Inception块。</li>
<li>第四模块串联了5个Inception块。</li>
<li>第五模块串联了2 个Inception块。</li>
<li>第五模块的后面紧跟输出层，使用全局平均池化层来将每个通道的高和宽变成1，最后接上一个输出个数为标签类别数的全连接层。</li>
</ul>
<hr>
<blockquote>
<p>说明：<br>在原作者的论文中添加了图中所示的softmax1和softmax2两个辅助分类器，如下图所示，训练时将三个分类器的损失函数进行加权求和，以缓解梯度消失现象。这里的程序作了简化，没有加入辅助分类器。</p>
</blockquote>
<hr>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/9d0794b330934bc9be72cba9f056d62eb77d3ba6c2ac450fae64cf86d86f2e04" width = "800"></center>
<center><br>图9：GoogLeNet模型网络结构示意图</br></center>

<p><br></br></p>
<p>GoogLeNet的具体实现如下代码所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GoogLeNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear</span><br><span class="line"><span class="comment">## 组网</span></span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Inception块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c0, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inception模块的实现代码，</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        c1,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        c2,图(b)中第二条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c2[0]是1x1卷积的输出通道数，c2[1]是3x3</span></span><br><span class="line"><span class="string">        c3,图(b)中第三条支路卷积的输出通道数，数据类型是tuple或list, </span></span><br><span class="line"><span class="string">               其中c3[0]是1x1卷积的输出通道数，c3[1]是3x3</span></span><br><span class="line"><span class="string">        c4,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 依次创建Inception块每条支路上使用到的操作</span></span><br><span class="line">        self.p1_1 = Conv2D(in_channels=c0,out_channels=c1, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p2_1 = Conv2D(in_channels=c0,out_channels=c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = Conv2D(in_channels=c2[<span class="number">0</span>],out_channels=c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p3_1 = Conv2D(in_channels=c0,out_channels=c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = Conv2D(in_channels=c3[<span class="number">0</span>],out_channels=c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.p4_1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = Conv2D(in_channels=c0,out_channels=c4, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 新加一层batchnorm稳定收敛【批量归一化层】</span></span><br><span class="line">        self.batchnorm = paddle.nn.BatchNorm2D(c1+c2[<span class="number">1</span>]+c3[<span class="number">1</span>]+c4)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 支路1只包含一个1x1卷积</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 支路2包含 1x1卷积 + 3x3卷积</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        <span class="comment"># 支路3包含 1x1卷积 + 5x5卷积</span></span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        <span class="comment"># 支路4包含 最大池化和1x1卷积</span></span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 将每个支路的输出特征图拼接在一起作为最终的输出结果</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.batchnorm(paddle.concat([p1, p2, p3, p4], axis=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        <span class="comment"># GoogLeNet包含五个模块，每个模块后面紧跟一个池化层</span></span><br><span class="line">        <span class="comment"># 第一个模块包含1个卷积层</span></span><br><span class="line">        self.conv1 = Conv2D(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool1 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第二个模块包含2个卷积层</span></span><br><span class="line">        self.conv2_1 = Conv2D(in_channels=<span class="number">64</span>,out_channels=<span class="number">64</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv2_2 = Conv2D(in_channels=<span class="number">64</span>,out_channels=<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool2 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第三个模块包含2个Inception块</span></span><br><span class="line">        self.block3_1 = Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>)</span><br><span class="line">        self.block3_2 = Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool3 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第四个模块包含5个Inception块</span></span><br><span class="line">        self.block4_1 = Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_2 = Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_3 = Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_4 = Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>)</span><br><span class="line">        self.block4_5 = Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 3x3最大池化</span></span><br><span class="line">        self.pool4 = MaxPool2D(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第五个模块包含2个Inception块</span></span><br><span class="line">        self.block5_1 = Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        self.block5_2 = Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 全局池化，用的是global_pooling，不需要设置pool_stride</span></span><br><span class="line">        self.pool5 = AdaptiveAvgPool2D(output_size=<span class="number">1</span>)</span><br><span class="line">        self.fc = Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))</span><br><span class="line">        x = self.pool3(self.block3_2(self.block3_1(x)))</span><br><span class="line">        x = self.block4_3(self.block4_2(self.block4_1(x)))</span><br><span class="line">        x = self.pool4(self.block4_5(self.block4_4(x)))</span><br><span class="line">        x = self.pool5(self.block5_2(self.block5_1(x)))</span><br><span class="line">        x = paddle.reshape(x, [x.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = GoogLeNet()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(model.parameters()))</span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters(), </span><br><span class="line">                                weight_decay=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_G, train_losses_G = train_pm(model, opt)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练过程中Loss的变化曲线</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">&quot;GoogLeNet-train loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(iters_G, train_losses_G, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>通过运行结果可以发现，使用GoogLeNet在眼疾筛查数据集iChallenge-PM上，loss能有效的下降。</p>
<h1 id="五、ResNet"><a href="#五、ResNet" class="headerlink" title="五、ResNet"></a>五、ResNet</h1><p>ResNet是2015年ImageNet比赛的冠军，将识别错误率降低到了3.6%，这个结果甚至超出了正常人眼识别的精度。</p>
<p>通过前面几个经典模型学习，我们可以发现随着深度学习的不断发展，模型的层数越来越多，网络结构也越来越复杂。那么是否加深网络结构，就一定会得到更好的效果呢？从理论上来说，假设新增加的层都是恒等映射，只要原有的层学出跟原模型一样的参数，那么深模型结构就能达到原模型结构的效果。换句话说，原模型的解只是新模型的解的子空间，在新模型解的空间里应该能找到比原模型解对应的子空间更好的结果。但是实践表明，增加网络的层数之后，训练误差往往不降反升。</p>
<p>Kaiming He等人提出了残差网络ResNet来解决上述问题，其基本思想如 <strong>图10</strong>所示。</p>
<ul>
<li>图10(a)：表示增加网络的时候，将$x$映射成$y&#x3D;F(x)$输出。</li>
<li>图10(b)：对图10(a)作了改进，输出$y&#x3D;F(x) + x$。这时不是直接学习输出特征$y$的表示，而是学习$y-x$。<ul>
<li>如果想学习出原模型的表示，只需将$F(x)$的参数全部设置为0，则$y&#x3D;x$是恒等映射。</li>
<li>$F(x) &#x3D; y - x$也叫做残差项，如果$x\rightarrow y$的映射接近恒等映射，图10(b)中通过学习残差项也比图10(a)学习完整映射形式更加容易。</li>
</ul>
</li>
</ul>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/e10f22f054704daabf4261ab46719629a36749631db74eb0a368499de3e5d3d6" width = "500"></center>
<center><br>图10：残差块设计思想</br></center>

<p><br></br></p>
<p>图10(b)的结构是残差网络的基础，这种结构也叫做<strong>残差块（Residual block）</strong>。输入$x$通过跨层连接，能更快的向前传播数据，或者向后传播梯度。通俗的比喻，在火热的电视节目《王牌对王牌》上有一个“传声筒”的游戏，排在队首的嘉宾把看到的影视片段表演给后面一个嘉宾看，经过四五个嘉宾后，最后一个嘉宾如果能表演出更多原剧的内容，就能取得高分。我们常常会发现刚开始的嘉宾往往表演出最多的信息（类似于Loss），而随着表演的传递，有效的表演信息越来越少（类似于梯度弥散）。如果每个嘉宾都能看到原始的影视片段，那么相信传声筒的效果会好很多。类似的，由于ResNet每层都存在直连的旁路，相当于每一层都和最终的损失有“直接对话”的机会，自然可以更好的解决梯度弥散的问题。</p>
<p>残差块的具体设计方案如 <strong>图11</strong> 所示，这种设计方案也常称作瓶颈结构（BottleNeck）。1*1的卷积核可以非常方便的调整中间层的通道数，在进入3*3的卷积层之前减少通道数（256-&gt;64），经过该卷积层后再恢复通道数(64-&gt;256)，可以显著减少网络的参数量。这个结构（256-&gt;64-&gt;256）像一个中间细，两头粗的瓶颈，所以被称为“BottleNeck”。<br><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/322b26358d43401ba81546dd134a310cfb11ecafb3314aab88b5885ff642870b" width = "500"></center>
<center><br>图11：残差块结构示意图</br></center>

<p><br></br></p>
<p>下图表示出了ResNet-50的结构，一共包含49层卷积和1层全连接，所以被称为ResNet-50。</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/8f42b3b5b7b34e45847a9c61580f1f8239a80ca6fa67448e8baeeb0209a2d556" width = "1000"></center>
<center><br>图12：ResNet-50模型网络结构示意图</br></center>

<p><br></br></p>
<p>ResNet-50的具体实现如下代码所示：</p>
<hr>
<p><strong>说明</strong> ：</p>
<p>带泄露修正线性单元（Leaky ReLU）函数是经典（以及广泛使用的）的ReLu激活函数的变体，该函数输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢），解决了Relu函数进入负区间后，导致神经元不学习的问题。</p>
<p><br></br></p>
<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-studio-static-online.cdn.bcebos.com/d75c6e01a5db456a990bc6060647b8a3a9ff83e3797a45469aca303485de1bcb" width = "500"></center>
<center><br>图13：Leaky ReLU</br></center>

<p><br></br></p>
<p><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/functional/leaky_relu_cn.html#leaky-relu">paddle.nn.functional.leaky_relu(x, negative_slope&#x3D;0.01, name&#x3D;None)</a></p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet模型代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------定义卷积批归一化块-------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNLayer</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_channels,</span></span><br><span class="line"><span class="params">                 num_filters,</span></span><br><span class="line"><span class="params">                 filter_size,</span></span><br><span class="line"><span class="params">                 stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 act=<span class="literal">None</span></span>):</span><br><span class="line">       </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        num_channels, 卷积层的输入通道数</span></span><br><span class="line"><span class="string">        num_filters, 卷积层的输出通道数</span></span><br><span class="line"><span class="string">        filter_size, 卷积核大小</span></span><br><span class="line"><span class="string">        stride, 卷积层的步幅</span></span><br><span class="line"><span class="string">        groups, 卷积核组数，默认groups=1不使用分组卷积</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建卷积层</span></span><br><span class="line">        self._conv = nn.Conv2D(</span><br><span class="line">            in_channels=num_channels,</span><br><span class="line">            out_channels=num_filters,</span><br><span class="line">            kernel_size=filter_size,</span><br><span class="line">            stride=stride,</span><br><span class="line">            padding=(filter_size - <span class="number">1</span>) // <span class="number">2</span>,</span><br><span class="line">            groups=groups,</span><br><span class="line">            bias_attr=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建BatchNorm层</span></span><br><span class="line">        <span class="comment"># ResNet中使用了BatchNorm层，在卷积层的后面加上BatchNorm以提升数值稳定性</span></span><br><span class="line">        self._batch_norm = paddle.nn.BatchNorm2D(num_filters)</span><br><span class="line">        </span><br><span class="line">        self.act = act</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self._conv(inputs)</span><br><span class="line">        y = self._batch_norm(y)</span><br><span class="line">        <span class="keyword">if</span> self.act == <span class="string">&#x27;leaky&#x27;</span>:</span><br><span class="line">            y = F.leaky_relu(x=y, negative_slope=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.act == <span class="string">&#x27;relu&#x27;</span>:</span><br><span class="line">            y = F.relu(x=y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;--------定义残差块---------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 每个残差块会对输入图片做三次卷积，然后跟输入图片进行短接</span></span><br><span class="line"><span class="comment"># 如果残差块中第三次卷积输出特征图的形状与输入不一致，则对输入图片做1x1卷积，将其输出形状调整成一致</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BottleneckBlock</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_channels,</span></span><br><span class="line"><span class="params">                 num_filters,</span></span><br><span class="line"><span class="params">                 stride,</span></span><br><span class="line"><span class="params">                 shortcut=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BottleneckBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 创建第一个卷积层 1x1</span></span><br><span class="line">        self.conv0 = ConvBNLayer(</span><br><span class="line">            num_channels=num_channels,</span><br><span class="line">            num_filters=num_filters,</span><br><span class="line">            filter_size=<span class="number">1</span>,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 创建第二个卷积层 3x3</span></span><br><span class="line">        self.conv1 = ConvBNLayer(</span><br><span class="line">            num_channels=num_filters,</span><br><span class="line">            num_filters=num_filters,</span><br><span class="line">            filter_size=<span class="number">3</span>,</span><br><span class="line">            stride=stride,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 创建第三个卷积 1x1，但输出通道数乘以4</span></span><br><span class="line">        self.conv2 = ConvBNLayer(</span><br><span class="line">            num_channels=num_filters,</span><br><span class="line">            num_filters=num_filters * <span class="number">4</span>,</span><br><span class="line">            filter_size=<span class="number">1</span>,</span><br><span class="line">            act=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果conv2的输出跟此残差块的输入数据形状一致，则shortcut=True</span></span><br><span class="line">        <span class="comment"># 否则shortcut = False，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> shortcut:</span><br><span class="line">            self.short = ConvBNLayer(</span><br><span class="line">                num_channels=num_channels,</span><br><span class="line">                num_filters=num_filters * <span class="number">4</span>,</span><br><span class="line">                filter_size=<span class="number">1</span>,</span><br><span class="line">                stride=stride)</span><br><span class="line"></span><br><span class="line">        self.shortcut = shortcut</span><br><span class="line"></span><br><span class="line">        self._num_channels_out = num_filters * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self.conv0(inputs)</span><br><span class="line">        conv1 = self.conv1(y)</span><br><span class="line">        conv2 = self.conv2(conv1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果shortcut=True，直接将inputs跟conv2的输出相加</span></span><br><span class="line">        <span class="comment"># 否则需要对inputs进行一次卷积，将形状调整成跟conv2输出一致</span></span><br><span class="line">        <span class="keyword">if</span> self.shortcut:</span><br><span class="line">            short = inputs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            short = self.short(inputs)</span><br><span class="line">        y = paddle.add(x=short, y=conv2)</span><br><span class="line">        y = F.relu(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;--------定义ResNet模型--------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers=<span class="number">50</span>, class_dim=<span class="number">1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        layers, 网络层数，可以是50, 101或者152</span></span><br><span class="line"><span class="string">        class_dim，分类标签的类别数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 支持的resnet层数</span></span><br><span class="line">        supported_layers = [<span class="number">18</span>, <span class="number">34</span>, <span class="number">50</span>, <span class="number">101</span>, <span class="number">152</span>]</span><br><span class="line">        <span class="keyword">assert</span> layers <span class="keyword">in</span> supported_layers, \</span><br><span class="line">            <span class="string">&quot;supported layers are &#123;&#125; but input layer is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(supported_layers, layers)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> layers == <span class="number">18</span>:</span><br><span class="line">            <span class="comment">#ResNet18包含多个模块，其中第2到第5个模块分别包含2、2、2、2个残差块</span></span><br><span class="line">            depth = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">34</span> <span class="keyword">or</span> layers == <span class="number">50</span>:</span><br><span class="line">            <span class="comment">#ResNet34/50包含多个模块，其中第2到第5个模块分别包含3、4、6、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">101</span>:</span><br><span class="line">            <span class="comment">#ResNet101包含多个模块，其中第2到第5个模块分别包含3、4、23、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> layers == <span class="number">152</span>:</span><br><span class="line">            <span class="comment">#ResNet152包含多个模块，其中第2到第5个模块分别包含3、8、36、3个残差块</span></span><br><span class="line">            depth = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 残差块中使用到的卷积的输出通道数</span></span><br><span class="line">        num_filters = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ResNet的第一个模块，包含1个7x7卷积，后面跟着1个最大池化层</span></span><br><span class="line">        self.conv = ConvBNLayer(</span><br><span class="line">            num_channels=<span class="number">3</span>,</span><br><span class="line">            num_filters=<span class="number">64</span>,</span><br><span class="line">            filter_size=<span class="number">7</span>,</span><br><span class="line">            stride=<span class="number">2</span>,</span><br><span class="line">            act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.pool2d_max = nn.MaxPool2D(</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            stride=<span class="number">2</span>,</span><br><span class="line">            padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ResNet的第二到第五个模块c2、c3、c4、c5</span></span><br><span class="line">        self.bottleneck_block_list = []</span><br><span class="line">        num_channels = <span class="number">64</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(depth)):</span><br><span class="line">            shortcut = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth[block]):</span><br><span class="line">                <span class="comment"># c3、c4、c5将会在第一个残差块使用stride=2；其余所有残差块stride=1</span></span><br><span class="line">                <span class="comment"># add_sublayer方法：返回一个由所有子层组成的列表</span></span><br><span class="line">                <span class="comment"># 字符串格式化: %d把后面对应的值用整数的形式显示</span></span><br><span class="line">                bottleneck_block = self.add_sublayer(</span><br><span class="line">                    <span class="string">&#x27;bottleneck_block_%d_%d&#x27;</span> % (block, i),</span><br><span class="line">                    BottleneckBlock(</span><br><span class="line">                        num_channels=num_channels,</span><br><span class="line">                        num_filters=num_filters[block],</span><br><span class="line">                        stride=<span class="number">2</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> block != <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span>, </span><br><span class="line">                        shortcut=shortcut))</span><br><span class="line">                num_channels = bottleneck_block._num_channels_out</span><br><span class="line">                self.bottleneck_block_list.append(bottleneck_block)</span><br><span class="line">                shortcut = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在c5的输出特征图上使用全局池化</span></span><br><span class="line">        self.pool2d_avg = paddle.nn.AdaptiveAvgPool2D(output_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stdv用来作为全连接层随机初始化参数的方差</span></span><br><span class="line">        <span class="keyword">import</span> math</span><br><span class="line">        stdv = <span class="number">1.0</span> / math.sqrt(<span class="number">2048</span> * <span class="number">1.0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建全连接层，输出大小为类别数目，经过残差网络的卷积和全局池化后，</span></span><br><span class="line">        <span class="comment"># 卷积特征的维度是[B,2048,1,1]，故最后一层全连接的输入维度是2048</span></span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">2048</span>, out_features=class_dim,</span><br><span class="line">                      weight_attr=paddle.ParamAttr(</span><br><span class="line">                          initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        y = self.conv(inputs)</span><br><span class="line">        y = self.pool2d_max(y)</span><br><span class="line">        <span class="keyword">for</span> bottleneck_block <span class="keyword">in</span> self.bottleneck_block_list:</span><br><span class="line">            y = bottleneck_block(y)</span><br><span class="line">        y = self.pool2d_avg(y)</span><br><span class="line">        y = paddle.reshape(y, [y.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        y = self.out(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;bottleneck_block_%d_%d&#x27;</span> % (<span class="number">1.2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>&#39;bottleneck_block_1_1&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = ResNet(layers=<span class="number">18</span>)</span><br><span class="line">model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.001</span>, </span><br><span class="line">                                momentum=<span class="number">0.9</span>, </span><br><span class="line">                                parameters=model.parameters(), </span><br><span class="line">                                weight_decay=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 启动训练过程</span></span><br><span class="line">iters_R, train_losses_R = train_pm(model, opt)</span><br></pre></td></tr></table></figure>


<p>通过运行结果可以发现，使用ResNet在眼疾筛查数据集iChallenge-PM上，loss能有效的下降。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h1><ul>
<li>卷积神经网络（CNN）是一类使用卷积层的网络。</li>
<li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和池化层。</li>
<li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li>
<li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li>
<li>LeNet是最早发布的卷积神经网络之一，先使用卷积层学习图片空间信息，然后使用全连接层来转换到类别空间。</li>
<li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li>
<li>今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。</li>
<li>尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。</li>
<li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li>
<li>块的使用使得网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li>
<li>我们发现深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效。</li>
<li>不同的超参数可以得到不同复杂度的变种。</li>
<li>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状（超参数）的卷积层和最大池化层来并行抽取信息，并使用$1×1$卷积层减少每像素级别上的通道维数从而降低模型复杂度，从而降低计算复杂度。</li>
<li>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li>
<li>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</li>
<li>GoogLeNet使用了9个Inception块，这是第一个达到上百层的网络，且在后续有了一系列的改进。</li>
<li>利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h1><p>[1] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learn- ing applied to document recognition. Proc. of the IEEE, 86(11):2278–2324, 1998 </p>
<p>[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. </p>
<p>[3] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014b. </p>
<p>[4]Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolu- tions. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015. </p>
<p>[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016a. </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space">小漁头&amp;小戴</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.dai2yutou.space/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F/">http://blog.dai2yutou.space/2023/01/01/深度学习5.2-图像分类中经典模型的组网方式/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.dai2yutou.space" target="_blank">小漁头|小戴</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/paddle/">paddle</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E5%8D%B7%E7%A7%AF%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/">深度学习基础_卷积基本概念及经典模型复现</a></div><div class="post_share"><div class="social-share" data-image="https://picbed.dai2yutou.space/web_img/19.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习5.1-从全连接层到卷积</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.3-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习5.3-图像识别模型关键组件之数据处理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF/" title="深度学习5.1-从全连接层到卷积"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-01</div><div class="title">深度学习5.1-从全连接层到卷积</div></div></a></div><div><a href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.3-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="深度学习5.3-图像识别模型关键组件之数据处理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-01</div><div class="title">深度学习5.3-图像识别模型关键组件之数据处理</div></div></a></div><div><a href="/2023/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05.4-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF%E4%B8%8E%E5%BE%AE%E8%B0%83/" title="深度学习5.4-图像识别模型关键组件之图像增广与迁移学习"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-01</div><div class="title">深度学习5.4-图像识别模型关键组件之图像增广与迁移学习</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="深度学习2.1-线性回归模型的实现"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.1-线性回归模型的实现</div></div></a></div><div><a href="/2022/12/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02.2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/" title="深度学习2.2-神经网络中的分类任务"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-20</div><div class="title">深度学习2.2-神经网络中的分类任务</div></div></a></div><div><a href="/2022/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" title="深度学习1.1-深度学习概论"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-18</div><div class="title">深度学习1.1-深度学习概论</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="animate__fadeIn card-info card-widget wow" data-wow-delay="0" data-wow-duration="" data-wow-iteration="" data-wow-offset="" style="visibility: visible; animation-name: fadeIn;"><div class="author-info-top"><div class="card-info-avatar"><a class="avatar-img" data-pjax-state="" href="/about"><img class="entered loaded" alt="avatar" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apple-touch-icon.jpg" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;"/></a><div class="author-status-box"><div class="author-status"><g-emoji class="g-emoji" alias="palm_tree" fallback-src="/img/tree_icon.png">🐟</g-emoji><span>摸鱼中~</span></div></div></div></div><div class="author-info__sayhi" id="author-info__sayhi">晚安😴！我是</div><h1 class="author-info__name">XiaoYutou|XiaoDai</h1><div class="author-info__description">热爱生活点滴，分享时刻精彩。</div><a id="card-info-btn" data-pjax-state="" onclick="pjax.loadUrl(/about/)"><i></i><span style="padding-left:32px;font-weight:600;font-size:large">了解更多<i class="faa-passing animated" style="padding-left:-2px;display:inline-block;vertical-align:middle;"><span style="height:28px;width:28px;fill:currentColor;position:relative;top:-1.5px">💨</span></i></span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xiaoyutoua" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2143191301@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>主域名:<a target="_blank" rel="noopener" href="https://www.dai2yutou.space">小漁头|小戴</a><br><span>技术问题欢迎交流🧐</span><span color="#3eb8be">VX:yuguolong_001</span></center></div><div id="welcome-info"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%AD%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%84%E7%BD%91%E6%96%B9%E5%BC%8F"><span class="toc-text">图像分类中经典模型的组网方式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81LeNet"><span class="toc-text">一、LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-LeNet%E5%9C%A8%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">1.1 LeNet在手写数字识别上的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E8%AE%AD%E7%BB%83%E3%80%91"><span class="toc-text">【训练】</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E9%A2%84%E6%B5%8B%E3%80%91"><span class="toc-text">【预测】</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-LeNet%E5%9C%A8%E7%9C%BC%E7%96%BE%E8%AF%86%E5%88%AB%E6%95%B0%E6%8D%AE%E9%9B%86iChallenge-PM%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">1.2 LeNet在眼疾识别数据集iChallenge-PM上的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="toc-text">1）数据集准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%BC%89%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%9B%BE%E7%89%87"><span class="toc-text">2）查看数据集图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%EF%BC%89%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%99%A8"><span class="toc-text">3）定义数据读取器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%EF%BC%89%E5%90%AF%E5%8A%A8%E8%AE%AD%E7%BB%83"><span class="toc-text">4）启动训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81AlexNet"><span class="toc-text">二、AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet%E7%9A%84%E9%87%8D%E8%A6%81%E6%9E%84%E6%88%90"><span class="toc-text">AlexNet的重要构成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%89%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E6%94%B9%E5%8F%98"><span class="toc-text">1）激活函数的改变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%BC%89%E5%AE%B9%E9%87%8F%E6%8E%A7%E5%88%B6%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2）容量控制和预处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81VGG"><span class="toc-text">三、VGG</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81GoogLeNet"><span class="toc-text">四、GoogLeNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81ResNet"><span class="toc-text">五、ResNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/16/c-%E6%8F%90%E9%AB%98%E7%BC%96%E7%A8%8B/" title="c++提高编程"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="c++提高编程"/></a><div class="content"><a class="title" href="/2023/05/16/c-%E6%8F%90%E9%AB%98%E7%BC%96%E7%A8%8B/" title="c++提高编程">c++提高编程</a><time datetime="2023-05-16T03:49:20.000Z" title="发表于 2023-05-16 11:49:20">2023-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/16/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82Python%E8%A3%85%E9%A5%B0%E5%99%A8/" title="一文弄懂Python装饰器"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/article_img/Python/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一文弄懂Python装饰器"/></a><div class="content"><a class="title" href="/2023/05/16/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82Python%E8%A3%85%E9%A5%B0%E5%99%A8/" title="一文弄懂Python装饰器">一文弄懂Python装饰器</a><time datetime="2023-05-16T03:21:03.000Z" title="发表于 2023-05-16 11:21:03">2023-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/15/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/" title="微信小程序入门到入土"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/24.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="微信小程序入门到入土"/></a><div class="content"><a class="title" href="/2023/05/15/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/" title="微信小程序入门到入土">微信小程序入门到入土</a><time datetime="2023-05-14T16:21:33.000Z" title="发表于 2023-05-15 00:21:33">2023-05-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 小漁头&小戴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.6/translate/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer src="/js/light.js"></script><canvas id="universe"></canvas><script defer src="/js/starry_sky.js"></script><script defer src="/js/console.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script async data-pjax src="/js/card_author.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JzK9w99AgP1g6fso",ck:"JzK9w99AgP1g6fso"})</script><script type="text/javascript" src ="/js/reward.js" ></script><script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.6.16/dist/sweetalert2.all.min.js"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="/js/txmap.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = 'b16a1fa0e63c46a4b8f28abfb06ae3fe';
  var gaud_map_key = 'e2b04289e870b005374ee030148d64fd&s=rsv3';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/3.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-02</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">英文水平不高，咋翻译论文？</a><div class="blog-slider__text">英文水平不高，咋翻译论文？</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/02/02/论文翻译/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/web_background2.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-17</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">🐌博客搭建学习笔记</a><div class="blog-slider__text">这是再搭建博客已经写文章时遇到的bug和对博客的一些必要操作，不定时更新哦~</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/17/博客搭建学习笔记/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/9.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">Butterfly外挂标签</a><div class="blog-slider__text">本文是撰写博客文章时可能会用到的外挂标签汇总，放到一起，便于查阅和使用</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/Butterfly外挂标签/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/2.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-12-09</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">第一篇文章</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/12/09/hello-world/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-20</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">Hexo发生error：spawn failed错误的解决方法</a><div class="blog-slider__text">Hexo发生error：spawn failed错误的解决方法</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/20/erro_spawn_failed/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">Hexo博客备份与恢复</a><div class="blog-slider__text">本文旨在解决在不同电脑上都能维护博客或配置、发布的内容丢失可恢复的问题。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/06/Hexo博客备份与恢复/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picbed.dai2yutou.space/web_img/10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-24</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">Echarts社区地址</a><div class="blog-slider__text">一些Echarts图标的开源网站。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/24/Echarts社区地址/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '2');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__bounceInRight');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/about/'|| '/about/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.dai2yutou.space/api?xiaoyutoua",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xiaoyutoua')
    }
  </script><!-- hexo injector body_end end --><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>